quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,Changed to use the environment variable directly. I also updated the PR description with details on how to install mkl and make Hail load it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440:133,load,load,133,https://hail.is,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440,1,['load'],['load']
Performance,"Changes since last review:; - Method now takes expressions for call and (optionally) scores.; - Block matrix and table of scores annotated and collected from source.cols() sent to Python, processed using int indices, column names restored on python side (thanks @tpoterba); - Fixed bug that silently dropped `n_samples / block_size` proportion of pairs, Python test checks it; - Extended Python tests to compare k and scores paths, test counts, min_kinship, maf, block_size; - Tuned tolerances on comparison with R from Python; - Extended to general column key, removing unique key check, noted in docs; - MEMORY_AND_DISK caching as default (thanks @konradjk) on Scala side; - The diagonal fix meant phi is computed with parallelism up to the number of diagonal blocks, rather than parallelism 1. But that's still likely a bottleneck as phi requires computing and point-wise dividing two big gram matrices. I now write phi to disk and read it back in, which squares the parallelism up to the number of blocks in phi. I think this should also improve the stability of the many downstream calculations derived from phi, esp. if pre-emptibles are used. No longer cacheing phi, but I left caching on the other matrices. @konradjk let us know how this version compares next time you run it.; - Noted in FIXME room for further improvement when fusing blocks: `replace join with zipPartitions, throw away lower triangular blocks sooner, avoid the nulls`; - Updated docs accordingly; - Deleted a bunch of code in PCRelate and PCRelateSuite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104:477,Tune,Tuned,477,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104,3,"['Tune', 'bottleneck', 'cache']","['Tuned', 'bottleneck', 'cacheing']"
Performance,"Changes:; - Add WaitableSharedPool that allows you to submit a collection of jobs (which start running as soon as they are submitted) against a shared AsyncWorkerPool and then `wait()` at the end for all submitted jobs to complete.; - Use in scheduler threads. I also unscientifically tweaked a few of the settings:; - Use queue_size=100 in shared async worker pool in scheduler. This is 1x the parallelism, which seems like plenty. (Queue size is how much pending work to queue up before blocking.); - Drop work per iteration from 1000 to 300.; - Increase the number of database connections to 50. This is 1/2 the parallelism in the scheduler, the argument being half the work goes into servicing web requests (e.g. talking to workers) and the other half talking to the database. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7961:434,Queue,Queue,434,https://hail.is,https://github.com/hail-is/hail/pull/7961,2,"['Queue', 'queue']","['Queue', 'queue']"
Performance,"ChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.writ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2129,concurren,concurrent,2129,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"Check partitioning during joins, so we don't have issues trying to load one partition thousands of times",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3291:67,load,load,67,https://hail.is,https://github.com/hail-is/hail/issues/3291,1,['load'],['load']
Performance,"Closes #4527; Closes #4761. This is a workaround to prevent issues with MatrixUnionRows when the; entries arrays are in different places in the rvRowType in each of the; children. Furthermore, it prevents issues if the entries array is; pruned and then re-added later in rebuild, where it will often be; inserted, likely by MatrixMapRows, at the end of the rvRowType. This; rearrangement caused the type equality assertion in Optimize to fail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4891:426,Optimiz,Optimize,426,https://hail.is,https://github.com/hail-is/hail/pull/4891,1,['Optimiz'],['Optimize']
Performance,"Closes hail-is/hail-production-issues#9. Details [here](https://github.com/hail-is/hail-production-issues/issues/9#issuecomment-1049356188). This is my bad. One of the query service PRs allowed spec writing to happen in; parallel with DB insertion (which reduces latency a bit), but if the spec fails; to write or is cancelled, then the DB has a spec token that points at a; cloud object which does not necessarily exist. I think we do not need the spec token, but removing it does not seem likely; to improve performance much. We still need to hit the DB to get the start_job_id.; There was some discussion about the necessity of the token [here](https://github.com/hail-is/hail/pull/7949#discussion_r370406517).; I think that discussion came to the wrong conclusion. GCS is atomic and globally; consistent. Writing to an already present spec object is atomic. The only issue; I forsee is the possibility that the spec is different the second time. The spec; should have the same semantic content, but if the characters are different the; spec index could be very briefly wrong. We could fix this by storing both the; spec and the index in one GCS file. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11404:263,latency,latency,263,https://hail.is,https://github.com/hail-is/hail/pull/11404,2,"['latency', 'perform']","['latency', 'performance']"
Performance,Closing for now until we're happy with the performance of the new v3 billing projects tables.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12848#issuecomment-1608186980:43,perform,performance,43,https://hail.is,https://github.com/hail-is/hail/pull/12848#issuecomment-1608186980,1,['perform'],['performance']
Performance,Closing for now until we're happy with the performance of the new v3 tables for billing project queries.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1608187955:43,perform,performance,43,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1608187955,1,['perform'],['performance']
Performance,"Closing in favor of #13434. GCS lists blobs in lexicographical order, so this optimization won't help at all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1679137737:78,optimiz,optimization,78,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1679137737,1,['optimiz'],['optimization']
Performance,Closing since this isn't on the winning side of performance yet.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2342#issuecomment-340501283:48,perform,performance,48,https://hail.is,https://github.com/hail-is/hail/pull/2342#issuecomment-340501283,1,['perform'],['performance']
Performance,Closing this in favor of adding a loop mechanism to #13475. We should revisit this PR at a later date for performance improvements in MJC.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1702822114:106,perform,performance,106,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1702822114,1,['perform'],['performance']
Performance,Code cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426:5,cache,cache,5,https://hail.is,https://github.com/hail-is/hail/pull/5426,1,['cache'],['cache']
Performance,"Code: . ```; import hail as hl; import time; import argparse; from slack_utils import send_message. def flip_text(base):; """"""; :param StringExpression base: Expression of a single base; :return: StringExpression of flipped base; :rtype: StringExpression; """"""; return hl.cond(base == 'A', 'T',; hl.cond(base == 'T', 'A',; hl.cond(base == 'C', 'G',; hl.cond(base == 'G', 'C', base)))). def main(pheno):; # load ldpred sumstats file; sumstats = hl.import_table('gs://ukbb_prs/sumstats/UKB_'+pheno+'_LDPred.txt', delimiter='\s+', impute=True, min_partitions=100). # create locus and alleles columns and key by locus; sumstats = (sumstats.annotate(locus=hl.parse_locus(sumstats.chrom[6:] + "":"" + hl.str(sumstats.pos)),; alleles=[sumstats.nt1,sumstats.nt2]); .key_by('locus')). # write the sumstats table; sumstats.write('gs://ukbb_prs/sumstats/temp.kt', overwrite=True). # read the sumstats table; sumstats = hl.read_table('gs://ukbb_prs/sumstats/temp.kt'). # remove leading zeros from contigs; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}. # import bgen(s); mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{22}_v3.bgen',; ['dosage'],; sample_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # load scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/hail-0.1/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # merge sumstats on bgen matrixtable; mt = mt.annotate_rows(ss=sumstats[mt.locus]). # handle strand/sign flips -- score in terms of alt allele; mt = mt.annotate_rows(beta = hl.case(); .when(((mt.alleles[0] == mt.ss.nt1) &; (mt.alleles[1] == mt.ss.nt2)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt1) &; (flip_text(mt.alleles[1]) == mt.ss.nt2)),; (-1*mt.ss.ldpred_inf_beta)); .when(((mt.alleles[0] == mt.ss.nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:404,load,load,404,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['load'],['load']
Performance,"Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx\n (GetTupleElement 0 (Ref __iruid_454)))))))))))\n2022-11-15 20:30:18.168 root: INFO: Prune: MakeStruct: eliminating field '__iruid_455'\n2022-11-15 20:30:18.178 root: INFO: optimize optimize: compileLowerer, initial IR: after: IR size 56:\n(MakeTuple (0)\n (Let __iruid_484\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (Let __iruid_485\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_486\n __iruid_487\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct (__iruid_458 (Ref __iruid_484)))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0\n (GetField __iruid_458 (Ref __iruid_487)))))\n (StreamFor __iruid_488\n (StreamMap __iruid_489\n (StreamRange -1 True\n (GetField start (Ref __iruid_486))\n (GetField end (Ref __iruid_486))\n (I32 1))\n (MakeStruct (idx (Ref __iruid_489))))\n (Begin\n (SeqOp 0 (Collect (CollectStateSig +PInt32))\n ((GetField idx (Ref __iruid_488)))))))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (NA String))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_484))))\n (StreamFor __iruid_490\n (ToStream True (Ref __iruid_485))\n (Begin\n (CombOpValue 0 (Collect (CollectStateSig +PInt32))\n (GetTupleElement 0 (Ref __iruid_490))))))\n (Let __iruid_491\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_491)))))))))\n2022-11-15 20:30:18.180 root: INFO: optimize optimize: compileLowerer, after InlineApplyIR: before: IR size 56: \n(MakeTuple (0)\n (Let __iruid_484\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PI",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:4747,optimiz,optimize,4747,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36042,cache,cached,36042,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, num",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6851,cache,cached,6851,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5661,cache,cached,5661,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/309:1495,load,loadClass,1495,https://hail.is,https://github.com/hail-is/hail/issues/309,3,['load'],['loadClass']
Performance,"Consider a NxM matrix, X, where N << M. The product X ⨯ Xᵀ has size NxN. If the block size is chosen on the order of N, say, N/2, then the number of partitions in the product is four. The number of partitions in X is much much larger. As a result, we miss out on some parallelism because four nodes must perform a tremendous number of multiplications. When N << M, we may consider using tree aggregations until N and M are the same order of magnitude. In particular, we can break the sum into chunks and use a tree aggregation. Consider:. ```; +---------+ +----+ +----+; | A | | B | = | C |; +---------+ | | +----+; | |; +----+; ```; We can slice A column-wise into three chunks and B column-wise into three chunks:. ```; +---------+ +----+ +----+; | : : | | .. | 1 = | C |; +---------+ | | 2 +----+; 1 2 3 | ˙˙ | 3; +----+; ```. multiply the chunks and then sum them:. ```; +----+ +----+ +----+ +----+ +----+ +----+ +----+; | A1 | | B1 | + | A2 | | B2 | + | A3 | | B3 | = | C |; +----+ +----+ +----+ +----+ +----+ +----+ +----+; ```. This would be a tree aggregate with tree of size three. I think we should trigger this whenever N < M/10, and we should generate a 10-ary tree of summations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1975:304,perform,perform,304,https://hail.is,https://github.com/hail-is/hail/issues/1975,1,['perform'],['perform']
Performance,"Consider this pipeline, which measures the speed of single-core decoding.; ```; import hail as hl; hl.init(master='local[1]'); vds = hl.vds.read_vds(...); vds.variant_data._force_count_rows(); ```. On a 2021 MacBook Pro, [YourKit reports](https://github.com/hail-is/hail/pull/13787#issuecomment-1756358633) ~60MiB/s of bandwidth and 100% CPU use. Substantial amounts of time are reported in Zstd and I/O, #13840 endeavors to address those issues. Even with these issues addressed, we anticipate decoding to use a substantial portion of CPU time. In particular, our decoders perform a fair number of branches to handle missingness (consider that an array is stored as: length, bitset indicating which values are non-missing, the non-missing values). #13787 demonstrated a 50% reduction in run-time ([see benchmarks from before this change](https://github.com/hail-is/hail/pull/13776)) primarily due to replacing iteration (which branches O(N) times) with bitset-driven iteration over the missing bits (which branches O(N_PRESENT) times). Unfortunately, using the ideas in #13787 to improve struct decoding is tricky because struct fields are heterogenous. We could generate 16 different decoders and branch on 4 bits of missingness, but that is fairly large code. However, if we are decoding large arrays of structs (such as a whole partition or an entries array), we could ""transpose"" the data and store a struct of arrays, with one array per-field. This representation has several benefits:. 1. We may use the bitset-driven iteration from #13787.; 2. For fixed-width contiguous element types, we can use `memcpy` for rapid decoding.; 3. O(1) skipping off the unused fields of O(N) structs. ; 5. General purpose compression should perform better due to locality of data types.; 6. We have the opportunity to apply data-type aware compression on each array. This change does require a novel set of PTypes to map the Array(Struct) operations onto the physical Struct(Array) representation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13841:574,perform,perform,574,https://hail.is,https://github.com/hail-is/hail/issues/13841,2,['perform'],['perform']
Performance,"Consider this snippet; ```; broken_ht = hl.import_table('../data/bikes.csv'); # Look at the first 3 rows; broken_ht.show(3); ```; [`bikes.csv`](https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv), but I removed the diacritic characters (AFAICT), the first line is now this:; ```; Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles). ```. This is the output:; ```; 2019-01-30 16:19:59 Hail: INFO: Reading table with no type imputation; Loading column 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)' as type 'str' (type not specified). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-8-5e24c8175e7d> in <module>; 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; ----> 3 broken_ht.show(3). </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-848> in show(self, n, width, truncate, types, handler). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in show(self, n, width, truncate, types, handler); 1330 Handler function for data string.; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5222:561,Load,Loading,561,https://hail.is,https://github.com/hail-is/hail/issues/5222,1,['Load'],['Loading']
Performance,"Consider, for example, this deploy: https://ci.hail.is/batches/7956812. `test-dataproc-37` succeeded but `test-dataproc-38` failed (it timed out b/c the master failed to come online). You can see the error logs for the cluster here: https://cloudlogging.app.goo.gl/t1ux8oqy11Ba2dih7. It states a certain file either did not exist or we did not have permission to access it. [`test_dataproc-37`](https://batch.hail.is/batches/7956812/jobs/193) and [`test_dataproc-38`](https://batch.hail.is/batches/7956812/jobs/194) started around the same time and both uploaded four files into:. gs://hail-30-day/hailctl/dataproc/ci_test_dataproc/0.2.121-7343e9c368dc/. And then set it to public read/write. The public read/write means that permissions are not the issue. Instead, the issue is that there must be some sort of race condition in GCS which means that if you ""patch"" (aka overwrite) an existing file, it is possible that a concurrent reader will see the file as not existing. Unfortunately, I cannot confirm this with audit logs of the writes and read because [public objects do not generate audit logs](https://cloud.google.com/logging/docs/audit#data-access).; > Publicly available resources that have the Identity and Access Management policies [allAuthenticatedUsers](https://cloud.google.com/iam/docs/overview#allauthenticatedusers) or [allUsers](https://cloud.google.com/iam/docs/overview#allusers) don't generate audit logs. Resources that can be accessed without logging into a Google Cloud, Google Workspace, Cloud Identity, or Drive Enterprise account don't generate audit logs. This helps protect end-user identities and information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13573:811,race condition,race condition,811,https://hail.is,https://github.com/hail-is/hail/pull/13573,2,"['concurren', 'race condition']","['concurrent', 'race condition']"
Performance,Constants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7544,Optimiz,Optimize,7544,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,"Context (which however conflates a few different issues): https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Error.20__C1188collect_distributed_array/near/276162426. Running the `prepare_pext` step of the https://github.com/broadinstitute/gnomad-browser pipeline using Hail 0.2.105 results in the following error:. ```; 2022-12-01 05:56:34.825 Hail: INFO: Loading <StructExpression of type struct{ensg: str, symbol: str, max_pexts: str, Spleen: str, Brain_FrontalCortex_BA9_: str, SmallIntestine_TerminalIleum: str, Artery_Coronary: str, Skin_SunExposed_Lowerleg_: str, Brain_Hippocampus: str, Esophagus_Muscularis: str, Brain_Nucleusaccumbens_basalganglia_: str, Artery_Tibial: str, Brain_Hypothalamus: str, Adipose_Visceral_Omentum_: str, Brain_CerebellarHemisphere: str, Nerve_Tibial: str, Breast_MammaryTissue: str, Liver: str, Skin_NotSunExposed_Suprapubic_: str, AdrenalGland: str, Pancreas: str, Lung: str, Pituitary: str, Muscle_Skeletal: str, Colon_Transverse: str, Artery_Aorta: str, Heart_AtrialAppendage: str, Adipose_Subcutaneous: str, Esophagus_Mucosa: str, Heart_LeftVentricle: str, Brain_Cerebellum: str, Brain_Cortex: str, Thyroid: str, Stomach: str, WholeBlood: str, Brain_Anteriorcingulatecortex_BA24_: str, Brain_Putamen_basalganglia_: str, Brain_Caudate_basalganglia_: str, Colon_Sigmoid: str, Esophagus_GastroesophagealJunction: str, Brain_Amygdala: str, mean_proportion: str}> fields. Counts by type:; str: 42; Traceback (most recent call last): (0 + 1) / 1]; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/genes.py"", line 327, in <module>; run_pipeline(pipeline); File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1066>"", line 2, in write",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:385,Load,Loading,385,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['Load'],['Loading']
Performance,"Context.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:13597,load,loading,13597,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loading']
Performance,"Context:; For LDPrune, we need to compute a correlation matrix for the variants in the dataset, and then get an entries table from that matrix. Since computing the correlation matrix is slow, we need some infrastructure to help us compute only the rows in the entries table that we need to perform the filtering for LDPrune.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3185:290,perform,perform,290,https://hail.is,https://github.com/hail-is/hail/pull/3185,1,['perform'],['perform']
Performance,Convert BGEN asserts to ifs for performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3771:32,perform,performance,32,https://hail.is,https://github.com/hail-is/hail/pull/3771,1,['perform'],['performance']
Performance,Correct import_bgen docs based on the optimization I made last week (empty `entry_fields` takes a fast path that doesn't parse the genotypes),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3531:38,optimiz,optimization,38,https://hail.is,https://github.com/hail-is/hail/pull/3531,1,['optimiz'],['optimization']
Performance,"Could there be some functionality to make it easier to run GWAS on a large set of phenotypes at once? For instance, in metabolomics data sets there can be around 10,000 phenotypes (many of which are highly correlated or chemically related) and you'd like to see GWAS results from all of these. Rather than submitting 10,000 separate jobs, could there be a way to optimize this analysis by considering them all together? I'm sure this would be useful in other fields besides metabolomics.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/620:363,optimiz,optimize,363,https://hail.is,https://github.com/hail-is/hail/issues/620,1,['optimiz'],['optimize']
Performance,"Creating network namespaces can often take hundreds of milliseconds (and sometimes seconds with `iptables` contention), so Batch takes this off the job hot path by pre-allocating namespaces. All job namespaces are configured identically and there is a fixed number of ""slots"" on any batch worker (`CORES * 4`), so pre-allocation and asynchronous recycling of namespaces is fairly straight-forward so long as we never attempt to run more containers on a worker than the number of slots (which the scheduling system should prohibit). However, since we started running long-lived JVM containers (#11397), the number of containers running on a given worker can easily be *greater* than `N_SLOTS`. On a 16-core machine, we create 30 JVMs that sit idle waiting for JVMJobs all the while occupying a precious network namespace. I thought for the longest time that #13402 was a race condition so was trying to trigger it through a barrage of quick jobs. Turns out all it took was running >34 long-running jobs on a single 16-core worker. In a dev deploy of `main`, running a batch with 35 quarter-core `sleep 150` jobs fails with a single job timing out waiting for a network. On this branch, I am able to run the same 35 job batch as well as a batch with 64 quarter-core jobs. Unfortunately, we don't have a great way to test ""run all these jobs at once on the same worker"". Resolves #13402",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13678:870,race condition,race condition,870,https://hail.is,https://github.com/hail-is/hail/pull/13678,1,['race condition'],['race condition']
Performance,"Current master on representative dataset:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: running: count; [Stage 2:======================================================>(661 + 2) / 663]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 5.760s; filtervariants intervals: 386.191ms; count: 32.617s; total: 38.763s; ```. and new:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: pruned 0 redundant intervals; hail: info: interval filter loaded 67 of 663 partitions; hail: info: running: count; [Stage 2:=======================================================> (65 + 2) / 67]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 9.911s; filtervariants intervals: 445.193ms; count: 3.356s; total: 13.712s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200:764,load,loaded,764,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200,1,['load'],['loaded']
Performance,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:315,optimiz,optimizations,315,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812,6,"['concurren', 'optimiz']","['concurrent', 'optimizations']"
Performance,"Currently there is a race condition in which we can shut down the filestore before; the aiohttp app stops accepting connections. If that happens, jobs will get; partially scheduled on the worker, but there will be no working file store so the; jobs cannot complete successfully. The partially scheduled jobs in turn leave the; worker in a bad, non-idle state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10949:21,race condition,race condition,21,https://hail.is,https://github.com/hail-is/hail/pull/10949,1,['race condition'],['race condition']
Performance,"Currently this is optimized to:. ```; (TableMapRows (idx) 1; (TableRange 5 5); (MakeStruct; (idx; (GetField idx; (Ref Struct{idx:Int32} row))); (x; (I32 5)))); ```. But clearly it should be an InsertFields, not a MakeStruct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4001#issuecomment-408232868:18,optimiz,optimized,18,https://hail.is,https://github.com/hail-is/hail/issues/4001#issuecomment-408232868,2,['optimiz'],['optimized']
Performance,"Currently, tasks to schedule new instances are put on the event loop inside the `Pool` and `JobPrivateInstanceManager` constructors. `Pool.create` and `JobPrivateInstanceManager.create` first instantiate an object of their respective type and then load existing instances from the database into the in-memory instance collection. This could potentially cause the create instances loop to trigger while we're drawing ""existing"" instances, which causes the assertion error in https://github.com/hail-is/hail-tasks/issues/24 when the create instances loop and load instances query race to add the instance to the in-memory data structure. This change moves the task creation from the constructor to the `create` method, so we don't start creating instances until all existing instances are accounted for. I think I would have liked to simply pass the constructor a list of instances, but we can't create an `Instance` without an `InstanceCollection`. Resolves hail-is/hail-tasks#24. I also threw in a bit of cleanup, i.e. removing some variable assignments that didn't seem very helpful and resolving a lint issue where we used `items` where we could just use `values`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11766:248,load,load,248,https://hail.is,https://github.com/hail-is/hail/pull/11766,2,['load'],['load']
Performance,"Currently, the MJS and MJC requests from the worker to the driver for a given job can race, as they are run as independent asyncio tasks. This results in unnecessary database load and deadlocks between the MJS and MJC SQL procedures. Rather than address the procedures directly, we enforce that we will never run MJS and MJC concurrently. The system is resilient to never receiving an MJS (as MJC will add any attempt data if not present), so we can make the following changes to the worker:; - Serialize the submission of MJS and MJC requests by having the MJC task wait on the MJS future; - Give up retrying MJS once the job has completed because we will instead just send an MJC. This could potentially reduce the database load for very short jobs. I ran a load test of 10k `true` jobs and `sleep 5` jobs a few times against my namespace and saw 0 deadlocks 🎉",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11824:175,load,load,175,https://hail.is,https://github.com/hail-is/hail/pull/11824,4,"['concurren', 'load']","['concurrently', 'load']"
Performance,Custom parser for variant (key) in LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2419:35,Load,LoadVCF,35,https://hail.is,https://github.com/hail-is/hail/pull/2419,1,['Load'],['LoadVCF']
Performance,D$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13004,Load,LoadMatrix,13004,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"DD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/jav",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3332,concurren,concurrent,3332,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['concurren'],['concurrent']
Performance,DD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:297); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3480:13642,concurren,concurrent,13642,https://hail.is,https://github.com/hail-is/hail/issues/3480,2,['concurren'],['concurrent']
Performance,DD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13067,Load,LoadMatrix,13067,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"D`, possibly with some non-empty key. This is consistent with the rule that the `rvd` must always have a stronger/longer key than the `TableType`.; * **small tweaks** - Now I start working through the `TableIR` nodes, rewriting them to remove explicit uses of `UnpartitionedRVD`. The general plan is to sandwich the rvd logic between `toOrderedRVD` and `toOldStyleRVD`. The first takes an `UnpartitionedRVD` to an `OrderedRVD` with empty key (and leaves `OrderedRVD`s alone), and the second takes an `OrderedRVD` to an `UnpartitionedRVD` if its key was empty, and leaves it alone otherwise. Once they're all rewritten this way, I redefine `toOldStyleRVD` to always return `OrderedRVD`, and `UnpartitionedRVD` is no longer used.; * **remove `TableUnkey`** - With `UnpartitionedRVD` going away, `TableUnkey` is no longer necessary, it's equivalent to keying by an empty key.; * **small tweaks** - these next two rewrite more `TableIR` nodes; * **Merge master** - the big one; * **tweak MatrixColsTable** - 1) Optimize `coerce` by checking if the requested key is empty, avoiding a scan in that case. 2) Optimize `sortedColsValue` by checking if the column key is empty, avoiding the sort in that case. 3) Simplify `colsRVD`, removing the case on the type of the `RVD`, just calling `coerce` and letting the previous optimizations avoid unnecessary work.; * **`distinctByKey` fix** - While looking over `TableIR` implementations, I noticed a bug in `distinctByKey`: you need to be sure no key is split across multiple partitions. To be sure the empty key edge case still works, I added a test to check that `strictify` on an empty-key partitioner will always collapse everything to one partition.; * **Flipped switch** - redifines `toOldStyleRVD` to just return the `OrderedRVD` unchanged, and asserts that `TableValue.rvd` is always an `OrderedRVD`.; * **rest of the `TableIR` tweaks** - added a factory method `OrderedRVD.unkeyed` to replace `UnpartitionedRVD.apply`.; * the rest are simple tidying up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4319:1927,Optimiz,Optimize,1927,https://hail.is,https://github.com/hail-is/hail/pull/4319,2,"['Optimiz', 'optimiz']","['Optimize', 'optimizations']"
Performance,"Dan and I have profiled the second query and have a couple of thoughts. We've identified one source of slowness in the compiler and generated code that relates to how we handle I/O. This is currently under active development and hope will lead to decent performance improvements. The change itself is significant, however, so I can't comment on timelines for when to expect the work to be complete by. There may be some code tweaks that do less work as discussed. I'll have a play with the source code in `hail_search` and report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1821675629:254,perform,performance,254,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1821675629,1,['perform'],['performance']
Performance,Data formats that use Characters (including old VDSes) will now be loaded as `TString`s. Resolves #1710,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1745:67,load,loaded,67,https://hail.is,https://github.com/hail-is/hail/pull/1745,1,['load'],['loaded']
Performance,"Default persist (used by ServiceBackend) currently does nothing. As we've discussed, there's more work to flesh out a cache/persist strategy in the new setting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5158:118,cache,cache,118,https://hail.is,https://github.com/hail-is/hail/pull/5158,1,['cache'],['cache']
Performance,"Defines a TStream/PStream type stub. I've omitted some number of things that other types need to define, as the purpose of the stream type is going to be to ensure that we're never fully instantiating collections where we shouldn't be, e.g. all the rows in a table partition. To that end, I've omitted definitions for ordering since I don't forsee a need for ordering on the entire stream (as opposed to on the element, or a subset thereof), as well as generators for annotations, etc. It basically otherwise mimics the PArray/TArray definitions, but I've made it extend Type/PType directly since most of the extra methods on containers seem irrelevant to streams, having mostly to do with e.g. length and loading specific elements. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5610:706,load,loading,706,https://hail.is,https://github.com/hail-is/hail/pull/5610,1,['load'],['loading']
Performance,"Deploy commits don't need to cleanup which adds some latency to this PR. We should probably use xargs -P4 to delete instances 4-way parallel. This PR is ~46 minutes, including all the cleanup time, where as deploys are 46 minutes *without the cleanup time*. Notice two things: (1) the service backend is again the critical path (2) some local backend tests took quite a while to get scheduled. Seems fishy to me that it took ~16 minutes to find a core for the local backend tests to run on. Anyway, seems good to use more fine-grained parallelism. This should help keep the cluster large-ish and turning over fast so that users get a great experience during the work day. ---. A deploy commit:. <img width=""2032"" alt=""Screen Shot 2023-05-17 at 17 30 55"" src=""https://github.com/hail-is/hail/assets/106194/9c00365e-1079-451c-bd85-e10561e715c1"">. This PR:. <img width=""2032"" alt=""Screen Shot 2023-05-17 at 17 34 40"" src=""https://github.com/hail-is/hail/assets/106194/fa7751be-3986-4361-89ea-e322760176bf"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1552119464:53,latency,latency,53,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1552119464,1,['latency'],['latency']
Performance,"Deploying grafana in our GKE cluster gives us instant and easy access to the stackdriver backend with the same querying capabilities of our current front-end, but without the clutter and insanely slow load times. See [here](https://internal.hail.is/dgoldste/grafana/d/TVkleyLMk/detailed-service-resource-utilization?orgId=1) for some example dashboards I set up to look at resources across our services (credentials are the default admin/admin). This alleviates the immediate pain of using the console (for metrics only, not logging), but my longer aim is that getting more regular use out of our metrics can reveal deeper pain points of our monitoring stack and if/where we need to eat up more responsibility from google monitoring. This is a StatefulSet, so configuration through the UI will persist and is done manually. If we find that our dashboards are stable and boilerplate enough, I'd like to move to a code-based dashboard configuration. Sadly, `check-yaml` does not appreciate our jinja templating in yaml, so I've removed it for now. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10013:201,load,load,201,https://hail.is,https://github.com/hail-is/hail/pull/10013,1,['load'],['load']
Performance,Discovered because I accidentally had an old build directory in $HAIL/hail/python that was sneakily being rsync'd into the build/deploy directory and then silently picked up by bdist_wheel as a build cache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12008:200,cache,cache,200,https://hail.is,https://github.com/hail-is/hail/pull/12008,1,['cache'],['cache']
Performance,"Do you mean when showing the log in the UI or the final upload to blob storage?. I think that's a great idea for the UI, where we show a truncated view in the page and the download button provides a way to stream the log file without loading it into memory on the front-end. In terms of the final upload, I'm a little wary about making a breaking change like that. It's probably true that if you're spewing tons of logs as a user you probably want to not do that. But if we move later to hosting logs in user-provided buckets instead of our own bucket there's no reason why they shouldn't be able to write large logs if they want to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1570846197:234,load,loading,234,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1570846197,1,['load'],['loading']
Performance,Do you not want to make this a separate script that gets loaded?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6468#issuecomment-505496680:57,load,loaded,57,https://hail.is,https://github.com/hail-is/hail/pull/6468#issuecomment-505496680,1,['load'],['loaded']
Performance,Docker requires the `-f` tag to purge an image from its cache if multiple tags reference the same image ID. Checked to make sure that this couldn't accidentally disturb the worker container and added an assert because we should never even try to remove the worker image.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10681:56,cache,cache,56,https://hail.is,https://github.com/hail-is/hail/pull/10681,1,['cache'],['cache']
Performance,"Does PCA do a scan? Either way, do you want to do the same optimization for TableAggregate?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12334#issuecomment-1280733235:59,optimiz,optimization,59,https://hail.is,https://github.com/hail-is/hail/pull/12334#issuecomment-1280733235,1,['optimiz'],['optimization']
Performance,Don't construct intermediate string.; Print full exception on metadata load failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/899:71,load,load,71,https://hail.is,https://github.com/hail-is/hail/pull/899,1,['load'],['load']
Performance,"Don't presist in repartition/coalesce.; Fixed Int overflow bug in OrderedRDD.coalesce.; Convert to GenotypeStream in cache, persist.; Fixed filteralleles annotation bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/915:117,cache,cache,117,https://hail.is,https://github.com/hail-is/hail/pull/915,1,['cache'],['cache']
Performance,"Don't split variable-length encoded ints across compression blocks. Save a comparison in the inner loop. As usual, has seemingly negligible effect on performance. I'm sure it would be significant on the C side. I don't encode as signed yet. It makes the termination condition more complicated and I want to think on it a bit more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2430:150,perform,performance,150,https://hail.is,https://github.com/hail-is/hail/pull/2430,1,['perform'],['performance']
Performance,Dramatically improves performance of PC Relate. Some prior discussion at #2280 .,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2365:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/2365,1,['perform'],['performance']
Performance,"EDIT: The problem in this message is still valid, but the fix described here was replaced following the conversation with Tim. Ok, second pruning bug fixed. This commit illustrates the change, probably worth looking at in isolation: https://github.com/hail-is/hail/pull/9578/commits/f4ce01d59a56bef7496490145e66d274f51139e6. Essentially, the problem was that we were rebuilding an `InsertFields(child, newFields, _)` node with a requested type of empty struct. The old rebuilding strategy handled this by recursively rebuilding the `child` node, then filtering `newFields` to include only those in the requested type. However, if the `child` node can't be filtered (say it's a `Ref` of a certain type), and one of the fields being inserted is overwriting a field in the child, you need to make sure you do the overwriting so the typing works out. Happy to explain more if this is unclear. I also added an optimization to just drop the whole `InsertFields` if the requested type is the empty struct, not sure if that's good pruning manners though. . CC @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9578#issuecomment-708568970:905,optimiz,optimization,905,https://hail.is,https://github.com/hail-is/hail/pull/9578#issuecomment-708568970,1,['optimiz'],['optimization']
Performance,"END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; ```. There was nothing in the IR that stood out when I examined it, but I will admit that I'm not the best at digging into it. ### Version. https://github.com/chrisvittal/hail/tree/vds/repro-example. ### Relevant log output. ```shell; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadByte(Memory.java:130); E 	at is.hail.annotations.Region$.loadByte(Region.scala:28); E 	at is.hail.annotations.Region$.loadBit(Region.scala:86); E 	at __C23148collect_distributed_array_matrix_native_writer.__m23333split_ToArray(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region478_486(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region16_503(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region14_529(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.uti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:2704,load,loadBit,2704,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['load'],['loadBit']
Performance,Either sphinx cached incorrectly or it's not writing the added text: https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/18456:id/www/hail/expr/hail.expr.TInt.html#hail.expr.TInt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806:14,cache,cached,14,https://hail.is,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806,1,['cache'],['cached']
Performance,Ensuring consistent cache keys across builds is challenging. We have to at least move the build-info file out of the JAR but there seemed to be other minor inconsistencies too,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14005#issuecomment-1874171278:20,cache,cache,20,https://hail.is,https://github.com/hail-is/hail/pull/14005#issuecomment-1874171278,1,['cache'],['cache']
Performance,"Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1620,Concurren,ConcurrentRestrictions,1620,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['Concurren'],['ConcurrentRestrictions']
Performance,"Environment:; - Spark 3.2.0; - Scala 2.12.15. Running: ; ```; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.2.0; ```; I get the error:; ```BUILD SUCCESSFUL in 2m 5s; 3 actionable tasks: 3 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; make: *** No rule to make target 'check-pip-lockfiles', needed by 'install-on-cluster'. Stop.; ```. Issue is fixed for me by renaming `install-on-cluster: $(WHEEL) check-pip-lockfiles` -> `install-on-cluster: $(WHEEL) check-pip-lockfile` on line 344 of hail/Makefile. Many thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12568:697,cache,cache,697,https://hail.is,https://github.com/hail-is/hail/issues/12568,1,['cache'],['cache']
Performance,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2881:510,cache,cache,510,https://hail.is,https://github.com/hail-is/hail/pull/2881,1,['cache'],['cache']
Performance,"Error messages from GCR and AR are different, and sometimes it looks like this from GCR:. ```; ""unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12708:151,perform,perform,151,https://hail.is,https://github.com/hail-is/hail/pull/12708,1,['perform'],['perform']
Performance,"Error summary: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. ```; In [4]: tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-3a6cee08b392> in <module>; ----> 1 tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:671,load,loads,671,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['load'],['loads']
Performance,Error: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(Context,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:1622,load,loadLibrary,1622,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['load'],['loadLibrary']
Performance,"Error: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2460,Load,LoadVCF,2460,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.221ms self 0.221ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.795ms self 0.010ms children 3.785ms %children 99.73%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:133056,Optimiz,OptimizePass,133056,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.563ms self 0.563ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 4.599ms self 0.009ms children 4.590ms %children 99.81%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:148843,Optimiz,OptimizePass,148843,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.030ms self 0.030ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:105040,Optimiz,Optimize,105040,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:90602,Optimiz,Optimize,90602,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:69423,Optimiz,Optimize,69423,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.210ms self 0.210ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 47.463ms self 0.201ms children 47.262ms %children 99.58%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 6.134ms self 6.134ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:37965,Optimiz,Optimize,37965,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,Evaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass/is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass#after total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.600ms self 0.007ms children 0.594ms %children 98.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:179913,Optimiz,OptimizePass,179913,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.scala:1008); at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:647); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); at is.hail.variant.MatrixTable.write(MatrixTable.scala:1238); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.r,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:7222,Load,LoadVCF,7222,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,"Example of tooling/debug (Firefox, built in to Chrome as well). <img width=""1308"" alt=""screen shot 2018-12-11 at 9 57 24 pm"" src=""https://user-images.githubusercontent.com/5543229/49844151-c0b46a80-fd8f-11e8-8ed5-de0d8293d538.png"">. Hidden cost of isomorphic SSR in React: small bundle hit: currently sends serialized state with initial / first page render. In case of scorecard, costs a few KB for the full /json response. No such cost incurred from client-side requests, but if the page that fetches those resources is the first-hit page, would incur loading indicator (on any other page could be prefetched).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-446443905:553,load,loading,553,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-446443905,1,['load'],['loading']
Performance,Example stack trace:; ```; 2022-02-08 18:09:30 root: ERROR: IllegalArgumentException: requirement failed; From java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:52); 	at is.hail.rvd.RVDPartitioner.extendKeySamePartitions(RVDPartitioner.scala:141); 	at is.hail.expr.ir.LoweredTableReader$$anon$2.coerce(TableIR.scala:382); 	at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); 	at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1790); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:581); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1304); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1035); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:394); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:547); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:69); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77). ```. called from here:; https://github.com/hail-is/hail/blob/d2f87d81dd1af43617740309e354d4bac8c672e0/hail/src/main/scala/is/hail/expr/ir/TableIR.scala#L382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11335:554,Load,LoadVCF,554,https://hail.is,https://github.com/hail-is/hail/issues/11335,1,['Load'],['LoadVCF']
Performance,Executor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11301,Load,LoadVCF,11301,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,Exporting profile225 on my laptop:; exportvcf (.bgz) -- 4m45s; exportplink (before changes) -- 34s; exportplink (after changes) -- 46s. Weird behavior going on there... I'd have thought that performance would go up.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/136#issuecomment-184022844:191,perform,performance,191,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-184022844,1,['perform'],['performance']
Performance,F build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux FS.cpp -MG -M -MF build/FS.d -MT build/FS.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Encoder.cpp -MG -M -MF build/Encoder.d -MT build/Encoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Decoder.cpp -MG -M -MF build/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_te; st.o; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/Na,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:9521,cache,cache-tests,9521,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cache-tests']
Performance,FS.open(FS.scala:569); E 	at is.hail.io.fs.FS.open$(FS.scala:569); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.index.IndexReader$.readTypes(IndexReader.scala:65); E 	at is.hail.io.bgen.LoadBgen$.$anonfun$getBgenFileMetadata$2(LoadBgen.scala:208); E 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); E 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); E 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); E 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); E 	at is.hail.io.bgen.LoadBgen$.getBgenFileMetadata(LoadBgen.scala:207); E 	at is.hail.io.bgen.MatrixBGENReader$.apply(LoadBgen.scala:387); E 	at is.hail.io.bgen.MatrixBGENReader$.fromJValue(LoadBgen.scala:365); E 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:116); E 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1815); E 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1738); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:2164); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2136); E 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:2164); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$2(Backend.scala:186); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.annotations.RegionP,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:3355,Load,LoadBgen,3355,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,1,['Load'],['LoadBgen']
Performance,"FWIW, I think most of the regression was this PR: https://github.com/broadinstitute/seqr/pull/3792/files. We kind of knew the performance was worse but also we decided that we needed to fix the bug more than we needed performance to be good :/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1917741862:126,perform,performance,126,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1917741862,2,['perform'],['performance']
Performance,"FWIW, this pipeline was performing these checks perhaps as many as 10 times per genotype, which is obviously unreasonable. Nonetheless, sending the RG along as a literal should improve the speed of these operations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13862#issuecomment-1773407789:24,perform,performing,24,https://hail.is,https://github.com/hail-is/hail/issues/13862#issuecomment-1773407789,1,['perform'],['performing']
Performance,"FYI, @lgruen we're abandoning the memory service entirely as we've started to experience similar issues to you all as people start to use QoB here at Broad. Daniel has some thoughts on a better cache layer. More soon!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12784#issuecomment-1470475331:194,cache,cache,194,https://hail.is,https://github.com/hail-is/hail/pull/12784#issuecomment-1470475331,1,['cache'],['cache']
Performance,"FYI, I create a `yDummy` of all zeros in order to very simply reuse the regression utils we have. Effect on performance is negligible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1655#issuecomment-292960736:108,perform,performance,108,https://hail.is,https://github.com/hail-is/hail/pull/1655#issuecomment-292960736,2,['perform'],['performance']
Performance,"FYI, I performance tested this + https://github.com/hail-is/hail/pull/8326 and gained a few percent:. ```; Harmonic mean: 91.0%; Geometric mean: 95.3%; Arithmetic mean: 96.8%; Median: 97.8%; ```. So this should go in and we leave the memoization optimization as a TODO.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8245#issuecomment-601652867:7,perform,performance,7,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-601652867,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"FYI, includes missing dependencies for batch_deploy and ci_deploy on create_accounts that can cause race condition failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460:100,race condition,race condition,100,https://hail.is,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460,1,['race condition'],['race condition']
Performance,"FYI, the PR shows up in my scorecard.hail.is queue if I'm an assignee.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7386#issuecomment-547133616:45,queue,queue,45,https://hail.is,https://github.com/hail-is/hail/pull/7386#issuecomment-547133616,1,['queue'],['queue']
Performance,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303:1357,load,loadClass,1357,https://hail.is,https://github.com/hail-is/hail/issues/303,3,['load'],['loadClass']
Performance,"Failure in image_fetcher_image:. ```; Traceback (most recent call last):; File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 279, in run; await docker_call_retry(self.container.start); File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 86, in docker_call_retry; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start; data=kwargs; File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'read unix @->@/containerd-shim/moby/7f911041e4fcc5ea78b6b3979d1232d0954193bced0e1e85acd2133b80cc463e/shim.sock: read: connection reset by peer: unknown'); ```. I will add this to the retry list. Another reason to drop docker ASAP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320:534,load,loads,534,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320,1,['load'],['loads']
Performance,"FatalError: HailException: optimization changed type!; before: Matrix{global:+Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh38),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{CIEND:Array[Int32],CIPOS:Array[Int32],CS:String,END:Int32,IMPRECISE:Boolean,MC:Array[String],MEINFO:Array[String],MEND:Int32,MLEN:Int32,MSTART:Int32,SVLEN:Array[Int32],SVTYPE:String,TSD:String,AC:Array[Int32],AF:Array[Float64],NS:Int32,AN:Int32,EAS_AF:Array[Float64],EUR_AF:Array[Float64],AFR_AF:Array[Float64],AMR_AF:Array[Float64],SAS_AF:Array[Float64],DP:Int32,AA:String,VT:Array[String],EX_TARGET:Boolean,MULTI_ALLELIC:Boolean,STRAND_FLIP:Boolean,REF_SWITCH:Boolean,DEPRECATED_RSID:Array[String],RSID_REMOVED:Array[String],GRCH37_38_REF_STRING_MATCH:Boolean,NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH:Boolean,GRCH37_POS:Int32,GRCH37_REF:String,ALLELE_TRANSFORM:Boolean,REF_NEW_ALLELE:Boolean,CHROM_CHANGE_BETWEEN_ASSEMBLIES:Array[String]},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh38),old_alleles:Array[String]},entry:Struct{GT:Call}}; after: Matrix{global:+Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh38),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{CIEND:Array[Int32],CIPOS:Array[Int32],CS:String,END:Int32,IMPRECISE:Boolean,MC:Array[String],MEINFO:Array[String],MEND:Int32,MLEN:Int32,MSTART:Int32,SVLEN:Array[Int32],SVTYPE:String,TSD:String,AC:Array[Int32],AF:Array[Float64],NS:Int32,AN:Int32,EAS_AF:Array[Float64],EUR_AF:Array[Float64],AFR_AF:Array[Float64],AMR_AF:Array[Float64],SAS_AF:Array[Float64],DP:Int32,AA:String,VT:Array[String],EX_TARGET:Boolean,MULTI_ALLELIC:Boolean,STRAND_FLIP:Boolean,REF_SWITCH:Boolean,DEPRECATED_RSID:Array[String],RSID_REMOVED:Array[String],GRCH37_38_REF_STRING_MATCH:Boolean,NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH:Boolean,GRCH37_POS:Int32,GRCH37_REF:String,ALLELE_TRANSFORM:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4524:59,optimiz,optimization,59,https://hail.is,https://github.com/hail-is/hail/issues/4524,1,['optimiz'],['optimization']
Performance,"Feedback welcome, not for merging. Goal with ScoreCovariance was to get a working interface rather than optimize speed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2219:104,optimiz,optimize,104,https://hail.is,https://github.com/hail-is/hail/pull/2219,1,['optimiz'],['optimize']
Performance,Felt a little silly that I can't run `hailctl curl default batch /` and see some HTML output. Perhaps more importantly this would allow a more sophisticated client-side UI to call `/api` methods if it wants to perform an action that isn't a page reload. This would also allow us to proxy the backend in local development environments (though I had written this prior to our recent discussion. Stacking on the CSRF PR #13604,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13616:210,perform,perform,210,https://hail.is,https://github.com/hail-is/hail/pull/13616,1,['perform'],['perform']
Performance,"FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: RuntimeException: Stream is already closed.; E ; E Java stack trace:; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distribu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:13874,concurren,concurrent,13874,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"Final conclusion here: Since batch 1 is discontinued in favor of batch 2, and we don't know how prometheus will react to batch 2's logging load (should be better since there's not constant kubernetes junk), let's not start a new node pool for now, and request 20 Gbs of memory as done here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6774#issuecomment-521758111:139,load,load,139,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-521758111,1,['load'],['load']
Performance,"First crack at supporting multi phenotype logistic regression. No matrix optimizations, as is implemented in multi phenotype linear regression, but I attempt to follow a similar approach as far as far as API and single call of mapPartitions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5072:73,optimiz,optimizations,73,https://hail.is,https://github.com/hail-is/hail/pull/5072,1,['optimiz'],['optimizations']
Performance,First step in fixing: https://github.com/hail-is/hail/issues/5358. @chrisvittal FYI this should improve the performance of multiple aggregations across samples in MatrixTable.anntoate_rows.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5598:108,perform,performance,108,https://hail.is,https://github.com/hail-is/hail/pull/5598,1,['perform'],['performance']
Performance,"First, @cristinaluengoagullo, thank you for your contribution! This is awesome. Second, I think this PR will need a little work before it can go in. Let me describe the situation:. We have now stopped work on Hail 0.1 and are now making only critical bug fixes. I think we can accept small feature additions, but we're optimizing for stability over features now. All new development has moved to master/0.2 beta. If you do make changes to 0.1, they should be forward ported to 0.2 if you want them to be carried forward. In addition, there are two problems with your PR:. 1. It is quite large. We prefer contributions to be single conceptual units. For example, a change to VEP should be separate from additions to the function registry. 2. The diff is somewhat confusing and I'm not 100% sure what is going on. It appears to include a large number of our own changes, it looks like from this commit: https://github.com/hail-is/hail/pull/3172/commits/e6f0b7f3a854f0fd64857876ab04375e570ba09f. However, given that the commit is under your name with a new commit message, I can't tell where those changes originated. Also, at least some (all?) of those changes already appear in 0.1, so I'm not sure why Github is displaying them, for example: https://github.com/hail-is/hail/pull/3172/files#diff-f11d07953ac5cd8bd8d4d3fd135a3efbR11. I think squashing your changes (and just your changes) and rebasing them onto the current 0.1 HEAD will fix the problem. Then we can take a closer look at the changes. Finally, it looks like your changed the VEP schema because you're invoking it with extra plugins. I think that's a no-go for us, but you could modify the VEP command (through an argument or in the properties file) to specify an alternate schema.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347:319,optimiz,optimizing,319,https://hail.is,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347,1,['optimiz'],['optimizing']
Performance,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7445:1326,race condition,race condition,1326,https://hail.is,https://github.com/hail-is/hail/pull/7445,1,['race condition'],['race condition']
Performance,Fix IR size printed in log statement after optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5441:43,optimiz,optimize,43,https://hail.is,https://github.com/hail-is/hail/pull/5441,1,['optimiz'],['optimize']
Performance,Fix LoadVCF exception message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2402:4,Load,LoadVCF,4,https://hail.is,https://github.com/hail-is/hail/pull/2402,1,['Load'],['LoadVCF']
Performance,Fix performance of Table.head,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5156:4,perform,performance,4,https://hail.is,https://github.com/hail-is/hail/pull/5156,1,['perform'],['performance']
Performance,Fix table/matrixtable construction performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4952:35,perform,performance,35,https://hail.is,https://github.com/hail-is/hail/pull/4952,1,['perform'],['performance']
Performance,Fix this issue:. ```; XMLHttpRequest cannot load https://github.com/hail-is/hail/blob/master/www/navbar.html. Origin http://discuss.hail.is is not allowed by Access-Control-Allow-Origin.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1024:44,load,load,44,https://hail.is,https://github.com/hail-is/hail/issues/1024,1,['load'],['load']
Performance,Fix wasSplit for BGEN loader [0.1],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2239:22,load,loader,22,https://hail.is,https://github.com/hail-is/hail/pull/2239,1,['load'],['loader']
Performance,Fixed reuse of code in loadElement in SJavaArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10901:23,load,loadElement,23,https://hail.is,https://github.com/hail-is/hail/pull/10901,1,['load'],['loadElement']
Performance,"Fixes #13346. Another user was confused by this: https://github.com/hail-is/hail/issues/14102. Unfortunately, the world appears to have embraced missing values in VCF array fields even though the single element case is ambiguous. In #13346, I proposed a scheme by which we can disambiguate many of the cases, but implementing it ran into challenges because LoadVCF.scala does not expose whether or not an INFO field was a literal ""."" or elided entirely from that line. Anyway, this error message actually points users to the fix. I also changed some method names such that every method is ArrayType and never TypeArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14105:357,Load,LoadVCF,357,https://hail.is,https://github.com/hail-is/hail/pull/14105,1,['Load'],['LoadVCF']
Performance,"Fixes #4251. @jbloom22 Is there anything else I should add? Maybe something about the relative performance of each approach? I also thought about using two separate linreg aggregator annotations, but that didn't seem better than the group_by approach.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4458:95,perform,performance,95,https://hail.is,https://github.com/hail-is/hail/pull/4458,1,['perform'],['performance']
Performance,"Fixes #5095 and tips Google that we don't want it to index /devel /stable (also 301 redirects are cached, should be faster).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5344:98,cache,cached,98,https://hail.is,https://github.com/hail-is/hail/pull/5344,1,['cache'],['cached']
Performance,"Fixes #5449. We don't have machinery for testing performance behavior of something; like show() right now, so I can't test it easily. But I did verify by; hand.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5468:49,perform,performance,49,https://hail.is,https://github.com/hail-is/hail/pull/5468,1,['perform'],['performance']
Performance,Fixes #7063. I forgot to change the cached methods in Emit to account for return type when I changed it to allow a single function to specify multiple return types.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7066:36,cache,cached,36,https://hail.is,https://github.com/hail-is/hail/pull/7066,1,['cache'],['cached']
Performance,Fixes O(N) performance of mt.entries().show(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7867:11,perform,performance,11,https://hail.is,https://github.com/hail-is/hail/pull/7867,1,['perform'],['performance']
Performance,"Fixes connection timeout after 8 hours. . When we transition to aiomysql, will port well to a pooled connection version (`async with self.pool.acquire() as conn:`. Even now however, the time it takes to acquire a connection is not the bottleneck during login. cc @danking assigned you as well in case you're on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5815:235,bottleneck,bottleneck,235,https://hail.is,https://github.com/hail-is/hail/pull/5815,1,['bottleneck'],['bottleneck']
Performance,"For #604: I changed the max-width to 80em from 45em. If this is not wide enough, then we should probably remove the max-width property. For #605: It was extremely difficult to replicate the issue, but I believe it's because the mathjax and jquery operations are running asynchronously and the mathjax finishes before the jquery code has finished populating the DOM. I added a ""defer"" attribute to the mathjax script loading, so the script downloads in the background, but doesn't get executed until the DOM has been populated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/630:416,load,loading,416,https://hail.is,https://github.com/hail-is/hail/pull/630,1,['load'],['loading']
Performance,"For an aggregate, or for collect specifically? You can for aggregators that are commutative. Most are. Aggregate/collect just happens not to be. I personally think this is a mistake/over-optimization, and we should (1) focus on making shuffles fast, and (2) when you're collecting, you can just do the key_by sort locally at the end anyway, so it is super cheap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448843503:187,optimiz,optimization,187,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448843503,1,['optimiz'],['optimization']
Performance,"For example,; ```; mt = hl.split_multi_hts(mt, permit_shuffle=True); mt = mt.annotate_cols(foo=hl.agg.sum(...)); ```; performs the aggregation in one giant partition. This is a known issue, and we are working on it. A workaround is to write the result of `split_multi` to disk before using it (which is a good idea regardless). The cause appears to the interaction of several things:; * `split_multi` computes two separate pieces, then merges them together with a `TableUnion`; * The aggregation doesn't care how the mt is keyed, and we propagate that upstream; * Because of that, the `TableUnion` ends up unioning two unkeyed tables; * `TableUnion`, as currently implemented, can only produce a result with a strict partitioner. In the case of no key fields, that means one partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13407:118,perform,performs,118,https://hail.is,https://github.com/hail-is/hail/issues/13407,1,['perform'],['performs']
Performance,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1098,load,loading,1098,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692,10,"['load', 'perform']","['load', 'loaded', 'loading', 'loads', 'performance']"
Performance,"For reference, here is the IR generated by `read_matrix_table().count_cols()` which I want to optimize to `I64(blah)`:. ```; (TableCount; (TableKeyBy (s) False; (TableParallelize None; (Let __cols_and_globals; (TableGetGlobals; (TableZipUnchecked; (TableMapGlobals; (TableMapGlobals; (TableRead ""/Users/tpoterba/data/profile.mt/rows"" True Table{global:Struct{},key:[locus,alleles],row:Struct{locus:Locus(GRCh37),alleles:Array[String]}}); (Literal Struct{} <literal value>)); (InsertFields; (Ref global); (__cols; (GetField rows; (TableCollect; (TableRead ""/Users/tpoterba/data/profile.mt/cols"" False Table{global:Struct{},key:[],row:Struct{s:String}})))))); (TableRead ""/Users/tpoterba/data/profile.mt/entries"" True Table{global:Struct{},key:[],row:Struct{`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[Struct{}]}}))); (MakeStruct; (rows; (ArrayMap elt; (ArraySort True; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref __cols_and_globals))); (I32 1)); (Let __cols_element; (ArrayRef; (GetField __cols; (Ref __cols_and_globals)); (Ref i)); (MakeStruct; (_1; (SelectFields (s); (Ref __cols_element))); (_2; (Ref __cols_element))))); (True)); (GetField _2; (Ref elt)))); (global; (SelectFields (); (Ref __cols_and_globals)))))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-449744021:94,optimiz,optimize,94,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449744021,1,['optimiz'],['optimize']
Performance,"For reference, here was the plink output from one of those tests:. ```. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); Recent cached web-check found...Problem connecting to web. Writing this text to log file [ /tmp/hail.3ouc7OzAKpSQ/plink.00001.log ]; Analysis started: Mon Jul 4 11:38:41 2016. ** Unused command line option: --vcf; ** Unused command line option: src/test/resources/sample.vcf; ** Unused command line option: --const-fid; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230289474:574,cache,cached,574,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230289474,1,['cache'],['cached']
Performance,"For some reason we delete secrets when re-creating them. This creates a race condition which Batch; appears to encounter whenever it is under heavy load during a deploy. @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I am using the command sequence suggested here: https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217:72,race condition,race condition,72,https://hail.is,https://github.com/hail-is/hail/pull/10217,2,"['load', 'race condition']","['load', 'race condition']"
Performance,"For the partitioning algorithm, I updated the test to confirm that all the individuals in the unrelated set are mutually unrelated. For PC-AiR, I updated the test to compare the loadings to PCA on just the unrelated individuals. The loadings are NumPy close. The scores are slightly different though because they are calculated differently. When there are related individuals, the scores are calculated by multiplying the loadings and the standardized genotypes. When there are no related individuals, the scores are calculated by multiplying the columns of the appropriate singular matrix with the eigenvalues. So for the scores, I just add a regression test. (In my testing, I observed that most of the scores were less than 1% different. However, there were a few differences that were larger around 20% or 30%. Not sure if this is a cause for concern because the calculation approaches are different and the SVD is approximate.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1977943502:178,load,loadings,178,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1977943502,3,['load'],['loadings']
Performance,"For ~1500 joint caller inputs, on what was a frankenbranch (I merged a bunch of stuff together, haven't pushed it). I saw that IR construction in python took around a minute. I'm not entirely sure where the time was being spent though. Also 1 minute is nothing compared to the amount of time we spend actually joint calling. Anyways, `run_combiner.py`; ```python3; def run_combiner(sample_list, json, out_path, tmp_path, summary_path=None, overwrite=False):; # make the temp path a directory, no matter what; tmp_path += f'/combiner-temporary/{uuid.uuid4()}/'; vcfs = [comb.transform_one(vcf); for vcf in hl.import_vcfs(sample_list, json, array_elements_required=False)]; combined = [comb.combine_gvcfs(mts) for mts in chunks(vcfs, MAX_COMBINER_LENGTH)]; if len(combined) == 1:; combined[0].write(out_path, overwrite=overwrite); else:; hl.utils.java.info(f'Writing combiner temporary files to: {tmp_path}'); ... # do more, but this stage isn't huge yet so :man_shrugging: ; ```; Relevant log:; ```; 2019-03-01 22:09:20 DAGScheduler: INFO: Job 0 finished: collect at LoadVCF.scala:1295, took 88.076400 s; 2019-03-01 22:10:19 Hail: INFO: Writing combiner temporary files to: gs://cdv-hail/combiner/tmp//combiner-temporary/dc741728-fdfd-49d9-a66e-94bd7b541879/; ```; Stage zero is tabix reading `sc.parallelize`. The next line is the logging line that I added, almost a minute apart. After that it's 30 seconds for the Optimizer and Lowerer and (printing hundreds of thousands of lines of IR).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5465#issuecomment-469789122:1066,Load,LoadVCF,1066,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469789122,2,"['Load', 'Optimiz']","['LoadVCF', 'Optimizer']"
Performance,"Forgot that we were importing navbar.css. We could go back to a separate navbar.css, but I think that is less optimal because I don't think you can include the html directly in xslt, and importing it using jquery can cause the navbar to flash immediately after page load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8489:266,load,load,266,https://hail.is,https://github.com/hail-is/hail/pull/8489,1,['load'],['load']
Performance,"Found the problem where EXISTS in a correlated subquery should be rewritten as IN; https://dev.mysql.com/doc/refman/5.7/en/optimizing-subqueries.html. ```; -> FROM jobs; -> INNER JOIN batches ON jobs.batch_id = batches.id; -> LEFT JOIN aggregated_job_resources; -> ON jobs.batch_id = aggregated_job_resources.batch_id AND; -> jobs.job_id = aggregated_job_resources.job_id; -> LEFT JOIN resources; -> ON aggregated_job_resources.resource = resources.resource; -> INNER JOIN job_attributes; -> ON jobs.batch_id = job_attributes.batch_id AND; -> jobs.job_id = job_attributes.job_id AND; -> job_attributes.`key` = 'name'; -> WHERE (jobs.batch_id = 14327) AND; -> (jobs.batch_id, jobs.job_id) IN (SELECT batch_id, job_id FROM job_attributes WHERE `key` = 'pheno' AND `value` = '50'); -> GROUP BY jobs.batch_id, jobs.job_id; -> ORDER BY jobs.batch_id, jobs.job_id ASC; -> LIMIT 50;; +----+-------------+--------------------------+------------+--------+--------------------------------------------------+--------------------------+---------+-----------------------------------------+------+----------+---------------------------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+--------------------------+------------+--------+--------------------------------------------------+--------------------------+---------+-----------------------------------------+------+----------+---------------------------------+; | 1 | SIMPLE | batches | NULL | const | PRIMARY | PRIMARY | 8 | const | 1 | 100.00 | Using temporary; Using filesort |; | 1 | SIMPLE | job_attributes | NULL | ref | PRIMARY,job_attributes_key_value | job_attributes_key_value | 1081 | const,const,const | 3057 | 100.00 | Using where |; | 1 | SIMPLE | jobs | NULL | eq_ref | PRIMARY,jobs_batch_id_state_always_run_cancelled | PRIMARY | 12 | const,batch.job_attributes.job_id | 1 | 100.00 | NULL |; | 1 | SIMPLE | job_attributes | NULL | eq_ref | PRIMARY,jo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9870:123,optimiz,optimizing-subqueries,123,https://hail.is,https://github.com/hail-is/hail/pull/9870,1,['optimiz'],['optimizing-subqueries']
Performance,"Francesco found that after filtering to the purcell 5k (6k sites after a `count`), the loadings file produced by PCA only contained 400 variants.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/806:87,load,loadings,87,https://hail.is,https://github.com/hail-is/hail/issues/806,1,['load'],['loadings']
Performance,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3447:62,load,loaded,62,https://hail.is,https://github.com/hail-is/hail/issues/3447,1,['load'],['loaded']
Performance,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6495:48,latency,latency,48,https://hail.is,https://github.com/hail-is/hail/issues/6495,1,['latency'],['latency']
Performance,From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2306,Load,LoadPlink,2306,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"From that VM, I can get into the container, install the jdk, then run jstack on one of the hung JVMs.; ```; # jstack 1433; ...; ""pool-1-thread-1"" #18 prio=5 os_prio=0 tid=0x00007f50c4f23000 nid=0x82c waiting on condition [0x00007f5084eeb000]; java.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:380,concurren,concurrent,380,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,7,['concurren'],['concurrent']
Performance,"From triage discussion. What this means: Instance destruction is slower than it needs to be, this can impact throughput but does not really impact reliability, and is uncommon enough to not be high priority at this time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380#issuecomment-1977202541:109,throughput,throughput,109,https://hail.is,https://github.com/hail-is/hail/issues/14380#issuecomment-1977202541,1,['throughput'],['throughput']
Performance,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3038:96,load,loaded,96,https://hail.is,https://github.com/hail-is/hail/issues/3038,1,['load'],['loaded']
Performance,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:111,cache,cache,111,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,8,"['cache', 'load']","['cache', 'loading']"
Performance,Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216262,concurren,concurrent,216262,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Generates a lot of bytecode and called in performance-sensitive places. Helps with profiling a bit.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10337:42,perform,performance-sensitive,42,https://hail.is,https://github.com/hail-is/hail/pull/10337,1,['perform'],['performance-sensitive']
Performance,"Getting this over the past few days when doing, well, basically any query. Log: [hail.log.txt](https://github.com/hail-is/hail/files/755839/hail.log.txt). ```; Caused by: java.lang.ClassNotFoundException: is.hail.sparkextras.ReorderedPartitionsRDDPartition; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1360:348,load,loadClass,348,https://hail.is,https://github.com/hail-is/hail/issues/1360,2,['load'],['loadClass']
Performance,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1518:948,concurren,concurrent,948,https://hail.is,https://github.com/hail-is/hail/issues/1518,4,"['concurren', 'queue']","['concurrent', 'queued']"
Performance,"Given our rate limit increases and turning on additional service tests, 5 concurrent PR batches is too much for the 4-core database to handle. This is a mitigation while we figure out the right way to maintain that load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11762:74,concurren,concurrent,74,https://hail.is,https://github.com/hail-is/hail/pull/11762,2,"['concurren', 'load']","['concurrent', 'load']"
Performance,"Given the breaking changes to interface, I'll make a discuss post and alert in various channels when merging. @liameabbott this may significantly improve performance when you pass a field as `x` in regression. I'll follow up directly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3289#issuecomment-378452971:154,perform,performance,154,https://hail.is,https://github.com/hail-is/hail/pull/3289#issuecomment-378452971,1,['perform'],['performance']
Performance,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10778:642,throughput,throughput,642,https://hail.is,https://github.com/hail-is/hail/pull/10778,1,['throughput'],['throughput']
Performance,Good catch! We removed this option a week or two ago because it prevented us from making performance improvements. . There's an example here which shows how to export the variantqc annotations with the expression-based `exportvariants` command:. http://discuss.hail.is/t/export-feature-added-splatting-structs-into-multiple-columns/59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1017#issuecomment-256799341:89,perform,performance,89,https://hail.is,https://github.com/hail-is/hail/issues/1017#issuecomment-256799341,1,['perform'],['performance']
Performance,"Good example. You can store physical key as a field, but you can also think of it as the result of an analysis, like prune and nesting depth, which you can compute in the optimizer as needed. Just another organizational option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6231#issuecomment-497760334:171,optimiz,optimizer,171,https://hail.is,https://github.com/hail-is/hail/pull/6231#issuecomment-497760334,1,['optimiz'],['optimizer']
Performance,"Google Cloud Storage documentation and [best practices] for object names; recommends avoiding sequential names like 'part-0nnnn'. We already use; UUIDs for randomness to avoid two tasks writing to the exact same; object, but by using the UUID as a prefix rather than a suffix we; (to a degree) uniformly distribute part file names over a range,; (hopefully) improving throughput. [best practices]: https://cloud.google.com/storage/docs/best-practices#naming",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836:368,throughput,throughput,368,https://hail.is,https://github.com/hail-is/hail/pull/10836,1,['throughput'],['throughput']
Performance,"Got it! You need to add the loop device. Now how to parse the correct thing from the xfs_info output is another story... ```; jigold@jg-file-cache:~$ xfs_info /mnt/test_xfs; meta-data=/dev/loop2 isize=512 agcount=4, agsize=65536 blks; = sectsz=512 attr=2, projid32bit=1; = crc=1 finobt=1 spinodes=0 rmapbt=0; = reflink=1; data = bsize=4096 blocks=262144, imaxpct=25; = sunit=0 swidth=0 blks; naming =version 2 bsize=4096 ascii-ci=0 ftype=1; log =internal bsize=4096 blocks=2560, version=2; = sectsz=512 sunit=0 blks, lazy-count=1; realtime =none extsz=4096 blocks=0, rtextents=0; ```. ```; jigold@jg-file-cache:~$ sudo docker run --rm --mount type=bind,source=/mnt/test_xfs,target=/host --cap-add SYS_ADMIN --security-opt apparmor:unconfined --device ""/dev/loop2:/dev/loop2:rwm"" test-xfs /bin/bash -c 'xfs_quota -x -c ""report -h"" /host'; Project quota on /host (/dev/loop2); Blocks; Project ID Used Soft Hard Warn/Grace; ---------- ---------------------------------; #0 4K 0 0 00 [------]; #200 0 0 0 00 [------]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004:141,cache,cache,141,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004,2,['cache'],['cache']
Performance,"Got it, will make sure next time I put prio:high label, the work gets done within <=2 days. Load balancing with other work, and will find a better balance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6952#issuecomment-526639135:92,Load,Load,92,https://hail.is,https://github.com/hail-is/hail/issues/6952#issuecomment-526639135,1,['Load'],['Load']
Performance,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:153,perform,performance,153,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096,12,"['bottleneck', 'cache', 'perform']","['bottleneck', 'bottlenecked', 'cache', 'caches', 'performance']"
Performance,"Great questions. The larger change here is that we're trying to decouple the Python front end from Scala/Java. Therefore, instead of calling directly into Python (e.g. getReferenceGenome), we're constructing a MatrixRead IR, which can either be passed to a service (ServiceBackend) or passed to Java as we did before (SparkBackend). I should also remark, py4j is incredibly slow, so batching calls to Python greatly improves performance in the current setup. So, for example, building an IR in Python, serializing it and parsing on Java is much faster than a series of py4j calls that builds the corresponding objects on the Java side one at a time. Some functions which used to be called from Python like importBgens are no longer used, but are still used by the Scala tests. Those got moved to TestUtils and are being phased out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5150#issuecomment-456682522:425,perform,performance,425,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-456682522,1,['perform'],['performance']
Performance,"Great, thanks. Should be ready to be reviewed now, the performance is now where I want it to be. (18x faster than aggregators for Caitlin's use case)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10632#issuecomment-875583903:55,perform,performance,55,https://hail.is,https://github.com/hail-is/hail/pull/10632#issuecomment-875583903,1,['perform'],['performance']
Performance,"Hail Version: 0.2.108. Running the line w/ Zstd compressed UKB bgen files:; `ht = hl.experimental.pc_project(; mt.GT,; loadings_ht.loadings,; loadings_ht.pca_af,; ); `; I get the error at the end of spark execution: ; `Exception in thread ""map-output-dispatcher-0"" Exception in thread ""map-output-dispatcher-1"" java.lang.UnsatisfiedLinkError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.recommendedCOutSize()J; `; (full error attached); [error.txt](https://github.com/hail-is/hail/files/10458606/error.txt). Any ideas?; Many Thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12608:131,load,loadings,131,https://hail.is,https://github.com/hail-is/hail/issues/12608,1,['load'],['loadings']
Performance,Hail cannot load several plink files in parallel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3975:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/issues/3975,1,['load'],['load']
Performance,"Hail failed to load VCF with haploid calls, such as those on chr Y or chrX nonPar male regions. According to VCF spec v4.2, those haploid calls should be represented as 0 or 1 instead of 0/0, or 1/1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1010:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/issues/1010,1,['load'],['load']
Performance,Hail failed to load haploid calls on chrX nonPar male or chrY regions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1010:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/issues/1010,1,['load'],['load']
Performance,Hail optimizer changes the type but does not show a difference in printable type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527:5,optimiz,optimizer,5,https://hail.is,https://github.com/hail-is/hail/issues/4527,1,['optimiz'],['optimizer']
Performance,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7063:1096,cache,cache,1096,https://hail.is,https://github.com/hail-is/hail/issues/7063,2,['cache'],['cache']
Performance,"Hail uses more than 51GiB of RAM to load ~82,000 results of a `collect_as_set` where the largest set is 12 entries. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/unexpected.20end.20of.20stream.20error/near/389506862",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13533#issuecomment-1709012282:36,load,load,36,https://hail.is,https://github.com/hail-is/hail/issues/13533#issuecomment-1709012282,1,['load'],['load']
Performance,Hail's Matrix Multiply Should Perform Better When Massively Reducing Size,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1975:30,Perform,Perform,30,https://hail.is,https://github.com/hail-is/hail/issues/1975,1,['Perform'],['Perform']
Performance,"Hail's optimizer should be smart enough to push `TableFilter` into a `TableExplode`. Consider these two equivalent pipelines on *tiny* data, a ten-by-ten matrix. ```; import hail as hl; mt = hl.balding_nichols_model(3, 10, 10); t = mt.entries(); t.filter(t.GT.is_hom_ref()).export('foo.tsv'); ```; ```; foo.tsv; merge time: 45.459ms; ```. ```; import hail as hl; mt = hl.balding_nichols_model(3,10,10); mt.filter_entries(mt.GT.is_hom_ref()).entries().export('foo2.tsv'); ```; ```; foo2.tsv; merge time: 23.856ms; ```. This will likely also require improving Hail's filter movement. I observed a `TableFilter` getting stuck behind a `TableMapGlobals`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6905:7,optimiz,optimizer,7,https://hail.is,https://github.com/hail-is/hail/issues/6905,1,['optimiz'],['optimizer']
Performance,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8636:316,cache,cache,316,https://hail.is,https://github.com/hail-is/hail/pull/8636,1,['cache'],['cache']
Performance,HailException: optimization changed type! after running split_multi(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4827:15,optimiz,optimization,15,https://hail.is,https://github.com/hail-is/hail/issues/4827,1,['optimiz'],['optimization']
Performance,Handler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAn,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215992,Optimiz,Optimize,215992,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,Handler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAn,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:200019,Optimiz,Optimize,200019,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,Handler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAn,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207971,Optimiz,Optimize,207971,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,Handler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAn,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:195367,Optimiz,Optimize,195367,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"Heh. At least in my version of Docker, those are implicitly relative to the root not the WORKDIR:; ```; (base) dking@wm28c-761 /tmp % cat Dockerfile ; FROM ubuntu:20.04; WORKDIR /foo/bar; VOLUME baz; (base) dking@wm28c-761 /tmp % docker build -t foo . ; [+] Building 0.1s (6/6) FINISHED ; => [internal] load build definition from Dockerfile 0.0s; => => transferring dockerfile: 34B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; => [internal] load metadata for docker.io/library/ubuntu:20.04 0.0s; => [1/2] FROM docker.io/library/ubuntu:20.04 0.0s; => CACHED [2/2] WORKDIR /foo/bar 0.0s; => exporting to image 0.0s; => => exporting layers 0.0s; => => writing image sha256:217748640e5c53f72b8de9917010e5742fb8bef99a37dcb13ec59a903cb5834c 0.0s; => => naming to docker.io/library/foo 0.0s. Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them; (base) dking@wm28c-761 /tmp % docker run foo /bin/sh -c 'pwd && ls -l . && ls -l / && ls -l /baz'; /foo/bar; total 0; total 56; drwxr-xr-x 2 root root 4096 May 9 15:06 baz; lrwxrwxrwx 1 root root 7 Oct 19 2022 bin -> usr/bin; drwxr-xr-x 2 root root 4096 Apr 15 2020 boot; drwxr-xr-x 5 root root 340 May 9 15:06 dev; drwxr-xr-x 1 root root 4096 May 9 15:06 etc; drwxr-xr-x 3 root root 4096 May 9 15:01 foo; drwxr-xr-x 2 root root 4096 Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 19 2022 lib -> usr/lib; drwxr-xr-x 2 root root 4096 Oct 19 2022 media; drwxr-xr-x 2 root root 4096 Oct 19 2022 mnt; drwxr-xr-x 2 root root 4096 Oct 19 2022 opt; dr-xr-xr-x 238 root root 0 May 9 15:06 proc; drwx------ 2 root root 4096 Oct 19 2022 root; drwxr-xr-x 5 root root 4096 Oct 19 2022 run; lrwxrwxrwx 1 root root 8 Oct 19 2022 sbin -> usr/sbin; drwxr-xr-x 2 root root 4096 Oct 19 2022 srv; dr-xr-xr-x 13 root root 0 May 9 15:06 sys; drwxrwxrwt 2 root root 4096 Oct 19 2022 tmp; drwxr-xr-x 10 root root 4096 Oct 19 2022 usr; drwxr-xr-x 11 root root 4096 Oct 19 2022 var; total 0; (base) dki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989:303,load,load,303,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989,7,"['CACHE', 'load']","['CACHED', 'load']"
Performance,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8908:461,perform,performance,461,https://hail.is,https://github.com/hail-is/hail/issues/8908,1,['perform'],['performance']
Performance,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:213,load,load,213,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374,1,['load'],['load']
Performance,"Here is Nirvana PR, as approved by the folks over at Illumina. They have some tweaks and changes they want to make in the works to improve performance and documentation, but this is functional and ready for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2127:139,perform,performance,139,https://hail.is,https://github.com/hail-is/hail/pull/2127,1,['perform'],['performance']
Performance,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:305,load,load,305,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['load'],['load']
Performance,"Here is what I get when invoking pyspark. $ pyspark; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/02 13:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/02 13:56:47 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.13 (default, Jul 18 2017 09:16:53); SparkSession available as 'spark'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996:469,load,load,469,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996,1,['load'],['load']
Performance,"Here's an implementation of a *split* SVCF-VCF courtesy of Tim P:; ```python3; import hail as hl. from constants import *. hl.init(log='/tmp/hail.log'). vds = hl.vds.read_vds(vds_path). hl.get_reference('GRCh38').add_sequence(get_fasta()). # load metadata structure from arbitrary input GVCF; metadata = hl.get_vcf_metadata(gvcf_paths[0]); metadata['format']['LEN'] = {; 'Description': 'Reference block length',; 'Number': '1',; 'Type': 'Integer',; }. # create reference VCF; rd = vds.reference_data; rd = rd.key_rows_by(locus=rd.locus, alleles=[rd.locus.sequence_context()]); rd = rd.transmute_entries(LEN=rd.END - rd.locus.position + 1); hl.export_vcf(rd, reference_svcr_vcf_path, metadata=metadata,; tabix=True). # create variant VCF; vd = vds.variant_data. # recode gvcf_info struct to top-level fields for compatibility with VCF format limitations; info_fields = list(vd.gvcf_info); mt = vd.transmute_entries(**{f'INFO_{x}': vd.gvcf_info[x] for x in info_fields}). # recode boolean info fields as integers to support VCF spec; bool_fields = [fd for fd in mt.entry if mt[fd].dtype == hl.tbool]; mt = mt.transmute_entries(**{fd: hl.int(mt[fd]) for fd in bool_fields}). def transform_number(number):; if number in {'A', 'R', 'G'}:; return f'LOCAL_{number}'; return number. for info_fd in info_fields:; info = metadata['info'][info_fd]; if info['Type'] == 'Flag':; info['Type'] = 'Integer'; info['Number'] = '1'; metadata['format'][f'INFO_{info_fd}'] = info. for d in metadata['format'].values():; d['Number'] = transform_number(d.get('Number')). hl.export_vcf(mt, variant_svcr_vcf_path, metadata=metadata,; tabix=True); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14010#issuecomment-1823581472:242,load,load,242,https://hail.is,https://github.com/hail-is/hail/issues/14010#issuecomment-1823581472,1,['load'],['load']
Performance,"Here's the link to the [headless service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) documentation and here's an [example blog post](https://techdozo.dev/grpc-load-balancing-on-kubernetes-using-headless-service/) where someone encountered the same issues we were facing with normal services. I think the documentation is motivation enough though: Envoy's `STRICT_DNS` setting would be considered a form of ""service discovery"" done through DNS. In order for Envoy to correctly make load balancing decisions, that DNS request should return all the viable IPs for an upstream instead of a single IP that points to kube proxy. Headless services do just that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094#issuecomment-1255233659:200,load,load-balancing-on-kubernetes-using-headless-service,200,https://hail.is,https://github.com/hail-is/hail/pull/12094#issuecomment-1255233659,2,['load'],"['load', 'load-balancing-on-kubernetes-using-headless-service']"
Performance,"Here's what _local_ looks like now. Note that I've already converted to a `vds` this time. ```; dking@wmb16-359 # rm -rf foo && time ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' ; hail: info: running: read -i profile.vds; [Stage 1:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:=====================================================> (210 + 4) / 214]hail: info: timing:; read: 3.047s; ibd: 4m35.1s; ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' 924.50s user 16.11s system 333% cpu 4:42.04 total; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/738#issuecomment-249995538:286,load,load,286,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-249995538,1,['load'],['load']
Performance,"Hey @anh151 !. I'm sorry you're having trouble with Hail. The message ""Container killed on request. Exit code is 137"" comes from Apache Spark, our underlying distributed compute framework. It indicates that the worker machines have insufficient RAM. In general, using worker machines with higher RAM-to-core ratios will help. If you're on GCP, try the n1-highmem family. Some other suggestions:. 1. Hail is a ""lazy"" system. Your entire pipeline is executed, from the beginning, when you run ""export"" or ""write"". That means that Hail has to do all of that work at once. You can ease the memory pressure by performing less operations at once, by writing an intermediate file (and reading back in and proceeding with it).; 2. We recommend against directly exporting from a complex operation (like group-by-aggregate). Instead, grab the cols table and write it to Hail's fast, binary, parallel format: `.cols().select('field_of_interest').write('my-cols.ht')`. Then read that table and export that: `hl.read_table('my-cols.ht').field_of_interest.export(...)`. Exporting to a text file requires more memory because we have to construct ASCII strings.; 3. Always use a compressed export: `.export('foo.tsv.bgz')` or `hl.export_vcf(..., 'foo.vcf.bgz')`. This won't help your memory problem, but you can avoid parsing strings to create loci by constructing an `hl.Locus` which is the Python-side representation of loci (`hl.locus` is the inside-Hail representation):; ```python3; def create_intervals(data):; return [; hl.Locus(chromosome, start, reference_genome=""GRCh38""); for i, (chromosome, start) in data[[""CHROM"", ""POS""]].iterrows(); ]; ```. Please reply here if you're still having problems after incorporating the above suggestions as it may indicate a more fundamental issue with Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485:605,perform,performing,605,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485,1,['perform'],['performing']
Performance,"Hey @illusional ! Sorry for the massive latency. OK, so, big apologies are in order, I totally lead you astray by mentioning the makefile. The Makefile *is* the source of hail version truth, but invoking the makefile inside an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:40,latency,latency,40,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,2,['latency'],['latency']
Performance,"Hey @jjfarrell,. Awesome! Thanks for the continued debugging support! So the `scores_expr` argument to [`hl.methods.pc_relate`](https://hail.is/docs/devel/methods/genetics.html?highlight=pc_relate#hail.methods.pc_relate) should be a numeric array. Every row must have the same length. All the scores must be non-missing. You can load a TSV into hail with [`hl.import_table`](https://hail.is/docs/devel/methods/impex.html?highlight=import_table#hail.methods.import_table). So in your case you probably want:; ```python; import hail as hl; scores = hl.import_table('/path/to/foo.tsv', impute=True); scores = scores.key_by('SampleId'); scores = scores.transmute(scores=[scores.PC1, scores.PC2, ..., scores.PC10]); dataset = # ...; rel = hl.pc_relate(; dataset.GT,; 0.02, ; scores_expr=scores[dataset.col_key].scores,; min_kinship=0.1); ```; ---; > Is it sorted by SampleId or indexed somehow?. So, the expression `scores_table[dataset.col_key]` indexes your TSV (loaded into Hail as a Table) by the column key of your dataset (the sample id). For this expression to make sense you have to define a key on your scores_table. I did that with `.key_by('SampleId')`. Since your `col_key` is also (I assume) a sample id they will be matched up as you expect. If the sample ids were mangled or changed by one of the tools that were used to generate the TSV, you'll need to un-mangle them, which you can, of course, use Hail to do as well, using something like `scores.key_by(scores.sample_id[0:10])` to take the first ten characters.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-390759209:329,load,load,329,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-390759209,2,['load'],"['load', 'loaded']"
Performance,"Hey @poneill !. `-O`/`PYTHONOPTIMIZE` is explicitly defined as the ""turn off asserts"" option in [the docs](https://docs.python.org/3/using/cmdline.html#cmdoption-O). If you disable asserts, you'll get even more inscrutable errors. I recommend against doing that. If you see any `assert(x, y)` in the code base, please file a PR or a bug. We'll fix it. We will not replace asserts with if-raise. ---. As to the bug you've found: yes this is a bug in Hail. We incorrectly assume that if there is at least one dataset with the right version and at least one dataset with the right reference genome that there's a dataset with the right version *and* reference genome. That logic is obviously false. I'll have someone fix this in the next couple weeks. As to the root issue: the Hail annotation database doesn't have a GRCh38 version of `gnomad_pca_variant_loadings` version 2.1. This is because [gnomAD](https://gnomad.broadinstitute.org/downloads#v2-liftover) hasn't published a GRCh38 version of their 2.1 variant loadings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952#issuecomment-1530239230:1013,load,loadings,1013,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1530239230,1,['load'],['loadings']
Performance,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:907,concurren,concurrent,907,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['concurren'],['concurrent']
Performance,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951:504,load,loaded,504,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951,2,['load'],['loaded']
Performance,Hi @barneyhill ! Sorry for the high latency. We don't do a great job of watching GitHub issues. In the future a post on https://discuss.hail.is will get a faster response. This sounds like a versioning issue. What is your environment? What version of Spark do you have?. This appears to be a fairly common issue that might be caused by class path issues or native library loading issues:; 1. https://github.com/luben/zstd-jni/issues/139; 2. https://github.com/luben/zstd-jni/issues/190; 3. https://github.com/luben/zstd-jni/issues/60,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12608#issuecomment-1452594239:36,latency,latency,36,https://hail.is,https://github.com/hail-is/hail/issues/12608#issuecomment-1452594239,2,"['latency', 'load']","['latency', 'loading']"
Performance,"Hi @danking, thanks for this. On the topic of asserts, there are really two interacting issues:. 1. Asserts are intended to ensure invariants, i.e. conditions that should always be true. In correct code, assertions should never raise so disabling them should have no consequences at runtime. In practice, however, they are often casually used to catch value errors, which can be expected to occur if a user-facing method receives bad/nonsensical inputs (e.g. here: https://github.com/hail-is/hail/blob/1940547d35ddddb084ad52684e36153c1e03a331/hail/python/hailtop/hailctl/dataproc/diagnose.py#L62); 2. Python's language design allows anyone calling your code to disable asserts for optimization purposes, because disabling asserts should never change the semantics of the program. Putting these two features together, you can arrive at a situation where a user thinks they're turning off asserts (which should never raise anyway) and instead stops catching value errors (whose absence can never be guaranteed). All that said, if the final answer is: ""if you invoke `-O` you deserve what's coming"", I'm happy to drop it :). Thanks for taking a look at the example. If I understand you correctly, it sounds like I passed the wrong inputs to the function, in which case it might be clearer to raise a ValueError instead of an AssertionError in the end. On a closer look, it seems like most of the instances of `assert(x, y)` are actually in scala code-- my mistake. Thanks again for looking into this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665:681,optimiz,optimization,681,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665,2,['optimiz'],['optimization']
Performance,"Hi @esebesty, can you ask this question on discuss.hail.is? We try to consolidate bug reports there and you will get a much faster response time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12630#issuecomment-1409342941:131,response time,response time,131,https://hail.is,https://github.com/hail-is/hail/issues/12630#issuecomment-1409342941,1,['response time'],['response time']
Performance,"Hi @jmarshall, the team talked about this issue in our standup today. We had some concerns about appropriateness of using this table as a long term storage area for larger metadata, and the likely developer effort and system downtime to perform the migration. So we currently don't plan on prioritizing this in the immediate future, but do let us know if you have any concerns about that - or if it ends up being impossible for you to work around this - and we might be able to reconsider (or maybe come up with alternative solutions). Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14702#issuecomment-2384128647:237,perform,perform,237,https://hail.is,https://github.com/hail-is/hail/issues/14702#issuecomment-2384128647,1,['perform'],['perform']
Performance,"Hi Jerome, this `AnnotationPathException` issue is something we've seen before. It seems to be caused sporadically by gradle's build caching, and can usually be fixed by running `gradle clean`. The tests that failed are probably the ones that require external tools available on the command line:; FisherExactSuite (requires Rscript); ImportPlinkSuite (requires plink 1.9); ExportPlinkSuite (requires plink 1.9); LoadBgenSuite (requires qctool)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240384398:413,Load,LoadBgenSuite,413,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240384398,1,['Load'],['LoadBgenSuite']
Performance,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:195,load,loadings-and-random-forest-classifier-on-your-dataset,195,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759,1,['load'],['loadings-and-random-forest-classifier-on-your-dataset']
Performance,"Hi all,. Here's the error message that I get when I go to install all of my python packages (scipy/uvloop/etc). ```; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-eb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:869,cache,cache,869,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['cache'],['cache']
Performance,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6012:693,Load,LoadVCF,693,https://hail.is,https://github.com/hail-is/hail/issues/6012,1,['Load'],['LoadVCF']
Performance,"Hi! . I know this is out of the blue, but we would like the ability to fetch resource_usage data from an endpoint programmatically to evaluate our job performance. I thought it might be worth suggesting this upstream to see if it's something you'd like too :). This PR:; 1. Use the internal method to fetch the dataframes for a job; 2. Transform the data frame to dictionary with `orient='split'`. And FWIW, here's how to convert it back into a dataframe:. ```python; import pandas as pd. response = {} # response from Hail Batch; dataframes = {; key: pd.DataFrame(data=values['data'], columns=values['columns']); for key, values in response.items(); }; ```. I tested this in on a dev deploy and it worked pretty well, but happy to add testing if you can direct me to a place to add it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14328:151,perform,performance,151,https://hail.is,https://github.com/hail-is/hail/pull/14328,1,['perform'],['performance']
Performance,"Hi!!!. - I was just delivered a wheel of Hail 0.2.128 loaded with this method from Chris, available at: gs://hail-30-day/hailctl/dataproc/cvittal-dev/0.2.128-2863e0eb192c/hail-0.2.128-py3-none-any.whl . Very helpful for removing those temp methods in the PR!. - it looks like the code will have a home inside of something that looks like the following: ; ```; qc = hl.struct(; **{; ann: hl.agg.filter(; expr,; vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; variant_ac=vmt.variant_ac,; variant_atypes=vmt.variant_atypes,; dp=vmt.DP,; ),; ); for ann, expr in argument_dictionary.items(); }; ); ```; inside of a hl.agg.filter call, and where argument_dictionary looks like {'vep_name':ht.vep.field , ""pass_all_vqc"": hl.len(vmt.filters) == 0} and so on. Just to give an idea what the usage is going to be.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1979150641:54,load,loaded,54,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1979150641,1,['load'],['loaded']
Performance,"Hi, ; I would like to use hail to perform clumping and threshold (C+T). After I search the Hail Docs, I just get how to perform pruning. ; In my opinion, I select the significant SNPs using `filter_rows` function, then select the independent SNPs using `ld_prune`. Is it correct? Thanks!. Best,; Sheng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11748:34,perform,perform,34,https://hail.is,https://github.com/hail-is/hail/issues/11748,2,['perform'],['perform']
Performance,"Hi, danking, the result is as follows:; ```; [root@tele-1 ~]# pyspark; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/15 08:58:31 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/15 08:58:31 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/15 08:58:31 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile(""/hail/test/BRCA1.raw_indel.vcf"").count(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 1008, in count; return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 999, in sum; return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 873, in fold; vals = self.mapPartitions(func).collect(); File ""/opt/Software/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367:466,load,load,466,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367,1,['load'],['load']
Performance,"Hi, not sure if this is the right avenue, but I'd also like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:702,load,load,702,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,1,['load'],['load']
Performance,"Hi,. Here are our latest updates to the johnc branch originally created by John Compitello. We have improved the overall annotation performance by increasing the default block_size to 500K. Please let us know if you have any questions. Best,; Shuli & the Nirvana team",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2300:132,perform,performance,132,https://hail.is,https://github.com/hail-is/hail/pull/2300,1,['perform'],['performance']
Performance,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/715:11,load,loading,11,https://hail.is,https://github.com/hail-is/hail/issues/715,1,['load'],['loading']
Performance,"High level take-aways:. - Hail docs now have syntax highlighting (we just needed to import pygments.css).; - Search works again.; - There are now only two root HTML templates: `site/templates/base.html` and; `web_common/web_common/templates/layout.html`. I cannot unify these further; because our services and our main websites actually differ significantly.; - The search/nav bar is now present in the HTML, no JS nonsense to; asynchronously load it into place after HTML rendering.; - Site now has a `make watch` which watches for changes and automatically; re-renders the HTML.; - Site now has a few make rules that facilitate experimenting with how the docs; are displayed within the context of the current development version of site's; CSS & HTML.; - XSLT is now only used by the C++ tests. Smaller things:. - Removed bootstrap dependencies. Did we ever actually use these?; - Removed ""clipboard.js"" dependency. Also not clear from where this came.; - Removed use of the `subtitle` tag, which isn't actually an HTML tag?. Future work:. - Simplify our CSS. It's not possible to logically reason about our CSS. And it; interacts in bad ways with the latent RTD themes. I want a unified Hail visual; theme.; - Clean up the search-related JavaScript in nav-bottom.html and; search.html. These both seem too complicated to just make search work. ---. The thrust of this PR is to restructure Hail's website and documentation to; entirely rely on Jinja2 templates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:443,load,load,443,https://hail.is,https://github.com/hail-is/hail/pull/9597,1,['load'],['load']
Performance,"Hm I only use neovim and I have a .cache dir, I wonder where it's from, maybe clangd or friends?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12414#issuecomment-1301375177:35,cache,cache,35,https://hail.is,https://github.com/hail-is/hail/pull/12414#issuecomment-1301375177,1,['cache'],['cache']
Performance,"Hmm, flake is failing on this; ```; + flake8 batch; batch/server/__init__.py:1:1: F401 '.server.serve' imported but unused; batch/server/__init__.py:1:1: F401 '.server.run_once' imported but unused; ```; But that's an incorrect bug. I wonder if the CI is loading the wrong python version. I'll check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5065#issuecomment-451266277:255,load,loading,255,https://hail.is,https://github.com/hail-is/hail/pull/5065#issuecomment-451266277,1,['load'],['loading']
Performance,"Hmm. this works: https://internal.hail.is/dking/site/vendors/vanta/viz.min.js, where is it not getting loaded correctly? I probably need another rule in dev deploy's site's nginx to fix the path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804:103,load,loaded,103,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804,1,['load'],['loaded']
Performance,"How many cores were you using?. Also, am I correct that this runs even when k > n, it's just not optimized for that case?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1973#issuecomment-320361030:97,optimiz,optimized,97,https://hail.is,https://github.com/hail-is/hail/pull/1973#issuecomment-320361030,1,['optimiz'],['optimized']
Performance,HttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.032ms self 0.028ms children 0.003ms %children 10.95%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.Eva,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:173820,Optimiz,Optimize,173820,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,HttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.050ms self 0.041ms children 0.010ms %children 19.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.Eva,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:30503,Optimiz,Optimize,30503,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,I actually quite like this with `tablify`. [Take a look at the differences before/after `tablify`](https://github.com/hail-is/hail/pull/2852/commits/31c48bfb8417279b28aa738dab6b56deb0c0b1e2). `tablify` obviously leads to heinous join graphs. We'll need to optimize these.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2852#issuecomment-362778686:256,optimiz,optimize,256,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-362778686,1,['optimiz'],['optimize']
Performance,"I added a `StreamLen` IR node. The justification for this were optimization issues in `Simplify` situations like. ```; case ArrayLen(ToArray(StreamMap(s, _, _))) => ArrayLen(ToArray(s)); ```. The above is not a valid optimization in all cases because if the elements of `s` are not realizable, the optimization will have created an invalid IR that will fail at emit time since elements of an array must be realizable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8783:63,optimiz,optimization,63,https://hail.is,https://github.com/hail-is/hail/pull/8783,3,['optimiz'],['optimization']
Performance,I added a discuss post for our users https://discuss.hail.is/t/i-get-a-negativearraysizeexception-when-loading-a-plink-file/899,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564#issuecomment-471018741:103,load,loading-a-plink-file,103,https://hail.is,https://github.com/hail-is/hail/issues/5564#issuecomment-471018741,1,['load'],['loading-a-plink-file']
Performance,"I added a new Grafana panel without alerts that hopefully will let us catch problems if jobs aren't getting scheduled in a timely manner. I think to have an alert, we'd want to measure what the average wait time of a job in the queue is which would require more infrastructure (keeping track of last state change). We can consider adding that now -- not sure how much work it would be.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12212:228,queue,queue,228,https://hail.is,https://github.com/hail-is/hail/pull/12212,1,['queue'],['queue']
Performance,I added an optimization so we prefer the default location if we haven't had many failures creating instances. This will resolve my concerns about unnecessarily charging users more with this change without giving them a way to prefer a cheaper region.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1170374342:11,optimiz,optimization,11,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1170374342,1,['optimiz'],['optimization']
Performance,I added the `forall` method and methods to look up individual elements without loading whole thing into memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9854#issuecomment-756355967:79,load,loading,79,https://hail.is,https://github.com/hail-is/hail/pull/9854#issuecomment-756355967,1,['load'],['loading']
Performance,I added the cleanup of old PR cached VEP images to the Archive Registry Cloud Run cleanup functions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1446721694:30,cache,cached,30,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1446721694,1,['cache'],['cached']
Performance,"I added this when I was hitting class too large exceptions in a previous PR, to see if it would help. I didn't end up using it there, but it still seems like a good optimization. @tpoterba I haven't thought about this very hard, would be interested in your thoughts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11004:165,optimiz,optimization,165,https://hail.is,https://github.com/hail-is/hail/pull/11004,1,['optimiz'],['optimization']
Performance,"I agree with this comment: https://stackoverflow.com/a/17329465/431282. > Lazy val is *not* free (or even cheap). Use it only if you absolutely need laziness for correctness, not for optimization. I think we should avoid lazy val. This was borne out when profiling `RegionValue` stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2316:183,optimiz,optimization,183,https://hail.is,https://github.com/hail-is/hail/pull/2316,1,['optimiz'],['optimization']
Performance,I also added a small optimization that doesn't copy the column annotations into the row unless we're aggregating. I did this for both `filter_rows` and `annotate_rows`. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3565:21,optimiz,optimization,21,https://hail.is,https://github.com/hail-is/hail/pull/3565,1,['optimiz'],['optimization']
Performance,"I also compared `variant_and_sample_qc_nested_with_filters_2` (33% worse on batch) between the two branches on my laptop, and could not detect a difference. I do think the make_ndarray range speedup is real -- there are a few benchmarks that indicate improvement that all are heavily dependent on the performance of the `StreamRange` implementation, which I think slightly improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10229#issuecomment-814325646:301,perform,performance,301,https://hail.is,https://github.com/hail-is/hail/pull/10229#issuecomment-814325646,1,['perform'],['performance']
Performance,"I also fixed a few buildImage's that had overly generous docker contexts, which, in my experience, really slows down image build due to loading the entire hail repository into the context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045:136,load,loading,136,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045,2,['load'],['loading']
Performance,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:393,cache,cache,393,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418,2,['cache'],['cache']
Performance,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1807:261,load,loadClass,261,https://hail.is,https://github.com/hail-is/hail/issues/1807,2,['load'],['loadClass']
Performance,"I am proposing a new iterator abstraction that I think should be preferred to Scala `Iterator` throughout most of the codebase, especially for iterators of region values. This is a low-level change, which could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3016:610,perform,performance,610,https://hail.is,https://github.com/hail-is/hail/pull/3016,1,['perform'],['performance']
Performance,"I am running AWS EMR 6.2.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.0.1; * Scala 2.12.10. On that cluster, I am building HAIL version 0.2.74-0c3a74d12093 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```. Compilation succeeds. I am able to load Hail into my Zeppelin notebook pyspark kernel; I am able to load and describe hail table stored in AWS S3. ```; %pyspark; # Import and launch Hail; import hail as hl; hl.init(sc); # Load files; ht_s = hl.read_table('s3://npm-hail/SG10K_Health_r5.3.1/n9770/SG10K_Health_r5.3.1.n9770.samples.ht'); # Desctibe; ht_s.describe(); ```. I am not able to show data. ```; %pyspark; ht_s.show(); ```. the `show()` call run infinitely into zeppelin and SparkUI note a started job, but no task launched. FYI hail version 0.2.60 on same EMR stack works without issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10832:341,load,load,341,https://hail.is,https://github.com/hail-is/hail/issues/10832,3,"['Load', 'load']","['Load', 'load']"
Performance,"I am using Hail 0.2.54. However, I also tested with the latest build.gradle file. I run the following make install command:; `make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.2`. However, I got this error message which did not appear before. ` > Could not resolve org.scalanlp:breeze-natives_2.11:+.; Required by:; project :; > Failed to list versions for org.scalanlp:breeze-natives_2.11.; > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error; * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.; * Get more help at https://help.gradle.org. BUILD FAILED in 29s; make: *** [build/libs/hail-all-spark.jar] Error 1`. It seems that is caused by https://repo.hortonworks.com/content/repositories/releases/ server is done.; I am wondering whether there is any maven substitute can be used temporarily to compile hail.jar?. Thanks in advance for your help.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9419:430,load,load,430,https://hail.is,https://github.com/hail-is/hail/issues/9419,1,['load'],['load']
Performance,"I assessed three query options, but keep in mind that the database is currently at nearly 100% CPU so these timings are high variance. I'm honestly pretty surprised that the first one isn't the fastest. It seems to me like the obvious SQL query and yet it does not perform as well as those using lateral joins. Option 2 and Option 3 both use lateral joins. Option 2 uses a LEFT JOIN followed by a `WHERE ... IS NOT NULL` which is definitionally an `INNER JOIN`. For that reason, I also explored Option 3, directly using `INNER JOIN`, which appears to be very slightly faster and lower variance. | Option | Timing (mean + stddev, in seconds) |; |--------|------------------------------------|; | 1 | 9.92 +- 2.02 |; | 2 | 5.35 +- 1.87 |; | 3 | 5.11 +- 1.15 |. ## Option 1; ```; SELECT DISTINCT; group_resources.batch_id,; group_resources.update_id,; group_resources.job_group_id; FROM job_group_inst_coll_cancellable_resources AS group_resources; INNER JOIN job_group_self_and_ancestors AS descendant; ON descendant.batch_id = group_resources.batch_id; AND descendant.job_group_id = group_resources.job_group_id; INNER JOIN job_groups_cancelled AS cancelled; ON descendant.batch_id = cancelled.id; AND descendant.ancestor_id = cancelled.job_group_id; ORDER BY group_resources.batch_id desc, group_resources.update_id desc, group_resources.job_group_id desc; LIMIT 1000;; ```; ```; 1000 rows in set (11.81 sec); 1000 rows in set (8.91 sec); 1000 rows in set (11.86 sec); 1000 rows in set (7.08 sec); ```; ## Option 2; ```; SELECT DISTINCT; group_resources.batch_id,; group_resources.update_id,; group_resources.job_group_id; FROM job_group_inst_coll_cancellable_resources AS group_resources; LEFT JOIN LATERAL (; SELECT; 1 AS cancelled; FROM job_group_self_and_ancestors AS descendant; INNER JOIN job_groups_cancelled AS cancelled; ON descendant.batch_id = cancelled.id; AND descendant.ancestor_id = cancelled.job_group_id; WHERE descendant.batch_id = group_resources.batch_id; AND descendant.job_group_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14366:265,perform,perform,265,https://hail.is,https://github.com/hail-is/hail/pull/14366,1,['perform'],['perform']
Performance,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2440:308,Load,LoadMatrix,308,https://hail.is,https://github.com/hail-is/hail/pull/2440,2,['Load'],['LoadMatrix']
Performance,"I believe [14366](https://github.com/hail-is/hail/pull/14366) played a part in motivating the lateral joins. For that query, the execution pans are identical. Here, I'm seeing better performance with the more natural query. I didn't go further for compatibility with user-defined filter operations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14629#issuecomment-2251282251:183,perform,performance,183,https://hail.is,https://github.com/hail-is/hail/pull/14629#issuecomment-2251282251,1,['perform'],['performance']
Performance,"I believe that this patch and the original version both prevent this. They would both lock the parent directory of `path` and thus would prevent concurrent copying. Also, it appears that as long as we don't use the `-d` option, and the filenames are unique, `gsutil rsync` kinda already does what we want. I also feel like the orignal approach was way too aggressive, it seems like it was serializing _all_ locked filesystem operations in the entire filesystem subtree since it would wait for the lock on every parent other than `/` of the requested file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366:145,concurren,concurrent,145,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366,1,['concurren'],['concurrent']
Performance,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564:340,Load,LoadPlink,340,https://hail.is,https://github.com/hail-is/hail/issues/5564,3,"['Load', 'load']","['LoadPlink', 'load']"
Performance,I can break this up further if you want. Big changes:. - change batch.py to support multi-line commands (use `{\n...\n}`); - change batch.py and job.py to support per-job environment variables (and add tests to test_batch.py); - add `partition` to hail top mirroring the implementation in Scala; - implement BatchPoolExecutor which attempts to faithfully implement the interface of concurrent.futures.Executor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9035:382,concurren,concurrent,382,https://hail.is,https://github.com/hail-is/hail/pull/9035,1,['concurren'],['concurrent']
Performance,"I changed that the `_update_token` is no longer cached. I thought it was the source of a bug, but it was something else that was an issue. However, I felt it was confusing and I didn't see what value it provided as we have retries on all of our client operations that would need the token and the token is not used in future operations to submit the update.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255509466:48,cache,cached,48,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255509466,1,['cache'],['cached']
Performance,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962:370,perform,performing,370,https://hail.is,https://github.com/hail-is/hail/pull/7962,3,"['cache', 'perform']","['cache', 'caches', 'performing']"
Performance,I changed this from 2=>1 in April of last year unintentionally while debugging; (it's easy to get interleaved prints/logs with 2 concurrent worker threads). https://github.com/hail-is/hail/pull/8535/files#diff-bf51d09b286fddaa730b426824ccb12dac8b9032e0c88bde81882f3cb1423df8R14,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10710:129,concurren,concurrent,129,https://hail.is,https://github.com/hail-is/hail/pull/10710,1,['concurren'],['concurrent']
Performance,I cleaned some things up in this PR. I think the major performance issue left with `list_jobs` and `list_batches` will be solved with an attributes table.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5934#issuecomment-486414885:55,perform,performance,55,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486414885,1,['perform'],['performance']
Performance,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3394:495,Load,LoadVCF,495,https://hail.is,https://github.com/hail-is/hail/pull/3394,1,['Load'],['LoadVCF']
Performance,"I cover the two primary methods: PC-Relate and KING. # PC-Relate. Suppose:. 1. We have $X_1$, an $M$ -by- $N$ matrix of genotypes, with $M$ variants and $N$ samples. Suppose we have a new dataset $X_2$ which strictly adds $l$ new variants and $k$ new samples to $X_1$. 2. We have a truncated-SVD of $U S V^T = X_1$ [1]. 3. We believe the ancestry space represented by the truncated-SVD still accurately represents the ancestry space of $X_2$. 4. We have already calculated the PC-Relate kinship matrix $\phi_1$ of $X_1$. We would like to calculate $\phi_2$ the kinship matrix of $X_2$ while only performing $O(k^2 M + kNM)$ work. ---. [1] PC-Relate, as presented in Conomos, et al., uses the PC scores and linear regression to define the ancestry space as follows.; 1. Calculate the first $k$ (different from $k$ above) PC scores of $X$ (which are defined in terms of the $k$-truncated SVD: $S V^T$).; 2. For each variant $s$, find the best linear fit using ordinary least squares for the equation $x_s = \alpha_s + \beta_s S V^T$. $\alpha$ is a scalar intercept term. $\beta_s$ is a $N$-vector. $x_s$ is the vector of genotypes for variant $s$ (the $s$-th row of $X$); 3. Defined the individual specific allele frequency for sample $i$ at variant $s$: $\mu_{is} = \widehat{\alpha_s} + \widehat{\beta_s} S V^T$. At one point, Patrick noticed that this rigamarole is unnecessary. The $k$-truncated SVD is the best rank $k$ approximation of $X$. I think our conclusion was that defining $\mu$ in terms of the $k$-truncated SVD is equivalent: $\mu = U S V^T$. # KING. Suppose again (1) and that we already have the $\phi_1$ KING's kinship estimator on $X_1$. We would like to calculate $\phi_2$, the KING kinship estimator matrix of $X_2$ while only performing $O(k^2 M + kNM)$ work. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13808:596,perform,performing,596,https://hail.is,https://github.com/hail-is/hail/issues/13808,2,['perform'],['performing']
Performance,"I created a separate PR to prove to ourselves that it actually triggers the behavior: https://github.com/hail-is/hail/pull/13400. I'm skeptical this will trigger the behavior. `submit` will wait for the bunch to be durably added, right? In that case, there's a *happens before* relationship between adding the first bunch and adding the second bunch. I think we need 10s of bunches to add in parallel so that the db is experiencing enough load that at least one bunch has reserved its job indices but its jobs have not been added while at the same time *at least two jobs* bunch is getting scheduled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399#issuecomment-1671784481:439,load,load,439,https://hail.is,https://github.com/hail-is/hail/pull/13399#issuecomment-1671784481,1,['load'],['load']
Performance,"I detect no performance difference on blanczos running the benchmark; locally. This pattern appears a lot in the NDArrayEmitter, though,; so we should fix it everywhere and see what happens!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9708:12,perform,performance,12,https://hail.is,https://github.com/hail-is/hail/pull/9708,1,['perform'],['performance']
Performance,"I dev deployed all *_image steps on a single worker running `main` and saw many fail with corrupted filesystems. I imagine this is because multiple jobs were extracting the same filesystem into the same place. The previous change to using a r/w lock for pulling and deleting images is correct, but we must lock on the image id when *extracting* the actual filesystem. With this change everything passed in my dev. The `BATCH_WORKER_IMAGE_ID` fix from before didn't actually work because of not properly escaping the `{` in the f-string. I also moved the `docker rmi` step to be first in the image cleanup process because I imagine if docker refuses to remove an image we shouldn't remove it from our own cache either.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10675:704,cache,cache,704,https://hail.is,https://github.com/hail-is/hail/pull/10675,1,['cache'],['cache']
Performance,"I did a little more digging into this today. I tried printing the size of the largest method in a class before splitting, before emitting bytecode. In the failing `test_can_process_wide_tables` test, the comparison of before/after this pr was consistent across all table widths in the test: some methods stay the same size, some increase by 7.4%. Only the largest table width triggers a ClassTooLarge exception. It's looking like this just created a small constant increase in code size, which we could make up with optimizations like constant folding, or longer term just fix the root problem by splitting up these giant classes. @tpoterba What are your thoughts on just disabling the largest wide-table test, vs spending more time understanding exactly how this is generating larger classes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10905#issuecomment-937106437:516,optimiz,optimizations,516,https://hail.is,https://github.com/hail-is/hail/pull/10905#issuecomment-937106437,1,['optimiz'],['optimizations']
Performance,"I didn't bust js yet because there are some external libraries (MathJax) that cary their own version strings and I don't want to break them. In testing seems safe, although I admit the regex isn't the most specific. In the worst case I believe we would append an unnecessary version string, which shouldn't break anything (just will cache the browser to reload the css instead of using cache)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6997:333,cache,cache,333,https://hail.is,https://github.com/hail-is/hail/pull/6997,2,['cache'],['cache']
Performance,"I didn't mean disable all optimization, I meant optimizations that specifically transform `TableCount` (e.g. the TableCount rewrite rules, the pruner can't prune the input to TableCount, etc.). As you note, it will also have to modify the implementation to run the RDD. Except that regression returns a table but force count returns a number. We could have set of 6 opaque operations: {Table, MatrixTable} => {Table, MatrixTable, Value} (skipping ones that don't actually appear).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387:26,optimiz,optimization,26,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"I do not know why. ```; # k logs -l app=batch | head; INFO	| 2018-10-26 17:04:45,840 	| server.py 	| <module>:44 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-10-26 17:04:45,844 	| server.py 	| <module>:53 | instance_id = 168f090933ba4db4ac6ba8d0add8460d; INFO	| 2018-10-26 17:04:45,849 	| server.py 	| run_forever:416 | run_forever: run target kube_event_loop; INFO	| 2018-10-26 17:04:45,850 	| server.py 	| run_forever:416 | run_forever: run target polling_event_loop; INFO	| 2018-10-26 17:04:45,850 	| server.py 	| run_forever:416 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; # k logs -l app=hail-ci | head; INFO	| 2018-10-26 16:47:18,826 	| environment.py 	| <module>:51 | BATCH_SERVER_URL http://batch.default; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:52 | SELF_HOSTNAME http://hail-ci; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:53 | REFRESH_INTERVAL_IN_SECONDS 60; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:54 | WATCHED_TARGETS [('hail-is/hail:master', True), ('hail-is/hail:0.1', True), ('hail-is/hail:bgen-changes', False), ('Nealelab/cloudtools:master', True)]; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:55 | INSTANCE_ID = ef1bb52a88dd49fb893869bf49063980; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:56 | CONTEXT = hail-ci-0-1; * Serving Flask app ""ci"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; ```. This obviously causes issues because CI is still waiting for batch jobs to finish.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4653:613,load,loading,613,https://hail.is,https://github.com/hail-is/hail/issues/4653,2,['load'],['loading']
Performance,"I do not. It's sort of hard to say without knowing how much memory the user is willing to devote to the cache, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3095#issuecomment-371992384:104,cache,cache,104,https://hail.is,https://github.com/hail-is/hail/pull/3095#issuecomment-371992384,1,['cache'],['cache']
Performance,"I don't quite understand this. To take advantage of this, it seems like we need also parallelize the tests. Where's the test parallelism set? Can we get a performance comparison, e.g. this PR test vs master?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7022#issuecomment-529491736:155,perform,performance,155,https://hail.is,https://github.com/hail-is/hail/pull/7022#issuecomment-529491736,1,['perform'],['performance']
Performance,I don't think it is used anymore. Builds are failing because it is returning 500. > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9418:94,load,load,94,https://hail.is,https://github.com/hail-is/hail/pull/9418,1,['load'],['load']
Performance,"I don't think this is the right solution. We should fix the control flow instead -- a ~40% performance degradation is pretty horrific, even as a stop-gap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9298#issuecomment-675614222:91,perform,performance,91,https://hail.is,https://github.com/hail-is/hail/pull/9298#issuecomment-675614222,1,['perform'],['performance']
Performance,"I ended up restructuring the summarizing to make the formatting easier. I also ended up putting the summary stuff on the expression; the vague goal is to allow a call to `table.summarize()` to cache all the summaries of the (nested) row fields, so that subsequent calls to e.g. `table.locus.summarize()` should be free.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7280:193,cache,cache,193,https://hail.is,https://github.com/hail-is/hail/pull/7280,1,['cache'],['cache']
Performance,I expect that supporting missingness on block matrix will make it extremely hard to have good performance and compatibility with other numerical libraries like numpy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6097#issuecomment-492704713:94,perform,performance,94,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492704713,1,['perform'],['performance']
Performance,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:454,cache,cache,454,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333,1,['cache'],['cache']
Performance,"I figured it out, `FS` is already searialazable, so we can just serialize the data into the class at compile time and deserialize it at load time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9044#issuecomment-653121962:136,load,load,136,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-653121962,1,['load'],['load']
Performance,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772:164,load,load,164,https://hail.is,https://github.com/hail-is/hail/pull/10772,1,['load'],['load']
Performance,"I find it compelling that this fixed the downloads. But I'd also like to understand why this changed worked. I will approve it to see if it unblocks Lindo while we keep discussing. > We needed to await cancelled tasks to handle the error that was raised inside the task. Right. We want to cancel the task and wait for it to finish, but we don't want any exceptions to be raised out. Your code appears to do that, but so does the previous code. Nothing in the documentation for `asyncio.wait` indicates it will raise exceptions: https://docs.python.org/3/library/asyncio-task.html#asyncio.wait. I also tested a short example:. ```; import asyncio; import sys. async def foo():; try:; print('A'); await asyncio.sleep(5); print('B'); return 5; finally:; print(sys.exc_info()). async def async_main():; print('creating task...'); t = asyncio.ensure_future(foo()); # wait for foo to sleep; await asyncio.sleep(1). # cancel foo in sleep; print('cancelling task...'); t.cancel(). print('waiting for task...'); await asyncio.wait([t]). print('done.'). asyncio.run(async_main()); ```. which prints:. ```; $ python3 foo.py; creating task...; A; cancelling task...; waiting for task...; (<class 'concurrent.futures._base.CancelledError'>, CancelledError(), <traceback object at 0x7f8cdef65e10>); done.; ```. The task is cancelled, and CancelledError is raised, but not propagated out. > 75% of his jobs would fail with this error. I'm actually confused where the cancellation error is coming from in the first place. If the code you're changing is the issue (and I think it is, too) then we only cancel if some other exception was raised, either by a task or in `__aexit__`. What's that exception? Can we print it out (enable more logging) in your test setup?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534#issuecomment-853115655:1185,concurren,concurrent,1185,https://hail.is,https://github.com/hail-is/hail/pull/10534#issuecomment-853115655,1,['concurren'],['concurrent']
Performance,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:673,load,loads,673,https://hail.is,https://github.com/hail-is/hail/issues/8029,1,['load'],['loads']
Performance,"I had a messy rebase. I ran optimize imports to clean up the import mess. It made a lot of changes to the imports, my apologies. It shouldn't affect the correctness in anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8362#issuecomment-616599742:28,optimiz,optimize,28,https://hail.is,https://github.com/hail-is/hail/pull/8362#issuecomment-616599742,1,['optimiz'],['optimize']
Performance,"I had to recreate from #7593 because I force-pushed after are rebase. cc: @cseed . Unfortunately, you're the only one around to review John. There's so many issues with this at current, but I think it would be better to get something in so I can actually start making forward progress towards something better. At a very practical level, I want various docker images to be cached instead of constantly rebuilding them as I try to develop this further.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7760:373,cache,cached,373,https://hail.is,https://github.com/hail-is/hail/pull/7760,1,['cache'],['cached']
Performance,"I have a branch where I've upgraded the dependency to libsimdpp-2.1 and resolved issues around depreciation warnings, this does solve the issue. We would need to discuss if we want to upgrade the dependency, and benchmark against the new version to see if it causes any performance regression. Branch is [here](https://github.com/chrisvittal/hail/tree/libsimdpp-2.1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780:270,perform,performance,270,https://hail.is,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780,1,['perform'],['performance']
Performance,"I have a implemented a highly concurrent Python asyncio filesystem that supports GCS and the local file system (and soon S3). It is my intention to ultimately replace the hadoop_* functions with this. The new thing feels pretty fast: copy benchmarks 2-5x faster than gsutil for example, esp. when working with lots of files. Some remarks:; - It is designed to do the minimal number of system calls/API calls per operation so there is serial loops like this anywhere in the code.; - Our short term goal is to use this for the input/output steps in Batch.; - It doesn't support Hadoop (and I'm not super exciting about maintaining that).; - Some things will be much faster because no round trip the JVM. ; - The interface is fully async, so we'll need to build some wrappers if you want a synchronous interface. The async interface will get you concurrency within operations (copy, rmtree), the sync interface only gets you currency within operations.; - The list files operation doesn't support globbing yet.; - There are no docs yet.; - Compared to Hadoop/POSIX, the interface is slightly lower level but it was designed to map well onto the filesystems we want to support. There is no `stat`, for example, but is statfile (which requires the input to be a file) and listfiles (which requires it to be a directory), although we could build that.; - I'd say the code is beta and not quite completely solid but getting close. Here is the AsyncFS interface: https://github.com/hail-is/hail/blob/main/hail/python/hailtop/aiotools/fs.py#L70. Here is an example creating a router filesystem that supports GCS and the local file system: https://github.com/hail-is/hail/blob/main/hail/python/test/hailtop/test_aiogoogle.py#L17. I'd be happy to chat more about what would make this attractive for you guys to switch to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10043#issuecomment-778364838:30,concurren,concurrent,30,https://hail.is,https://github.com/hail-is/hail/pull/10043#issuecomment-778364838,2,['concurren'],"['concurrency', 'concurrent']"
Performance,"I haven't checked for performance regressions. Tim, do you have a standard way of doing this? Given that you're on vacation tomorrow, @danking, could you help me understand your performance testing procedures?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-453376506:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453376506,2,['perform'],['performance']
Performance,"I haven't looked at this yet, I have a new use case in seqr for the table code. I need to be able to load data as `RDD[Annotation]` without pulling out a sample/variant key (or pull out a totally custom key). Can I do that easily with the new interface?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232252890:101,load,load,101,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232252890,1,['load'],['load']
Performance,I imagine a mismatch in number of partitions still causes a performance issue?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953#issuecomment-406020938:60,perform,performance,60,https://hail.is,https://github.com/hail-is/hail/issues/3953#issuecomment-406020938,1,['perform'],['performance']
Performance,"I introduced a performance bug when I made the BgenRecord eagerly decode the genotypes. The genotypes are not needed for fast keys. This change throws out Hadoop in favor of a Spark-based approach. I also did an instrumented profiling of `hl.import_bgen(...)._force_count_rows()` and found that almost all our time is spent in region value builder state manipulation. I included a small fix that makes `ArrayStack` actually use field references instead of method calls (this shaved about 1/10 off of force count rows time) (cc: @cseed). The real solution is to use a staged region value builder. I will follow this PR with a SRVB pull request. Finally, I will also hook it up to `MatrixRead` so that the dead fields pruner can prune bgen fields too (cc: @tpoterba)!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3893:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/3893,1,['perform'],['performance']
Performance,I keep losing track of this because it's not on my CI queue. I've cloned the branch and PR'ed under my user so it shows up in my queue: https://github.com/hail-is/hail/pull/13586,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12046#issuecomment-1710251980:54,queue,queue,54,https://hail.is,https://github.com/hail-is/hail/pull/12046#issuecomment-1710251980,2,['queue'],['queue']
Performance,"I left the changes to Query and Batch in separate commits for ease of review. I put these in the same PR because we don't really have standalone testing for JVM Jobs outside of Query-on-Batch so the FASTA use-case serves as a test here that cloudfuse is working properly for JVM Jobs. Would be great if Jackie you could review the batch commit and Tim could review the query commit. ## Hail Query; - Added support for the `FROM_FASTA_FILE` rpc and the service backend now passes sequence file information from RGs in every rpc; - Refactored the liftover handling in service_backend to not redundantly store liftover maps and just take them from the ReferenceGenome objects like I did for sequence files. This means that add/remove liftover/sequence functions on the Backend are just intended to sync up the backend with python, which is a no-op for the service backend.; - Don't localize the index file on fromFASTAFile/addSequence before creating the index object. `FastaSequenceIndex` just loads the whole file on construction so might as well stream it in from whatever storage it's in.; - FASTA caching is left alone because those files will be mounted and unmounted from the jvm container over the life of the job. JVM doesn't have to worry about disk usage because that's handled by Batch XFS quotas, so long as the service backend requests enough storage to fit the FASTA file. Batch will make sure that a given bucket (and therefore a given FASTA file) is mounted once per-user on a batch worker. ## Hail Batch; - Added support for read-only cloudfuse mounts for JVM jobs; - These mounts are shared between jobs on the same machine from the same user; - I did not change DockerJobs, but they could be very easily adapted to use this new mount-sharing code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:992,load,loads,992,https://hail.is,https://github.com/hail-is/hail/pull/12736,1,['load'],['loads']
Performance,"I listed everything I'm aware of, though it's possible that some internal changes may have made performance impacts I don't know about.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8292:96,perform,performance,96,https://hail.is,https://github.com/hail-is/hail/pull/8292,1,['perform'],['performance']
Performance,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:2,load,loaded,2,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,2,['load'],"['loaded', 'loading']"
Performance,I looked at the Spark worker logs by setting up the SSH tunnel (according to laurent's cloud post) and going to the Executor tab of the spark history server page. It seems we're running into this issue: https://issues.apache.org/jira/browse/SPARK-16845. This is definitely because Andrea's VDS has lots of annotations. Here's a quote from that Spark issue:. > I've been struggling to duplicate this and finally came up with a strategy that duplicates it in a spark-shell. It's a combination of a wide dataset with nested (array) structures and performing a union that seem to trigger it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267428726:544,perform,performing,544,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267428726,1,['perform'],['performing']
Performance,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:145,load,load,145,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,4,['load'],['load']
Performance,"I looked closer at the logic in the old combiner and realized it was permitting things within 1 window-size of the window to be binned. I wasn't doing this at all, so implemented it. This has nice performance properties, but uses more memory than the user requests, so I'm using just a 25% ""grace window"" plus the buffer to have both good performance and low memory usage. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 39.5% 57.617 22.773; table_aggregate_downsample_dense 26.6% 127.079 33.843; ----------------------; Geometric mean: 32.4%; Simple mean: 33.1%; Median: 33.1%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7197#issuecomment-538549413:197,perform,performance,197,https://hail.is,https://github.com/hail-is/hail/pull/7197#issuecomment-538549413,2,['perform'],['performance']
Performance,I made an issue for loading the bytes less often. https://github.com/hail-is/hail/issues/13811,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792#issuecomment-1761891070:20,load,loading,20,https://hail.is,https://github.com/hail-is/hail/issues/13792#issuecomment-1761891070,1,['load'],['loading']
Performance,"I made three more improvements last week (and I don't plan to do anything else now):. - [run hl.sort once](https://github.com/hail-is/hail/pull/5078/commits/57ed10c8d0145f2751dbbbccdfb93f9e983f674b), I realized that if you create an expression that calls `hl.sort(..)` and use it in many places, the sort gets in-lined and no optimizer pass lifts it to a global. I manually lifted it to globals. - [remove an unnecessary per-entry rename](https://github.com/hail-is/hail/pull/5078/commits/9e31a97a8deb0e3b0886f6b20d0d953b9e1b167d). - [remove an unnecessary allocation of a row struct](https://github.com/hail-is/hail/pull/5078/commits/61fd47bf1ee9fc09b9076da3e5acf87c5fc21fd5)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5078#issuecomment-456591269:326,optimiz,optimizer,326,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-456591269,1,['optimiz'],['optimizer']
Performance,I might still have a race condition when cleaning up the container despite using a lock. I'll see if I can reason about it tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11397#issuecomment-1067386677:21,race condition,race condition,21,https://hail.is,https://github.com/hail-is/hail/pull/11397#issuecomment-1067386677,1,['race condition'],['race condition']
Performance,"I mostly want this for debugging lowering on `LocalBackend`, but I added a SparkBackend implementation as well (goes through breeze and java literals, not going to be super performant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10398:173,perform,performant,173,https://hail.is,https://github.com/hail-is/hail/pull/10398,1,['perform'],['performant']
Performance,"I need this functionality for compacting the billing tables. I'm not sure how we get this privilege to `batch`, `batch-admin` etc., but this should be fine for me to at least measure any performance gains.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13165:187,perform,performance,187,https://hail.is,https://github.com/hail-is/hail/pull/13165,1,['perform'],['performance']
Performance,I need to be able to load tokens for multiple users for #9553 and would like to pipe it through the auth utilities in a reasonable way so I can use them. Let me know if this looks ok or if there's a better way to do it?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9585:21,load,load,21,https://hail.is,https://github.com/hail-is/hail/pull/9585,1,['load'],['load']
Performance,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7523:306,perform,performance,306,https://hail.is,https://github.com/hail-is/hail/pull/7523,14,"['load', 'perform', 'throughput']","['load', 'performance', 'throughput']"
Performance,"I picked the name since Cronus is the father of Zeus. Perhaps Saturn is more appropriate. Open to suggestions here. The UX flow:. 1. User loads up `https://hail.is/cronus` and sees a form with a button.; 2. Pressing the button starts a pod running Jupyter for the user that no one else has access to; 3. refreshing the page or going to `https://hail.is/cronus` again redirects to the Jupiter instance; 4. to get a fresh Jupyter instance, the user can clear their cookies. The components:. - a flask app (`cronus/cronus.py`) which launches pods and handles authentication (via cookies); - an nginx reverse proxy which uses `auth_request` to check the permissions with the flask app; - a pod running `Jupyter notebook` with hail `pip` installed. TODO:. - [x] add make targets to generate the `cronus-job` image (the jupyter notebook image); - [ ] maybe simplify the directives used in nginx? I kept throwing shit at it until it worked; - [ ] figure out how to teach flask url_for to use a root other than `/`. I don't know what HTTP proxy headers to set to inform it that it lives at a subdirectory of `hail.is`; - [ ] get rid of the button? creating a new pod needs to be a `POST` so that the web browser doesn't access twice or eagerly access it, etc. maybe I can use javascript on the root page to make the post request and redirect the page.; - [ ] testing? I could add some basic things, but the most time consuming and annoying thing was getting the reverse proxy settings right and testing that requires an nginx instance. @cseed I randomly assigned, should I be picking from you and Tim? What's the plan for review on these new things?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576:138,load,loads,138,https://hail.is,https://github.com/hail-is/hail/pull/4576,1,['load'],['loads']
Performance,"I plan to move this to `hl.dnd.DNDArray`. I had to make a couple changes to RVD and Table to make this work. They all; revolve around convincing Hail not to elide *critical* `key_by`s. The critical insight is that 1:1 partitioners (partitioners where each range; bound interval contains exactly one key) are special: permuting their keys is; free. I can take advantage of this by combining two changes:; 1. `RVD.enforceKey` is aware of these partitioners and avoids scans in that case; 2. Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:500,optimiz,optimizer,500,https://hail.is,https://github.com/hail-is/hail/pull/8864,2,['optimiz'],['optimizer']
Performance,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8229:650,load,load,650,https://hail.is,https://github.com/hail-is/hail/pull/8229,2,['load'],['load']
Performance,"I pulled out a flag to cache in java, but accidentally got rid of the thing it was actually doing. This should be fixed now; with a smaller test mt I'm seeing the number of allocated regions be consistent between combOps:. ```; ...; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6824:23,cache,cache,23,https://hail.is,https://github.com/hail-is/hail/pull/6824,1,['cache'],['cache']
Performance,"I pushed some addition changes: push requestedType into TableRead, expose (private) in Python for performance testing. On a chunk of gnomAD sites file, read count went from 19s (all fields) to 12s (keys + 1 int field). The Python changes should get removed once prune dead fields goes in. MatrixRead will require a bit more work (with the recent unification of matrix read/import IR nodes).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3667#issuecomment-392303741:98,perform,performance,98,https://hail.is,https://github.com/hail-is/hail/pull/3667#issuecomment-392303741,1,['perform'],['performance']
Performance,"I realize this looks like a lot of code changes, but it's mostly copying and pasting two SQL procedures and changing one line in each. This adds 4 bits of metadata to requests that then can be queried as extra metadata:; - batch_id; - job_id; - batch_operation; - job_queue_time. Should be self-explanatory except job_queue time is the time in which the job is first set to ready to when it was scheduled on the worker (exact moment is when the job config is made to send to the worker). Example logging query. Note that the search on ""batch_id"" is not optimized so you definitely want to add some kind of time limit that's short on the window to search. I can add my Python script that scrapes these logs and makes a Plotly figure in a separate PR once this goes in. ```; (; resource.labels.container_name=""batch""; resource.labels.namespace_name=""{namespace}""; ) OR (; resource.labels.container_name=""batch-driver""; resource.labels.namespace_name=""{namespace}""; ) OR (; resource.type=""gce_instance""; logName:""worker.log""; labels.""compute.googleapis.com/resource_name"":""{namespace}""; ); jsonPayload.batch_id=""{batch_id}""; timestamp >= ""{start_timestamp}"" {end_timestamp}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13219:553,optimiz,optimized,553,https://hail.is,https://github.com/hail-is/hail/pull/13219,1,['optimiz'],['optimized']
Performance,"I rebuilt the disk image from scratch and it's 2.1 GB. I'm not sure we can do much better than that if we cache the batch2 image on there. However, I found if we don't do that, it adds up to an extra minute.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7216#issuecomment-539671078:106,cache,cache,106,https://hail.is,https://github.com/hail-is/hail/pull/7216#issuecomment-539671078,1,['cache'],['cache']
Performance,"I removed jinja and restored the build command. I can't see your last comment on docker/Makefile, GitHub won't load it for me, can you repost it here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9593#issuecomment-713905582:111,load,load,111,https://hail.is,https://github.com/hail-is/hail/pull/9593#issuecomment-713905582,1,['load'],['load']
Performance,"I removed the assertions about the chain of IRState properties. We could take the parameters in OptimizePass, but that seems a bit hacky/unnecessary. The IRStates are checked at runtime, so I'm confident we'll still be able to debug issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9030#issuecomment-651918109:96,Optimiz,OptimizePass,96,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651918109,1,['Optimiz'],['OptimizePass']
Performance,"I renamed the array impls Array{Load, Addr}Impl.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4882#issuecomment-444322864:32,Load,Load,32,https://hail.is,https://github.com/hail-is/hail/pull/4882#issuecomment-444322864,1,['Load'],['Load']
Performance,"I see, here's the bit that does suggest the prefix:. > Use a naming convention that distributes load evenly across key ranges; > Auto-scaling of an index range can be slowed when using sequential names, such as object keys based on a sequence of numbers or timestamp. This occurs because requests are constantly shifting to a new index range, making redistributing the load harder and less effective. > In order to maintain a high request rate, avoid using sequential names. Using completely random object names gives you the best load distribution. If you want to use sequential numbers or timestamps as part of your object names, introduce randomness to the object names by adding a hash value before the sequence number or timestamp.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836#issuecomment-914242680:96,load,load,96,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914242680,3,['load'],['load']
Performance,"I see, it wasn't doing redundant work, just generating redundant IR by regenerating the IR to load covariates per set of phenotypes. This would only affect chained linear regression, since that's the only time there's more than one `one_y_field_name_set in y_field_names`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871:94,load,load,94,https://hail.is,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871,1,['load'],['load']
Performance,"I set the number of records pulled to 1000. I'm not sure if it makes a huge difference, but I decided it's better to favor any possible speed improvement rather than optimizing for an adversial user right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1277558454:166,optimiz,optimizing,166,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1277558454,1,['optimiz'],['optimizing']
Performance,"I staged `import_matrix_table` and achieved substantial performance improvements. A few changes were necessary:; - `FunctionBuilder` now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists. The main change is in `ImportMatrix.scala` which is both staged and based on scanning the string rather than using `String.split`. The approach is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Asi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:56,perform,performance,56,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,I talked to @danking about this earlier and he thinks this should be a separate change and the `IF` check for n_jobs = n_completed isn't buying us anything in preventing deadlocks or performance improvements. He's going to look over #11352 to make sure there's nothing we're missing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701807283:183,perform,performance,183,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701807283,1,['perform'],['performance']
Performance,I talked to Cotton about it and he said not to. But it's not clear how much of a difference that makes yet anyway. I think this version is pretty good and an improvement. Plus it'll add a benchmark which we can work on optimizing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849:219,optimiz,optimizing,219,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849,2,['optimiz'],['optimizing']
Performance,"I temporarily added this test to `OrderedRDD.coerce`:. ```; fastKeys match {; case Some(fastKeys) =>; assert(fastKeys.partitions.length == rdd.partitions.length). val A = fastKeys.mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); val B = rdd.map(_._1); .mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); assert(A == B); case None =>; }; ```. It is too expensive to run all the time. It also fails for `LoadVCF`, which doesn't filter the symbolic variants in `justVariants`. This can be fixed by filtering once beforehand.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248632396:472,Load,LoadVCF,472,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248632396,1,['Load'],['LoadVCF']
Performance,I think I understand the problem and I think there is a latent concurrency bug in the handling of randomness. I need to dig in a little more.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426#issuecomment-467141785:63,concurren,concurrency,63,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467141785,1,['concurren'],['concurrency']
Performance,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488:602,perform,performance,602,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488,2,['perform'],['performance']
Performance,"I think Tim is right about the cause. And I prefer to keep no default entry so that users aren’t surprised that they’ve lost information from the bgen. And the docs already say “For best performance, include precisely those fields required for your analysis.“",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3612#issuecomment-389885493:187,perform,performance,187,https://hail.is,https://github.com/hail-is/hail/issues/3612#issuecomment-389885493,1,['perform'],['performance']
Performance,"I think Tim's suggestion and Cotton's #1 are the same, basically? Stash the (possibly) uncompressed bytes in `data` and then decompress only in `getValue` if necessary. This gets us back to previous performance, but we still pay to copy the data even if we never read it. If this is impacting people, we should do that because it seems low-risk and high-value. As I think we all do, I prefer #3 as the long term solution. I found spreading the code across two methods a little confusing. I think ideally there would be just one method that decodes and writes into the RVB. I can pick up a proper re-write this/next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862#issuecomment-401787058:199,perform,performance,199,https://hail.is,https://github.com/hail-is/hail/issues/3862#issuecomment-401787058,1,['perform'],['performance']
Performance,I think given the performance I'll need to take another try at this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2089#issuecomment-321885309:18,perform,performance,18,https://hail.is,https://github.com/hail-is/hail/pull/2089#issuecomment-321885309,1,['perform'],['performance']
Performance,"I think that brittleness is gone now, and was fixed by us doing method; splitting in compiler. But double checking would be great. On Wed, Nov 17, 2021, 5:04 PM Patrick Schultz ***@***.***>; wrote:. > Yeah, I guess I should benchmark, since you've observed some strange; > brittleness in the linreg performance before.; >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/11070#issuecomment-972121769>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADJCWES5I5IHLD7O6ECMZIDUMQRFBANCNFSM5IICVNFQ>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11070#issuecomment-972127901:299,perform,performance,299,https://hail.is,https://github.com/hail-is/hail/pull/11070#issuecomment-972127901,1,['perform'],['performance']
Performance,"I think that optimization is worth keeping for linear regression, i care less about the others since they're more bottlenecked elsewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4535#issuecomment-430343623:13,optimiz,optimization,13,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430343623,2,"['bottleneck', 'optimiz']","['bottlenecked', 'optimization']"
Performance,"I think that's right, though we serialize other potentially private information. I think we ought to have a per-organization (Hail billing project?) cache, but also not very high priority. I'd be pretty chuffed to learn we're running important enough stuff that people are attempting timing attacks on our cache to learn what queries other people are executing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10309#issuecomment-821250865:149,cache,cache,149,https://hail.is,https://github.com/hail-is/hail/pull/10309#issuecomment-821250865,4,['cache'],['cache']
Performance,"I think the database insert didn't perform as well with > 1000 jobs per insert in the front_end create_jobs, but I can't figure out where I got that. Feel free to reject this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7604:35,perform,perform,35,https://hail.is,https://github.com/hail-is/hail/pull/7604,1,['perform'],['perform']
Performance,"I think the only two things I'm stuck on are:; (a) Do we want users to pass a VEPConfig instead of a `config` dictionary (and add documentation)?; (b) What is the best way to expose the VEP command interface so a user can customize it to their setup? I wanted to do something like this, but I don't see how to do this with the bash script being called with an argument `/bin/bash -c ""...."" csq` or `/bin/bash -c ""..."" vep`. ```python3; vep_85_grch37_command = '''; #!/bin/bash. if [ $VEP_CONSEQUENCE -ne 0 ]; then; vcf_or_json=""--vcf""; else; vcf_or_json=""--json""; fi. export VEP_COMMAND=/vep/vep \; ${VEP_INPUT_FILE:+--input_file $VEP_INPUT_FILE} \; --format vcf \; ${vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir=${VEP_DATA_DIR} \; --plugin LoF,human_ancestor_fa:${VEP_DATA_DIR}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:${VEP_DATA_DIR}/loftee_data/phylocsf_gerp.sql,gerp_file:${VEP_DATA_DIR}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT. exec vep.py ""$@""; '''. supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfig(; 'hail-qob-vep-grch37-us-central1',; ['us-central1'],; HAIL_GENETICS_QOB_VEP_GRCH37_IMAGE,; '/vep_data/',; {},; VEPConfig.default_vep_json_typ,; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""vep""],; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""csq_header""],; True,; 'gcp',; ),; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947:735,cache,cache,735,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947,1,['cache'],['cache']
Performance,"I think this PR is just about as good as it's going to get for now. From looking at the Grafana API metrics, I think I was hitting the maximum scheduler throughput. The get running cancellable jobs is around 40ms each call for 5000 jobs while the getting the job head queue is 123ms. If the 40ms becomes a problem, then we can pull less records (see explanation below) or we can not do a json array agg and figure out the regions using bit shifting. When we did the load tests yesterday getting the job head queue was around 1-2 seconds with us each having 20k records. I think we just have to keep an eye on it. I did some further optimization of the scheduler by allowing it to pull up to 10000 jobs from the database to try and schedule before it hits its fair share of jobs scheduled. This helps a lot with efficiency to use the existing capacity if there are jobs further down the queue that are schedulable. I know it's a bit of a departure from what we've done in the past, but I think since we're going in order of fair share now and pulling more jobs from the database isn't that expensive, then this is fine. Happy to make this number 1000 even. 300 was too small though. Jobs at the front of the queue will eventually be able to run because the next iteration of the autoscaler will create the correct instances for those jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928:153,throughput,throughput,153,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928,7,"['load', 'optimiz', 'queue', 'throughput']","['load', 'optimization', 'queue', 'throughput']"
Performance,"I think this is a good change but the partitioner hint won't fix Xiao's problem. Here's why:. His OOM error comes from the way we do ordered joins. His workflow was basically annotatevariants table x10, so each partition of the left ended up pulling 10 128M (compressed, so really more) chunks into memory, and boom goes the dataflow. Each table was sorted, so the partitioner hint is never applied. . I'm not sure how we can fix this without a query optimizer. It certainly seems like our current model is dangerous. If I were hand-optimizing his workflow, I might want to shuffle small text files against the vds partitioner regardless of sortedness",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248592567:451,optimiz,optimizer,451,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248592567,2,['optimiz'],"['optimizer', 'optimizing']"
Performance,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193:18,race condition,race condition,18,https://hail.is,https://github.com/hail-is/hail/pull/8193,5,"['cache', 'load', 'race condition']","['cache', 'loads', 'race condition']"
Performance,"I think this is what was wrong with the `git_make_bash_image` taking a minute each time. Since every image without a `publishAs` uses `ci-intermediate`, the `ci-intermediate:cache-PR-X` tag is left pointing to whichever anonymous image built last in the PR run. This is certainly never `git_make_bash_image`, so every time it gets rebuilt, the cache-from that it is using points to an an image whose layers do not include a layer that is `RUN apt-get update && apt-get install -y git make bash`. If this PR runs twice, hopefully we'll see the first step go super quick.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12285:174,cache,cache-PR-X,174,https://hail.is,https://github.com/hail-is/hail/pull/12285,2,['cache'],"['cache-PR-X', 'cache-from']"
Performance,"I think we should continue with another review and then a load test. I'm still a bit hesitant about the query change, but we can keep an eye on it. I'm still get errors with the typing:. ```; (venv) jigold@wm349-8c4 hail % make -C hail/python check; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; python3 -m pylint --rcfile ../../pylintrc hailtop --score=n; python3 -m mypy --config-file ../../setup.cfg hailtop; hailtop/batch/backend.py:481: error: Incompatible types in assignment (expression has type ""Union[str, List[str], None]"", variable has type ""Optional[List[str]]""); Found 1 error in 1 file (checked 146 source files); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464:58,load,load,58,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464,1,['load'],['load']
Performance,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856:310,latency,latency,310,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856,2,['latency'],['latency']
Performance,"I think you're slightly misinterpreting this bit:. > Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. > Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. > Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. > Tim Poterba: ok, I think I'm convinced. In order to do a lazy bgen ptype, we need the *value*, not the type, to hold a region handle. PType doesn't need to be associated with a region -- that's a concept that doesn't really make sense given that PType is really a memory layout spec and codegen interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575727189:327,load,loadElement,327,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575727189,2,['load'],['loadElement']
Performance,I think your approach is better. I was over optimizing for the case where we have lots of active users.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1613675806:44,optimiz,optimizing,44,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1613675806,1,['optimiz'],['optimizing']
Performance,"I thought the purpose of the cache was to cache the latest version in production. Let's take service-base as an example. There's the deployment in production that we care about. But every PR is now going to change the cache each time to what it thinks service-base is. This means that the last 4 layers for service-base will change for every time we run a test PR and it changes hailtop, gear, or web-common. If you don't like this change, then feel free to close it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213:29,cache,cache,29,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213,3,['cache'],['cache']
Performance,"I thought we were going to load the genome reference upon initializing the HailContext. What happens if someone has two VDS's in the same session, but one is GRCh37 and the other is 38? Then we would have to ensure the reference is set to the correct one for every operation on the vds. It wouldn't be too difficult to add a decorator to the Python VDS to set the global reference on each method call. Then for a Join we can check the references are the same explicitly. If we go this route, I think we should not have the HailContext have the reference parameter, and instead have it as an optional parameter on `import_vcf`, `import_plink`, ...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1789#issuecomment-302429644:27,load,load,27,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302429644,1,['load'],['load']
Performance,I totally wanted this when I originally wrote the bgen optimization.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3773#issuecomment-398157431:55,optimiz,optimization,55,https://hail.is,https://github.com/hail-is/hail/pull/3773#issuecomment-398157431,1,['optimiz'],['optimization']
Performance,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862:682,load,load,682,https://hail.is,https://github.com/hail-is/hail/issues/3862,1,['load'],['load']
Performance,"I tried benchmarking this change and didn't see much of a difference in timings in my contrived high throughput example. However, I do think this index is better because I believe MySQL does the order by first and then filters records. @danking Can you take a look at this and make sure the index is actually an improvement. The speed of the query is linearly related to the number of records in the limit. So I think if we need to get the query speed back to under 10ms then we revert back to pulling a smaller number of records rather than 1000. I think 300 is fine and gets us to 10ms. I just didn't want to pull 10 jobs and then none of them are schedulable but the 100th one is. We can revisit this if the scheduler becomes the bottleneck after your changes to the gateway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12350:101,throughput,throughput,101,https://hail.is,https://github.com/hail-is/hail/pull/12350,2,"['bottleneck', 'throughput']","['bottleneck', 'throughput']"
Performance,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009:368,load,loaded,368,https://hail.is,https://github.com/hail-is/hail/issues/8009,2,['load'],['loaded']
Performance,"I understand what is going on now. The issue is that the temp directory is getting removed after the first batch.run(), but we're assuming those input files are still there. I think we should just clear the files and definitions cache after submission.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1426386771:229,cache,cache,229,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1426386771,2,['cache'],['cache']
Performance,"I use a Mac and try to install hail.; I use Mojave; I installed pyenv to modify my python versions.; I installed Python 3.7.9 since you recommend to use Python 3.7 as the latest version.; I then did a pip install hail, and it fails with pyspark:. Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1.tar.gz (215.7 MB); ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-pip-egg-info-vlaj8k6d; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:285,cache,cached,285,https://hail.is,https://github.com/hail-is/hail/issues/9742,1,['cache'],['cached']
Performance,"I used filters for the following images when I've run the Azure cleanup script, but we should double check these make sense still in light of changing how we use ""cache"" and there aren't any additional images or ones that we don't want to delete that are in this list:. ```; --filter 'auth:.*' \; --filter 'base:.*' \; --filter 'base_spark_3_2:.*' \; --filter 'batch:.*' \; --filter 'batch-driver-nginx:.*' \; --filter 'batch-worker:.*' \; --filter 'benchmark:.*' \; --filter 'blog_nginx:.*' \; --filter 'ci:.*' \; --filter 'ci-intermediate:.*' \; --filter 'ci-utils:.*' \; --filter 'create_certs_image:.*' \; --filter 'echo:.*' \; --filter 'grafana:.*' \; --filter 'hail-base:.*' \; --filter 'hail-build:.*' \; --filter 'hail-buildkit:.*' \; --filter 'hail-run:.*' \; --filter 'hail-run-tests:.*' \; --filter 'hail-pip-installed-python37:.*' \; --filter 'hail-pip-installed-python38:.*' \; --filter 'hail-ubuntu:.*' \; --filter 'memory:.*' \; --filter 'monitoring:.*' \; --filter 'notebook:.*' \; --filter 'notebook_nginx:.*' \; --filter 'prometheus:.*' \; --filter 'service-base:.*' \; --filter 'service-java-run-base:.*' \; --filter 'test-ci:.*' \; --filter 'test-monitoring:.*' \; --filter 'test-benchmark:.*' \; --filter 'website:.*' \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349:163,cache,cache,163,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349,1,['cache'],['cache']
Performance,I used the gsutil storage bandwidth tool and confirmed we get 1.2 Gibit / second upload and download speeds from within a 1 core job and 10 Gi storage. Adding more cores didn't change anything. I ran a test job with the copy tool on a 10 Gi random file and matched 1.2 Gibit / second. I'm wondering if the problem is actually workload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write thr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:903,Latency,Latency,903,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Latency'],['Latency']
Performance,"I want a cluster pod resource waste metric. Consider one pod. if its request is 100mCPU and its actual load is 10mCPU we're ""wasting"" 90mCPU. I want to know the distribution of wasted pod CPU. I want to know the top 10 wasteful pods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6509:103,load,load,103,https://hail.is,https://github.com/hail-is/hail/issues/6509,1,['load'],['load']
Performance,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209:70,perform,performance,70,https://hail.is,https://github.com/hail-is/hail/pull/2209,1,['perform'],['performance']
Performance,"I want to annotate a field like this:; ```Gene_Conseq_MAF=(va.annot.gene + ""\n"" + va.annot.most_severe_csq + ""\nMAF:"" + str(va.lmmreg.maf))```; (so a string with \n) such that when I load in R, the top loci will be highlighted as so. However, the exported .gz file actually has new lines at each “\n""; is there a way to avoid this?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1132:183,load,load,183,https://hail.is,https://github.com/hail-is/hail/issues/1132,1,['load'],['load']
Performance,"I want to track performance of logistic regression. Currently, on my laptop these two benchmarks clock in at 11 seconds for Breeze and 62 seconds for ndarrays. Now that I have a metric I'll try and optimize.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10492:16,perform,performance,16,https://hail.is,https://github.com/hail-is/hail/pull/10492,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,I want to use this in optimization.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7139:22,optimiz,optimization,22,https://hail.is,https://github.com/hail-is/hail/pull/7139,1,['optimiz'],['optimization']
Performance,"I was able to figure out how to remove all the ""network shuffle""s and ""coerced sorted dataset""s and that improved the time down to 73 seconds, so a big improvement! I would still hope to improve performance a bit more, being reliably under a minute would be helpful. Here are the logs from that search, let me know what else I can do to help improve the performance or to help you figure it out: ; [hail-search.log](https://github.com/hail-is/hail/files/13310449/hail-search.log). PR is here if you are interested: https://github.com/broadinstitute/seqr/pull/3717",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1804152779:195,perform,performance,195,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1804152779,2,['perform'],['performance']
Performance,I was asked to agree to some license after clearing my gradle cache. I do not think anyone needs this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10099:62,cache,cache,62,https://hail.is,https://github.com/hail-is/hail/pull/10099,1,['cache'],['cache']
Performance,"I was going to suggest we leave this out for 0.2. With better-optimized tables, there will be very little reason to have one-dimensional matrices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2649#issuecomment-355347069:62,optimiz,optimized,62,https://hail.is,https://github.com/hail-is/hail/pull/2649#issuecomment-355347069,1,['optimiz'],['optimized']
Performance,"I was having trouble figuring out how to handle the token and the attributes in hailtop.batch_client.aioclient.Batch. When we create an update from a Batch that already existed perhaps in a different process, we don't have the attributes and token. I made a contract where `commit_update` always returns the token and attributes regardless of whether the BatchBuilder already has that infromation. However, we could also get that information available lazily and cache the result. In addition, the `n_jobs` returned to the client are the number of jobs that are committed and not the same as the `n_jobs` in the batches table. Things to do before merging:; 1. Get rid of the batch updates additions to the UI2. ; 2. Double check the GCP LogsExplorer to make sure there are no silent error messages especially with regards to cancellation.; 3. Have @danking look over the SQL stored procedure for `commit_batch_update` to make sure that query is going to perform as good as what is possible given the complexity of the check.; 4. Run a test batch with the old client (I just checked out the current version of main). You need to make sure both create and create-fast are accounted for and succeed. I've been using the following script to make sure we're using the slow path in addition to the fast path with a regular small test job:. ```python3; from hailtop.batch import ServiceBackend, Batch; import secrets. backend = ServiceBackend(billing_project='hail'); b = Batch(backend=backend); # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; for i in range(8):; j1 = b.new_job(); long_str = secrets.token_urlsafe(256 * 1024); j1.command(f'echo ""{long_str}"" > /dev/null'); batch = b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347:463,cache,cache,463,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347,2,"['cache', 'perform']","['cache', 'perform']"
Performance,"I was seeing crashes on workers in a local install because the Hail jar was loaded by a Spark class loader rather than the system class loader, so the toString in:. > String name = ClassLoader.getSystemResource(""include"").toString();. was failing with a null pointer exception. Fixed this two ways: don't unpack includes on the worker (compilation should only happen on the master) and use the same class loader that loaded the NativeCode object. Also some reformatting and style changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4554:76,load,loaded,76,https://hail.is,https://github.com/hail-is/hail/pull/4554,5,['load'],"['loaded', 'loader']"
Performance,"I went with your suggestions, changing `until` to `to` and using `foldLeft` rather than `flatMap` since it's cleaner on memory (even if slightly slower still). We can always speed these back up in any use case where they become a bottleneck, but right now they won't be. In @maccum 's pruning case there is a rectangle (window) per variant, but also a more appropriate single-pass algorithm. Definitely a performance hit on a rather heinous example using my original code versus your versions, but it's nothing compared to the distributed block matrix computations that follow in pipelines:; ```; val gp = GridPartitioner(512, 100000, 100000); val rnd = new scala.util.Random; def rects = Array.fill(100000){; val i = rnd.nextInt(90000); val j = rnd.nextInt(20000); Array[Long](i, i + 10000, i, i + 10000); }. outer: keep; inner: array. time: 66.642ms; time: 66.549ms; time: 64.039ms; time: 74.439ms. outer: for; inner: array. time: 1.251s; time: 1.389s; time: 1.439s; time: 1.353s. outer: keep; inner: for. time: 723.906ms; time: 715.612ms; time: 721.161ms; time: 707.852ms. outer: for; inner: for. time: 1.820s; time: 1.842s; time: 2.011s; time: 1.718s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094#issuecomment-372717222:230,bottleneck,bottleneck,230,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372717222,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:16,load,load,16,https://hail.is,https://github.com/hail-is/hail/issues/2070,1,['load'],['load']
Performance,"I would prefer that there was some way to abstract over this behavior, but I don't see any ""mutually exclusive"" tag in Args4j. A common `FilterOptions` class might help, but we'd still have to call a ""check options"" method in each `Command` and linear inheritance limits the scalability of this approach to other common options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/622#issuecomment-240831961:275,scalab,scalability,275,https://hail.is,https://github.com/hail-is/hail/pull/622#issuecomment-240831961,1,['scalab'],['scalability']
Performance,"I'd argue that it's broadly useful, but rather the issue is that it's useful at a lower level of abstraction (e.g. how it's used in `ld_prune`, composed with `sparsify_row_intervals`). So I see why the `hl` namespace is may be too high level, but it's also strange to put in experimental a function that is used in non-experimental (as well as experimental) methods. One option is to underscore the method for use outside experimental, but expose through the experimental module. Another is to put it in a submodule, like genetics. I'd like to expose more high-level applications directly (e.g. an `ld_matrix` function that takes an optional `radius` and `coord_expr` and returns the sparse block matrix), and we can think about re-implementing in terms of scans once they come online (deriving the stops from the starts) and zipping the intervals and blocks without ever localizing should memory or performance be an issue.; I don't want to hide it entirely in the meantime as several groups want to make use of it already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873#issuecomment-401468652:900,perform,performance,900,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401468652,1,['perform'],['performance']
Performance,"I'd like to give the user the ability to authenticate to our services from within a batch job. The specific use case I need it for is for the query service to be able to cache things with the memory service, but it seems like it could be more broadly applicable. I'm unsure whether this is the correct way to do it. This is currently not exposed in the python user interface (only in BatchClient), but I can pipe the option through in this PR if we want to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437:170,cache,cache,170,https://hail.is,https://github.com/hail-is/hail/pull/9437,1,['cache'],['cache']
Performance,"I'd like to move master to Spark 2 and Scala 2.11. These changes get us as close as possible. They include:. - remove SparkExport, use reflection to get path of partition when loading from parquet; - remove SparkManager; - upgrade to Kudu 1.1.0 (Spark 2 support). The distance between this and Spark 2 is very small, see https://github.com/hail-is/hail/commit/95a588cfa72391d4303bf6891fd017ec211989db. When the master moves to Spark 2, we can maintain a spark1 branch until the on-prem machines get upgraded. Ideally, the spark1 branch could get rebased automatically as part of the CI, although I'm not quite sure how we'd handle conflicts. Alternatively, we could maintain a spark2 -> spark1 diff in the repo that gets applied as part of testing. Fixes https://github.com/hail-is/hail/issues/1117",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1124:176,load,loading,176,https://hail.is,https://github.com/hail-is/hail/pull/1124,1,['load'],['loading']
Performance,"I'd like your initial feedback before I start testing this on Azure. A substantially earlier version seemed to work fine on GCP with dev deploy. The major conceptual change I made is a `resource` now contains a `prefix` and a `version`. The `resource_name` is just `{prefix}/{version}`. The prefixes for GCP are the same as they were before and don't vary by region. However, the new prefixes for Azure are region specific. The version is `1` for all current resources. . I added a `latest_resource_versions` table that has the prefix mapped to the latest version. This is used to generate the current resource names. There is a new CloudResourceManager that is in charge of managing the spot billing pricing cache and updating the prices in the cache and the database from the cloud provider's API. Since I couldn't easily rename resources to products everywhere in the database due to anonymous foreign key constraints, I had to rename the existing `CloudResourceManager` to `CloudDriverAPI`. Feel free to suggest a better name. The GCPResourceManager is a skeleton right now, but we'll have to flesh it out in the new year when GCP moves to spot billing with varying prices. For the `AzureResourceManager`, I use a new pricing client to grab the latest vm and disk prices. I support all possible disk prices, but for now, I limited the VM query to just get the machine types we support right now. In the future, we could get all VM prices, but the query is around 40 seconds for that compared to 2 seconds now. I was worried if we had such a slow query that blocked driver startup, that would be bad and this is fine for now. There are two classes I added: a `Resource` and a `Price`. The Price is only implemented for Azure and is used to store cost results from the pricing API. The resource has a couple of different mixin classes with an abstract method to generate the quantified resource depending on the type (ex: ComputeResourceMixin). Then there's `AzureDiskResource`, `AzureVMResource`, e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11092:709,cache,cache,709,https://hail.is,https://github.com/hail-is/hail/pull/11092,2,['cache'],['cache']
Performance,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2288:251,perform,performance,251,https://hail.is,https://github.com/hail-is/hail/pull/2288,2,['perform'],['performance']
Performance,I'd really like to see size + performance benchmarks here -- I think the `matrix_table_decode_and_count` and `matrix_table_decode_and_count_just_gt` ones will be interesting.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7803#issuecomment-571308864:30,perform,performance,30,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571308864,1,['perform'],['performance']
Performance,"I'll do a performance test, but there's still foreign key constraints on these rows. They're just redundant. We don't need a check on both `batches` and `attempts`. The rows in `attempts` wouldn't have been inserted without the check in `batches`. All of these proposed changes don't change anything about data integrity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811:10,perform,performance,10,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811,1,['perform'],['performance']
Performance,I'll queue up type hints. > add the timestamp of the last status change?. Can you be a bit more specific? You want the time on the status posted to go PR statuses?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6007#issuecomment-488146766:5,queue,queue,5,https://hail.is,https://github.com/hail-is/hail/pull/6007#issuecomment-488146766,1,['queue'],['queue']
Performance,"I'll stew on this a little further and I have yet to look closely at the queries themselves, but my first two questions are:. 1. I'm not opposed to adding tokens to the `batches_n_jobs_in_complete_states` table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. > (C) The new server code deploys with the new mark_batch_complete code that runs periodically. Eventually the newly completed batches since the migration will get set to ""complete"". It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, *then* remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412:317,perform,performance,317,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"I'll take this change, yeah. I also think it's worth running some experiments to see if this actually makes a performance difference...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836#issuecomment-914666716:110,perform,performance,110,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914666716,1,['perform'],['performance']
Performance,"I'm closing for now. After more careful benchmarking, this is a slight regression. I think this is the right approach, but we don't have enough facilities for writing performant linear algebra in python/hail yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11070#issuecomment-973300626:167,perform,performant,167,https://hail.is,https://github.com/hail-is/hail/pull/11070#issuecomment-973300626,1,['perform'],['performant']
Performance,I'm closing this for now until I make sure that I haven't impacted the performance elsewhere in the system.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7336#issuecomment-544184220:71,perform,performance,71,https://hail.is,https://github.com/hail-is/hail/pull/7336#issuecomment-544184220,1,['perform'],['performance']
Performance,"I'm confused by your description. The memoize occurs inside `copyFromAddress`, and `idx` never changes in the body. If `copyFromAddress` is called inside a loop in which `idx` is changing, the memoize will be reevaluated each iteration. Is the issue that the memoize forces `typ.loadField(srcOff, 1)` outside of the `typ.isFieldMissing(cb, srcOff, 1)` check?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11805#issuecomment-1112087768:279,load,loadField,279,https://hail.is,https://github.com/hail-is/hail/pull/11805#issuecomment-1112087768,1,['load'],['loadField']
Performance,I'm debating whether this PR and #12848 are necessary right now. It's going to take a lot of effort to run the migrations with the number of rows involved and deduping these tables will not impact any query performance. The original goal was to have a consistent set of tables with the billing tables that needed to be deduped.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1607879959:207,perform,performance,207,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1607879959,1,['perform'],['performance']
Performance,I'm fairly certain this was an intermittent race condition in GitHub that we have no control over. Closing. Re-open if we see it again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429134998:44,race condition,race condition,44,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429134998,1,['race condition'],['race condition']
Performance,"I'm happy with this, ready for review. I haven't yet added the `-o ""-""` option yet to write to stdout, but I'll put it in my queue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/119#issuecomment-169488520:125,queue,queue,125,https://hail.is,https://github.com/hail-is/hail/pull/119#issuecomment-169488520,1,['queue'],['queue']
Performance,I'm having a pretty hard time with the binary format. It's also incredibly slow to load the data in the UI when the data is being recorded properly. My approach must not be right. Can you double check this is what you had in mind?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11368#issuecomment-1057541994:83,load,load,83,https://hail.is,https://github.com/hail-is/hail/pull/11368#issuecomment-1057541994,1,['load'],['load']
Performance,I'm in favor of keeping this as a feature. It means we can optimize things way more and more easily.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4738#issuecomment-436428753:59,optimiz,optimize,59,https://hail.is,https://github.com/hail-is/hail/issues/4738#issuecomment-436428753,1,['optimiz'],['optimize']
Performance,"I'm keeping my LD extension branch separate until we add a proper sparse block matrix implementation, but I pulled out these functions on GridPartitioner since (i) they're some of the logic we'll need to make sparse block matrix useful and (ii) Meredith just built a step to compute variant windows in LDPrune, which can then be combined with this logic as part of her optimization strategy to not compute unneeded blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094:369,optimiz,optimization,369,https://hail.is,https://github.com/hail-is/hail/pull/3094,1,['optimiz'],['optimization']
Performance,"I'm merging this so Sali can use the new interface and better performance, and I am going to add an additional test in another PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2544#issuecomment-351097552:62,perform,performance,62,https://hail.is,https://github.com/hail-is/hail/pull/2544#issuecomment-351097552,1,['perform'],['performance']
Performance,I'm missing something. Why shouldn't test deployments benefit from and contribute to the cache? Why isolate them somewhere else?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152542361:89,cache,cache,89,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152542361,1,['cache'],['cache']
Performance,"I'm not 100% sure I know the implications of this change with regards to race conditions. I think it's okay? Also, why do we not include the timings if the job is deleted in `__exit__`? This is the current code:. ```python3; class ContainerStepManager:; def __init__(self, timing: Dict[str, float], is_deleted: Callable[[], bool], ignore_job_deletion: bool = False):; self.timing: Dict[str, float] = timing; self.is_deleted = is_deleted; self.ignore_job_deletion = ignore_job_deletion. def __enter__(self):; if self.is_deleted() and not self.ignore_job_deletion:; raise JobDeletedError(); self.timing['start_time'] = time_msecs(). def __exit__(self, exc_type, exc, tb):; if self.is_deleted() and not self.ignore_job_deletion:; return; finish_time = time_msecs(); self.timing['finish_time'] = finish_time; self.timing['duration'] = finish_time - self.timing['start_time']; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11082:73,race condition,race conditions,73,https://hail.is,https://github.com/hail-is/hail/pull/11082,1,['race condition'],['race conditions']
Performance,"I'm not really sure, either. I figured as long as every type was in there somewhere at least once and some things were missing, it would be fine. This is just to check backward compatibility, so it's hard to imagine that (1) the current version passes all the tests, (2) it can load a reasonable collections of types and values from an old version, but (3) only, say, missing calls are broken.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3043#issuecomment-369993008:278,load,load,278,https://hail.is,https://github.com/hail-is/hail/pull/3043#issuecomment-369993008,1,['load'],['load']
Performance,"I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411:369,load,loads,369,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411,1,['load'],['loads']
Performance,I'm not sure this will make such a performance difference in the common case -- the genotype-level downcode/subset operation will dominate runtime,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484:35,perform,performance,35,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484,1,['perform'],['performance']
Performance,"I'm not sure what the class loading issues are, there's probably some dependency conflict that's been introduced by pulling in avro. Need to investigate, and possibly relocate avro.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10793#issuecomment-902307018:28,load,loading,28,https://hail.is,https://github.com/hail-is/hail/pull/10793#issuecomment-902307018,1,['load'],['loading']
Performance,"I'm obviously sympathetic to this (I've even argued matrix should not have row/col annotations at all!) But I have a few questions about how this will work:. The main interface problem I see is that there are naturally columnless matrices in our domain (sites files). This PR was motivated by some code that Konrad sent me. Are we going to have two versions of VEP and export_vcf, for example?. Second, it seems we can have either interface purity or performance in the short term. I loosely think the latter is better (I would) so we can productively move people off 0.1 even if that means the interface is less stable (e.g. eventually we'll deprecate/remove the drop functions). I want to say I can be convinced otherwise, but changing a range join to a table shuffle seems like a non-starter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2649#issuecomment-355362499:451,perform,performance,451,https://hail.is,https://github.com/hail-is/hail/pull/2649#issuecomment-355362499,1,['perform'],['performance']
Performance,I'm opening this PR so I can get feedback more quickly by being at the front of the queue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12263:84,queue,queue,84,https://hail.is,https://github.com/hail-is/hail/pull/12263,1,['queue'],['queue']
Performance,I'm planning to remove flags in the service because they have bad idempotency properties. Is there a reason to not make the cached FS the default one?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9484#issuecomment-794121319:124,cache,cached,124,https://hail.is,https://github.com/hail-is/hail/pull/9484#issuecomment-794121319,1,['cache'],['cached']
Performance,I'm pretty sure this is due to the database getting overloaded by concurrent PRs. I'm gonna retry the builds.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11747#issuecomment-1096631068:66,concurren,concurrent,66,https://hail.is,https://github.com/hail-is/hail/pull/11747#issuecomment-1096631068,1,['concurren'],['concurrent']
Performance,"I'm still not sure why writing to block matrix after a deep filter performs so badly, but in the meantime I want to document the fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4359:67,perform,performs,67,https://hail.is,https://github.com/hail-is/hail/pull/4359,1,['perform'],['performs']
Performance,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/321:1097,concurren,concurrent,1097,https://hail.is,https://github.com/hail-is/hail/issues/321,2,['concurren'],['concurrent']
Performance,"I'm trying to stop having us call `Region.loadBit` everywhere in `EBaseStruct.decode`. First step of that is not calling `setFieldMissing` and `setFieldPresent` everywhere. PRing for tests right now on first round of doing this, there are still some calls that need to be removed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10994:42,load,loadBit,42,https://hail.is,https://github.com/hail-is/hail/pull/10994,1,['load'],['loadBit']
Performance,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251732459:135,load,loaded,135,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459,1,['load'],['loaded']
Performance,"I've been supporting Hana as much as I can, but she needs someone who can be more dedicated and responsive than me. She uses a k8s cluster. She has a SEQR frontend deployment. She also has a Hail deployment (statefulset maybe?). The Hail pod has an SSD mounted read-only. That SSD has all the SEQR data in Hail Table form. There are many tables with annotations (variant metadata, like ""probability this variant is damaging"" or ""likely causes this to happen to the protein""). There are also ""per-family"" tables which contain all the sequences within a single family. Many queries are directly against a particular family. Those tables are small and quick to read. There's also one giant table containing all the sequences from all the families. That table is large and expensive to read. A lot of our engineering work has been around making sure queries against that table are fast. Tim, at one point, had enough of her system locally that he could experiment with running queries on his laptop against his SSD. He hacked on the queries themselves and on Hail itself until the bandwidth was fast enough that the queries should complete fast enough on the full dataset. Fast enough varies but generally a couple tens of seconds is OK. The work here is to pair with Hana to diagnose performance issues and make changes until the queries are acceptably fast. The first thing I would do is update her to the latest Hail (with the array decoder improvement as well as the memory overhead stuff on which Daniel is working). Then, with Hana's help, test the timing of some queries. If the queries are still too slow, your options are:; 1. Check the log files and the IR. Are there unnecessary shuffles? Is the code really large? Can we do less work maybe?; 2. Have Hana help you replicate her setup locally. You just need a slice of the data and enough of SEQR to run a query. Now hook up a profiler. What's slow? Can we do something about that?. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:1646,perform,performance,1646,https://hail.is,https://github.com/hail-is/hail/issues/13882,1,['perform'],['performance']
Performance,I've hacked a 'solution' by adding an explicit check at https://github.com/hail-is/hail/blob/607d2b4aa032c24db033359eb6f92da976a8d9f2/src/main/scala/org/broadinstitute/hail/io/vcf/HtsjdkRecordReader.scala#L59 but am not sure of the performance penalty. All tests still pass at least! :). See pull request https://github.com/hail-is/hail/pull/1066,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1065#issuecomment-258770482:232,perform,performance,232,https://hail.is,https://github.com/hail-is/hail/issues/1065#issuecomment-258770482,1,['perform'],['performance']
Performance,"I've left all the instances of `region.loadX` untouched (and left the methods on the region object, but this should let us avoid piping through region objects when we just need to read something. (Broken out from #6580)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6644:39,load,loadX,39,https://hail.is,https://github.com/hail-is/hail/pull/6644,1,['load'],['loadX']
Performance,"I've merged a stack of changes to this branch including:; - Cleaned up tests, including refactoring, making Balding-Nichols covariates deterministic and removing lots of extra test code; - Reorder args in Scala to match Python, related bug fixes; - Improved large N performance by using single array D rather than A and B; - Moved dense versus sparse matching outside of loop; - Improved Python docs and Scala remarks; - Debugged test failure only occurring in Spark 2.1.0, which turned out to be related to accuracy of Davies. I've increased accuracy to 1e-8 which is enough to make current tests pass. Once this goes in, I'll make PRs to:; - Allow users to set accuracy and iterations on Davies, will use same defaults as R: 1e-6 and 10k.; - Add number of variants per key as column.; - Fix behavior to finish running even if some groups are too big upper bound, or if Cholesky fails. Document this behavior. Less urgently, but to keep in mind:; - If bottleneck, improve performance of Gramian computation in large N case using blocking; - Improve Davies C code",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707:266,perform,performance,266,https://hail.is,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707,3,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"I've moved `check_entry_indexed` outside the conditional and added a note to remove the conditional entirely once select_entries on a field is free. I think it's reasonable to keep the conditional until then, at least in the block matrix case where needlessly invoking the compiler on every double may effect performance. If you feel strongly, I'll remove it from PCA and test the performance impact on write BlockMatrix (though I'd rather just leave the latter unperturbed).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3262#issuecomment-377698921:309,perform,performance,309,https://hail.is,https://github.com/hail-is/hail/pull/3262#issuecomment-377698921,2,['perform'],['performance']
Performance,I've realised that hailtop is not the place for contributed methods that use batch if we want to restrict hail imports. Think about how strange it is to exclude our own Batch cookbook example (gwas clumping) from a viable contributed module (in that case no tests on gwas.py could be performed directly).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-670772749:284,perform,performed,284,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670772749,1,['perform'],['performed']
Performance,"I've rebased John's branch, added the default block_size change by @shulik7 at Nirvana, and made a few additional small changes including adding the @typecheck_method and @record_method decorators (the latter is now required, else history goes histrionic). The configuration file, init script, and resource files are in`gs://hail-common/nirvana`. The init script `gs://hail-common/nirvana/nirvana-init-GRCh37.sh` makes local copies of the resource files and .net: ; ```; #!/bin/bash. mkdir -p /nirvana/Data/Cache; mkdir -p /nirvana/Data/References; mkdir -p /nirvana/Data/SupplementaryDatabase. #Data is copied for use with Nirvana 1.6.2 as of June 19 2017; gsutil -m cp -r gs://hail-common/nirvana/Data/Cache/24/GRCh37 /nirvana/Data/Cache; gsutil -m cp gs://hail-common/nirvana/Data/References/5/Homo_sapiens.GRCh37.Nirvana.dat /nirvana/Data/References; gsutil -m cp -r gs://hail-common/nirvana/Data/SupplementaryDatabase/39/GRCh37 /nirvana/Data/SupplementaryDatabase; gsutil -m cp -r gs://hail-common/nirvana/netcoreapp1.1 /nirvana; gsutil -m cp gs://hail-common/nirvana/nirvana-cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:507,Cache,Cache,507,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,3,['Cache'],['Cache']
Performance,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you don’t want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433:753,load,load,753,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433,4,['load'],['load']
Performance,IR-ify MT.dropRows. Optimize Filter(True()) and Filter(False()),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3908:20,Optimiz,Optimize,20,https://hail.is,https://github.com/hail-is/hail/pull/3908,1,['Optimiz'],['Optimize']
Performance,"If I change my performance test to write using fast LZ4, the time drops to 26.2s and the profile looks like:. ![Screen Shot 2020-05-26 at 1 43 34 PM](https://user-images.githubusercontent.com/106194/82932751-e5d27400-9f56-11ea-9356-086f57d58fba.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864#issuecomment-634173911:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-634173911,1,['perform'],['performance']
Performance,"If a many-partitions heavily-filtered matrix table is converted to a block matrix with `write_from_entry_expr`, parallelism is lost and kills performance. In the extreme case, imagine a MT with 4096 partitions, each with 1M rows, which are filtered to 1 row. There will be 1-way parallelism in the write. . We need to checkpoint the intermediate matrix if the loss of parallelism is above some threshold.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6995:142,perform,performance,142,https://hail.is,https://github.com/hail-is/hail/issues/6995,1,['perform'],['performance']
Performance,"If cols() sorts on the column key, then this optimization doesn't work for expressions with scans since they'd scan in a different order. (I also pulled out the `ContainsAgg` check for the next one, since MatrixFilterCols can't handle aggregations anyways.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4295:45,optimiz,optimization,45,https://hail.is,https://github.com/hail-is/hail/pull/4295,1,['optimiz'],['optimization']
Performance,"If ready for review, ""assign"" someone and it'll show up in their CI queue :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12913#issuecomment-1518350329:68,queue,queue,68,https://hail.is,https://github.com/hail-is/hail/pull/12913#issuecomment-1518350329,1,['queue'],['queue']
Performance,If the query plans are identical I’d expect identical performance. Maybe the DB version was incremented and the optimizer is better? These queries are only interesting when the system is under enough load to have “cancellable resources” i.e. running jobs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14366#issuecomment-2251347683:54,perform,performance,54,https://hail.is,https://github.com/hail-is/hail/pull/14366#issuecomment-2251347683,3,"['load', 'optimiz', 'perform']","['load', 'optimizer', 'performance']"
Performance,"If there is no work to do, the scheduler threads should wait. This is likely causing the database load. run_if_changed isn't waiting if there is no work to do, so all three threads are spinning as fast as possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7955:98,load,load,98,https://hail.is,https://github.com/hail-is/hail/pull/7955,1,['load'],['load']
Performance,If you can make this IR rewrite pass determinism with/without optimization I will be totally confident in this change; https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/ir/Simplify.scala#L294,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4104#issuecomment-411794524:62,optimiz,optimization,62,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411794524,1,['optimiz'],['optimization']
Performance,"Images are stored in a global cache, so we have to be careful that a private image pulled by one user isn't executed by another user. Therefore, pull each time for private images. We could speed this up by having a per-user private image cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7456:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/7456,2,['cache'],['cache']
Performance,"Implement an arbitrary inner product operation. This is the critical piece of; infrastructure that underlies all relatedness inference algorithms. The typical; inner product on real number matrices is defined as:. ```; L_ij : matrix of shape a by b; M_kl : matrix of shape b by c; N_il : matrix of shape a by c, the inner product of L and M. N_il = Sum_k (L_ik * M_kl); ```. This PR allows the user to define what `*` means and what `Sum` means. For; example, the KING paper defines an estimator for relatedness of homogeneous; populations called KING-homo. KING-homo's numerator is given by; `score_difference` below. ```python3; mt = hl.balding_nichols_model(2, 5, 5); mt = mt.select_entries(genotype_score=hl.float(mt.GT.n_alt_alleles())); da = hl.experimental.dnd.array(mt, 'genotype_score', block_size=3); score_difference = da.T.inner_product(; da,; lambda l, r: sqr(l - r),; lambda l, r: l + r,; hl.float(0),; hl.agg.sum; ); ```. The rest of KING-homo is just manipulation of row fields. Eventually we need to implement `ndarray_inner_product(self, other, product, sum)`; which does a cache-friendly pass over the data but applies arbitrary user; product and sum operations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9080:1092,cache,cache-friendly,1092,https://hail.is,https://github.com/hail-is/hail/pull/9080,1,['cache'],['cache-friendly']
Performance,"Implemented image untagging for image cleanup steps (like is done in GCR) for Azure. Since old layers still should be used for caching, this just removes the tag used for an image in a test build. We can then do something like [here](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auto-purge#run-in-an-on-demand-task) where you can purge untagged layers that are older than some number of weeks where we believe they're no longer relevant to the layer cache. I also switched out the `registry-push-credentials` that CI uses to build images from the ACR admin login to CI's service principal and eliminated the admin login from the ACR terraform resource. I dev deployed CI and manually verified after a deploy that a tag that was cleaned up no longer showed up in acr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11100:482,cache,cache,482,https://hail.is,https://github.com/hail-is/hail/pull/11100,1,['cache'],['cache']
Performance,Importing from `batch_configuration` means that for this cache to be used you must define all environment variables that batch depends on. I severed this connection and fixed a use of the k8s cache in bootstrap.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11162:57,cache,cache,57,https://hail.is,https://github.com/hail-is/hail/pull/11162,2,['cache'],['cache']
Performance,Impose sorted partitioning on RDDs for huge performance gains,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/555:44,perform,performance,44,https://hail.is,https://github.com/hail-is/hail/pull/555,1,['perform'],['performance']
Performance,Improve performance of BaldingNicholsModel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2314:8,perform,performance,8,https://hail.is,https://github.com/hail-is/hail/issues/2314,1,['perform'],['performance']
Performance,Improve performance of VCF Header Check,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2138:8,perform,performance,8,https://hail.is,https://github.com/hail-is/hail/pull/2138,1,['perform'],['performance']
Performance,Improve performance usage of StringTableReader by implementing StringTablePartitionReader instead of using rvbs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10753:8,perform,performance,8,https://hail.is,https://github.com/hail-is/hail/pull/10753,1,['perform'],['performance']
Performance,"Improved the optimizer so the IR generated for:. ```; mt = hl.import_vcf('sample.vcf'); mt.info.CCC.show(); ```. is reasonable. Run optimizer in compile, so we optimize (transformed) agg and seq ops. Simplify runs:; - propagate Begins up if possible; - inline single-use Lets; - (Apply annotate ...) => InsertFields if possible; - Turn MakeStruct of a bunch of FieldRefs into InsertFields. The optimizer now turns this:. ```; (TableMapGlobals Struct{} ""{}""; (MatrixRowsTable; (MatrixMapRows None None; (MatrixRead None False False ...); (Let __uid_1; (MakeStruct; (<expr>; (GetField CCC; (GetField info; (Ref ... va))))); (ApplyIR annotate; (MakeStruct; (locus; (GetField locus; (Ref ... va))); (alleles; (GetField alleles; (Ref ... va)))); (MakeStruct; (<expr>; (GetField `<expr>`; (Ref Struct{`<expr>`:Int32} __uid_1)))))))); (MakeStruct)); ```. I shit you not, that's literally what's generated by:. ```; import hail as hl; mt = hl.import_vcf('sample.vcf'); mt.info.CCC.show(); ```. into:. ```; (TableMapGlobals Struct{} ""{}""; (MatrixRowsTable; (MatrixMapRows None None; (MatrixRead ... False False ...); (InsertFields; (SelectFields (locus alleles); (Ref ... va)); (<expr>; (GetField CCC; (GetField info; (Ref ... va))))))); (MakeStruct)); ```. The only thing that's missing is to push the MatrixRowsTable into the MatrixMapRows. @tpoterba, I thought you had code for this? What happened?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3966:13,optimiz,optimizer,13,https://hail.is,https://github.com/hail-is/hail/pull/3966,4,['optimiz'],"['optimize', 'optimizer']"
Performance,Improves performance of GVCF import significantly:; ```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; import_gvcf_force_count 81.2% 68.737 55.833; import_and_transform_gvcf 79.9% 75.692 60.464; ----------------------; Harmonic mean: 80.5%; Geometric mean: 80.6%; Arithmetic mean: 80.6%; Median: 80.6%; ```. Stacked on #8382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8383:9,perform,performance,9,https://hail.is,https://github.com/hail-is/hail/pull/8383,1,['perform'],['performance']
Performance,"In Hail, we need to know the dataset schema statically (e.g. when run `import_vcf` before you process the data). There are two ways to do it: use the header or impute. Impute means scanning the entire dataset (it may be the very last variant has a field that appears nowhere else in the file). Even more, you need to scan all the genotypes to determine the the types since types aren't declared outside of the header. We have an impute option for loading TSVs, but not for VCFs. I agree it would be good to add. I'm going to close this in favor of this issue: https://github.com/hail-is/hail/issues/3499",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467#issuecomment-386413979:447,load,loading,447,https://hail.is,https://github.com/hail-is/hail/issues/3467#issuecomment-386413979,1,['load'],['loading']
Performance,"In QoB, the Query Driver and all Query Worker jobs reside in the same batch. Currently, the Query Driver submits worker jobs and then waits for them to finish before collecting their results from GCS. The way it waits is by polling the status of the batch and waiting for the number of completed jobs to reach `n_total_jobs - 1` (to account for itself). This is both awkward and prevents a couple potentially valuable pieces of functionality: you cannot run multiple concurrent Query Drivers, and you cannot take advantage of Batch's `cancel_after_n_failures` functionality because you do not want to cancel the Query Driver itself. The introduction of job groups addresses both these problems, as the Query Driver can create a job group for the stage of worker jobs and then await its completion. This PR makes a job group for the query driver and then the query driver creates a nested job group per stage of workers that it creates. Utilizing `cancel_after_n_failures` to fail a stage early is future work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14432:467,concurren,concurrent,467,https://hail.is,https://github.com/hail-is/hail/pull/14432,1,['concurren'],['concurrent']
Performance,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/463:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/issues/463,1,['load'],['load']
Performance,"In both cases we had 279 cores, 1:1 core:partition. With 37 million variants. ```; nSamples 2,535; nVariants 37,237,504; ```. ```; dking@cluster-2-m:~/hail$ spark-submit --class org.broadinstitute.hail.driver.Main build/libs/hail-all-spark.jar read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.vds/ ibd -o gs://hail-1kg/ALL.1KG.qc.genome --parallel-write; hail: info: running: read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.vds/; [Stage 1:=================================================> (5 + 1) / 6]hail: info: running: ibd -o gs://hail-1kg/ALL.1KG.qc.genome --parallel-write; [Stage 8:======================================================>(278 + 1) / 279]hail: info: timing:; read: 27.197s; ibd: 16m21.3s; total: 16m48.5s; ```. With 76 thousand variants. ```; nSamples 2,535; nVariants 76,628; ```. ```; dking@cluster-2-m:~/hail$ spark-submit --class org.broadinstitute.hail.driver.Main build/libs/hail-all-spark.jar read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.filtered.AF005.callrate98.p1000.vds/ filtervariants expr --keep -c 'v.start % 200 == 0' ibd -o gs://hail-1kg/ALL.1KG.callrate98.p1000.qc.genome; hail: info: running: read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.filtered.AF005.callrate98.p1000.vds/; [Stage 1:======================================================>(269 + 1) / 270]hail: info: running: filtervariants expr --keep -c 'v.start % 200 == 0'; hail: info: running: ibd -o gs://hail-1kg/ALL.1KG.callrate98.p1000.qc.genome; [Stage 8:======================================================>(278 + 1) / 279]hail: info: while writing:; gs://hail-1kg/ALL.1KG.callrate98.p1000.qc.genome; merge time: 3.590s; hail: info: timing:; read: 40.466s; filtervariants expr: 288.900ms; ibd: 40.869s; total: 1m21.6s; ```. I seem to have forgot the `--parallel-write` flag on the second invocation. That should make it a bit faster if writing is the bottleneck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-261081642:1840,bottleneck,bottleneck,1840,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261081642,1,['bottleneck'],['bottleneck']
Performance,"In particular, isn't it possible that you have the n-1th and nth job racing to complete. Everyone else is already done. Call the n-1th job's transaction T1 and the nth job's transaction T2. Both race down to this statement in MJC:; ```; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id AND n_completed = batches.n_jobs;; ```. That will now need to have a sum(n_completed) over all tokens. The isolation level is repeatable read. Assume T1 and T2 generate non equal tokens. T1 and T2 may both snapshot the state of the database before either T1 or T2 executes. T1 and T2 will necessarily see the changes they've made (which affect distinct rows because they have distinct tokens), but neither is required to see the changes the other has made. I think the only way to guarantee that at least one of T1 or T2 sees the database with sum(n_completed) == n_jobs is for both of them to LOCK IN SHARE MODE when doing the sum(n_completed). That will cause lock contention. Maybe that's OK? In the worst case you could have this happen:. 1. Job 1 executes all the way to just before the sum(n_completed).; 2. Job 2 executes all the way to modifying the volatile state.; 3. Job 1 blocks waiting for Job 2 to modify the volatile state.; 4. Job 3 executes all the way to modifying the volatile state.; 5. Job 1 and 2 now wait for Job 3 to modifying the volatile state.; 6. ...; 7. Job 1, 2, 3, n-1 now all wait for Job n to modify the volatile state.; 8. Job 1...n finally execute the sum, all in parallel. I guess that's not terrible, it just means that the latency of Job 1 is extended as long as other jobs can race in before it grabs a shared lock on all the rows.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916:1596,latency,latency,1596,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916,1,['latency'],['latency']
Performance,"In readBlock, membership was checked in a LinkedHashMap, then if it was; found, the value was retrieved from the cache. This lead to a data race; where a value could be evicted between the check and retrival. The; solution is to grab the block value out of the map, and if it is not; null, return it, otherwise, grab the block string, put it in the map,; and return it. Co-authored-by: Tim Poterba <tpoterba@broadinstitute.org>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9427:113,cache,cache,113,https://hail.is,https://github.com/hail-is/hail/pull/9427,1,['cache'],['cache']
Performance,"In the Sphinx theme which the Hail docs use, the search page does not show anything unless a query has been provided and a search performed. https://github.com/readthedocs/sphinx_rtd_theme/blob/master/sphinx_rtd_theme/search.html. Thus, the Search link on the [home page of the Hail 0.2 docs](https://hail.is/docs/0.2/index.html) leads to a [blank page](https://hail.is/docs/0.2/search.html). ![image](https://user-images.githubusercontent.com/1156625/74118640-44333c80-4b8a-11ea-9147-7a0d188d44a0.png). To avoid confusion, this change removes the link to the search page from the home page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8065:130,perform,performed,130,https://hail.is,https://github.com/hail-is/hail/pull/8065,1,['perform'],['performed']
Performance,"In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586380409:299,load,loadElement,299,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586380409,1,['load'],['loadElement']
Performance,"In the former, do you mean to run terraform in CI? The latter seems easier to me given our current system. That being said, our current system isn't great since it does not let you have more than 1 concurrent PR that wants to make terraform changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13489#issuecomment-1691966175:198,concurren,concurrent,198,https://hail.is,https://github.com/hail-is/hail/pull/13489#issuecomment-1691966175,1,['concurren'],['concurrent']
Performance,"In the spirit of shrinking the `Backend` functionality to a core set of functions, move code cache access via execute context.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14691:93,cache,cache,93,https://hail.is,https://github.com/hail-is/hail/pull/14691,1,['cache'],['cache']
Performance,"In the spirit of spreading out review burden a bit, I'm gonna pick up review for this PR. I'll be sure to read up on all the stacked conversations so I'm cache'd in before I review this one. I'll stick a review on here and you can dismiss when this is ready for a look :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14018#issuecomment-1888002011:154,cache,cache,154,https://hail.is,https://github.com/hail-is/hail/pull/14018#issuecomment-1888002011,1,['cache'],['cache']
Performance,"In this PR, I rewrite `linear_regression_rows_nd` to use `_map_partitions` instead of `_group_within_partitions`. By doing this, I've eliminated the need to do a `key_by` at the end of `linear_regression_rows_nd`. I also think this makes the code clearer. . This PR also makes a few seemingly random changes that are actually bug fixes:. 1. When emitting `Apply` nodes, we were grabbing the `Code[Region]` from the first argument to the `MethodBuilder`. However, the assumption that the first argument will always be a `Region` seems to no longer be true. As such, we just construct a `CodeParam` from the `StagedRegion` we have available. . 2. In the NDArrayEmitter, I want to make sure I call the local `emit` method that passes off to `emitWithRegion`, for the same reason as 1: (Can't trust first argument to be a `Region`). 3. In `EmitStream`, I need to use `memoizeField` instead of `memoize`, because regular `memoize` saves to a `LocalRef`, and that will get reset to 0 when `next` is called on a stream. Lesson: don't trust locals for things that must live between elements of a stream. I feel like you have a better idea of how the Stream stuff gets emitted than I do Patrick. I'm curious if what I wrote in `process_block` could be written in a way that would lead to better code getting emitted, as I still need to figure out how to squeeze more performance out of this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9469:1358,perform,performance,1358,https://hail.is,https://github.com/hail-is/hail/pull/9469,1,['perform'],['performance']
Performance,Incorporates grafana which we did not have at the time of previous revision and removes the nginx config that is no longer used. We now use Envoy as our load balancer. I'll make a separate dev doc explaining the gateways.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14400:153,load,load,153,https://hail.is,https://github.com/hail-is/hail/pull/14400,1,['load'],['load']
Performance,Increase verbosity of Compile optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5749:30,optimiz,optimization,30,https://hail.is,https://github.com/hail-is/hail/pull/5749,1,['optimiz'],['optimization']
Performance,"Indeed, I'm getting similar timing and also:; ```; 2018-04-29 13:15:04 BLAS: WARN: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; 2018-04-29 13:15:04 BLAS: WARN: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS; ```; We still have it in the Dataproc image. I wonder what happened locally.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385266528:93,load,load,93,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385266528,2,['load'],['load']
Performance,"InfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1154,load,loadClass,1154,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,1,['load'],['loadClass']
Performance,"Installing hail from the whl is going to break this cache every time so we end up installing the gcloud sdk on every build, which takes 40 seconds. It'd be nice to also move the extra pip packages further up the image but I think they're ordered this way so that pip gives us hail-compatible versions. I think the best fix would be to track these packages in a requirements.txt that is pinned and constrained on the hail package's requirements.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12301:52,cache,cache,52,https://hail.is,https://github.com/hail-is/hail/pull/12301,1,['cache'],['cache']
Performance,"Interesting to see the benchmarks, thanks. I didn't realize there were any per-variant usages, I figured these were per-RDD. That makes me more okay with the original, but it's completely up to you. On a side note, I can't wait until we can work in C++, where using library facilities to simplify code isn't such a performance hit!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094#issuecomment-372723395:315,perform,performance,315,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372723395,2,['perform'],['performance']
Performance,"Intern types rather than recording singletons.; Remove canonical field stuff from LoadVCF (unused), move to LoadGDB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2426:82,Load,LoadVCF,82,https://hail.is,https://github.com/hail-is/hail/pull/2426,2,['Load'],"['LoadGDB', 'LoadVCF']"
Performance,Is there a noticeable performance improvement?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1566#issuecomment-287546850:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/1566#issuecomment-287546850,1,['perform'],['performance']
Performance,It appears (see [1] and [2]) that compiling AVX2 instructions (which hail uses to calculate IBD quickly) on a Mac using some versions of MacPorts GCC doesn't work. The Hail team recommends compiling with Clang when on Mac OS X. We _do not recommend_ removing AVX2 compatibility (either by adding `-mno-avx` or removing `-march=native`) because the AVX2 instructions are vital to IBD performance. [1] http://stackoverflow.com/questions/10327939/error-no-such-instruction-while-assembling-project-on-mac-os-x; [2] https://github.com/Theano/Theano/issues/1980,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1341:383,perform,performance,383,https://hail.is,https://github.com/hail-is/hail/issues/1341,1,['perform'],['performance']
Performance,"It appears jinja2 is not the bottleneck:; ```; # seq 0 30000 | jq '{""batch"": {""jobs"": [inputs | {""job_id"": ., ""state"": ""Running"", ""exit_code"": null, ""duration"": null, ""batch_id"": 1}], ""id"": 1}}' > batches; # time j2 -f json ../batch/batch/templates/batch.html batches > foo.html && open foo.html; j2 -f json ../batch/batch/templates/batch.html batches > foo.html 0.30s user 0.06s system 95% cpu 0.374 total; ```. The website does take about ~2 seconds to fully load though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6548#issuecomment-508150362:29,bottleneck,bottleneck,29,https://hail.is,https://github.com/hail-is/hail/issues/6548#issuecomment-508150362,2,"['bottleneck', 'load']","['bottleneck', 'load']"
Performance,"It definitely looks like ""ZONE_RESOURCE_POOL_EXHAUSTED"" is the cause of these GPU test failures. In this case it looks like it took ~4 minutes to successfully get a VM (after two exhaustion errors) & schedule the job. By then, our uniform 6 minute timeout per test left us with just two minutes. It looks like the job actually did succeed in the worker (seems to have taken ~2 minutes, seems long, does testing for CUDA do some kind of initialization work?). Looks like backing that off to 10 minutes might be just enough to eventually get us a GPU. Might be worth pulling that into its own build.yaml test job so that it does not block the queue of other tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739:641,queue,queue,641,https://hail.is,https://github.com/hail-is/hail/pull/13739,1,['queue'],['queue']
Performance,It doesn't make sense to be able to delete or cancel an individual job since they must be part of a batch now. I also deleted `list_jobs` since a job must be a part of a batch. I left in `job.wait()` because I felt the tests in `test_dag` were important and shouldn't be deleted and the wait functionality is needed for there not to be race conditions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6023:336,race condition,race conditions,336,https://hail.is,https://github.com/hail-is/hail/pull/6023,1,['race condition'],['race conditions']
Performance,"It exists on LoadVCF in Scala, but isn't exposed in Python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3341:13,Load,LoadVCF,13,https://hail.is,https://github.com/hail-is/hail/issues/3341,1,['Load'],['LoadVCF']
Performance,"It is possible there is a race condition, though I have not witnessed this before. In fact, it seems rather reasonable that GitHub had some intermittent slow down that delayed repository creation or ability to find said repository temporarily.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153:26,race condition,race condition,26,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153,1,['race condition'],['race condition']
Performance,"It is true that the optimizer first runs whole-stage, and that could be killing you, while this caches per-stage. I say we run again with this and see where we are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426#issuecomment-467105630:20,optimiz,optimizer,20,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467105630,2,"['cache', 'optimiz']","['caches', 'optimizer']"
Performance,"It looks like I accidentally ""comment"" reviewed rather than ""approve"" reviewed. Further commits to either feature or master will invalidate CI results, though there is a race condition that is only fixable by requiring everyone to rebase on top of latest master before merges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-302577366:170,race condition,race condition,170,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-302577366,1,['race condition'],['race condition']
Performance,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:17,cache,cache,17,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082,2,['cache'],['cache']
Performance,It looks like pylint/flake8 don't like the import structure. I say we just import them! I don't think it affects performance in any meaningful way.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8968#issuecomment-644398412:113,perform,performance,113,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-644398412,1,['perform'],['performance']
Performance,It looks like the prebuilt hail/hail/prebuilt/lib/linux-x86-64/libhail.so is what requires GLIBC 2.14. What would it take to get this compiled with 2.12?. ```; objdump -p libhail.so. libhail.so: file format elf64-x86-64. Program Header:; LOAD off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**21; filesz 0x0000000000023e1d memsz 0x0000000000023e1d flags r-x; LOAD off 0x00000000000246f8 vaddr 0x00000000002246f8 paddr 0x00000000002246f8 align 2**21; filesz 0x0000000000000de0 memsz 0x0000000000001038 flags rw-; DYNAMIC off 0x0000000000024ca8 vaddr 0x0000000000224ca8 paddr 0x0000000000224ca8 align 2**3; filesz 0x00000000000001f0 memsz 0x00000000000001f0 flags rw-; NOTE off 0x00000000000001c8 vaddr 0x00000000000001c8 paddr 0x00000000000001c8 align 2**2; filesz 0x0000000000000024 memsz 0x0000000000000024 flags r--; EH_FRAME off 0x000000000001ffa4 vaddr 0x000000000001ffa4 paddr 0x000000000001ffa4 align 2**2; filesz 0x00000000000007dc memsz 0x00000000000007dc flags r--; STACK off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**4; filesz 0x0000000000000000 memsz 0x0000000000000000 flags rw-; RELRO off 0x00000000000246f8 vaddr 0x00000000002246f8 paddr 0x00000000002246f8 align 2**0; filesz 0x0000000000000908 memsz 0x0000000000000908 flags r--. Dynamic Section:; NEEDED libstdc++.so.6; NEEDED libm.so.6; NEEDED libgcc_s.so.1; NEEDED libc.so.6; INIT 0x000000000000b1c8; FINI 0x000000000001ea38; INIT_ARRAY 0x00000000002246f8; INIT_ARRAYSZ 0x0000000000000010; FINI_ARRAY 0x0000000000224708; FINI_ARRAYSZ 0x0000000000000008; GNU_HASH 0x00000000000001f0; STRTAB 0x0000000000003690; SYMTAB 0x0000000000000c90; STRSZ 0x0000000000005524; SYMENT 0x0000000000000018; PLTGOT 0x0000000000225000; PLTRELSZ 0x0000000000000e10; PLTREL 0x0000000000000007; JMPREL 0x000000000000a3b8; RELA 0x0000000000009038; RELASZ 0x0000000000001380; RELAENT 0x0000000000000018; VERNEED 0x0000000000008f38; VERNEEDNUM 0x0000000000000004; VERSYM 0x0000000000008bb4;,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-443244929:238,LOAD,LOAD,238,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-443244929,2,['LOAD'],['LOAD']
Performance,It looks like the repo create succeeded. Is there a race condition and it isn't ready yet?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-428992101:52,race condition,race condition,52,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428992101,1,['race condition'],['race condition']
Performance,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4948:95,load,loading,95,https://hail.is,https://github.com/hail-is/hail/issues/4948,6,['load'],"['load', 'loading']"
Performance,"It seems like the discussion on this PR has blown out Github's UI, haha, loading and navigating this page is increasingly painful. When you're ready for another look, do you mind squashing these changes and opening a new PR to start with a clean state?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-415862955:73,load,loading,73,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-415862955,1,['load'],['loading']
Performance,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752:1040,load,loaded,1040,https://hail.is,https://github.com/hail-is/hail/pull/10752,1,['load'],['loaded']
Performance,"It takes one minute to build the docs *even if nothing has changed since the; last build*. There are a few things that lengthen the feedback cycle:. - We defeat Sphinx's input cache by deleting and re-copying over all the source; files.; - We defeat Sphinx's output cache by `mv`ing the output to a new location.; - We check that Hail is installed (at a cost of two seconds) *every* time we; build the docs. This isn't necessary, Sphinx prints a reasonable message; (""cannot import ..."") if Hail is not installed.; - We create a wheel file every time we build the docs at a cost of several; seconds.; - We recreate the tutorials tar even if it has not changed. Instead, I propose this PR:. - Do not copy the source files.; - Copy the output to the new location.; - Do not check hail is installed.; - Do not even install Hail.; - Use Make to check if the tutorial tar need be recreated. Regarding not installing Hail: even install-editable takes two seconds. It is; the developer's responsibility to ensure the right version of Hail is; installed. When you check out a branch just run `make install-editable`; once. Then edit the docs to your heart's desire, never re-install Hail. With this PR it takes ~3.5 seconds to rebuild the docs if nothing has; changed. We do work proportional to the number of changed files, not; proportional to all files. Sphinx itself takes 2-3 seconds, so we can't do much; better than this. Dice came up Patrick, but I imagine @tpoterba has thoughts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9348:176,cache,cache,176,https://hail.is,https://github.com/hail-is/hail/pull/9348,2,['cache'],['cache']
Performance,"It was really for performance: we want to remove the filtered allele(s) from R, A and G-based annotations. Now for the gnomAD VDS there are ~ 40 of these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1203#issuecomment-268331577:18,perform,performance,18,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268331577,1,['perform'],['performance']
Performance,"It was wrong for me to check `hasNext` before checking `childIterationCount != blockSize`. If you look at https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1306, you can see that `LoadVCF`'s `hasNext` loads values into memory. By calling `hasNext` and then not actually processing that value, I was loading a value into memory and then invalidating the pointer before it processed in the next group of rows. . Thanks to @tpoterba for helping me figure this out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8743:185,Load,LoadVCF,185,https://hail.is,https://github.com/hail-is/hail/pull/8743,4,"['Load', 'load']","['LoadVCF', 'loading', 'loads']"
Performance,"It would lessen the load on aiohttp, to something that may be better suited to handling large messages. Then store the result in gcs, and respond with the address of the object",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5898#issuecomment-484219314:20,load,load,20,https://hail.is,https://github.com/hail-is/hail/issues/5898#issuecomment-484219314,1,['load'],['load']
Performance,"It's a successful build for a previous version of master. I'm working on a PR to note when ci2 builds are out of date. We only update on approved PR at a time, so it is in a queue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361:174,queue,queue,174,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361,1,['queue'],['queue']
Performance,"It's good that you documented it in #3706. When fixed I can simplify `tie_breaker` to `hl.signum(r.twice_maf - l.twice_maf)`, but I don't expect that to make a noticeable performance difference in the scheme of the full computation so it's not my highest priority.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3704#issuecomment-395117104:171,perform,performance,171,https://hail.is,https://github.com/hail-is/hail/pull/3704#issuecomment-395117104,2,['perform'],['performance']
Performance,"It's interesting, this reading problem is basically a little configuration language, but somehow it's really hard to write the performance we want without lots of code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255:127,perform,performance,127,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255,1,['perform'],['performance']
Performance,It's just a string so `json.loads` fails on it. Not sure why I did that anyway. This has been broken on CI for a bit now. CI still manages fine because it checks everything on an interval but the callback helps it respond immediately to when batches finish for a PR test or deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12631:28,load,loads,28,https://hail.is,https://github.com/hail-is/hail/pull/12631,1,['load'],['loads']
Performance,It's wildly unsafe. It's better to scope the unsafety in; `IEmitCode.handle` for `loadField`. Also add `PNDArrayValue.shapes` to handle the previous use case of; `PBaseStructValue.apply`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9289:82,load,loadField,82,https://hail.is,https://github.com/hail-is/hail/pull/9289,1,['load'],['loadField']
Performance,Iterate on optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5862:11,optimiz,optimization,11,https://hail.is,https://github.com/hail-is/hail/pull/5862,1,['optimiz'],['optimization']
Performance,Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:7711,concurren,concurrent,7711,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['concurren'],['concurrent']
Performance,Iterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSet,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:4771,concurren,concurrent,4771,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['concurren'],['concurrent']
Performance,"Jobs with large logs (>2GiB-ish) can break workers because the current worker code attempts to load the whole log as `bytes` before uploading it to blob storage. This loading into `bytes` also plagues the batch front end when loading logs from blob storage to present to the user.; ; This updates the worker and front end to always stream through logs, never load them into memory. Additionally, in order to make page loads in the UI reasonable, we limit the length of the log that is shown in the UI, with some advice to download the file if it's too large to render on the page. Fixes #13329",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14076:95,load,load,95,https://hail.is,https://github.com/hail-is/hail/pull/14076,5,['load'],"['load', 'loading', 'loads']"
Performance,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8177:157,load,loading,157,https://hail.is,https://github.com/hail-is/hail/pull/8177,3,['load'],"['loadElementToIRIntermediate', 'loading']"
Performance,"Just a heads up, any image that relies on pip, may want to pin to version 18.1 (if you want to use --no-cache-dir, as was done in notebook). https://github.com/pypa/pip/issues/6197",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5238:104,cache,cache-dir,104,https://hail.is,https://github.com/hail-is/hail/issues/5238,1,['cache'],['cache-dir']
Performance,"Just starting to explore this PR. I compared this in-progress CI run to 7126798 (from #12737). The time to service backend starting is ~7 minutes. In the other PR, its ~8 minutes. I suppose that's because this PR isn't hitting any caches, right?. Hmm, it also seems like the critical path to the service backend test is through `build_hail_jar_and_wheel_only`. I wonder if we double the cores, would the time halve? On my laptop a fresh build is like 3m. <img width=""1512"" alt=""Screen Shot 2023-03-07 at 10 27 05"" src=""https://user-images.githubusercontent.com/106194/223501120-ea93c58b-f47f-4e49-8405-8a53d97d76e7.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458571966:231,cache,caches,231,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458571966,1,['cache'],['caches']
Performance,"Just to clarify, I was seeing an infinite loop in Interpret for a 0-ary function via calls to Interpret => Compile => Optimize => FoldConstants => Interpret.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6050#issuecomment-490304549:118,Optimiz,Optimize,118,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-490304549,1,['Optimiz'],['Optimize']
Performance,"Just to clarify, Spark 2 is preferred. We'll be dropping Spark 1 support in a few of weeks. Spark 2 has a bunch of performance improvements and features we want to take advantage of in the coming months.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272507277:115,perform,performance,115,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272507277,1,['perform'],['performance']
Performance,"Keep deleting! You can get rid of `LoadVCF` `readerBuilder` argument and, in fact, all the `ReaderBuilder`s and `AbstractRecordReader`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/82#issuecomment-159316557:35,Load,LoadVCF,35,https://hail.is,https://github.com/hail-is/hail/pull/82#issuecomment-159316557,1,['Load'],['LoadVCF']
Performance,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:171,perform,performance,171,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569,4,"['cache', 'perform']","['cache', 'performance']"
Performance,Keeping the most recent 10 `deploy-`m `pr-` and `dev-` images seems reasonable. I think we also use the `cache-` prefix. We maybe should keep 10 each of those?. Anything that's untagged is absolutely good to delete.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13441#issuecomment-1679605182:105,cache,cache,105,https://hail.is,https://github.com/hail-is/hail/issues/13441#issuecomment-1679605182,1,['cache'],['cache']
Performance,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3017:598,Load,LoadMatrix,598,https://hail.is,https://github.com/hail-is/hail/issues/3017,2,['Load'],['LoadMatrix']
Performance,"Konrad is seeing bookend problems related to table joins with different data ranges. For example, trying to left join a table with only chr22 and a table with all chromosomes loads all chr1-21 partitions with the first partition of the left. This is a problem for any type of join, but we can easily optimize for left/right by removing partitions from the opposite table that cannot possibly overlap any partition of the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3685:175,load,loads,175,https://hail.is,https://github.com/hail-is/hail/issues/3685,2,"['load', 'optimiz']","['loads', 'optimize']"
Performance,"L. This will only impact users who build; <code>cryptography</code> from source (i.e., not from a <code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/138da90c8450446b19619e3faa77b9da54c34be3""><code>138da90</code></a> workaround scapy bug in downstream tests (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8218"">#8218</a>) (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8228"">#8228</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/69527bc7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:2987,load,loading,2987,https://hail.is,https://github.com/hail-is/hail/pull/12668,8,['load'],['loading']
Performance,LD prune optimization & question,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697:9,optimiz,optimization,9,https://hail.is,https://github.com/hail-is/hail/pull/3697,1,['optimiz'],['optimization']
Performance,LD prune performance is unacceptable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4506:9,perform,performance,9,https://hail.is,https://github.com/hail-is/hail/issues/4506,1,['perform'],['performance']
Performance,"Large trees of union_cols have to concatenate entries arrays pairwise, creating a lot of junk in memory. A `MatrixMultiWayUnionCols` IR node should be straightforward to lower to `TableMultiWayZipJoin`, such that concatenating the entries arrays is completely deforested. We could then optimize nested `MatrixUnionCols` to a single `MatrixMultiWayUnionCols`, and/or expose a `multi_way_union_cols` method in python. https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/how.20does.20union_cols.20work/near/165089884",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6066:286,optimiz,optimize,286,https://hail.is,https://github.com/hail-is/hail/issues/6066,1,['optimiz'],['optimize']
Performance,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:35,Perform,Performing,35,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635,1,['Perform'],['Performing']
Performance,Let me do some experiments and see whether there is a performance cost. The third post down seemed to think the way I wrote it there shouldn't be a performance penalty and it should still use the index. I'm not sure whether I believe that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12275#issuecomment-1268740914:54,perform,performance,54,https://hail.is,https://github.com/hail-is/hail/pull/12275#issuecomment-1268740914,2,['perform'],['performance']
Performance,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:263,perform,performance,263,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['perform'],['performance']
Performance,"Let's chat today to make a plan for getting these reviewed. I certainly want to see size + performance benchmarks -- I'm more than a little concerned that this change by itself will be *slower* for the 1kg matrix tables -- you'll do several java object allocations per matrix entry to decode with these (for AD and PL arrays), where previously we were doing none.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7821#issuecomment-574201074:91,perform,performance,91,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-574201074,1,['perform'],['performance']
Performance,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218:70,perform,performant,70,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218,2,['perform'],['performant']
Performance,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:392,race condition,race condition,392,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816,2,"['cache', 'race condition']","['cache', 'race condition']"
Performance,Lightening Tim's review load a bit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12901#issuecomment-1520908811:24,load,load,24,https://hail.is,https://github.com/hail-is/hail/pull/12901#issuecomment-1520908811,1,['load'],['load']
Performance,Linear Regression ND Performance Fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666:21,Perform,Performance,21,https://hail.is,https://github.com/hail-is/hail/pull/9666,1,['Perform'],['Performance']
Performance,"Load VCF PP as Hail PL. Load GT and GQ normally, but for the purpose of input validation, interpret them with respect to PP rather than PL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/349:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/issues/349,2,['Load'],['Load']
Performance,Load VCFs faster,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4744:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/pull/4744,1,['Load'],['Load']
Performance,Load less data in simple cases,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3626:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/pull/3626,1,['Load'],['Load']
Performance,Load partially missing calls as missing in import_vcf.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2404:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/pull/2404,1,['Load'],['Load']
Performance,Load vcf file error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/issues/6747,1,['Load'],['Load']
Performance,LoadMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2246:0,Load,LoadMatrix,0,https://hail.is,https://github.com/hail-is/hail/pull/2246,2,['Load'],['LoadMatrix']
Performance,LoadVCF fails with non-standard bases,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:0,Load,LoadVCF,0,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,LoadVCF should error on call fields that are not Number=1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3008:0,Load,LoadVCF,0,https://hail.is,https://github.com/hail-is/hail/issues/3008,1,['Load'],['LoadVCF']
Performance,"Loader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1674,Concurren,ConcurrentRestrictions,1674,https://hail.is,https://github.com/hail-is/hail/issues/11705,6,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,Loadmatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2431:0,Load,Loadmatrix,0,https://hail.is,https://github.com/hail-is/hail/pull/2431,1,['Load'],['Loadmatrix']
Performance,Logging improvements: log context calling Optimize and IR size,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5428:42,Optimiz,Optimize,42,https://hail.is,https://github.com/hail-is/hail/pull/5428,1,['Optimiz'],['Optimize']
Performance,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:978,optimiz,optimization,978,https://hail.is,https://github.com/hail-is/hail/pull/7719,4,"['Optimiz', 'optimiz']","['Optimize', 'optimization', 'optimizations']"
Performance,"Looks good, I say we put it in. I think in the future we should talk somewhere about things important to performance though that new users often get wrong (not caching appropriately, repartitioning, etc.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1900#issuecomment-307388969:105,perform,performance,105,https://hail.is,https://github.com/hail-is/hail/pull/1900#issuecomment-307388969,1,['perform'],['performance']
Performance,"Looks like slightly slower in 0.1; ```; 2018-06-26 01:47:57 Hail: INFO: Number of BGEN files parsed: 1; 2018-06-26 01:47:57 Hail: INFO: Number of samples in BGEN files: 487409; 2018-06-26 01:47:57 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-06-26 01:49:08 Hail: INFO: Coerced almost-sorted dataset; 2018-06-26 01:49:08 Hail: INFO: No multiallelics detected.; 2018-06-26 01:49:09 Hail: INFO: interval filter loaded 27 of 586 partitions; ```. I'll kill what I have and run a single regression, since this will take a long time.; ```; 2018-07-18 15:39:30 Hail: INFO: Number of BGEN files parsed: 1; 2018-07-18 15:39:30 Hail: INFO: Number of samples in BGEN files: 487409; 2018-07-18 15:39:30 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-07-18 15:40:37 Hail: INFO: Coerced almost-sorted dataset; 2018-07-18 15:40:39 Hail: INFO: interval filter loaded 5 of 293 partitions; 2018-07-18 15:43:13 Hail: WARN: 126215 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:43:13 Hail: INFO: linear_regression: running on 361194 samples for 110 response variables y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:06 Hail: WARN: 132571 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:06 Hail: INFO: linear_regression: running on 354838 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:59 Hail: WARN: 132781 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:59 Hail: INFO: linear_regression: running on 354628 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:45:42 Hail: WARN: 133165 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:45:42 Hail: INFO: linear_regression: running on 354244 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965:431,load,loaded,431,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965,2,['load'],['loaded']
Performance,"Looks like we probably want to add the following to benchmark somewhere:. ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OPENBLAS_NUM_THREADS=1; export OMP_NUM_THREADS=1; export VECLIB_MAXIMUM_THREADS=1; ```. Trying to test if setting veclib lower fixes things, but apparently Apple caches the result of the environment variable somewhere so it's unclear whether me setting it is working",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384:304,cache,caches,304,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384,1,['cache'],['caches']
Performance,"Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:79328,concurren,concurrent,79328,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:56149,concurren,concurrent,56149,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:97698,concurren,concurrent,97698,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:87062,concurren,concurrent,87062,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:63293,concurren,concurrent,63293,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 3 requested; 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 3 from BlockManagerMaster.; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 3; 2019-01-22 13:11:57 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56628) with ID 18; 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18, partition 15,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:132774,concurren,concurrent,132774,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:71889,concurren,concurrent,71889,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:119654,concurren,concurrent,119654,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115242,concurren,concurrent,115242,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:77876,concurren,concurrent,77876,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60503,concurren,concurrent,60503,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:96246,concurren,concurrent,96246,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:85610,concurren,concurrent,85610,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:67647,concurren,concurrent,67647,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:131322,concurren,concurrent,131322,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:70437,concurren,concurrent,70437,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:124008,concurren,concurrent,124008,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:113790,concurren,concurrent,113790,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:80780,concurren,concurrent,80780,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:57601,concurren,concurrent,57601,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:93344,concurren,concurrent,93344,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:89964,concurren,concurrent,89964,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:66195,concurren,concurrent,66195,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:128420,concurren,concurrent,128420,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:73341,concurren,concurrent,73341,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:122556,concurren,concurrent,122556,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:112338,concurren,concurrent,112338,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:105019,concurren,concurrent,105019,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,LoweringPipeline#apply total 4.179ms self 1.415ms children 2.764ms %children 66.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.490ms self 0.008ms children 1.482ms %children 99.45%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:193892,Optimiz,Optimize,193892,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	| ; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4660,concurren,concurrent,4660,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,"MContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:1533,perform,performs,1533,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['perform'],['performs']
Performance,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2093:134,optimiz,optimize,134,https://hail.is,https://github.com/hail-is/hail/pull/2093,2,['optimiz'],['optimize']
Performance,"Main change: add `var mark: Int` to `BaseIR`.; On profiling the benchmark `matrix_multi_write_nothing`, I noticed a significant amount of time was spent ; - iterating through zipped arrays in requiredness ; - Adding and removing elements from `HashSet`s.; In fact, half the time spent in requiredness was removing ir nodes from the `HashSet` set used as the queue! With this change, requiredness runs like a stabbed rat!. Explanation of `mark`:; This field acts as a flag that analyses can set. For example:; - `HasSharing` can use the field to see if it has visited a node before.; - `Requiredness` uses this field to tell if a node is currently enqueued. The `nextFlag` method in `IrMetadata` allows for analyses to get a fresh value they can set the `mark` field. ; This removes the need to traverse the IR after analyses to re-zero every `mark` field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13991:358,queue,queue,358,https://hail.is,https://github.com/hail-is/hail/pull/13991,1,['queue'],['queue']
Performance,Make Optimize less noisy when coming from Compile.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3999:5,Optimiz,Optimize,5,https://hail.is,https://github.com/hail-is/hail/pull/3999,1,['Optimiz'],['Optimize']
Performance,Make the parser faster; add support for cached IR nodes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4195:40,cache,cached,40,https://hail.is,https://github.com/hail-is/hail/pull/4195,1,['cache'],['cached']
Performance,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6201:647,cache,cache-from,647,https://hail.is,https://github.com/hail-is/hail/pull/6201,1,['cache'],['cache-from']
Performance,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4311,Cache,CacheManager,4311,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,6,"['Cache', 'concurren']","['CacheManager', 'concurrent']"
Performance,"Masahiro got this error message: . ```; File ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8161:774,load,loads,774,https://hail.is,https://github.com/hail-is/hail/issues/8161,1,['load'],['loads']
Performance,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7729:811,perform,performance,811,https://hail.is,https://github.com/hail-is/hail/pull/7729,1,['perform'],['performance']
Performance,"Maybe some Google API cannot handle 3 batch-drivers under full load?. Also, this docker inspect thing is just pervasive and extraordinarily annoying.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13155:63,load,load,63,https://hail.is,https://github.com/hail-is/hail/pull/13155,1,['load'],['load']
Performance,Maybe this is a better solution?. https://cloud.google.com/container-registry/docs/pulling-cached-images,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9660#issuecomment-719759217:91,cache,cached-images,91,https://hail.is,https://github.com/hail-is/hail/pull/9660#issuecomment-719759217,1,['cache'],['cached-images']
Performance,Maybe we can use GraalVM Native Image? https://docs.oracle.com/en/graalvm/enterprise/20/docs/reference-manual/native-image/Limitations/#native-image-compatibility-and-optimization-guide,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13675#issuecomment-1728007156:167,optimiz,optimization-guide,167,https://hail.is,https://github.com/hail-is/hail/issues/13675#issuecomment-1728007156,2,['optimiz'],['optimization-guide']
Performance,Maybe we should just cloud fuse the relevant bucket?. I am curious how much of a difference trimming a few files makes. I kinda figured this was mostly latency bound not bandwidth bound.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12371#issuecomment-1289868322:152,latency,latency,152,https://hail.is,https://github.com/hail-is/hail/pull/12371#issuecomment-1289868322,1,['latency'],['latency']
Performance,"Memory's local storage is a simple cache, so it is safe for k8s to; evict it from the node if the node is underutilized. https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10864:35,cache,cache,35,https://hail.is,https://github.com/hail-is/hail/pull/10864,1,['cache'],['cache']
Performance,Merge after #2147 Priority queue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2148:27,queue,queue,27,https://hail.is,https://github.com/hail-is/hail/pull/2148,1,['queue'],['queue']
Performance,Metadata loading fix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/899:9,load,loading,9,https://hail.is,https://github.com/hail-is/hail/pull/899,1,['load'],['loading']
Performance,"MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1853,Load,LoadPlink,1853,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,Minor expr optimizations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2477:11,optimiz,optimizations,11,https://hail.is,https://github.com/hail-is/hail/pull/2477,1,['optimiz'],['optimizations']
Performance,"More common case is to first do some QC, that's the case I was optimizing for. What about:. `import ... filtergenotypes --remove -c 'g.gq < 20' variantqc filtervariants --remove -c 'va.qc.callRate < 0.2' exportplink ...`. Still, I thought it might be better, too. Must be the double persist.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/136#issuecomment-184148300:63,optimiz,optimizing,63,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-184148300,1,['optimiz'],['optimizing']
Performance,"More timing info. I tried a QR / TriSolve approach for `fit` as well and it performed worse so removed it. I believe this because solving tiny systems (dimension number of covariates) is dwarfed by time spent working with vectors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:76,perform,performed,76,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833,1,['perform'],['performed']
Performance,"Mostly code reorg. Also:. moved rewriters to ir objects; call Optimize before intepreting; removed Filter{Rows, Cols} rules (non-IR), those should get folded back into the MT methods like other AST-based rules; re-enabled Fitler{Rows, Cols}IR fusion rules since logical and/or is back",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3330:62,Optimiz,Optimize,62,https://hail.is,https://github.com/hail-is/hail/pull/3330,1,['Optimiz'],['Optimize']
Performance,"Mostly infrastructure. Added NewAST base class for Matrix and KeyTable ASTs with a primitive term rewriting engine. This should eventually be a base for AST, too (because we'll want to rewrite value expressions, too). I broke VariantMetadata into two parts: VSMMetadata (static types/metdata for VSM) and VSMLocalValue (part of MatrixValue that is computed/stored on master and broadcast). Added MatrixRead, FilterSamples and FilterVariants matrix AST nodes. Simple optimizer that pushes filters into read and some minor optimizations of filters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778:466,optimiz,optimizer,466,https://hail.is,https://github.com/hail-is/hail/pull/1778,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"Moved key table type to Row, and misc performance improvements",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1583:38,perform,performance,38,https://hail.is,https://github.com/hail-is/hail/pull/1583,1,['perform'],['performance']
Performance,"Moves multi-pod deployments over to using Headless Services, which enables client-side load-balancing to the underlying pods. See #12095 for more context. The reason I put this in its own PR is that Kubernetes won't let me apply the `clusterIP: None` changes to existing `Services`, and I must delete the `Service` resources first. I can manually delete and apply new headless services in a way that is compatible with what is currently on main and with just a few seconds of downtime, but I should do this manually just before this PR merges.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094:87,load,load-balancing,87,https://hail.is,https://github.com/hail-is/hail/pull/12094,1,['load'],['load-balancing']
Performance,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2136:731,Load,LoadVCF,731,https://hail.is,https://github.com/hail-is/hail/issues/2136,6,['Load'],['LoadVCF']
Performance,"Much better! I understand what's going on now. Just to make sure I understand where the performance improvements are, we don't wait for all JVMs to be intitialized before accepting JVM jobs and the queue is FIFO so we reuse the same JVMs that are warm already?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13870#issuecomment-1775410645:88,perform,performance,88,https://hail.is,https://github.com/hail-is/hail/pull/13870#issuecomment-1775410645,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"Multiple users in HPC, Possible Concurrency/Threading problem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:32,Concurren,Concurrency,32,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['Concurren'],['Concurrency']
Performance,"My large test worked in my namespace. The docs were able to build. They're a bit confusing with the enum object, but I'm not sure how to easily fix it. The key things to look for are the scheduler query matches the sort order of the control loop query. If that's off, then instances will thrash. Once you're good with this then we can do a load test sometime tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1270642090:340,load,load,340,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1270642090,1,['load'],['load']
Performance,"NB, this is a stacked PR. To see just these changes see [this commit](https://github.com/hail-is/hail/pull/12883/commits/ae51e0a9af12e4c89a44e7ce3235f3f665ff4830). ---. [VPC Flow Logs](https://cloud.google.com/vpc/docs/flow-logs):. > VPC Flow Logs records a sample of network flows sent from and received by VM instances, including; > instances used as Google Kubernetes Engine nodes. These logs can be used for network monitoring,; > forensics, real-time security analysis, and expense optimization. I found the collection process the most elucidating part of the documentation. My summary of that; process follows:. 1. Packets are sampled on the network interface of a VM. Google claims an average sampling rate of; 1/30. This rate reduces if the VM is under load. This rate is immutable to us. 2. Within an ""aggregation interval"", packets are aggregated into ""records"" which are keyed (my term); by source & destination. There are currently six choices for aggregation interval: 5s, 30s, 1m,; 5m, 10m, and 15m. 3. Records are sampled. The sampling rate is a user configured floating point number (precision; unclear) between 0 and 1. 4. Metadata is optionally added to the records. The metadata captures information about the source; and destination VM such as project id, VM name, zone, region, GKE pod, GKE service, and geographic; information of external parties. The user may elect to receive all metadata, no metadata, or a; specific set of metadata fields. 5. The records are written to Google Cloud Logging. The pricing of VPC Flow Logs is described at the [network pricing page](https://cloud.google.com/vpc/network-pricing#network-telemetry). Notice that, if logs are only sent to Cloud Logging (not to BigQuery, Pub/Sub, or Cloud Storage):. > If you store your logs in Cloud Logging, logs generation charges are waived, and only Logging charges apply. I believe in this phrase ""logs generation charges"" refers to *VPC Flow logs* generation charges. The Google Cloud Logging [pricing page]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12883:487,optimiz,optimization,487,https://hail.is,https://github.com/hail-is/hail/pull/12883,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"NG: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatRe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:1909,load,loadClass,1909,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['load'],['loadClass']
Performance,Needed by https://github.com/hail-is/hail/pull/2097. Will address performance once Row is gone and we're pure unsafe.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2089#issuecomment-322304632:66,perform,performance,66,https://hail.is,https://github.com/hail-is/hail/pull/2089#issuecomment-322304632,1,['perform'],['performance']
Performance,New load and extracts for hail datasets,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6803:4,load,load,4,https://hail.is,https://github.com/hail-is/hail/pull/6803,1,['load'],['load']
Performance,"Nice, it works now! I flipped through uses of `EmitCode.setup`, looking for places that might cause problems when `m` was more than a load of a boolean. I only found a couple, and fixed them. Also note that `EmitCode.fromI` also makes an empty `setup`, with all the work done in `m`, so we were already assuming `m` can do work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9925#issuecomment-769276725:134,load,load,134,https://hail.is,https://github.com/hail-is/hail/pull/9925#issuecomment-769276725,1,['load'],['load']
Performance,No detectable difference in performance:. ```; $ hail-bench compare /tmp/before.json /tmp/after2.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; matrix_table_decode_and_count 101.5% 4.216 4.278; ----------------------; Geometric mean: 101.5%; Simple mean: 101.5%; Median: 101.5%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7143:28,perform,performance,28,https://hail.is,https://github.com/hail-is/hail/pull/7143,1,['perform'],['performance']
Performance,No error checking on LoadVCF if user-provided header file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2437:21,Load,LoadVCF,21,https://hail.is,https://github.com/hail-is/hail/pull/2437,1,['Load'],['LoadVCF']
Performance,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248646084:74,load,loading,74,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084,1,['load'],['loading']
Performance,"Non preemptible for that reason seems fine. . However, I think I have a fundamental misunderstanding. In my experience thus far, Prometheus will need more than 30GB of RAM if anyone runs thousands of pods on our cluster for an hour or more. Is that not your understanding? Last time I ran the test Prometheus wasn’t able to start after crashing. Also the per-pod Graphana graphs loaded noticeably slower than everything else.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6774#issuecomment-519120787:379,load,loaded,379,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-519120787,1,['load'],['loaded']
Performance,Non specific performance issue,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/207#issuecomment-301788879:13,perform,performance,13,https://hail.is,https://github.com/hail-is/hail/issues/207#issuecomment-301788879,1,['perform'],['performance']
Performance,"Not all of the Hail Tables and MatrixTables that are publicly available on the gnomAD [downloads](https://gnomad.broadinstitute.org/downloads) page are currently available in the Datasets API/Annotation DB. . This PR makes the following changes to the datasets available via the Hail Datasets API/Annotation DB:. - Add `gnomad_genome_sites` Table, versions: 3.1.1, 3.1.2; - Add `gnomad_hgdp_1kg_subset_dense` MatrixTable, version: 3.1.2; - Rename `gnomad_hgdp_1kg_callset` MatrixTable to `gnomad_hgdp_1kg_subset_dense`, version: 3.1; - Add `gnomad_hgdp_1kg_subset_sparse` MatrixTable, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_sample_metadata` Table, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_variant_annotations` Table, version: 3.1.2; - Add `gnomad_variant_co-occurrence` Table, version: 2.1.1; - Add `gnomad_pca_variant_loadings` Table, versions: 2.1, 3.1. Other general changes:. - Add/update the schema `.rst` files, for the datasets listed above, for the [docs](https://hail.is/docs/0.2/datasets/schemas.html); - Set the example dataset loaded in `hl.experimental.load_dataset` to be the most recent `gnomad_hgdp_1kg_subset_dense` MatrixTable (version 3.1.2)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11608:1051,load,loaded,1051,https://hail.is,https://github.com/hail-is/hail/pull/11608,1,['load'],['loaded']
Performance,"Not sure this is better, but basically my logic is to kill the `crun run` process first. Wait for it to be killed. Set the process = None. Then check if a container exists. If the container exists, then do a deep kill of all child subprocesses. I'm not sure I completely understand where the race condition could occur here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10855#issuecomment-942624612:292,race condition,race condition,292,https://hail.is,https://github.com/hail-is/hail/pull/10855#issuecomment-942624612,1,['race condition'],['race condition']
Performance,"Note this PR replaces the previous [Feature/sas token merge](https://github.com/hail-is/hail/pull/12877) because the original PR branch got jacked up beyond repair. All the comments on the earlier PR are responded to there and addressed in the code for this one. This PR is to enable `hail-az/https` Azure file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; - Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; - Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new azure-mgmt-storage package requirement.; - Updated `AzureAsyncFS` to use `(account, container, credential)` tuple as internal `BlobServiceClient` cache key; - Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token; - Update `RouterFS.ls` function and associated listfiles function to allow for trailing query strings during path traversal; - Update `AsyncFS.open_from` function to handle query-string urls in zero-length case; - Change to existing behavior: `LocalAsyncFSURL.__str__` no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; - Updated `InputResource` to not include the SAS token as part of the destination file name; - Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions to respect the new model, where it is no longer safe to extend URLs by just appending new segments with `+ ""/""` because there may be a query string, and added `'sas/azure-https'` test case to the fixture. Running tests for the SAS case requires some new test variables to allow the test code to generate SAS toke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140:909,cache,cache,909,https://hail.is,https://github.com/hail-is/hail/pull/13140,1,['cache'],['cache']
Performance,"Now all loaders have class names `Load*` and all Suites have class names `Import*Suite` and `Export*Suite`. Plink, BGEN, and GEN were inconsistent within and with VCF, Matrix, GDB, MatrixParser, etc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2955#issuecomment-367512565:8,load,loaders,8,https://hail.is,https://github.com/hail-is/hail/pull/2955#issuecomment-367512565,2,"['Load', 'load']","['Load', 'loaders']"
Performance,"Now that test databases are hosted on their own servers instead of the single cloud-hosted MySQL, we can ramp up the parallelism both in our tests and in the number of PRs that we run at once. I recall that even before we had this DB bottleneck we still restricted the number of PRs running at once for cost reasons, but if that's not the case we could remove that restriction entirely.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12974:234,bottleneck,bottleneck,234,https://hail.is,https://github.com/hail-is/hail/pull/12974,1,['bottleneck'],['bottleneck']
Performance,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520:129,perform,perform,129,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520,4,"['load', 'perform']","['load', 'perform']"
Performance,O: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2584) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishAndReturnString(UTF8StreamJsonPars,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2985,concurren,concurrent,2985,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,"OK, I added some code to empty the cache when we `hl.stop`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1241334157:35,cache,cache,35,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1241334157,1,['cache'],['cache']
Performance,"OK, I figured out what was happening. The problem wasn't with cerberus (although I'm happy to with my change), it is that json.dump always converts a dictionary key into a string. I had with a key None, and it got turned into the string 'null' in json, because json object values must string keys:. ```; >>> import json; >>> d = {None: 5, 'foo': None}; >>> json.loads(json.dumps(d)); {'null': 5, 'foo': None}; ```. I remove the broken test. Note, I pushed two more changes that probably need a proper review:; - moved jobs validation to batch (from batch_client), I'd been meaning to do that,; - and I wrote the batch validator explicit in the style of the jobs validator (I'd be meaning to do that, too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836:362,load,loads,362,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836,1,['load'],['loads']
Performance,"OK, I left in the optimization in multi-way join but also added it to RVD.repartition. This is the best I can see how to do. I realize I wanted `satisfiesAllowedOverlap(key.length - 1)`, not `satisfiesAllowedOverlap(0)`. It's just the case I was working with had one key. Fixed. That might clarify some confusion. Also added a strictify to generate. I think the existing code was wrong without that. @patrick-schultz can you take another look?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424#issuecomment-467080098:18,optimiz,optimization,18,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467080098,1,['optimiz'],['optimization']
Performance,"OK, I made a suite of additional changes:; - create docker/requirements.txt,; - batch doesn't use conda,; - pr-builder (rebuilt) installs docker/requirements.txt (same requirements as base image),; - put Spark in base image, removed spark-base image,; - set ENV IN_HAIL_CI=1 in hail-ci scripts,; - pull remote images and use --cache-from, allow push and deploy in CI, build locally only otherwise",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687:327,cache,cache-from,327,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687,1,['cache'],['cache-from']
Performance,"OK, I reimplemented the sync-er in Python. This works well enough though it would benefit from something that waited for changes to settle down and did one copy-restart. Currently, you can queue up a bunch of changes and it sometimes take as long as 5 seconds for the whole system to settle down enough that you can refresh and get the new page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089:189,queue,queue,189,https://hail.is,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089,1,['queue'],['queue']
Performance,"OK, I think I'm done. To be honest, I don't like the latest version any better than what I had a week; ago: expressing the buffer management with ownership and move semantics results in about 30-40; extra lines of code to do something which was expressed more concisely in the naked-pointer; version (and we've taken it from being understandable to most programmers, to being understandable only by people familiar with C++ ownership/move-semantics idioms). And I don't; think there's going to be any measurable performance improvement from what we started with. But it is what it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-397410855:512,perform,performance,512,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-397410855,1,['perform'],['performance']
Performance,"OK, I will run the stress test later this week. It sounds like the stress test is really important to convincing us that batch is correct. I want to migrate that from a manual process into a normal test. I'm going to add a performance test that requires a 10,000 or 1,000 job /bin/true batch to finish in ~8 or 0.8 minutes. Let's see if we can add correctness checks to that that satisfy the needs of stress.py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985#issuecomment-951320966:223,perform,performance,223,https://hail.is,https://github.com/hail-is/hail/pull/10985#issuecomment-951320966,1,['perform'],['performance']
Performance,"OK, crap, this is pretty broken. I think the code cache is holding onto broadcast pointers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10969#issuecomment-942292720:50,cache,cache,50,https://hail.is,https://github.com/hail-is/hail/pull/10969#issuecomment-942292720,1,['cache'],['cache']
Performance,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385:864,cache,cache,864,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385,4,['cache'],['cache']
Performance,"OK, if I add the following additional dependencies in gradle:. ```; 	include(dependency('net.sourceforge.f2j:arpack_combined_all:0.1')); 	include(dependency('com.github.fommil.netlib:native_system-java:1.1')); 	include(dependency('com.github.fommil.netlib:netlib-native_system-linux-x86_64:1.1')); 	include(dependency('com.github.fommil.netlib:netlib-native_ref-linux-x86_64:1.1')); 	include(dependency('com.github.fommil:jniloader:1.1')); ```. it correctly loads on Linux:. > 2018-04-30 00:13:07 JniLoader: INFO: successfully loaded /tmp/jniloader8608320282306924695netlib-native_system-linux-x86_64.so. I'll test the analog on OSX tomorrow. Are you sure we're getting natives on Dataproc now? This definitely worked in the past. I get a 4x speedup (in the 1024 cases):. 214 ms ± 19.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). Now we're only 35x slower.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385310865:458,load,loads,458,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385310865,2,['load'],"['loaded', 'loads']"
Performance,"OK, now passes the tests. Not sure if `minRep` should be called in `VariantSubgen` or in `LoadBGenTest` (as I did). All depends on how `VariantSubgen` is used (e.g. if testing minRep)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402:90,Load,LoadBGenTest,90,https://hail.is,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402,1,['Load'],['LoadBGenTest']
Performance,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8205:227,scalab,scalable,227,https://hail.is,https://github.com/hail-is/hail/pull/8205,6,"['multi-thread', 'optimiz', 'perform', 'scalab']","['multi-threaded', 'optimizer', 'optimizes', 'performance', 'scalable']"
Performance,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:259,race condition,race condition,259,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312,1,['race condition'],['race condition']
Performance,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8247:606,load,load,606,https://hail.is,https://github.com/hail-is/hail/pull/8247,8,['load'],"['load', 'loadElement', 'loadField']"
Performance,"OK, this passes all the tests except for `test_vcf_parser_golden_master__gvcf_GRCh37` which inexplicably hangs. I've marked that as skip. I've attached the WIP tag because the longest tests now take 47 minutes. I'll leave this PR up as a canary for when a `main` change fails service tests. However, I won't merge it until we improve test latency. cc: @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11444#issuecomment-1069516954:339,latency,latency,339,https://hail.is,https://github.com/hail-is/hail/pull/11444#issuecomment-1069516954,1,['latency'],['latency']
Performance,"OK, this should be ready to go. I had to make one more fix, the HailClassLoader parent had to be the Hail jar class loader (via the class loader of HailClassLoader) instead of the system class loader (the default), otherwise AsmFunctionN was not found when executing compiled code. I'm not quite sure why this isn't failing elsewhere. I'm running dockerized vanilla Spark 2.2.0.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4554#issuecomment-430455067:116,load,loader,116,https://hail.is,https://github.com/hail-is/hail/pull/4554#issuecomment-430455067,3,['load'],['loader']
Performance,"OK.; ```; [root@tele-1 ~]# pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 09:10:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 09:10:21 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 09:10:21 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 09:10:21 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:659,load,load,659,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,1,['load'],['load']
Performance,"Oh yeah, to get my code to work you need to comment out line 778 `gene_id=ch_ht.gene_ids,` in `_annotated_comp_het_variant`. It doesn't break search to be missing that annotation, it just has some downstream display affects that I would need to fix if I actually wanted to use the code, but given the performance hit I wasn't sure it was worth figuring that out as this code may be too slow to use. I was not able to get the code you provided here to run either, but one concern I have with it is that the unique combinations are computed per gene, but if you have a pair of variants that are each in the same 2 genes, you would get the pair twice in the results, one for each gene. The error I get when I run the code you provide is; ```; ""Key type mismatch: cannot index table with given expressions:; Table key: <<<empty key>>>; Index Expressions: locus<GRCh38>, array<str>, set<str>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, struct{z_score: float32}, struct{region_type_ids: array<int32>}, locus<GRCh37>, str, array<struct{amino_acids: str, canonical: int32, codons: str, gene_id: str, hgvsc: str, hgvsp: str, transcript_id: str, biotype_id: int32, consequence_term_ids: array<int32>, is_lof_nagnag: bool, lof_filter_ids: array<int32>, transcript_rank: int32}>, str, int64, struct{PHRED: float32}, struct{alleleId: int32, conflictingPathogenicities: array<struct{pathogenicity_id: int32, count: int32}>, goldStars: int32, pathogenicity_id: int32, assertion_ids: array<int32>}, struct{REVEL_score: float32, VEST4_score: float32, MutPred_score: float32, SIFT_pred_id: int32, Polyphen2_HVAR_pred_id: int32, MutationTaster_pred_id: int32, fathmm_MKL_coding_pred_id: int32}, struct{Eigen_phred: float32}, struct{AF_POPMAX: float3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465:301,perform,performance,301,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465,1,['perform'],['performance']
Performance,"Oh, I didn't realize we're running pre- and post-condition checks on every pass. I would think the main reason to require the post-condition of one pass to match the pre-condition of the next (besides general hygiene) is to only have to do the check once. Anyways, I agree this seems fine for now. I think the root of the issue is that we're assuming that `Optimize` preserves all possible `IRState`s, which is a pretty bold claim. Eventually we should probably refactor `Optimize` to separate out rules that apply to different `IRState`s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9030#issuecomment-651982433:357,Optimiz,Optimize,357,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651982433,2,['Optimiz'],['Optimize']
Performance,"Oh, good catch. This is something we can also test for in the performance test suite: compute and use twice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5416#issuecomment-466480355:62,perform,performance,62,https://hail.is,https://github.com/hail-is/hail/pull/5416#issuecomment-466480355,1,['perform'],['performance']
Performance,"Oh, sorry, I see the problem. I had a PR recently that changed the Array to IndexedSeq, it seems like this didn't get retested/blocked. The usual race condition (although I don't know why this didn't get re-run, that was ages ago.) We really need to drop TC and get a better model.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3877#issuecomment-401561294:146,race condition,race condition,146,https://hail.is,https://github.com/hail-is/hail/pull/3877#issuecomment-401561294,1,['race condition'],['race condition']
Performance,"Oh, that's a dumb and easily fixed bug. Sorry about that. The enlargeToRange should only be used for right and outer joins. With that fixed, it will only load partitions from the right that could possibly overlap those of the left. There is definitely room for optimization in making a smart choice of partitioner for the join result, not just using the (possibly enlarged) left partitioner, but that is a harder problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3685#issuecomment-393174426:154,load,load,154,https://hail.is,https://github.com/hail-is/hail/issues/3685#issuecomment-393174426,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"Ok, I think this is better. Every read, we cache both file pointers that can refer to the end of the current block. If we would use the smaller one, use the bigger one instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9304#issuecomment-676303474:43,cache,cache,43,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676303474,1,['cache'],['cache']
Performance,"Ok, so Cotton's new thing means emitting separate methods by hand is not a thing we do anymore. But there are two factors hurting the benchmark. . One is that the benchmark is hiding the fact that we are spending ~25 seconds serializing and de-serializing JSON for this ndarray. So the real comparison is more like 55 seconds vs 75 seconds, which is a roughly 25% speed improvement. . The other is that `hl.nd.ones` is just an alias for `hl.nd.array(hl.range(shape_product)).map(lambda x: 1).reshape((n_rows, n_cols))`. This is going to create a bunch of row major data, copy it to column major in a pretty cache inefficient way during the reshape, then do the additions. So that's eating some of the time too. We should probably have a way for all the constant methods to not go through regular array. . Anyway, 25% improvement + better interface is a win for now, we can revisit ways to make this faster in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293:607,cache,cache,607,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293,1,['cache'],['cache']
Performance,"Ok. famous last words, but I think we're in good shape here. I think Daniel if you can do one last pass on the most recent changes, that would be great and then we'll have time to do a load test to make sure the behavior is still good and instances aren't thrashing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1271995498:185,load,load,185,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1271995498,1,['load'],['load']
Performance,Old but relevant: http://stackoverflow.com/questions/30004295/dataframe-save-sqlcontext-load-loses-nullable-status-of-schema,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1421#issuecomment-281760846:88,load,load-loses-nullable-status-of-schema,88,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-281760846,1,['load'],['load-loses-nullable-status-of-schema']
Performance,"On 100M row Table created with:. ```; import hail as hl; mt = hl.utils.range_table(10000000, n_partitions=32); mt = mt.annotate(sq = mt.idx * mt.idx, name = hl.str(mt.idx)); mt.write('long.ht', overwrite=True); ```. I timed performance. ```; In [2]: %%timeit; ...: hl.read_table('long.ht')._force_count(); ```. which was identical:. outputmetrics: 5.41 s ± 131 ms per loop (mean ± std. dev. of 7 runs, 1 loop each); master: 5.41 s ± 118 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). Since I'm adding output metrics here, I also time writing 1M rows. Nearly identical:. master: 2.88 s ± 80 ms per loop (mean ± std. dev. of 7 runs, 1 loop each); outputmetrics: 2.89 s ± 81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3567#issuecomment-388563769:224,perform,performance,224,https://hail.is,https://github.com/hail-is/hail/pull/3567#issuecomment-388563769,1,['perform'],['performance']
Performance,"On Azure, one of the tests timed out with 500 responses from the server. I'll need to debug in GCP, but the PR queue is long right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1960073457:111,queue,queue,111,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1960073457,1,['queue'],['queue']
Performance,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ​I get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:413,cache,cached,413,https://hail.is,https://github.com/hail-is/hail/issues/10197,3,['cache'],['cached']
Performance,"On a test of 1-2 partitions with 5000 samples, this takes the second stage of a densify from 2 minutes down to 1.4 minutes (only loading GT).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6967:129,load,loading,129,https://hail.is,https://github.com/hail-is/hail/pull/6967,1,['load'],['loading']
Performance,"On std::unique_ptr, I may be a contrarian, but I don't care what the ""C++ community"" thinks about it.; If you buy into using std::unique_ptr<T>, then everyone who writes or reads the code has to get ; their head around the massively confusing and counter-intuitive concept of move semantics (a ; form of assignment which modifies the source) *and* the somewhat bizarre terminology and syntax; used to express that in C++. And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). . I would be fine with that extra learning curve and complexity if unique_ptr<T> solved a difficult; problem. But - by definition! - it doesn't. It only works for the easy case where you have one; pointer to each object. And anywhere that you *might* want to use unique_ptr<T>, shared_ptr<T> provides a superset; of the functionality at only a small extra cost in memory and runtime. So my rule is, if you need; a smart pointer, use shared_ptr<T>. And if there's some place where the memory or performance; cost of shared_ptr<T> is truly proved to be painful, then use a few raw pointers where absolutely; necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396320515:1145,perform,performance,1145,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396320515,2,['perform'],['performance']
Performance,"On this note, could the keys that the join was actually performed on be printed separately? Would make a nice sanity check",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4485#issuecomment-429003951:56,perform,performed,56,https://hail.is,https://github.com/hail-is/hail/issues/4485#issuecomment-429003951,1,['perform'],['performed']
Performance,"Once the PR's for Coalesce and Die go in, the only remaining item for the InferPType pass is CastRename...then I think we can start performing this pass during compilation/interpretation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6952#issuecomment-573314993:132,perform,performing,132,https://hail.is,https://github.com/hail-is/hail/issues/6952#issuecomment-573314993,1,['perform'],['performing']
Performance,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:324,load,load,324,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842,1,['load'],['load']
Performance,"One more time, with feeling! (was: #10072). - [x] (@tpoterba) a1f3b2a5c9 add fails_service_backend; - [ ] (@tpoterba, @cseed) dc0bee7ce1 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) 4b663be367 [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) d3c1f0987c [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) aab6ba98be [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) a1619cff36 [query-service] make user cache thread-safe; - [ ] (@tpoterba) c315fcb0b1 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) 912c21f709 [shuffler] log ShuffleCodecSpec anytime it is created; - [x] (@daniel-goldstein) c2495837e7 [scala-lsm] bugfix: least key may equal greatest key; - [x] (@daniel-goldstein) 5fb3db703e [services] discovered new transient error; - [x] (@daniel-goldstein) 9cd0999938 [shuffler] more assertions in ShuffleClient; - [x] (@daniel-goldstein) a71a3c9b8c [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [x] (@daniel-goldstein) 41b06aeaa8 [query-service] move hail.jar earlier in Dockerfile; - [x] (@daniel-goldstein) 8df4029698 [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 0354e1f557 [query-service] simplify socket handling; - [x] (@jigold) 6690a4decc [batch] teach JVMJob where to find the hail configuration files; - [x] (@daniel-goldstein) ae2e3d2996 [query-service] switch to services team approved logging; - [ ] (@tpoterba) b18f86e647 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 6d5d0b68af [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) 0d42df8b08 [query-service] run tests against query service; - [x] (@jigold) f9d361e686 [query-service] aiohttp.ClientSession must be created in async code; - [ ] (@cseed) c35f2e10e3 [query-service][hail][build.yaml] address miscellaneous comments from cotton; - [x]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:498,cache,cache,498,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['cache'],['cache']
Performance,"One obvious point of optimization that Dan already identified is the way we were keying our data is causing full shuffles, as we were keying the data by a string variant ID in the form `1-32683987-ACTCTT-A` instead of locus-allele. Changing back to keying on locus-allele fixes this issue for our more straightforward searches, but we have a search that looks for pairs of possible compound heterozygous variants in the same gene, and that still is resulting in 2 full shuffles. I'm a little at a loss for how to fix this, because we are grouping by an unsorted field so I'm not sure how to prevent us from working with an unsorted dataset. The offending code right now is as follows (somewhat simplified for readability):. ```; primary_variants = hl.agg.filter(ch_ht[HAS_ALLOWED_ANNOTATION], hl.agg.collect(ch_ht.row)); secondary_variants = hl.agg.filter(ch_ht[HAS_ALLOWED_SECONDARY_ANNOTATION], hl.agg.collect(ch_ht.row)); ch_ht = ch_ht.group_by('gene_ids').aggregate(v1=primary_variants, v2=secondary_variants); ch_ht = ch_ht.explode(ch_ht.v1); ch_ht = ch_ht.explode(ch_ht.v2); ch_ht = ch_ht.annotate(grouped_variants=hl.sorted([ch_ht.v1, ch_ht.v2], key=lambda v: (v.locus, v.alleles))); ch_ht = ch_ht.key_by(; locus=ch_ht.grouped_variants[0].locus, ; alleles=ch_ht.grouped_variants[0].alleles,; locus2=ch_ht.grouped_variants[1].locus, ; alleles2=ch_ht.grouped_variants[1].alleles,; ); ch_ht = ch_ht.distinct(); ...; # more filtering and annotating; ...; return ch_ht._key_by_assert_sorted('locus', 'alleles'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1776044401:21,optimiz,optimization,21,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1776044401,2,['optimiz'],['optimization']
Performance,"One option is to extend NormalizeNames to take a prefix, and use a uid as a prefix when calling NormalizeNames inside ForwardLets. . Another option is to add an option to NOT run NormalizeNames inside the optimizer, when we known that there cannot be name collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5710#issuecomment-483001807:205,optimiz,optimizer,205,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001807,1,['optimiz'],['optimizer']
Performance,One option is to treat IR execution as a job pushed into a queue; would have the nice property of persistence in the case that apiserver dies before request is fulfilled,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5898#issuecomment-484217879:59,queue,queue,59,https://hail.is,https://github.com/hail-is/hail/issues/5898#issuecomment-484217879,1,['queue'],['queue']
Performance,Only if you're doing something like write or PCA. Definitely query optimizer territory,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/238#issuecomment-256328775:67,optimiz,optimizer,67,https://hail.is,https://github.com/hail-is/hail/issues/238#issuecomment-256328775,1,['optimiz'],['optimizer']
Performance,Optimize IR generated in ArrayAgg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5765:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/5765,1,['Optimiz'],['Optimize']
Performance,Optimize MatrixMapCols with aggregation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4174:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/4174,1,['Optimiz'],['Optimize']
Performance,Optimize MatrixValue.entriesRVD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3950:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/3950,1,['Optimiz'],['Optimize']
Performance,Optimize PCA to LinReg transition,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/60:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/60,1,['Optimiz'],['Optimize']
Performance,Optimize TSV export using local compression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1707:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/1707,1,['Optimiz'],['Optimize']
Performance,Optimize TableKeyBy(TableFilterIntervals(TableKeyBy)),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6147:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/6147,1,['Optimiz'],['Optimize']
Performance,Optimize import annotations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/824,1,['Optimiz'],['Optimize']
Performance,"Optimize lowered TableHead to not scan a partition past the requested number of rows. Currently TableHead performs a loop, calculating the lengths of the first 4 partitions, then the first 16, etc. If we fix it to not count a partition multiple times, instead starting each loop at the first uncounted partition, we can further optimize to stop scanning a partition after the number of rows still needed (requested number minus sum of partitions counted so far). Note that there is no similar optimization to TableTail. We must compute the full length of each partition, to compute how many rows to drop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9637:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/9637,4,"['Optimiz', 'optimiz', 'perform']","['Optimize', 'optimization', 'optimize', 'performs']"
Performance,Optimize sampleqc and various.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/93:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/93,1,['Optimiz'],['Optimize']
Performance,Optimized BGEN import.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1551:0,Optimiz,Optimized,0,https://hail.is,https://github.com/hail-is/hail/pull/1551,1,['Optimiz'],['Optimized']
Performance,Optimized coalesce for VSMs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/681:0,Optimiz,Optimized,0,https://hail.is,https://github.com/hail-is/hail/pull/681,2,['Optimiz'],['Optimized']
Performance,Optimized sampleqc for (fixed) VSM structure. Added; downsamplevariants. Make sure all file IO goes through hadoop IO; interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/93:0,Optimiz,Optimized,0,https://hail.is,https://github.com/hail-is/hail/pull/93,1,['Optimiz'],['Optimized']
Performance,OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:5473,concurren,concurrent,5473,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['concurren'],['concurrent']
Performance,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:7337,concurren,concurrent,7337,https://hail.is,https://github.com/hail-is/hail/issues/3015,7,"['Load', 'concurren']","['LoadVCF', 'concurrent']"
Performance,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11040:11,cache,cache,11,https://hail.is,https://github.com/hail-is/hail/pull/11040,2,['cache'],['cache']
Performance,"Out of curiosity, what was the reason that this kind of functionality shouldn't be a core method? . I saw [pc_project](https://github.com/macarthur-lab/gnomad_hail/blob/537cb9dd19c4a854a9ec7f29e552129081598399/utils/generic.py#L105) (mentioned in this [thread](https://discuss.hail.is/t/pca-to-output-allele-frequencies-alongside-loadings/439/4)) and that function stands out to me amongst all the gnomad other utilities as being particularly useful for many applications. Separately, where do most contrib functions end up? Other than gnomad, would you recommend any good collections of generic functionality?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/442#issuecomment-587074535:330,load,loadings,330,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-587074535,1,['load'],['loadings']
Performance,"Oy! If you `grep netlib hail.log`, do you see natives loading or failing to load? I'll check on my end too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385265623:54,load,loading,54,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385265623,2,['load'],"['load', 'loading']"
Performance,PCA loadings don't drop scores/eigenvalues,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5554:4,load,loadings,4,https://hail.is,https://github.com/hail-is/hail/issues/5554,1,['load'],['loadings']
Performance,"PIs for configuring Grafana settings.</li>; <li>api-change:<code>rbin</code>: This release adds support for Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.29.15</h1>; <ul>; <li>bugfix:Endpoints: Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.29.14</h1>; <ul>; <li>api-change:<code>route53</code>: Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/botocore/commit/f0dd67f9b7cc2791f301f3fd135f0c97d9c66bae""><code>f0dd67f</code></a> Merge branch 'release-1.29.16'</li>; <li><a href=""https://github.com/boto/botocore/commit/22c3cb362c0ef00c6de404140f06a14d0e195f39""><code>22c3cb3</code></a> Bumping version to 1.29.16</li>; <li><a href=""https://github.com/boto/botocore/commit/4aa5f864b62b6193ed0729a4ac71c010877fe377""><code>4aa5f86</code></a> Update to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/a7e153d9c822fae5c55d30ef476bdf4f55a4d027""><code>a7e153d</code></a> Merge branch 'release-1.29.15'</li>; <li><a href=""https://github.com/boto/botocore/commit/a942b57854dd35a37766d7973c3fb980a2de4068""><code>a942b57</code></a> Merge branch 'release-1.29.15' into develop</li>; <li><a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12503:1465,latency,latency,1465,https://hail.is,https://github.com/hail-is/hail/pull/12503,1,['latency'],['latency']
Performance,"PRing for test suite, but it's mostly working. . Todo:. - [x] Optimization for already sorted tables; - [x] ~~Handle sort by descending~~ (deferred to subsequent PR); - [x] Handle tables with no partitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11050:62,Optimiz,Optimization,62,https://hail.is,https://github.com/hail-is/hail/pull/11050,1,['Optimiz'],['Optimization']
Performance,PTypes 21: Remove TContainer.loadElement (with Code),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4628:29,load,loadElement,29,https://hail.is,https://github.com/hail-is/hail/pull/4628,1,['load'],['loadElement']
Performance,PTypes 23: Convert more references to TContainer.loadElement,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4631:49,load,loadElement,49,https://hail.is,https://github.com/hail-is/hail/pull/4631,1,['load'],['loadElement']
Performance,Part 1 of chipping away at config.mk. This puts the two make targets for building the vm image in GCP into a single script. It loads variables that used to come from config.mk from kubernetes. Added a convenience function to offer a confirmation prompt before running the script. Here's an example:. ```; (hailenv) dgoldste@wmce3-cb7 hail % $HAIL/batch/gcp-create-worker-image.sh; Building image with properties:; Version: 12; Project: hail-vdc; Zone: us-central1-a; Are you sure? [y/N] n; (hailenv) dgoldste@wmce3-cb7 hail %; ```. Tested by running with a high image version number (3010 to be precise),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11327:127,load,loads,127,https://hail.is,https://github.com/hail-is/hail/pull/11327,1,['load'],['loads']
Performance,"Patrick and I discussed this on Friday 03/29. Two observations:. 1. We have to make sure key encodings on both sides of a join are comparable. ; Say we're performing a join of two streams a and b, and the key contains an array field.; After analysis, we discover that a has missing elements and n b does not.; We have to unify the encoding plan such that we encode missingness for both a and b.; Otherwise comparison of the two encodings will be meaningless. 2. We have to encode nulls differently for either side of a join.; We need to ensure that null key fields on the left don't match with null key fields on the right (ie have a left and right null encoding).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14403#issuecomment-2027422718:155,perform,performing,155,https://hail.is,https://github.com/hail-is/hail/pull/14403#issuecomment-2027422718,1,['perform'],['performing']
Performance,Performance boost for sites-only VDS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1824:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/pull/1824,1,['Perform'],['Performance']
Performance,"Performance is close, if slightly worse. Could be laptop load differences. Insignificant, this is a great balance. {""config"": {""cores"": 1, ""version"": ""0.2.28-42f5ab7d9617"", ""timestamp"": ""2019-12-04 19:20:11.757847"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [25.609369775000005, 25.694102771999994, 26.285334770000006]}]}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098,2,"['Perform', 'load']","['Performance', 'load']"
Performance,"Performance note:; to do an aggregation - export sites pipeline, master took 7m, this branch took 14s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/954#issuecomment-253405048:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/pull/954#issuecomment-253405048,1,['Perform'],['Performance']
Performance,Performance optimizations for shuffling small rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4457:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/pull/4457,2,"['Perform', 'optimiz']","['Performance', 'optimizations']"
Performance,Performance regression in import_vcf - sorting coercion now happens on every evaluation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9064:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/issues/9064,1,['Perform'],['Performance']
Performance,Persist and cache return a VDS so we can persist again,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1380:12,cache,cache,12,https://hail.is,https://github.com/hail-is/hail/pull/1380,1,['cache'],['cache']
Performance,"Picking up where #13776 left off. CHANGELOG: improved speed of reading hail format datasets from disk. This PR speeds up decoding arrays in two main ways:; * instead of calling `arrayType.isElementDefined(array, i)` on every single array element, which expands to; ```scala; val b = aoff + lengthHeaderBytes + (i >> 3); !((Memory.loadByte(b) & (1 << (i & 7).toInt)) != 0); ```; process elements in groups of 64, and load the corresponding long of missing bits once; * once we have a whole long of missing bits, we can be smarter than branching on each bit. After flipping to get `presentBits`, we use the following psuedocode to extract the positions of the set bits, with time proportional to the number of set bits:; ```; while (presentBits != 0) {; val idx = java.lang.Long.numberOfTrailingZeroes(presentBits); // do something with idx; presentBits = presentBits & (presentBits - 1) // unsets the rightmost set bit; }; ```. To avoid needing to handle the last block of 64 elements differently, this PR changes the layout of `PCanonicalArray` to ensure the missing bits are always padded out to a multiple of 64 bits. They were already padded to a multiple of 32, and I don't expect this change to have much of an effect. But if needed, blocking by 32 elements instead had very similar performance in my benchmarks. I also experimented with unrolling loops. In the non-missing case, this is easy. In the missing case, I tried using `if (presentBits.bitCount >= 8)` to guard an unrolled inner loop. In both cases, unrolling was if anything slower. Dan observed benefit from unrolling, but that was combined with the first optimization above (not loading a bit from memory every element), which I beleive was the real source of improvement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13787:330,load,loadByte,330,https://hail.is,https://github.com/hail-is/hail/pull/13787,5,"['load', 'optimiz', 'perform']","['load', 'loadByte', 'loading', 'optimization', 'performance']"
Performance,Please don't approve yet. Some initial experiments are showing that the performance here is pretty terrible - probably related to more interpreter usage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5091:72,perform,performance,72,https://hail.is,https://github.com/hail-is/hail/pull/5091,1,['perform'],['performance']
Performance,"Plus:; - Boolean ldc (load constant) instructions need an int, not a boolean. JVM seems OK with it, but the asm bytecode verifier rejects it.; - In Apply codegen, the zip in function lookup was potentially truncating the arguments, selecting an incorrect function. Fix, and simplify the definition of `methods`.; - Fix/simplify asm error reporting from asm in lir Emit. The old code was crashing inside asm. I used the new code to debug some bytecode issues, it works well.; - compute max locals/stack, needed by the asm verifier (CheckClass). @konradjk This fixes your class not found issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8537:22,load,load,22,https://hail.is,https://github.com/hail-is/hail/pull/8537,1,['load'],['load']
Performance,PoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:81,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4462,Load,LoadVCF,4462,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,Possible optimization is to only run the long query that figures out the ready jobs if the n_commits > 0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10484#issuecomment-845443102:9,optimiz,optimization,9,https://hail.is,https://github.com/hail-is/hail/pull/10484#issuecomment-845443102,1,['optimiz'],['optimization']
Performance,Pretty horrible performance. Most of the time is spent in the IR evaluation to annotate entries:. ```; (Let __iruid_65; (InsertFields; (Ref row); None; (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap __iruid_66; (ArrayRange; (I32 0); (I32 10); (I32 1)); (Literal Struct{} <literal value>)))); (InsertFields; (Ref __iruid_65); None; (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap __iruid_67; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (InsertFields; (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref __iruid_65)); (Ref __iruid_67)); None; (x; (ApplyBinaryPrimOp Add; (GetField col_idx; (ArrayRef; (GetField __cols; (Ref global)); (Ref __iruid_67))); (GetField row_idx; (Ref __iruid_65))))))))))); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6541:16,perform,performance,16,https://hail.is,https://github.com/hail-is/hail/pull/6541,1,['perform'],['performance']
Performance,Priority queue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2147:9,queue,queue,9,https://hail.is,https://github.com/hail-is/hail/pull/2147,1,['queue'],['queue']
Performance,"Probably we should iterate on this PR, accumulating speed boosts until we get a satisfying PR latency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1552120374:94,latency,latency,94,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1552120374,1,['latency'],['latency']
Performance,Problem loading plink file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/715:8,load,loading,8,https://hail.is,https://github.com/hail-is/hail/issues/715,1,['load'],['loading']
Performance,"Problem: Currently in the LD Matrix case I compute V, multiply it through the genotype matrix to get U, then subset columns of U. U is probably bigger than V though, so this could limit the number of eigenvectors that can be used. Probably should just subset columns of V to the desired number of eigenvectors for performance and to increase maximum number of eigenvectors available.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-316386605:314,perform,performance,314,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-316386605,1,['perform'],['performance']
Performance,Pushed another commit: use the gradle cache when building Hail for apiserver.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5670#issuecomment-475490709:38,cache,cache,38,https://hail.is,https://github.com/hail-is/hail/pull/5670#issuecomment-475490709,1,['cache'],['cache']
Performance,"Pushed one more fix: a batch test failed on job.wait() where /status threw 500. It was a running job, so batch hit the worker. The job was error, so it threw an exception and the container was being deleted. There was a race condition getting the container status:. ```; if self.container:; ... self.get_container_status() ...; ```. and deleting the container:. ```; if self.container:; ... call self.container.delete(); self.container = None; ```. If the delete happens between the check for self.container being defined and the call to self.container.show inside get_container_status, show throws 404. Thus, I modified get_container_status to return None on 404.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491:220,race condition,race condition,220,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491,1,['race condition'],['race condition']
Performance,PutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:8364,concurren,concurrent,8364,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['concurren'],['concurrent']
Performance,"Python CLI tools like `hailctl` suffer from slow startup times, which infuriate me. This is in part because the first thing that happens is python has to recursively load all imported packages, since imports are traditionally done at the top-level. Very conveniently, setting the `PYTHONPROFILEIMPORTTIME` environment variable will cause python to emit a profile to stderr, which you can visualize with tools like [tuna](https://github.com/nschloe/tuna). So running. ```; PYTHONPROFILEIMPORTTIME=1 hailctl dev config show 2> profile.log; tuna profile.log; ```. gave me this. <img width=""1576"" alt=""Screen Shot 2022-01-28 at 2 58 28 PM"" src=""https://user-images.githubusercontent.com/24440116/151614364-d57a4478-1516-4397-ac72-4f2b9c6c081b.png"">. showing that importing `aiohttp` is responsible for half the time it takes me to run `hailctl dev config show`, which is literally just printing a local file!! There's no reason this shouldn't be instantaneous, but reducing it to ~300ms, which this change did, is fine enough for me for now. Generally people don't care about import time because most applications are long-lived and what does a few seconds matter, so `pylint` by default wants us to put imports at the top level. I would say this is a valid exception.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11293:166,load,load,166,https://hail.is,https://github.com/hail-is/hail/pull/11293,1,['load'],['load']
Performance,"Quality of life improvement, wrapper around saving an ndarray and loading through numpy. Most changes are in the tests which previously could only extract elements from ndarrays to assert the transformation worked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6020:66,load,loading,66,https://hail.is,https://github.com/hail-is/hail/pull/6020,1,['load'],['loading']
Performance,"RCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2200,cache,cache,2200,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['cache'],['cache']
Performance,RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:3867,concurren,concurrent,3867,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['concurren'],['concurrent']
Performance,RVD$$anonfun$orderedJoin$1.apply(KeyedOrderedRVD.scala:56); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$26.apply(ContextRDD.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$26.apply(ContextRDD.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-eb5d13fe97fc; Error summary: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:12898,concurren,concurrent,12898,https://hail.is,https://github.com/hail-is/hail/issues/4055,2,['concurren'],['concurrent']
Performance,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:21727,concurren,concurrent,21727,https://hail.is,https://github.com/hail-is/hail/issues/3063,2,['concurren'],['concurrent']
Performance,Re-assigned to Amanda to balance the load from Patrick.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2458#issuecomment-346038240:37,load,load,37,https://hail.is,https://github.com/hail-is/hail/pull/2458#issuecomment-346038240,1,['load'],['load']
Performance,"Re. the questions about the PCA step, I think you'll be beter off modifying `_hwe_normalized_blanczos`. For one thing, this ensures that PC-AiR always returns results in the same form as normal PCA. More importantly, `_hwe_normalized_blanczos` performs the SVD using a ""tall-skinny matrix"" representation, which is just a table of matrices (2d ndarrays). This is more efficient than using block matrices for several reasons that aren't directly relevant here. The result of the SVD is computed as local numpy ndarrays. Given these forms of the data, projecting the related sampled onto the computed PCs should be straightforward and efficient. But once everything is converted to tables and matrixtables, it's much harder and does a lot of redundant work. Let me know if you want to schedule a time to walk through the PCA internals and where you can plug in to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230:244,perform,performs,244,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230,1,['perform'],['performs']
Performance,"Re: this interface:; ```scala; def apply(i: Int): Option[Int] = {; setGenotype(i); if (hasGT) Some(getGT) else None; }; ```; It's entirely for performance reasons. We never want to allocate or process `Option`s anywhere, and there's some overhead we can avoid with calling `setGenotype(i)` twice if we use two methods for `hasGtIdx(i: Int): Boolean ` and `getGtIdx(I: Int): Int`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365:143,perform,performance,143,https://hail.is,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365,1,['perform'],['performance']
Performance,Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:10942,concurren,concurrent,10942,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10315:256,latency,latency,256,https://hail.is,https://github.com/hail-is/hail/pull/10315,2,"['cache', 'latency']","['cacheable', 'latency']"
Performance,"Ready for another look, I rewrote queue as a linked list. I'll add `linkedlistof()` typecheckers later, when we fix the typecheck copying problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2657#issuecomment-356039630:34,queue,queue,34,https://hail.is,https://github.com/hail-is/hail/pull/2657#issuecomment-356039630,1,['queue'],['queue']
Performance,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-241153168:303,perform,performance,303,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168,2,['perform'],['performance']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:2834,concurren,concurrent,2834,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:57746,concurren,concurrent,57746,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:59196,concurren,concurrent,59196,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:56294,concurren,concurrent,56294,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60648,concurren,concurrent,60648,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:82375,concurren,concurrent,82375,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:80925,concurren,concurrent,80925,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:78021,concurren,concurrent,78021,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:79473,concurren,concurrent,79473,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 3 requested; 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 3 from BlockManagerMaster.; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 3; 2019-01-22 13:11:57 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56628) with ID 18; 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18, partition 15, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:132919,concurren,concurrent,132919,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:131467,concurren,concurrent,131467,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:130015,concurren,concurrent,130015,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:128565,concurren,concurrent,128565,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:90109,concurren,concurrent,90109,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:85755,concurren,concurrent,85755,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:87207,concurren,concurrent,87207,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:88657,concurren,concurrent,88657,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:97843,concurren,concurrent,97843,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:96391,concurren,concurrent,96391,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:93489,concurren,concurrent,93489,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:94939,concurren,concurrent,94939,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:66340,concurren,concurrent,66340,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:64888,concurren,concurrent,64888,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:63438,concurren,concurrent,63438,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:67792,concurren,concurrent,67792,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.had,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:74936,concurren,concurrent,74936,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:70582,concurren,concurrent,70582,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:72034,concurren,concurrent,72034,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:73486,concurren,concurrent,73486,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.had,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115387,concurren,concurrent,115387,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:113935,concurren,concurrent,113935,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:112483,concurren,concurrent,112483,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:111031,concurren,concurrent,111031,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:122701,concurren,concurrent,122701,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:121249,concurren,concurrent,121249,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:119799,concurren,concurrent,119799,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:124153,concurren,concurrent,124153,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107945,concurren,concurrent,107945,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102258,concurren,concurrent,102258,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103712,concurren,concurrent,103712,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:105164,concurren,concurrent,105164,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:145073,concurren,concurrent,145073,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140712,concurren,concurrent,140712,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143619,concurren,concurrent,143619,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142166,concurren,concurrent,142166,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.sche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198480,concurren,concurrent,198480,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:194025,concurren,concurrent,194025,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:197026,concurren,concurrent,197026,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195572,concurren,concurrent,195572,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.5 KB, free: 21.2 GB); 2019-01-22 13:12:04 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q03.scc.bu.edu:36955 (size: 24.5 KB, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177531,concurren,concurrent,177531,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:176077,concurren,concurrent,176077,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173169,concurren,concurrent,173169,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174623,concurren,concurrent,174623,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition 25, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166982,concurren,concurrent,166982,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165528,concurren,concurrent,165528,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162621,concurren,concurrent,162621,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:164075,concurren,concurrent,164075,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153852,concurren,concurrent,153852,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149491,concurren,concurrent,149491,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150945,concurren,concurrent,150945,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152399,concurren,concurrent,152399,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(14, scc-q05.scc.bu.edu, 42935, None); 2019-01-22 13:12:06 BlockManagerMaster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:189030,concurren,concurrent,189030,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187576,concurren,concurrent,187576,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:186122,concurren,concurrent,186122,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184668,concurren,concurrent,184668,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,Reassigning this since Arcturus has a lot of reviews queued up. I think it's a relatively straightforward one though.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8496#issuecomment-612919449:53,queue,queued,53,https://hail.is,https://github.com/hail-is/hail/pull/8496#issuecomment-612919449,1,['queue'],['queued']
Performance,"Reduces necessary template for a new page to:. ```xslt; <?xml version=""1.0"" encoding=""ISO-8859-15""?>; <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">. <xsl:import href=""template.xslt""/>. <xsl:template name=""page-title"">Foo Bar</xsl:template>; <xsl:template name=""meta-description"">; <meta name=""description"" content=""Hail Foo Bar Baz""/>; </xsl:template>. </xsl:stylesheet>. ```. also moves scripts around to reduce blocking html loading. https://developers.google.com/speed/docs/insights/BlockingJS. cc @mkveerapen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8455:461,load,loading,461,https://hail.is,https://github.com/hail-is/hail/pull/8455,1,['load'],['loading']
Performance,Refactor LoadVCF to use MatrixRead,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840:9,Load,LoadVCF,9,https://hail.is,https://github.com/hail-is/hail/pull/3840,1,['Load'],['LoadVCF']
Performance,Refactored table reader coercion and caching mechanism. ### What changed?. - Removed `shouldCacheQueryInfo` method from `Backend` class; - Introduced `CoercerCache` in `ExecuteContext`; - Refactored `LoweredTableReader.makeCoercer` to return a function instead of a class; - Removed local caching in `GenericTableValue` and `LoweredTableReader`; - Added `NoCaching` utility . ### Why make this change?. This change aims to optimize table reader coercion by:; - Centralizing caching logic in `ExecuteContext`; - Allowing more flexible caching strategies across different backend implementations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14696:423,optimiz,optimize,423,https://hail.is,https://github.com/hail-is/hail/pull/14696,1,['optimiz'],['optimize']
Performance,"Reference</code> and <code>NativeLibrary</code> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1440"">#1440</a>: Support for LoongArch64 - <a href=""https://github.com/Panxuefeng-loongson""><code>@​Panxuefeng-loongson</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1444"">#1444</a>: Update embedded libffi to 1f14b3fa92d4442a60233e9596ddec428a985e3c and rebuild native libraries - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1438"">#1438</a>: Handle arrays in structures with differing size - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1442"">#1442</a>: Handle race condition in <code>c.s.j.p.win32.PdhUtil#PdhEnumObjectItems</code> - <a href=""https://github.com/dbwiddis""><code>@​dbwiddis</code></a>.</li>; </ul>; <h2>Important Changes</h2>; <ul>; <li><code>Memory#dispose</code>, <code>CallbackReference#dispose</code> and <code>NativeLibrary#dispose</code>; were called by the <code>Object#finalize</code> override. These calls were replaced by; the use of a cleaner. It is not guaranteed anymore, that <code>dispose</code> is called; on subclasses on finalization.</li>; </ul>; <h1>Release 5.11.0</h1>; <h2>Features</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1398"">#1398</a>: Increase <code>c.s.j.p.win32.Sspi#MAX_TOKEN_SIZE</code> on Windows 8/Server 2012 and later - <a href=""https://github.com/dbwiddis""><code>@​dbwiddis</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1403"">#1403</a>: Rebuild AIX binaries with libffi 3.4.2 (other archite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:2124,race condition,race condition,2124,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['race condition'],['race condition']
Performance,"Regarding having only one version that returns the triple (V, S, U), returning the eigenvalues S is no extra work. For the record, computing U (the loadings) given V requires multiplying the input m x n RowMatrix A times an n x k Breeze matrix: U = A * (V * S^-1). k is small so this is RowMatrix multiply is done with a broadcast (rather than BlockMatrix with shuffle).; ```; if (computeU) {; // N = Vk * Sk^{-1}; val N = new BDM[Double](n, sk, Arrays.copyOfRange(u.data, 0, n * sk)); var i = 0; var j = 0; while (j < sk) {; i = 0; val sigma = sigmas(j); while (i < n) {; N(i, j) /= sigma; i += 1; }; j += 1; }; val U = this.multiply(Matrices.fromBreeze(N)); SingularValueDecomposition(U, s, V); } else {; SingularValueDecomposition(null, s, V); }; ```; This extra step ought to be fast relative to computing V since the later requires many rounds of such multiplications.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348539919:148,load,loadings,148,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348539919,1,['load'],['loadings']
Performance,"Regardless, we should publish `hailgenetics/hail` for each supported Python version. We should probably use that by default instead of python-dill and we should probably cache the most recent version on the workers since it's almost certainly the most popular image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13558#issuecomment-1707150748:170,cache,cache,170,https://hail.is,https://github.com/hail-is/hail/issues/13558#issuecomment-1707150748,1,['cache'],['cache']
Performance,RelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.241ms self 0.241ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:112808,Optimiz,OptimizePass,112808,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,RelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.295ms self 0.295ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:98370,Optimiz,OptimizePass,98370,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,RelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.348ms self 0.348ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:77191,Optimiz,OptimizePass,77191,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,RelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.409ms self 0.409ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:62793,Optimiz,OptimizePass,62793,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,RelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.450ms self 0.450ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:54265,Optimiz,OptimizePass,54265,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,RelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.607ms self 0.607ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:45737,Optimiz,OptimizePass,45737,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"Releases version 0.2.56. Stacked on #9373, since I'm mainly releasing for performance improvements in #9363, #9373, and #9374.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9386:74,perform,performance,74,https://hail.is,https://github.com/hail-is/hail/pull/9386,1,['perform'],['performance']
Performance,Remove Optimize method signature for BlockMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5834:7,Optimiz,Optimize,7,https://hail.is,https://github.com/hail-is/hail/pull/5834,1,['Optimiz'],['Optimize']
Performance,"Remove the `Begin` node, as its behavior can now be represented by the `Let` node. Besides removing redundant nodes, this will also make the new ssa-style text representation simpler. The `Begin` node emmitter performed method splitting, emitting groups of 16 children in seperate methods. This preserves that behavior by doing a similar optimization in the `Let` emitter. This is a significant change in how we split generated code into methods, so we should watch out for how this affects things.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14068:210,perform,performed,210,https://hail.is,https://github.com/hail-is/hail/pull/14068,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,Rename EmitValue.get to EmitValue.load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9732:34,load,load,34,https://hail.is,https://github.com/hail-is/hail/pull/9732,1,['load'],['load']
Performance,"Renamed and moved `datasets/annotation_db.json` config file to `hail/experimental/datasets.json` and modified urls in `dataset[path]` to use a region parameter to load datasets from bucket in the appropriate region. Modified `load_datasets()` function to no longer use the `config_file` parameter, and to require user to specify `region` parameter. The checked-in `hail/experimental/datasets.json` file will now be used as the config file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9411:163,load,load,163,https://hail.is,https://github.com/hail-is/hail/pull/9411,1,['load'],['load']
Performance,"Reopening this after some changes, mostly to see whether it works with g++-4.8.3 as installed; on the CI machines. The src/main/c/Makefile now builds a libboot.so with -fabi-version=2, which should work against; systems with g++-3.4.0 or later, and both libhail_abi_v2.so and libhail_abi_v9.so. The NativeCode; initialization then figures out which one to load. In theory this should work on MacOS systems back to MacOS 10.9 (Mavericks), which was the first; to use libc++ instead of libstdc++, and on Linux systems with g++3.4.0 or later. By default these libraries are built with ""-march=sandybridge"", which would work on all MacBook Pro's; released since 2011 (and is also the first cpu with AVX). In the medium term I favor the idea of packaging a known good tested compiler into the release, but ; believe that probably won't become critical until we're attempting whole-stage compilation, since the; generated PackDecoder's so far are relatively straightforward code and max out at about 2K lines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412736583:356,load,load,356,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412736583,1,['load'],['load']
Performance,Reopening. Needed by https://github.com/hail-is/hail/pull/2097. Will address performance once Row is gone and we're full unsafe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2098:77,perform,performance,77,https://hail.is,https://github.com/hail-is/hail/pull/2098,1,['perform'],['performance']
Performance,"Replace uses of `new SFooCode(...).memoize(cb)` with one of; * `new SFooValue(...)`; * for pointer types, `pType.loadCheapSCode(cb, addr)`. With `memoize` no longer used, all `SCode` methods are unused. I delete all the class bodies, but leave the concrete `SCode` classes, because `SSettable.store(cb, SValue)` is still implemented as `SSettable.store(cb, sv.get)`. Fixing that will be the next PR. Delete `loadCheapSCodeField`, replace uses with `cb.memoizeField(loadCheapSCode(...))`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11240:113,load,loadCheapSCode,113,https://hail.is,https://github.com/hail-is/hail/pull/11240,3,['load'],"['loadCheapSCode', 'loadCheapSCodeField']"
Performance,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550:243,perform,perform,243,https://hail.is,https://github.com/hail-is/hail/pull/8550,1,['perform'],['perform']
Performance,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2803:519,load,loadInt,519,https://hail.is,https://github.com/hail-is/hail/issues/2803,2,['load'],"['loadInt', 'loadLength']"
Performance,"Replying to cseed on code-reuse and cacheing/locking. a) Even with whole-stage codegen, there's a possibility that during development a user; will be tweaking a query in ways which don't change all the stages. And in that case the re-use; would give hits on some stages. The plan is not really to aim at structuring things to get a; high level of re-use, but just to opportunistically exploit re-use which happens to occur -; e.g. if the early stages of an analysis involve reading an existing file and filtering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:36,cache,cacheing,36,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,4,"['bottleneck', 'cache']","['bottleneck', 'cacheing']"
Performance,"Required for adequate performance in forthcoming lowered MatrixIRs involving the cols. There are a few changes here. 1) LiftLiterals is renamed to LiftNonCompilable. This pass functions to lift non-compilable IRs into MapGlobals nodes.; 2) Introduced EvaluateNonCompilable pass. This evaluates all non-compilable nodes, and replaces them either with primitives (I32, Str, NA), or with references to a struct of compound values.; 3) TableMapGlobals uses EvaluateNonCompilable and the compiler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5125:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/5125,1,['perform'],['performance']
Performance,Resolves a performance problem with _to_table that caused a re-scan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3105:11,perform,performance,11,https://hail.is,https://github.com/hail-is/hail/pull/3105,1,['perform'],['performance']
Performance,"Revert ""[auth] Cache user sessions in memory""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12148:15,Cache,Cache,15,https://hail.is,https://github.com/hail-is/hail/pull/12148,1,['Cache'],['Cache']
Performance,"Reverts hail-is/hail#14374. #14373, #14377 and #14379 appear to have restored Batch's throughput.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14387:86,throughput,throughput,86,https://hail.is,https://github.com/hail-is/hail/pull/14387,1,['throughput'],['throughput']
Performance,Reverts hail-is/hail#9738. Concurrently creating certs doesn't work when OpenSSL assigns consecutive serial numbers.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10141:27,Concurren,Concurrently,27,https://hail.is,https://github.com/hail-is/hail/pull/10141,1,['Concurren'],['Concurrently']
Performance,"Reverts hail-is/hail#9874. This is invalid, doesn't handle the fact that negative ones can be present in the shape. Unless we handle that in python/IR, we can't do this optimization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9877:169,optimiz,optimization,169,https://hail.is,https://github.com/hail-is/hail/pull/9877,1,['optimiz'],['optimization']
Performance,"Right now it's not entirely clear what to do to ensure that a build works on the dataflow cluster, as well as to generate a set of benchmarks that capture a set of performance statistics well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/156:164,perform,performance,164,https://hail.is,https://github.com/hail-is/hail/issues/156,1,['perform'],['performance']
Performance,"Right now the Grafana is exposed as a k8s service speaking http with only the grafana auth. This puts an nginx sidecar in front of Grafana to bring TLS all the way through the the Grafana pod and perform dev authentication. This required adding an api endpoint to auth that can verify a connection based on the session and not an Authorization header. Other services like `router-resolver` have gotten away with not having this since they construct the Authorization header in python before hitting the `userinfo` endpoint, but this seems like a straightforward addition that will make it easier for internal authentication such as this case. This does another deviant thing which is using a `runImage` step to template the nginx config instead of templating inside the container at container start time (like router and site currently do). It is a little janky, because there are essentially two jinja passes, one in CI to render the shell script for the job, and then the jinja line in the job itself to render the nginx config. But this looks to be the most straightforward way I could figure out without adding another `build.py` Step type and even in that case it would have to be some sort of no-op job.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10139:196,perform,perform,196,https://hail.is,https://github.com/hail-is/hail/pull/10139,1,['perform'],['perform']
Performance,"Right now, we perform a full scan in `to_dense_mt`, we have information to do less work and densify in a single pass. - [ ] Expose partitioning in python; - [ ] For each partition in the variants table, use `ref_block_max_length` to determine the full reference interval necessary to densify that partition; - [ ] Use `map_partitions` of the variants and `query_table` on the reference to get two streams with all information necessary to densify.; - [ ] Join the streams and use the current algorithm/scan to do the work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14499:14,perform,perform,14,https://hail.is,https://github.com/hail-is/hail/issues/14499,1,['perform'],['perform']
Performance,"Rude, the check boxes don't get marked automatically. Anyway, all of this is done except the meta GWAS which isn't a performance issue and I will create a separate issue for that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4020#issuecomment-422943490:117,perform,performance,117,https://hail.is,https://github.com/hail-is/hail/issues/4020#issuecomment-422943490,1,['perform'],['performance']
Performance,"Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/late",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12502:1620,latency,latency,1620,https://hail.is,https://github.com/hail-is/hail/pull/12502,2,['latency'],['latency']
Performance,"S$: INFO: close: gs://neale-bge/foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index; 2023-09-22 19:11:12.656 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permissio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9750,concurren,concurrent,9750,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,Safe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(Lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7358,Optimiz,Optimize,7358,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,"Saves memory on the master but has an unfortunate side effect of; drastically multiplying the number of times a tabix file is read, as it; is read once per partition per vcf rather than once per vcf. This change places tabix reading in a more critical path of the gVCF; merger, I would appreciate a more detailed performance audit of that; code, in addition to looking over this change. From my measurements it looks like we pay a 20-30 second cost per partition of 100 gVCFs. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5596:313,perform,performance,313,https://hail.is,https://github.com/hail-is/hail/pull/5596,1,['perform'],['performance']
Performance,"SciPy 1.11.1 Release Notes</h1>; <p>SciPy <code>1.11.1</code> is a bug-fix release with no new features; compared to <code>1.11.0</code>. In particular, a licensing issue; discovered after the release of <code>1.11.0</code> has been addressed.</p>; <h1>Authors</h1>; <ul>; <li>Name (commits)</li>; <li>h-vetinari (1)</li>; <li>Robert Kern (1)</li>; <li>Ilhan Polat (4)</li>; <li>Tyler Reddy (8)</li>; </ul>; <p>A total of 4 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; <h1>SciPy 1.11.0 Release Notes</h1>; <p>SciPy <code>1.11.0</code> is the culmination of 6 months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.11.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.9+</code> and NumPy <code>1.21.6</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>Several <code>scipy.sparse</code> array API improvements, including <code>sparse.sparray</code>, a new; public base class distinct from the older <code>sparse.spmatrix</code> class,; proper 64-bit index support, and numerous deprecations paving the way to a; modern sparse array experience.</li>; <li><code>scipy.stats</code> added tools for survival analysis, multiple hypothe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13228:1278,optimiz,optimizations,1278,https://hail.is,https://github.com/hail-is/hail/pull/13228,1,['optimiz'],['optimizations']
Performance,"Scorecard: Use Sanic, cache fully-shaped GitHub response",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242:22,cache,cache,22,https://hail.is,https://github.com/hail-is/hail/pull/5242,1,['cache'],['cache']
Performance,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:32,load,load,32,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527,1,['load'],['load']
Performance,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:517,load,load,517,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['load'],['load']
Performance,See here for the problem I'm solving with this PR: https://dev.hail.is/t/ndarray-matmul-performance-improvements/176. cc @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8037:88,perform,performance-improvements,88,https://hail.is,https://github.com/hail-is/hail/pull/8037,1,['perform'],['performance-improvements']
Performance,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/812:1641,Optimiz,Optimize,1641,https://hail.is,https://github.com/hail-is/hail/issues/812,1,['Optimiz'],['Optimize']
Performance,Seems the `testImplementation` doesn't configure the classpath for tests correctly - javatests are currently failing with `Error: Could not find or load main class org.testng.TestNG`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249:148,load,load,148,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249,1,['load'],['load']
Performance,"SelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the above rule needs care wrt aggregations, namely the let must be pushed under the aggregation, but no further. All these rules need to be careful because we don't want to lose the ability to send something through BLAS. Perhaps these rules should be left entirely to the ""pipeline""-level (see: Arcturus' recent work) optimizer (after translation to tables of small tensors is complete, at which point BLAS operations are explicit). ## Compilation. At first, we pattern match the items that can be represented via BlockMatrix, err'ing on unrepresentable expressions. C2[[ TensorMap(u, ApplyUnaryPrimOp(Plus(), Ref(""e""), F64(n))) ]]; =; u.scalarAdd(n). C2[[ TensorMap2(u, v, ApplyBinaryPrimOp(Plus(), Ref(""l""), Ref(""r""))) ]]; =; u.add(v). C2[[ TensorContraction(u, v, (ApplyAggOp Sum () None ((ApplyBinaryPrimOp `*` (Ref l) (Ref r))))) ]]; =; u.dot(v)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5195:2316,optimiz,optimizer,2316,https://hail.is,https://github.com/hail-is/hail/issues/5195,1,['optimiz'],['optimizer']
Performance,Service tests have high latency to the JVM. Let's not separately call `hl.eval` so many times when we don't need to.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10448:24,latency,latency,24,https://hail.is,https://github.com/hail-is/hail/pull/10448,1,['latency'],['latency']
Performance,"Should be able to load file from export_blah types='/path/to/types' directly into TextTableConfig (or new hc.import_table) without having to explicitly load as string, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1743:18,load,load,18,https://hail.is,https://github.com/hail-is/hail/issues/1743,2,['load'],['load']
Performance,"Should be good to go. There were two problems:. I needed to make the encoder/decoder `@transient lazy`. The encoder/decoder call generated code but can't be serialized. The make functions handle serialization and loading of the generated code. Also, RegionValueAggregators used in scans can have result called multiple times, so I needed to add a MemoryBuffer.clearPos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5414#issuecomment-466625216:213,load,loading,213,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466625216,2,['load'],['loading']
Performance,Should be ready to go now. I'll add a better description in the morning. I also haven't benchmarked anything yet; would be good to do that and maybe tune things like amount of unrolling. @danking Could you share how you created the datatset you were using for benchmarking?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13787#issuecomment-1756261102:149,tune,tune,149,https://hail.is,https://github.com/hail-is/hail/pull/13787#issuecomment-1756261102,1,['tune'],['tune']
Performance,Should i run benchmarks before merging this? Changing the way things are copied around seems potentially prone to performance regression.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9989#issuecomment-773452910:114,perform,performance,114,https://hail.is,https://github.com/hail-is/hail/pull/9989#issuecomment-773452910,1,['perform'],['performance']
Performance,"Shuffles function as a persist in most cases. I don't completely understand when they don't... Is this a case when shuffle won't persist?. 1. Split multi, something tiny moves, it's put in memory; 2. Use this thing a couple times. It's cached; 3. Do a big shuffle with something else, which uses lots of memory and evicts the split-multi shuffled variants; 4. now you need to recompute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1814#issuecomment-301555125:236,cache,cached,236,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301555125,1,['cache'],['cached']
Performance,"Shuffles have wide dependencies, and it appears that Spark does not enforce that the Nth partition of a shuffled RDD is computed on the same node as the Nth partition of its parent. To keep RDDs `Ordered`, we sometimes need to shuffle, even though the typical key doesn't move at all. It would likely provide a sizable performance gain to enforce that partitions stay on the same node after the shuffle: this way, network traffic (often the rate-limiting step) is kept to a minimum. To do this, we need to optionally override the `getPreferredLocations` function of the ShuffledRDD created in `OrderedRDD.apply` to provide the preferred locations of the parent RDD. This flag should be used in `splitmulti`. **NB:** it's possible this won't actually help much, since if there are 3 preferred hosts for the parent, we may only have a 1/3 chance of landing on the same one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/723:319,perform,performance,319,https://hail.is,https://github.com/hail-is/hail/issues/723,1,['perform'],['performance']
Performance,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:530,load,load,530,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639,1,['load'],['load']
Performance,"Slowly getting rid of the regions being threading through non-allocating functions, part i + 1. This changes the method signature of CodeOrdering method signatures from f(region1, v1, region2, v2) to f(v1, v2). Some other function signatures (e.g. PInterval.loadStart) were also updated as necessary. No functionality has been changed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6700:258,load,loadStart,258,https://hail.is,https://github.com/hail-is/hail/pull/6700,1,['load'],['loadStart']
Performance,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:296,perform,perform-,296,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713,2,"['optimiz', 'perform']","['optimization', 'perform-']"
Performance,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:1874,perform,performance,1874,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125,1,['perform'],['performance']
Performance,"So I'm moderately unhappy with this for various reasons, and almost didn't PR it. We don't generally respect IR identity (e.g. optimize copies everything), so I'm not sure how useful memoizing partition counts on the IR actually is. We could try to carry this information forward inside copy. In that cases that (MatrixIR child) has the same partition counts as child, we can actually push the computed partition counts down into child. But all of this is getting pretty complicated ... just to optimize count? I wonder if it is worth it. Yes, Count and PartitionCounts have different requirements. I think we either need both, or we need to recognize in (Sum (PartitionCounts child)) that child doesn't need to preserve order. (An analysis pass that determines which IR need to preserve order in general will be better than determining this syntactically from context with a rule like (Count (Unkey child)). How do you optimize (Count (Filter (Unkey ...))?). Hmm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3891#issuecomment-402858809:127,optimiz,optimize,127,https://hail.is,https://github.com/hail-is/hail/pull/3891#issuecomment-402858809,3,['optimiz'],['optimize']
Performance,"So in this case, does the PType method loadLength do anything with a region? If so, is that region stored in the value (Scala object) passed to loadLength? Such as `loadLength(address: RegionOwnedAddress)` where RegionOwnedAddress contains (region, Int).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575731997:39,load,loadLength,39,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575731997,3,['load'],['loadLength']
Performance,"So then, loadLength would take instead of a Long, a RegionOwnedAddress, which would say be a (region, Int) tuple correct? Because I'm not exactly sure what Hail values we are referring to in this context, unless we mean ""a Scala value that points to some data that the Hail program needs to access"" (since currently we pass around memory addresses that are Scala primitives, and call methods on PTypes...so I thought the proposal was to give the PType management of regions, so that the caller would not need to think about this, which is closer to what I had envisioned).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575729333:9,load,loadLength,9,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575729333,1,['load'],['loadLength']
Performance,"So you want folks to do this for callRate?; `call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)')`. Is there a performance difference?. Also, this is grammatically off:; ""Perform aggregation queries over genotypes, and returns python objects."". https://hail.is/hail/hail.VariantDataset.html?highlight=query_genotypes#hail.VariantDataset.query_genotypes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1503#issuecomment-284854399:122,perform,performance,122,https://hail.is,https://github.com/hail-is/hail/pull/1503#issuecomment-284854399,2,"['Perform', 'perform']","['Perform', 'performance']"
Performance,"So, the deserializer uses camel case due to the underlying YAML-centered focus of k8s. I sympathize that this is confusing. We [perform this mapping explicitly in batch](https://github.com/hail-is/hail/blob/master/batch/batch/client.py#L167-L179). There's a pair of issues that [track the confusing behavior of `to_dict`](https://github.com/kubernetes-client/python/issues/683) and [provide a workaround](https://github.com/kubernetes-client/python/issues/390):. ```; >>> x = kube.client.V1PodSpec(containers=[], service_account_name='foo'); >>> api.sanitize_for_serialization(x); {'containers': [], 'serviceAccountName': 'foo'}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5738#issuecomment-479527243:128,perform,perform,128,https://hail.is,https://github.com/hail-is/hail/pull/5738#issuecomment-479527243,1,['perform'],['perform']
Performance,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:75,race condition,race condition,75,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732,1,['race condition'],['race condition']
Performance,"Some context:. We have 3 kinds of IRs currently: ; - Value IRs (MakeStruct, I32, ApplyComparisonOp, etc); - TableIRs (TableRange, TableFilter, etc); - MatrixIRs (MatrixMapRows, MatrixExplodeCols, etc). One of the passes of the optimizer is to reformulate MatrixIRs in terms of TableIRs, and we're in the middle of this push to write lowerers for each MatrixIR node (which means writing an algorithm in LowerMatrixTable.scala, and removing the node's 'execute' method). Here I implement lowering MatrixAnnotateRowsTable as either TableIntervalJoin or TableLeftJoinRightDistinct (a future PR should split MatrixAnnotateRowsTable into 2 nodes like Table has, probably). The one bit of extra complexity in Python comes from implementing foreign-key joins explicitly (this was previously handled by the IR node itself).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075:227,optimiz,optimizer,227,https://hail.is,https://github.com/hail-is/hail/pull/5075,1,['optimiz'],['optimizer']
Performance,Some links:. http://findbugs.sourceforge.net/; https://docs.gradle.org/current/userguide/findbugs_plugin.html; https://github.com/sksamuel/scalac-scapegoat-plugin; https://stackoverflow.com/questions/22617713/whats-the-current-state-of-static-analysis-tools-for-scala; https://stackoverflow.com/questions/1598882/are-there-any-tools-for-performing-static-analysis-of-scala-code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/81:337,perform,performing-static-analysis-of-scala-code,337,https://hail.is,https://github.com/hail-is/hail/issues/81,1,['perform'],['performing-static-analysis-of-scala-code']
Performance,"Some useful reading:; - [Phantom References in Java Reference Objects](http://www.kdgregory.com/index.php?page=java.refobj#PhantomReferences); - [java.lang.ref](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/package-summary.html#reachability); - [java.lang.ref.PhantomReference](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/PhantomReference.html). The structure is now this:; - `RegionPool` becomes `RegionPoolNativeMemoryOwner`; - a new `RegionPool` class is simply a (unique) reference to a `RegionPoolNativeMemoryOwner`; - `RegionPoolNativeMemoryFreer` is a `PhantomReference` to `RegionPool`; - There is just one instance of all three classes in memory at any given time.; - A single thread is blocking on the reference queue waiting for something to free. How it works:; - The reachability of `RegionPool` defines the reachability the native memory.; - When the `RegionPool` becomes unreachable, the JVM places the corresponding `RegionPoolNativeMemoryFreer` on its reference queue.; - The `cleaner` thread removes the phantom reference from the queue; - At this point, the `RegionPool` is effectively gone, there are no references to it anywhere. Because of that fact, the `RegionPoolNativeMemoryFreer` can free all the native memory by calling `RegionPoolNativeMemoryOwner.free`.; - The `cleaner` thread removes the `RegionPoolNativeMemoryFreer` from the `refs` set which allows the `RegionPoolNativeMemoryFreer` itself to be GC'ed. We do leak the cleaner thread. It will live for the entirety of the JVM process, which seems fine since once you start Hail, you probably want to keep using Hail. I can't find a reference for this, but I'm fairly certain you need to prevent the PhantomReference itself from being GC'ed. That's why I have that set of refs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8203:744,queue,queue,744,https://hail.is,https://github.com/hail-is/hail/pull/8203,3,['queue'],['queue']
Performance,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:233,perform,performs,233,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400,2,['perform'],['performs']
Performance,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:214,load,load,214,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683,4,['load'],['load']
Performance,"Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2217,concurren,concurrent,2217,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,Special case UKB format in BGEN loader.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2491:32,load,loader,32,https://hail.is,https://github.com/hail-is/hail/pull/2491,1,['load'],['loader']
Performance,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2074:613,optimiz,optimization,613,https://hail.is,https://github.com/hail-is/hail/pull/2074,3,"['Load', 'optimiz']","['LoadVCF', 'optimization', 'optimizations']"
Performance,"Spinning up a VM takes around two minutes. Downloading fresh container images; takes additional time, maybe a whole minute. The cost of timing out is high: an otherwise passing PR test run may fail; demanding a bump and delaying merging of said PR by fifteen to twenty minutes. The cost of waiting two more minutes is that a resource deadlock may last; two extra minutes. We address deadlocks by scaling up and limiting concurrent; PR tests to four.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6878:420,concurren,concurrent,420,https://hail.is,https://github.com/hail-is/hail/pull/6878,1,['concurren'],['concurrent']
Performance,"Stacked on #12006 . This PR actually uses the new tables within the client application. The billing page should now load quickly. Note that the old aggregated billing tables are still in the database and being populated. The key thing to note is I switched how we are computing the cost. First, I aggregate the usage by resource before multiplying by the resource rate and summing. Therefore, the old and new numbers should be close, but not identical in the UI. The reviewer should double check that there are no references to `aggregated_*_resources$` tables (the ones that do not have the date interval) within the application code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11997:116,load,load,116,https://hail.is,https://github.com/hail-is/hail/pull/11997,1,['load'],['load']
Performance,Stacked on #12761. - This PR gets rid of the triggers needed for the migration as well as no longer needed columns; - There's some comments about the attempt resources `deduped_resource_id` column in the migration script. The goal is to swap the resource_id with the deduped_resource_id. This might be more complicated than just keeping both columns. I'm not sure. I also want to check the performance and blocking of the rename and generating the foreign key constraints.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12762:390,perform,performance,390,https://hail.is,https://github.com/hail-is/hail/pull/12762,1,['perform'],['performance']
Performance,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7035:473,load,load,473,https://hail.is,https://github.com/hail-is/hail/pull/7035,1,['load'],['load']
Performance,Stacked on: https://github.com/hail-is/hail/pull/8179. This adds ModuleBuilder. A module is a collection of classes whose bytecode should be loaded together (like a function and some associated dependent functions). Dependent functions now add themselves to the module.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8186:141,load,loaded,141,https://hail.is,https://github.com/hail-is/hail/pull/8186,1,['load'],['loaded']
Performance,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5540:781,cache,cache,781,https://hail.is,https://github.com/hail-is/hail/pull/5540,1,['cache'],['cache']
Performance,Still to do:; - What should the index file look like for 0 keys? I think it should be 0 bytes and the reader needs to look at the nKeys in the metadata first.; - Incorporate into LoadBGEN,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4049:179,Load,LoadBGEN,179,https://hail.is,https://github.com/hail-is/hail/pull/4049,1,['Load'],['LoadBGEN']
Performance,"Struct's loadField has been changed to load the address directly in the; case of the field being a collection type. PCanonicalIntervalValue was; using loadStart and loadEnd rather than startOffset and endOffset which; caused addresses to be loaded twice in the case of intervals of; collections, leading to segfaults. This code is currently unused, but will be used in EmitCodeOrdering #8725",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9032:9,load,loadField,9,https://hail.is,https://github.com/hail-is/hail/pull/9032,5,['load'],"['load', 'loadEnd', 'loadField', 'loadStart', 'loaded']"
Performance,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149:1316,load,load,1316,https://hail.is,https://github.com/hail-is/hail/pull/8149,1,['load'],['load']
Performance,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8581:789,load,load,789,https://hail.is,https://github.com/hail-is/hail/pull/8581,2,['load'],"['load', 'loaded']"
Performance,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8312:815,load,load,815,https://hail.is,https://github.com/hail-is/hail/pull/8312,2,"['load', 'optimiz']","['load', 'optimize']"
Performance,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8333:593,perform,performance,593,https://hail.is,https://github.com/hail-is/hail/pull/8333,1,['perform'],['performance']
Performance,"Summary: hailctl batch list --help; ```; usage: hailctl batch list [-h] [--query QUERY] [--limit LIMIT] [--all] [--before BEFORE] [--full] [--no-header] [-o O]. List batches. optional arguments:; -h, --help show this help message and exit; --query QUERY, -q QUERY; see docs at https://batch.hail.is/batches; --limit LIMIT, -l LIMIT; number of batches to return (default 50); --all, -a list all batches (overrides --limit); --before BEFORE start listing before supplied id; --full when output is tabular, print more information; --no-header do not print a table header; -o O specify output format (json, yaml, csv, tsv, or any tabulate format); ```. Details:; * Default listing to a limit of 50 records, once batch statuses are; cached from `list_batches`, this should result in 1 http request for the; default behavior of this tool.; * Teach --limit option to cap the number of records returned; * Teach --all to override --limit; * Teach --before to pass a last_batch_id query parameter to list_batches; * Teach --full to print all status information; * Teach --no-header to enable not printing a header for tabular output; * Teach -o {format} to change the output format the following are supported:; - json: always full json output, like hitting the list enpoint manually; - yaml: like json, but yaml!; - csv/tsv: simple comma/tab separated output for machine processing; - any python-tabulate output format, listed here:; https://github.com/astanin/python-tabulate#table-format. The only default that has been changed is the listing limit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9557:728,cache,cached,728,https://hail.is,https://github.com/hail-is/hail/pull/9557,1,['cache'],['cached']
Performance,"Support Zstdandard compression for hail input and output block buffers. Zstd is notable for having both very fast compression speed and adequate decompression speed, such that we expect to be network limited for decompression. Further tests may show that Zstd is more performant than LZ4, leading to a proper switch from one format to the other.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981:268,perform,performant,268,https://hail.is,https://github.com/hail-is/hail/pull/12981,1,['perform'],['performant']
Performance,"Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575732722:30,load,loadLength,30,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732722,2,['load'],"['loadAddress', 'loadLength']"
Performance,"Surfaced because sometimes k8s secrets 404 for CI pipelines and we got FK constraint failures because there is no batch 0. No danger of bad data being written, just noise and unnecessary database load. Thank you foreign key checks!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14373:196,load,load,196,https://hail.is,https://github.com/hail-is/hail/pull/14373,1,['load'],['load']
Performance,"Switch apiserver from flask to aiohttp. Mostly boilerplate, except calling into the JVM is blocking. Therefore, I execute JVM calls via a concurrent ThreadPoolExecutor with (a somewhat randomly selected) 16 threads. py4j is thread safe and executes each request on the server (Java) side in a separate thread: https://github.com/bartdag/py4j/blob/master/py4j-python/src/py4j/java_gateway.py#L898",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624:138,concurren,concurrent,138,https://hail.is,https://github.com/hail-is/hail/pull/5624,1,['concurren'],['concurrent']
Performance,"TEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:5451,Race Condition,Race Condition,5451,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['Race Condition'],['Race Condition']
Performance,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215:763,latency,latency,763,https://hail.is,https://github.com/hail-is/hail/pull/5215,2,['latency'],['latency']
Performance,"TableIRSuite extensively uses the function; ```; def collect(tir: TableIR): TableCollect = TableCollect(TableKeyBy(tir, FastIndexedSeq())); ```; to compare the result of a `TableIR` with the expected collection. But `TableCollect` makes no promises what order the results will be in, and in particular the optimizer is allowed to remove that `TableKeyBy`. This PR redefines that function to use the collect aggregator, which does promise the order rows are collected. It also fixes a small type error in the `TableJoin` lowering case that was uncovered.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9054:306,optimiz,optimizer,306,https://hail.is,https://github.com/hail-is/hail/pull/9054,1,['optimiz'],['optimizer']
Performance,TableImport should refuse to load non-splittable .gz,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/973:29,load,load,29,https://hail.is,https://github.com/hail-is/hail/issues/973,1,['load'],['load']
Performance,Tested in a dev deploy'd load test that # of add_attempt_resources queries == # of jobs instead of double as you can currently observe in default.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12461:25,load,load,25,https://hail.is,https://github.com/hail-is/hail/pull/12461,1,['load'],['load']
Performance,"Tests pushing to the cache is important. For example, if a PR adds a new apt-get dependency, only the first build should have to rebuild the image. Subsequent commits / retries should be fast.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152533590:21,cache,cache,21,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152533590,1,['cache'],['cache']
Performance,"TextTableReader.read takes (optional) nPartitions.; importannotations: new option -n/--npartitions.; Added hintPartition to OrderedRDD coerce and friends.; When loading variant annotations, load with as many partitions as the RDD we're going to join with, and hint with its partitioner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824:161,load,loading,161,https://hail.is,https://github.com/hail-is/hail/pull/824,2,['load'],"['load', 'loading']"
Performance,"Thank you both. If you're comfortable without performance testing at this stage (since this is part of a broader move, and optimization seems like it is out of scope, as it is mostly now a function of the Table implementation), this PR is approved, once the commented-out old code block is removed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-454049236:46,perform,performance,46,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454049236,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Thank you for taking a look!. > 1. I'm not opposed to adding tokens to the batches_n_jobs_in_complete_states table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. The issue is that I started with #13475 and after your insightful comment about keeping the batches and job groups tables in sync, I realized that rather than using the batch_after_update trigger to keep the job groups and batches table states identical, we should just go ahead and directly add a double update to the job groups and batches table wherever a batches update occurs in our current code base. Unfortunately, I got stuck with the MJC trigger with these lines of code:. ```sql; UPDATE batches_n_jobs_in_complete_states; SET n_completed = (@new_n_completed := n_completed + 1),; n_cancelled = n_cancelled + (new_state = 'Cancelled'),; n_failed = n_failed + (new_state = 'Error' OR new_state = 'Failed'),; n_succeeded = n_succeeded + (new_state != 'Cancelled' AND new_state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:221,perform,performance,221,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Thanks !! Can you please let us know about how to accomplish ""You can project the withheld samples by summing over all variants and multiplying the variant loadings by the withheld samples' GTs."" in Hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-1298367803:156,load,loadings,156,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-1298367803,1,['load'],['loadings']
Performance,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:159,perform,performance,159,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410,2,['perform'],['performance']
Performance,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240395647:488,Load,LoadBgenSuite,488,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647,1,['Load'],['LoadBgenSuite']
Performance,"Thanks Tim. The from/toCodes were completely wrong, should be good now. Do you think I should leave `loadFrom` unimplemented until there's a corresponding pType?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10614#issuecomment-895411901:101,load,loadFrom,101,https://hail.is,https://github.com/hail-is/hail/pull/10614#issuecomment-895411901,1,['load'],['loadFrom']
Performance,"Thanks for figuring this out! For dataproc, the startup script runs the code below the first time the data is used to generate the index for GRCh37 only. I ran the same dummy VEP command when I generated the QoB data for GRCh37. But we don't do this in GRCh38 on dataproc, so I didn't run this command for QoB as well. The fix is to add something similar as below to the GRCh38 dataproc script and then independently fix the QoB data for GRCh38. ```; # Run VEP on the 1-variant VCF to create fasta.index file -- caution do not make fasta.index file writeable afterwards!; cat /vep_data/loftee_data/1var.vcf | docker run -i -v /vep_data:/root/.vep \; ${VEP_DOCKER_IMAGE} \; perl /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/variant_effect_predictor.pl \; --format vcf \; --json \; --everything \; --allele_number \; --no_stats \; --cache --offline \; --minimal \; --assembly ${ASSEMBLY} \; -o STDOUT; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1832737456:849,cache,cache,849,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1832737456,1,['cache'],['cache']
Performance,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:382,cache,caches,382,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,4,"['Perform', 'cache', 'optimiz', 'perform']","['Performance', 'caches', 'optimizations', 'performance']"
Performance,"Thanks for pushing me to clean that up with the `consume` infrastructure, much neater / shorter. . However, I can't seem to write a working `_result` function in that style. I tried several different ways, all led to segfaults, not sure why. I ended up just inlining calls to `isFieldDefined` and `PBaseStruct.loadField` for that one, as I didn't want to spend more time digging into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-669386823:310,load,loadField,310,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669386823,1,['load'],['loadField']
Performance,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:59,optimiz,optimization,59,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790,2,['optimiz'],"['optimization', 'optimizer']"
Performance,Thanks! That helped. There a couple of other issues that came up that required some tinkering. I list them below in case any one runs into them also. 1. curl failed when trying to download the ibsimdpp lib. The workaround was to download it with wget and move it to the Make Directory. ```; wget --no-check-certificate https://storage.googleapis.com/hail-common/libsimdpp-2.0-rc2.tar.gz; mv libsimdpp-2.0-rc2.tar.gz src/main/c; ```. 2. Needed to compile with newer version of gcc; ```; module load gcc/7.2.0; ./gradlew -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar archiveZip. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411:493,load,load,493,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411,1,['load'],['load']
Performance,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/480#issuecomment-234027310:1667,load,loading,1667,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310,2,['load'],['loading']
Performance,"That test with `10000` fields takes about two minutes, so I didn't want to add that to the test suite. Although two minutes seems insanely slow, it's better than the quadratic scaling provided by the old implementation of `Infer(InsertFields(...))`. Moreover, these tests are now bottlenecked by the py4j boundary rather than constructing immutable maps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4484#issuecomment-426097277:280,bottleneck,bottlenecked,280,https://hail.is,https://github.com/hail-is/hail/pull/4484#issuecomment-426097277,1,['bottleneck'],['bottlenecked']
Performance,That was it. `loadPrimitive` also needed a call to `fundamentalType`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2564#issuecomment-351443642:14,load,loadPrimitive,14,https://hail.is,https://github.com/hail-is/hail/pull/2564#issuecomment-351443642,1,['load'],['loadPrimitive']
Performance,"That's a good question, do the semantics of the client library intend to support two different users modifying the objects simultaneously. We don't have an intended use case for it, so I'd say no. So `_status` cannot be out of date, it can only be stale. I'm only checking if it is complete. Once a Job is complete, its status cannot change. In the case it was deleted by someone else, if I didn't cache it would return 404, but we ruled that out. So I think this code is currently safe. Does that clarify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5812#issuecomment-481006033:398,cache,cache,398,https://hail.is,https://github.com/hail-is/hail/pull/5812#issuecomment-481006033,1,['cache'],['cache']
Performance,"The #1895 Fast Multiply PR is not merged yet, but the multiplication algorithm used there should be the foundation for a tree-aggregating multiply. In particular, [`BetterBlockMatrix.BlockMatrixMultiplyRDD`](https://github.com/danking/hail/blob/b4dda2386e342afe0da1cb809ce756bddd029074/src/main/scala/org/apache/spark/mllib/linalg/distributed/BetterBlockMatrix.scala). My thinking is to produce a layer of a new rdd, `BlockMatrixTreeMultiplyRDD`, which reduces the number of partitions by an order of magnitude in the manner given above. The description above doesn't account for the situation in which the smaller dimension has more than one block. In that situation, we would not perform any condensation of partitions along the smaller dimension (we can't! if we did there would be too few blocks in the output matrix, `C`) .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1975#issuecomment-313702288:682,perform,perform,682,https://hail.is,https://github.com/hail-is/hail/issues/1975#issuecomment-313702288,1,['perform'],['perform']
Performance,The 'details' page for Jenkins won't load. This branch passed on my local computer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/291#issuecomment-210482932:37,load,load,37,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210482932,1,['load'],['load']
Performance,"The Encoder/Decoder methods weren't handling non-struct values correctly (this is mostly fine from an Encoder/Decoder perspective, since they don't handle non-struct/array values). I fixed this in #6727 because I wanted to encode arbitrary values, and changed EmitPackDecoder/EmitPackEncoder to handle arbitrary values so that I could test this. I also pulled out some more peripheral changes from that PR, mostly defining some (currently unused, untested) methods like `Code.orEmpty` and a version of `newMethod` that lets you give the method a real name for ease of debugging, as well as some more changes to move more load/store methods off of region instances and onto the Region object.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6759:621,load,load,621,https://hail.is,https://github.com/hail-is/hail/pull/6759,1,['load'],['load']
Performance,"The Makefiles grab the `docker_prefix` from the global-config each time they are run so once we make the change in the global-config nothing else needs to change. Actually, I think this entire PR is not even needed. When CI does a deploy, the changes it applies to Kubernetes include fully qualified image names, e.g. `gcr.io/hail-vdc/batch:asdf1234`. If we were to swap out the `docker_prefix` global-config variable, CI would start to create new images that are pushed to the new repository (it would kill the cache for a single build but whatever), but the existing images would still exist and be undisrupted. The only images that need to exist in the new container registry when the switch is made are the images that we push on bootstrap which I am going to do manually anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486:512,cache,cache,512,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486,1,['cache'],['cache']
Performance,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7108:338,perform,performance,338,https://hail.is,https://github.com/hail-is/hail/pull/7108,1,['perform'],['performance']
Performance,The `Consume` was getting optimized away in (StreamLen(StreamMap ...)),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9612:26,optimiz,optimized,26,https://hail.is,https://github.com/hail-is/hail/pull/9612,1,['optimiz'],['optimized']
Performance,"The `CovarianceData` reordering stuff looks right, but makes me a little nervous. Can you add some tests to verify the data? Just load a cov file and verify a few entries by hand. Also, verify the entries are sorted after loading. Otherwise, looks good!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/155#issuecomment-182922376:130,load,load,130,https://hail.is,https://github.com/hail-is/hail/pull/155#issuecomment-182922376,2,['load'],"['load', 'loading']"
Performance,"The `UserData` dict is loaded from the `auth` database and passed around to different services, but mostly to look at the current user's username or some other metadata. Because it's in so many places I'm a little worried about it getting logged, which we can't do because it contains the user's `session_id`. But `userdata['session_id']` is used in so few places that I removed `session_id` from the dict and retrieve it explicitly where it's needed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13618:23,load,loaded,23,https://hail.is,https://github.com/hail-is/hail/pull/13618,1,['load'],['loaded']
Performance,"The capacity on the cache is pretty arbitrary, but given that bunches are going to get churned through very quickly and then never used again, it seemed nice to have the assertion that every layer of the cache is always small and shouldn't be an issue to search through in a blocking manner. I tested this with a dev-deployed load-test and observed the number of `get_token_start_id` queries drop from O(jobs) to ~4 per second at max throughput. No difference in profiling, this is just an attempt to reduce the number of queries we're hitting the database with.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12023:20,cache,cache,20,https://hail.is,https://github.com/hail-is/hail/pull/12023,4,"['cache', 'load', 'throughput']","['cache', 'load-test', 'throughput']"
Performance,"The changelog for [6.0.0](https://github.com/johnrengelman/shadow/releases/tag/6.0.0) claims performance improvements. In practice,; I save maybe a few second on the `shadowJar` step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10263:93,perform,performance,93,https://hail.is,https://github.com/hail-is/hail/pull/10263,1,['perform'],['performance']
Performance,"The code as written doesn't seem to allow this to happen. Did someone else bind to that port? It looks like it can happen if an unhandled exception occurs during docker stop or delete, in which case we free the port even though the container might still have the port open. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 354, in run; start_container, self.container); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 94, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.6/asyncio/tasks.py"", line 358, in wait_for; return fut.result(); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 142, in start_container; return await container.start(); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 170, in start; data=kwargs,; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'driver failed programming external connectivity on endpoint batch-20376-job-59-main (8a971634c54c03a1e7df1b4255814137c92e10d310b3d47a1fe6cb7432222ed0): Error starting userland proxy: listen tcp 0.0.0.0:46572: bind: address already in use'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8411:1146,load,loads,1146,https://hail.is,https://github.com/hail-is/hail/issues/8411,1,['load'],['loads']
Performance,"The creeping expansion of the interface is on me as we tried to not break 0.1. I'd appreciate discussing in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210:729,bottleneck,bottleneck,729,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210,2,['bottleneck'],['bottleneck']
Performance,"The current `_variants_per_file` interface isn't usable by mortals. We should have something like `hl.import_bgen('/path/to/bgen', ..., variants=ht.key)` where `ht` is a table with key locus and alleles. This can use the new index file format to get the set of variants to load. Depends on: https://github.com/hail-is/hail/issues/4018",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4019:273,load,load,273,https://hail.is,https://github.com/hail-is/hail/issues/4019,1,['load'],['load']
Performance,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10714:793,queue,queue,793,https://hail.is,https://github.com/hail-is/hail/pull/10714,1,['queue'],['queue']
Performance,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10683:369,load,loading,369,https://hail.is,https://github.com/hail-is/hail/pull/10683,1,['load'],['loading']
Performance,"The current logic of `key_by` and `distinct` to remove duplicate pairs is actually not properly deduplicating pairs in some cases. Since I know that thats not really the approach we want in the long run I didn't bother figuring out why, and instead tried implementing a version of the code that filters out duplicate pairs before exploding, which should result in less work overall. However, I somehow introduced an extra `Ordering unsorted dataset with network shuffle` and the overall performance of that search got slower by 15 seconds. . Here is the change I made. Let me know if you have a better approach for filtering out duplicates, or if you see any ways to reorganize this code to make it less shuffle-y; https://github.com/broadinstitute/seqr/commit/2e45403efc159b58cec723f86e6de7653d64cf5f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1828333357:487,perform,performance,487,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1828333357,1,['perform'],['performance']
Performance,"The current main version of the Query Service uses a fresh class loader for every query. This means each driver job and worker job starts with uncompiled classes for any class in Hail. This change uses a shared class loader for all jobs with the same SHA. This enables use of previously JIT'ed Hail classes. This noticeably improves no-op performance from ~8 seconds to ~3 seconds. Most of that remaining 3 seconds is due to Query-on-Batch and Batch, not Query. Currently, Hail generates classes using a counter. When a driver or worker re-uses an old class loader, it would mistakenly re-use classes generated by a previous Hail Query-on-Batch job because they share the same name. This PR avoids that entirely by using a fresh class loader per job for *generated* classes. This PR parameterizes the entire Hail Query system by a class loader. This class loader is passed in from the initiator of the driver or worker job. We could, eventually, re-use class loaders:; - across jobs for a single batch; - across jobs for a single user; - across jobs for a single billing project; - across all jobs. I think the first three are somewhat uncontroversial but we need to fix the class naming problem. The fourth introduces a new security risk. I think we have a lot of performance to squeeze out of QoB before we need to take that step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11212:65,load,loader,65,https://hail.is,https://github.com/hail-is/hail/pull/11212,9,"['load', 'perform']","['loader', 'loaders', 'performance']"
Performance,The docker file describes a sufficient environment to build and test hail 0.1. The Makefile wraps up Docker image production. The `hail-docs-trampoline.sh` delays the `git rev-parse` until the docs are actually built which allows `gradle downloadDependencies` to run without the `.git` folder present which allows me to cache some of the gradle dependencies once rather than per-build. `hail-ci-build-image` contains the name of a docker image in which to build and test hail 0.1. `hail-ci-build.sh` describes how to build and test hail 0.1 and populates the `artifacts` directory with the results and an index file.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4123:320,cache,cache,320,https://hail.is,https://github.com/hail-is/hail/pull/4123,1,['cache'],['cache']
Performance,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6545:695,concurren,concurrent,695,https://hail.is,https://github.com/hail-is/hail/issues/6545,1,['concurren'],['concurrent']
Performance,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:380,load,loads,380,https://hail.is,https://github.com/hail-is/hail/pull/9120,1,['load'],['loads']
Performance,"The fix is somewhat subtle and relies on PruneDeadFields, which is called (a) by the optimizer and (b) by the compiler, so this is safe. The important piece is that the process of upcasting strips requiredness. This isn't a great design, but I'm comfortable with it for now since I hope physical types will solve this in The Right Way™ before 0.2 release anyway. fixes #4134",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4265:85,optimiz,optimizer,85,https://hail.is,https://github.com/hail-is/hail/pull/4265,1,['optimiz'],['optimizer']
Performance,"The fix of the slow insert performance was to have `_close_batch` not submit N futures to try and `put_on_ready` queue. We were also using a ton of memory, but changing `get_jobs` in batches of 1000 helped the memory problem. I also split the create and delete into two separate pools so we don't block one operation with the other. My next goal is to figure out why the pods completing seems slow (maybe it's fine). I also think I haven't thought through the restart of batch2 carefully enough (do any pods get lost, performance, does it block?). . I think for now the performance of inserting 1 million jobs is acceptable. We're at approximately 20-30 seconds to build the DAG for 1 million jobs and then another 90-120 seconds to submit it. I'll try running it again over the weekend or on Monday 10-20 times to make sure it's consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7336#issuecomment-543974423:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/pull/7336#issuecomment-543974423,4,"['perform', 'queue']","['performance', 'queue']"
Performance,The foreign key constraints are also a safety belt which protect us from a corrupted database. How much does this improve performance? Let's explicitly record the cost-benefit analysis in the PR comments so we can refer back to it if necessary.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1163219995:122,perform,performance,122,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1163219995,1,['perform'],['performance']
Performance,"The fourth in a series of PCRelate Improvements. Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 36 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""phi"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2252:61,cache,cache,61,https://hail.is,https://github.com/hail-is/hail/pull/2252,1,['cache'],['cache']
Performance,"The full text search is there because we don't know how long the path name will be and I thought this was better for looking for prefix matches with ""LIKE"". Maybe I'm wrong. The cancellation_op_id is gone, but I'm still worried about the performance of some of these queries.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1439147838:238,perform,performance,238,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1439147838,1,['perform'],['performance']
Performance,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10314:213,load,load,213,https://hail.is,https://github.com/hail-is/hail/pull/10314,4,"['cache', 'load', 'multi-thread']","['cache', 'load', 'multi-threaded']"
Performance,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:212,load,load,212,https://hail.is,https://github.com/hail-is/hail/pull/10279,5,"['cache', 'load']","['cache', 'load', 'loadClass', 'loader']"
Performance,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:212,load,load,212,https://hail.is,https://github.com/hail-is/hail/pull/10390,4,"['cache', 'load', 'multi-thread']","['cache', 'load', 'multi-threaded']"
Performance,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:183,Load,LoadVCF,183,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496,2,['Load'],['LoadVCF']
Performance,"The last link has the right answer. For reasons not known to me, you must destroy the service and recreate the service to get correct behavior. You know you have correct behavior when the TCP Load Balancer in the GCP console shows most of your instances as unhealthy (because most of them are not hosting the service in question). This lead to at most 15 minutes of downtime and probably like 10 minutes, which seems unacceptable to me, but 🤷‍♀",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289:192,Load,Load,192,https://hail.is,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289,1,['Load'],['Load']
Performance,"The latest changes adopt the ""happy medium"" of having at all times either 0 or 1 NativeModule; objects corresponding to each lib, and a worker may get a shared ptr to what was constructed as; a master NativeModule. . In either case, the constructor is responsible for checking that lib already exists, or populating; it if it didn't exist. The big_mutex is held during constructors, in a way which a) protects the; module_table, and b) makes the transition from ""no lib file"" to ""complete and immutable lib file""; appear atomic, whether that occurs by invoking the makefile, or by writing binary data. Consequently, the makefile is simplified to just build $(MODULE).so without worrying about; atomicity, and the perl rename goes away. The loading is now done eagerly in the worker constructor, but still lazily for master-constructed; NativeModule's, since the most common lifecycle for a master is to construct it, do getKey, getBinary,; then throw it away without needing to load it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4211#issuecomment-417031457:740,load,loading,740,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-417031457,4,['load'],"['load', 'loading']"
Performance,"The layers of wtf really seem to have no end here. Hadoop at least *appears* to [include the configuration in the cache key](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3833-L3841) for its FileSystem cache, but it is actually just ignored by the constructor. Ergo, even if you stop the Hail context and try to start a new hail context with a new Hadoop Configuration, you'll get a filesystem configured by the first configuration. I'm looking for a way around this now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443:114,cache,cache,114,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443,2,['cache'],['cache']
Performance,"The main change is to the communication protocol between the client and; the driver and between the driver and the worker. In main, both the driver and the client send messages back to the client; and driver (respectively) by writing to a file in cloud storage. In both; cases, the file (in main) has one of these two structures:. 0x00 # is_success (False); UTF-8 encoded string # the stack trace. 0x01 # is_success (True); UTF-8 encoded string # JSON message to send back to the client or driver. In this PR, the success case does not change. The failure case becomes:. 0x00 # is_sucess (False); UTF-8 encoded string # short message; UTF-8 encoded string # expanded message; 4-byte signed integer # error id. The Python client (in `service_backend.py`) and the Driver (in; `ServiceBackendSocketAPI2`) changes to read this and raise the right error if; an error id is present. I also uncovered three unrelated problems that are fixed in this PR:; 1. PlinkVariant needs to be serializable because it is broadcasted.; 2. We open an input stream in LoadPlink which ought to be closed, but there is no mechanism to do so in the ServiceBackend. I just ignore it for now. cc: @tpoterba, I'm not sure what the right answer is here.; 3. Two uses of the broadcasted file system that should use the ExecuteContext's file system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11624:1046,Load,LoadPlink,1046,https://hail.is,https://github.com/hail-is/hail/pull/11624,1,['Load'],['LoadPlink']
Performance,"The master node should not need to store every partition's aggregator in memory. It can join them as soon as they arrive, blocking further IO until it is prepared to receive more aggregators. Is this possible in the Spark model? Are the iterators of an RDD lazily reading from the network or do they load all the data into memory before calling `RDD.compute`?. This is relevant to running `hl.sample_qc` on 100k whole genomes, split across 10,000 partitions (1 billion aggregators).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4526:300,load,load,300,https://hail.is,https://github.com/hail-is/hail/issues/4526,1,['load'],['load']
Performance,The match on method identity caused mismatches between the variable; stored in loaded across splits.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9947:79,load,loaded,79,https://hail.is,https://github.com/hail-is/hail/pull/9947,1,['load'],['loaded']
Performance,"The need to supply the region when reading objects via a pointer irked me. It turns out the only reason we do this is to determine whether or not we need to deep copy objects when copying to a region that may or may not be the same as the old region. It seemed to me perfectly reasonable to ask a region if it allocated an object rather than carry around an extra reference so I drafted this change, dependent on such a change not impacting performance too dramatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13881:441,perform,performance,441,https://hail.is,https://github.com/hail-is/hail/pull/13881,1,['perform'],['performance']
Performance,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8987:126,cache,cache,126,https://hail.is,https://github.com/hail-is/hail/pull/8987,1,['cache'],['cache']
Performance,"The new tar file is now in all VEP replicates for dataproc. The only change is it uses the indexed cache files and the tar file has the word ""_indexed"" in it. Otherwise, it should have the same contents / file structure as the non-indexed tar file that is there currently. I tested this as best as I could, but it would be prudent to give ourselves time when releasing this in case there is a problem in the release script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1881444531:99,cache,cache,99,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1881444531,1,['cache'],['cache']
Performance,The old query service would cache user information. The new query service just; constructs a new FS for every query (its not too expensive). None of this code; is used currently.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11651:28,cache,cache,28,https://hail.is,https://github.com/hail-is/hail/pull/11651,1,['cache'],['cache']
Performance,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10591:722,concurren,concurrent,722,https://hail.is,https://github.com/hail-is/hail/pull/10591,1,['concurren'],['concurrent']
Performance,The other option is to drop the literals added to the globals in references to globals inside the IRs. This will generally look like (GetField (DropFields globals (a b c ...) x) which will just get optimized to (GetField globals x).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-413500936:198,optimiz,optimized,198,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-413500936,1,['optimiz'],['optimized']
Performance,"The part of this that's not solved by #4104 is the preservation of relational values through optimization passes. For example, we expect the following to be true:; ```; ht = hl.utils.range_table(10, 3); ht1 = ht.annotate(x=hl.rand_unif(0, 1)); ht1.x.collect()[:5] == ht1.head(5).x.collect(); ```. but since there exist two rules ; ```; case TableHead(TableMapRows(child, newRow, newKey, preservedKeyFields), n) =>; TableMapRows(TableHead(child, n), newRow, newKey, preservedKeyFields). case TableHead(tr@TableRange(nRows, nPar), n) =>; if (n < nRows) TableRange(n.toInt, (nPar.toFloat * n / nRows).toInt.max(1)) else tr; ```; the underlying `TableRange` changes partitioning through the optimization pass and the results differ.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4017#issuecomment-412199170:93,optimiz,optimization,93,https://hail.is,https://github.com/hail-is/hail/issues/4017#issuecomment-412199170,2,['optimiz'],['optimization']
Performance,"The performance difference is driven by the number of rectangles and their size relative to block size. One use case is to partition the genome into perhaps 2k LD blocks, so 2k rectangles. Another use case is to view the genome as perhaps 10k overlapping windows. These will be handled fine. A third (as in @maccum LD prune) is to view each variant as having its own window around it, where the lower and upper bounds increase as the variant index increases. In that case, I think the algorithm should exploit the ordered-ness anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094#issuecomment-372727351:4,perform,performance,4,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372727351,1,['perform'],['performance']
Performance,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:583,perform,perform,583,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403,4,['perform'],['perform']
Performance,The problem is performance. You wanted to have an allocation-free text importer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/240#issuecomment-317567372:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/issues/240#issuecomment-317567372,1,['perform'],['performance']
Performance,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9163:747,load,load,747,https://hail.is,https://github.com/hail-is/hail/issues/9163,1,['load'],['load']
Performance,"The requiredness of the loaded code is the requiredness of the field type. However, if the struct itself is optional, the EmitCode needs to be optional too, regardless of the requiredness of the field.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9920#issuecomment-767221854:24,load,loaded,24,https://hail.is,https://github.com/hail-is/hail/pull/9920#issuecomment-767221854,1,['load'],['loaded']
Performance,The root issue seems to be that native libraries don't load properly when they are binding to instance-methods as opposed to class-methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5565#issuecomment-473009468:55,load,load,55,https://hail.is,https://github.com/hail-is/hail/issues/5565#issuecomment-473009468,1,['load'],['load']
Performance,"The separation makes that metric more accurate. Moreover, I check the queue before rescheduling. This saves a database call, a kubernetes call, and set churn (we remove then add back the job).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6799:70,queue,queue,70,https://hail.is,https://github.com/hail-is/hail/pull/6799,1,['queue'],['queue']
Performance,"The serialization changed briefly, still load VDSes written with that verison.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/729:41,load,load,41,https://hail.is,https://github.com/hail-is/hail/pull/729,1,['load'],['load']
Performance,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:536,perform,performance,536,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056,2,['perform'],['performance']
Performance,"The strictify is needed to ensure that the resulting entriesRVD is ordered by [row key, col key]. If the row key is empty, the only way to do this would be to explode the rows then shuffle to sort by col key. I'm not sure what behavior we want for this. The empty row key is just the extreme case of having too coarse a row key, so that the rows having a given row key are too large to fit one partition and sort the entries by [row key, col key]. Until we can dynamically estimate the data sizes, we have to make some assumption about when a key's worth of data fits on a partition, and otherwise we have to shuffle. If the col key was also empty, we could do this without a shuffle, because there is no ordering requirement on the entriesRVD. I could recognize and optimize that case as a partial solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4646#issuecomment-433468727:767,optimiz,optimize,767,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433468727,1,['optimiz'],['optimize']
Performance,The tests relying on Batch are getting slower because it takes a long time to download and build Docker images and we're putting more load on Batch. This will increase parallelism and reduce test failures due to timeouts.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441:134,load,load,134,https://hail.is,https://github.com/hail-is/hail/pull/9441,1,['load'],['load']
Performance,"The third in a series of PCRelate Improvements. Depends on #2249 . Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 40 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2253:79,cache,cache,79,https://hail.is,https://github.com/hail-is/hail/pull/2253,1,['cache'],['cache']
Performance,"The third in a series of PCRelate Improvements. Depends on #2249. Sprinkling cache on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 80 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```. Ready for a final look after #2270 lands. Creating a PR so that @konradjk and others can take it for a spin if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2280:77,cache,cache,77,https://hail.is,https://github.com/hail-is/hail/pull/2280,1,['cache'],['cache']
Performance,"The tie breaker function is, for all intents and purposes, a separate compilation unit. I think you should be able to call Compile.scala on that IR to produce a `(HailClassLoader, FS, Int, Region) => F)` on the driver, serialize that function with `mb.getObject`, and then initialize it with the (class loader, fs, 0, region) wherever the MaximalIndependentSet code is evaluated",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295#issuecomment-1272171870:303,load,loader,303,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1272171870,1,['load'],['loader']
Performance,"The two versions appear indistinguishable on my laptop. With the filesystem cache warm, the old version took 2m, the new one 1m58s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10245#issuecomment-809703007:76,cache,cache,76,https://hail.is,https://github.com/hail-is/hail/pull/10245#issuecomment-809703007,1,['cache'],['cache']
Performance,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:185,optimiz,optimization,185,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500,1,['optimiz'],['optimization']
Performance,The wheel container prevents this image from ever being cached.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8623:56,cache,cached,56,https://hail.is,https://github.com/hail-is/hail/pull/8623,1,['cache'],['cached']
Performance,"Then lift it is. For gnomAD performance, this is critical for @konradjk. I will think about what's involved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3486#issuecomment-386410873:28,perform,performance,28,https://hail.is,https://github.com/hail-is/hail/issues/3486#issuecomment-386410873,1,['perform'],['performance']
Performance,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3861:755,perform,performance,755,https://hail.is,https://github.com/hail-is/hail/issues/3861,2,"['load', 'perform']","['loaders', 'performance']"
Performance,"There are two problems with this:; - it is a massive de-optimization if we forward an expensive computation into a loop (e.g. arraymap); - in the above case, it is an error in if the body of the let is non-deterministic. To do this right we need to build a control flow graph.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4497:56,optimiz,optimization,56,https://hail.is,https://github.com/hail-is/hail/pull/4497,1,['optimiz'],['optimization']
Performance,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1165,cache,cached,1165,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857,1,['cache'],['cached']
Performance,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6930:1078,perform,performance,1078,https://hail.is,https://github.com/hail-is/hail/issues/6930,1,['perform'],['performance']
Performance,"There remain a couple questions that a solution should answer:; 1. TCP or Unix Domain Socket? Current consensus feels that TCP is a reasonable and more portable way to go (allows for backend deployment over the web/in K8s for example); 2. Should we use just TCP or also use HTTP? If the Java backends can multiplex requests, HTTP sounds favorable, otherwise it's unclear to me what advantages it would give us over TCP + JSON. Ergonomically HTTP might be easier, but one tends have certain default expectations of HTTP servers (I would *assume* an HTTP server should be able to serve requests concurrently, are we just going to use all `POST`s?, etc.). Either way this feels like a minor adjustment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491:593,concurren,concurrently,593,https://hail.is,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491,1,['concurren'],['concurrently']
Performance,There was a race condition where `crun run` could have been cancelled before the container was actually created. This change fixes this problem by checking whether the container exists before trying to delete it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10855:12,race condition,race condition,12,https://hail.is,https://github.com/hail-is/hail/pull/10855,1,['race condition'],['race condition']
Performance,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10946:74,concurren,concurrent,74,https://hail.is,https://github.com/hail-is/hail/pull/10946,2,['concurren'],['concurrent']
Performance,"These changes include a performance regression - instead of allocating memory once and filling in each member of the nested value (srvb) we are using struct constructors that allocate e.g. the locus an extra time and copy it into the container. I do not think it's worth creating constructors right now that prevent this regression -- the region value construction is much slower than the java calls here, and the right solution is coming down teh pike -- constructing containers using SStackStruct emit codes will have exactly the semantics we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644,1,['perform'],['performance']
Performance,"These kinds of ideas bring our local performance closer to PLINK, but I think there's a lot more low hanging fruit before we get here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1082#issuecomment-422389869:37,perform,performance,37,https://hail.is,https://github.com/hail-is/hail/issues/1082#issuecomment-422389869,1,['perform'],['performance']
Performance,"These queries are responsible for getting the paginated list of jobs in the batch UI. I believe this missing order by went unnoticed because the queries were naturally getting returned in order based on the jobs primary key, but we've recently seen a couple page loads of jumbled or missing jobs. Seems like something around the query stats changed and the database adjusted its plan slightly. Regardless, we want this order by explicit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14585:263,load,loads,263,https://hail.is,https://github.com/hail-is/hail/pull/14585,1,['load'],['loads']
Performance,These tests were timing out consistently now. I think 3 minutes is too strict given what we know about the performance of our current system.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13320:107,perform,performance,107,https://hail.is,https://github.com/hail-is/hail/pull/13320,1,['perform'],['performance']
Performance,"These two tests take a very long time in the service and do not benefit from the; massive horizontal scalability of Hail Query on Hail Batch. These tests also; somewhat frequently cause OOMs in the service backend, so the point is perhaps; moot. Nonetheless, I think this is an overall positive change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10943:101,scalab,scalability,101,https://hail.is,https://github.com/hail-is/hail/pull/10943,1,['scalab'],['scalability']
Performance,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3715:1522,bottleneck,bottleneck,1522,https://hail.is,https://github.com/hail-is/hail/pull/3715,1,['bottleneck'],['bottleneck']
Performance,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9320:205,optimiz,optimizes,205,https://hail.is,https://github.com/hail-is/hail/pull/9320,5,"['optimiz', 'perform']","['optimization', 'optimizes', 'perform', 'performance']"
Performance,"This PR adds an optimization where a two dimensional matrix multiply of matrices containing float32 or float64s will be performed by SGEMM or DGEMM respectively. In the future, we should use this during tensor multiplies as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7807:16,optimiz,optimization,16,https://hail.is,https://github.com/hail-is/hail/pull/7807,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"This PR adds stream nodes `StreamMultiMerge` and `StreamZipJoin`, which will be used to implement `TableUnion` and `TableMultiWayZipJoin`. The two nodes were so similar, both in implementation and in interface, that I thought bundling them into one PR would actually make it *easier* to review, but I can split them up if you disagree. The implementations of both nodes use tournament trees, a very simple data structure ideal for this problem. Think of it as a priority queue specialized to holding exactly `k` elements, so when you pop the top element, you must immediately replace it with a new value. A tournament tree is just what it sounds like. It is a complete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9033:471,queue,queue,471,https://hail.is,https://github.com/hail-is/hail/pull/9033,1,['queue'],['queue']
Performance,This PR adds support for optionally loading a dosage values as the call value for genotype loci. This is to support VCF files (such as the ones we use at 23andMe) that have dosages (`DS`) instead of genotype call information (typically `GT`).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5077:36,load,loading,36,https://hail.is,https://github.com/hail-is/hail/pull/5077,1,['load'],['loading']
Performance,"This PR adds the Azure AsyncFS. I added a new secret with the Azure credentials for a new service account, adopted the container `hail-test-****` as the test container, and made the container have a 1 day retention policy (we'll want to check and make sure this actually works). The performance seemed decent. We'll probably want to benchmark this formally at some point, but I think it's good enough for now to have something that works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10795:283,perform,performance,283,https://hail.is,https://github.com/hail-is/hail/pull/10795,1,['perform'],['performance']
Performance,"This PR attempts to simplify the use of TLS and HTTP(S) in Hail. The big changes; are in `hail/python/hailtop`. In particular I removed several functions with; confusingly overlapping functionality in `tls.py`. Instead, we now have three; functions:. - `internal_server_ssl_context`; - `internal_client_ssl_context`; - `external_client_ssl_context`. The client context is configured to seek certificates from its peers. Both; internal contexts load the Hail certificate chain specified in the Hail SSL; Config. The external client context does not load the hail certificate chain. I intend all Hail's HTTP(S) requests to use `httpx.py` (so named to not conflict; with modules named `http`). Again, I have simplified the landscape. We now have; two functions:. - `httpx.client_session`: The constructor for all asynchronous, HTTPS client; sessions.; - `httpx.blocking_client_session`: The constructor for all synchronous, HTTPS; client sessions. Both sessions have the exact same configuration parameters. The API is exactly; the same except the blocking client session replaces asynchronous methods with; synchronous ones. Both sessions accept the `aiohttp.ClientSession` constructor parameters. They; support one new parameter and modify the behavior of one old parameter.; - `retry_transient`: when set to `True` this parameter will retry all transient; errors in all requests made by this session. This defaults to `True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:444,load,load,444,https://hail.is,https://github.com/hail-is/hail/pull/9554,2,['load'],['load']
Performance,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:451,load,loadLength,451,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413,6,['load'],"['loadElement', 'loadLength']"
Performance,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092:308,perform,performance,308,https://hail.is,https://github.com/hail-is/hail/pull/1092,1,['perform'],['performance']
Performance,"This PR improves and consolidates several that were pending. It does a few things:. - all values in the file formats are stored natively (encoded), not using json,; - in particular, the globals and cols are not stored in the json, and not loaded when the read is performed by the user (but when it is executed by the backend).; - MT parts (globals, rows, cols, entries) are all stored in separate directories and each is itself a valid Table file; - extensions for MT and Table are no longer enforced; - the metadata now stores an ""RVDSpec"" which is a recipe for reading an RVD. This gives us the flexibility to define new RVD types (e.g. HashedRVD) or modify the existing classes (by forking and adding additional parameters, etc.); - the rows and entries are stored separately and zipped together on read. If dropSamples = true, don't even load the entries.; - created UnpartitionedRVD, cleaned up the RVD interface. fixes: #2810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2858:239,load,loaded,239,https://hail.is,https://github.com/hail-is/hail/pull/2858,3,"['load', 'perform']","['load', 'loaded', 'performed']"
Performance,"This PR is in @catoverdrive path toward backend refactor. I would like to get this in before any further changes occur, 2nd conflict since this was opened & passing all tests. @catoverdrive Can you help me understand why AddHadoopConfiguration went away? Its body appears inlined in GetHadoopConfiguration. As a result the filesystem object (hConf) is no longer passed around, which is moving things in the opposite direction of future PRs related to this one, which will explicitly pass FS objects to all methods that perform file system operations. . edit: The AddHadoopConfiguration default implementation (on the trait didn't go away, but the overriding implementation did, resulting in an apparent noop, and maybe a potential bug, although I don't understand this portion of the codebase as well as I should yet. ```scala; trait FunctionWithHadoopConfiguration {; def addHadoopConfiguration(hConf: SerializableHadoopConfiguration): Unit; }. // No overriding addHadoopConfiguration implementation; def getHadoopConfiguration: Code[SerializableHadoopConfiguration] = {; if (_hconf == null) {; cn.interfaces.asInstanceOf[java.util.List[String]].add(typeInfo[FunctionWithFS].iname); val confField = newField[FS]; val mb = new EmitMethodBuilder(this, ""addHadoopConfiguration"", Array(typeInfo[SerializableHadoopConfiguration]), typeInfo[Unit]); methods.append(mb); mb.emit(confField := mb.getArg[SerializableHadoopConfiguration](1)); _hconf = HailContext.sHadoopConf. def resultWithIndex(print: Option[PrintWriter] = None): Int => F = {; if (localHConf != null); f.asInstanceOf[FunctionWithHadoopConfiguration].addHadoopConfiguration(localHConf); ```. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573:519,perform,perform,519,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573,1,['perform'],['perform']
Performance,"This PR is the first iteration of an AsyncFS-based copy interface. It adds RouterAsyncFS.copy. The goal of these changes is to establish the interface and behavior. I expect several follow-on PRs:. - Revise the original copy interface proposal and add to dev-docs.; - ~~Parallelizes the transfers concurrently with async and across multiple threads.~~; - ~~After parallizing, copy will involve a lot of paralellism. Throwing an exception on the first error will be very non-deterministic. Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the sour",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:297,concurren,concurrently,297,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['concurren'],['concurrently']
Performance,"This PR is to enable `hail-az;` file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; 	- Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; 	- Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new `azure-mgmt-storage` package requirement.; 	- Updated `AzureAsyncFS` to use `(account, credential)` tuple as internal `BlobServiceClient` cache key; 	- Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token ; 	- Update `RouterFS.ls` function and associated `listfiles` function to allow for trailing query strings during path traversal ; 	- Change to existing behavior: `LocalAsyncFSURL.__str__`no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; 	- Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions; - Updated InputResource to not include the SAS token as part of the destination file name . `test_fs.py` has been updated to respect the new model, where it is no longer safe to extend URLs by just appending new segments with + ""/"" because there may be a query string. But actually running those tests for the SAS case will require some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`): ; ```; export HAIL_TEST_AZURE_ACCOUNT=hailtest; export HAIL_TEST_AZURE_CONTAINER=hail-test-4nxei; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=hailms02; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```; So the SAS case is disabled for now (`test_fs.py`):; ```; @pytest.fixture(param",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12877:629,cache,cache,629,https://hail.is,https://github.com/hail-is/hail/pull/12877,1,['cache'],['cache']
Performance,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10469:625,perform,performance,625,https://hail.is,https://github.com/hail-is/hail/pull/10469,1,['perform'],['performance']
Performance,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3280:91,load,loadings,91,https://hail.is,https://github.com/hail-is/hail/pull/3280,1,['load'],['loadings']
Performance,"This PR separates the ability to broadcast values in generated code on the backend from the collectDArray function. In order to support this, I did three things:. - allowed the function builder to build and store Encoder/Decoder objects, which wasn't strictly necessary but makes generating encoders/decoders for staged code much easier (I'll probably run some performance comparisons on this, shortly, but hopefully this doesn't introduce unacceptable amounts of overhead.); - added broadcast support on BackendUtils to be able to broadcast serialized Hail values with a given decoder; - rewrote the CollectDistributedArray codegen to exercise the broadcasting on BackendUtils instead of manually serializing/broadcasting/deserializing the globals in generated code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8040:361,perform,performance,361,https://hail.is,https://github.com/hail-is/hail/pull/8040,1,['perform'],['performance']
Performance,This PR tries compacting the v2 tables instead of the intended v3 tables to test CI performance improvements on tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13156:84,perform,performance,84,https://hail.is,https://github.com/hail-is/hail/pull/13156,1,['perform'],['performance']
Performance,"This PR tries to address the error we saw last Friday on Azure where there was a stuck worker that could not pull ubuntu:20.04 from Dockerhub. The error message in the worker logs was. ```; DockerError(500, 'Head \""https://haildev.azurecr.io/v2/ubuntu/manifests/20.04\"": denied: retrieving permissions failed'); ```. I looked at the system logs and the actual error message was this:. ```; Mar 03 16:56:12 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:56:12.112691249Z"" level=info msg=""Attempting next endpoint for pull after error: Head \""https://haildev.azurecr.io/v2/ubuntu/manifests/20.04\"": denied: retrieving permissions failed""; ```. Higher up in the logs was:. ```; Mar 03 16:54:50 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:54:50.520878176Z"" level=debug msg=""Fetching manifest from remote"" digest=""sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab"" error=""<nil>"" remote=""docker.io/library/ubuntu:20.04""; Mar 03 16:54:50 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:54:50.762789745Z"" level=debug msg=""Fetching manifest from remote"" digest=""sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab"" error=""ref moby/1/index-sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab locked: unavailable"" remote=""docker.io/library/ubuntu:20.04""; ```. My working hypothesis is described in detail here that the image cache with locks got corrupted with the simultaneous pulls: https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/Azure.20CI.20appears.20hung/near/339452619. To mitigate this, when we get the error ""denied: retrieving permissions failed"", we try and delete the image and then try pulling again once more before erroring gracefully. At least for now, this errors the user's job, but that's better than the status quo where the job is stuck.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12758:1450,cache,cache,1450,https://hail.is,https://github.com/hail-is/hail/pull/12758,1,['cache'],['cache']
Performance,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5979:269,optimiz,optimizer,269,https://hail.is,https://github.com/hail-is/hail/pull/5979,9,"['load', 'optimiz']","['loaded', 'optimizer']"
Performance,"This PR:. - Introduces mapping over NDArrays in JVM emitter. ; - Introduces the `NDArrayEmitter` class, mimicking the cxx analogue. This class is used as the basis of our `NDArray` deforesting efforts (see `emitLoops`); - Fixes a bug in `PStruct`'s load field function. ; - Introduces useful helper functions on `PNDArray`: `numElements` and `makeDefaultStrides`. Ready for review",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6983:249,load,load,249,https://hail.is,https://github.com/hail-is/hail/pull/6983,1,['load'],['load']
Performance,"This PR:. - Pushes code builders through PNDArray interface instead of method builders; - Starts switching away from `PNDArray.data.load` to using methods like `dataPArrayPointer` and `dataFirstElementPointer`. All `dataPArrayPointer` calls will go away when ndarrays are no longer backed by `PArray`. ; - Speeds up repeated calls to `loadElement` on `SNDArrayPointerSettable`, which speeds up linear regression nd benchmark ~12%. Now we are approximately 60% slower than breeze linear regression.'; - Removes `CodePTuple`, since all instances of its use are removed now and it predated the current `PCode` system",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9883:132,load,load,132,https://hail.is,https://github.com/hail-is/hail/pull/9883,2,['load'],"['load', 'loadElement']"
Performance,This actually lets us compile / optimize across aggregations!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3690#issuecomment-393366147:32,optimiz,optimize,32,https://hail.is,https://github.com/hail-is/hail/issues/3690#issuecomment-393366147,1,['optimiz'],['optimize']
Performance,"This added node performs the function that appears all over Python pipelines:; ```python; mt = mt.annotate_rows(foo = ht[mt.row_key]); ```; and ; ```; mt = mt.annotate_rows(foo = ht[mt.not_a_key_field]); ```. The code inside the execution is quite horrible when joining on a non-key field, but I expect it to be much faster than the previous Python implementation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3974#issuecomment-408132885:16,perform,performs,16,https://hail.is,https://github.com/hail-is/hail/pull/3974#issuecomment-408132885,1,['perform'],['performs']
Performance,"This adds a `label` column to pools, which can be used to select a subset of pools to consider when scheduling a job. The label can be specified for each job by setting the `_pool_label` attribute, e.g. `job._pool_label = 'seqr'` will consider all pools that have the `seqr` label. Note: this incurs a DB migration. `batch/sql/add-seqr-pools.sql` is an example for adding a copy of the default preemptible pools, with an additional `seqr` label applied. CPG limits the number of instances in those dedicated pools to prevent long running seqr loading pipeline jobs from starving other batches for resources. #assign services",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879:543,load,loading,543,https://hail.is,https://github.com/hail-is/hail/pull/11879,1,['load'],['loading']
Performance,This adds a control to CI where an operator can adjust the rate limit on a particular service in a particular namespace. CI stores and propagates that information when it generates the Envoy configs for `gateway` and `internal-gateway`. This enables operators to shield an overwhelmed batch driver by decreasing its rate limit without needing to push a code change. Increasing the rate limit can increase throughput if workers are being rate limited but the batch driver is not fully utilized.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14609:405,throughput,throughput,405,https://hail.is,https://github.com/hail-is/hail/pull/14609,1,['throughput'],['throughput']
Performance,"This adds a prometheus statefulset to track metrics like API request latency and uptime. It scrapes pods on a 15s interval and collects prometheus metrics from any container in a pod with `grafanak8sapp` label that exposes an https endpoint `/metrics`.; The batch front end was already exposing prometheus metrics, but I changed it up slightly. For any http endpoint there should be a single metric, `http_request_latency`. Prometheus adds app and namespace metrics so seeing latencies for batch in particular is just a filter applied to this single metric. You can track latency of an endpoint by adding the `@monitor_endpoint` decorator defined in `metrics.py`, which tracks latency as well as number of requests and status code per request, available in the `http_request_count` metric. I also added monitoring to all CI endpoints. This also includes an `up` metric for tracking uptime at the same 15s granularity. I'm not convinced prometheus will suit our finer-grained needs surrounding batch, but it should do well enough in the meantime for our more traditional SLIs and allows to focus on one problem at a time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10165:69,latency,latency,69,https://hail.is,https://github.com/hail-is/hail/pull/10165,3,['latency'],['latency']
Performance,"This adds about 800KB total to the repo. I added a notion of supportedCodec, which includes all the codecs except DirectCodec (which currently depends on the in memory representation of region values which we don't want to promise). FYI @tpoterba I also added a micro optimization: if you get a field of a struct declaration, just grab out the AST for the field. This fixed some massive expression size blowup in create_all_values_datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3043:268,optimiz,optimization,268,https://hail.is,https://github.com/hail-is/hail/pull/3043,1,['optimiz'],['optimization']
Performance,"This adds filter, filterCols, and filterRows to BlockMatrix, en route to moving kinship and LD matrix to from IRM to BlockMatrix. For example, filtering out rows/columns from a kinship matrix (e.g. to remove samples with missing covariate data); or filtering an LD matrix (e.g. to perform Leave-One-Chromosome-Out LMM analysis). Since the number of tasks equals the number of resulting partitions (blocks), these functions are suited to filtering out a small to medium subset (say, throwing out a few rows and columns, or half of the rows and columns) and may fail when filtering all but a small number of rows and columns due to all the blocks being sent to one executor. In the latter case, one can do better by writing the result of filterCols, then reading and applying filterRows. Testing locally, on a 16384 * 16384 matrix (block size 4096, 16 partitions, 128MB each), when only filtering columns (or rows), filterCols (or filterRows) tends to be faster than filter (likely due to less copying), and filterCols tends to be a bit faster than filterRows (which I think is due to lack of transpose and esp. better striding). When filtering both columns and rows, filter tends to be fastest but filter_columns followed by filter_rows is often comparable. It may be that the cost of an additional copy in the first step is offset by smaller strides resulting better caching in the second step. For example, filtering out every 31st row and column:; ```; filter; [9.550655126571655, 9.62321400642395]; filterCols.filterRows; [12.190248966217041, 12.07064700126648]; filterRows.filterCols; [14.239542007446289, 15.601837158203125]; ```; Filtering out every row and column with 0.5 probability, resulting in 4 partitions:; ```; filter; [4.625396966934204, 4.427164077758789, 4.435616970062256]; filterCols.filterRows; [4.647814035415649, 4.610292911529541, 4.655405044555664]; filterRows.filterCols; [4.866307973861694, 5.073392868041992, 5.470307111740112]; ```. On GCP, with 32 core highmem master and",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2647:281,perform,perform,281,https://hail.is,https://github.com/hail-is/hail/pull/2647,1,['perform'],['perform']
Performance,"This allows Spark 2 Hail to load partitioning information from VDSes; written by the Spark 1 version, but loading partitioning from previous; Spark 2 versions will now fail. Since there are far more VDSes in; uses written with Spark 1 in use, this seems like a good trade-off. There are now to serial version UIDs in the wild for Locus. I don't; see how to write code to load both of them (except maybe; loading/unloading different versions of the Locus class which seems; painful.) I would prefer a tool that converts the partitioning to; JSON instead (once support for JSON is ready). The partitioning information should be stored as JSON instead of Java; serialization, which is not a good long-term storage format.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1138:28,load,load,28,https://hail.is,https://github.com/hail-is/hail/pull/1138,4,['load'],"['load', 'loading']"
Performance,"This allows us to insert fields elsewhere in the struct, and; is necessary for a forthcoming Python IR generation change, which will fix the BGEN allocation performance problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5103:157,perform,performance,157,https://hail.is,https://github.com/hail-is/hail/pull/5103,1,['perform'],['performance']
Performance,This avoids confusing Docker cache behavior by baking the verison number into; the RUN command string.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10282:29,cache,cache,29,https://hail.is,https://github.com/hail-is/hail/pull/10282,1,['cache'],['cache']
Performance,"This branch:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 18.2 s per loop; ```. Master:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 27.7 s per loop; ```. I predict the performance improvement will increase with large datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1363:215,perform,performance,215,https://hail.is,https://github.com/hail-is/hail/pull/1363,1,['perform'],['performance']
Performance,"This can probably be better optimized, but it is part of a larger problem with the notify children not happening atomically. Working on a better fix, but that will take some time. Figured this is an improvement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6355:28,optimiz,optimized,28,https://hail.is,https://github.com/hail-is/hail/pull/6355,1,['optimiz'],['optimized']
Performance,"This change adds an nginx sidecar to the batch-driver pod for terminating TLS. TLS negotiation has proven a major bottleneck to scheduling performance, as the batch-driver currently spends up to 60% of its CPU time in handshakes with workers. Moving TLS termination into a sidecar that can leverage additional cores both reduced CPU pressure on the driver and allowed for a 3-4x increase in job-scheduling throughput. ## Benchmarking; Below are before-and-after profiles of the same benchmark (30,000 1s jobs) under the proposed higher rate limit, showing CPU time:; <img width=""1889"" alt=""Screen Shot 2022-03-21 at 5 10 13 PM"" src=""https://user-images.githubusercontent.com/24440116/159364769-6fd60840-5745-40ab-802e-68b8d4f32078.png"">; <img width=""1885"" alt=""Screen Shot 2022-03-21 at 5 12 33 PM"" src=""https://user-images.githubusercontent.com/24440116/159364787-ca7ec307-877d-479c-9c19-8746b5e82eab.png"">. Looking at Wall time, the before profile is nearly identical because at the current rate limit the driver uses 100% of its CPU shares under this benchmark. On this branch, CPU utilization drops to 40-60%, giving the following wall time profile:; <img width=""1879"" alt=""Screen Shot 2022-03-21 at 5 30 39 PM"" src=""https://user-images.githubusercontent.com/24440116/159367182-0830d6ff-3b6f-4fa7-8004-0fc43283ec4a.png"">. So we can be confident that driver CPU is no longer a bottleneck even in the increased rate limit. ## So what's the bottleneck now?; Since the higher rate limit still leaves the driver plenty of CPU room (I've seen it peak at 60% of a vCPU), why not crank it higher? Well, we're increasing concurrency so latent deadlocks start to be a bigger issue again. We start to see tens of deadlocks per second in the proposed rate limit and hundreds at higher rate limits. As a result, we're spending more cycles repeating queries instead of actually scheduling faster. Next steps should focus on eliminating deadlocks before we can continue to max out CPU use. ## Miscellaneous; We'v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638:114,bottleneck,bottleneck,114,https://hail.is,https://github.com/hail-is/hail/pull/11638,3,"['bottleneck', 'perform', 'throughput']","['bottleneck', 'performance', 'throughput']"
Performance,"This change anticipates the ContextRDD change wherein `RVD.rdd` will not; be an RDD. Moreover, enforcing an abstraction barrier at the level of; `RVD` will ease changes to the implementation of `RVD`. There are two remaining types of calls that I cannot eliminate:. - uses in BlockMatrix and OrderedRDD2: these two classes are building; new RDDs based on the RVD's rdd, these classes should be considered; within the implementation of the RVD abstraction. Because these two; classes are outside of `is.hail.rvd`, I cannot enforce an access; modifier on `RVD.rdd`. - uses by methods:. - LDPrune: it seems we need a ""GeneralRVD"". - Skat: it seems like some of this could be moved to python actually;; but there is some matrix math that cannot be moved until the expr; lang has efficient small-matrix ops. - MatrixTable.same: I could probably move this if I re-implemented; forall in terms of RVD.aggregate?. - MatrixTable.annotateRowsIntervalTable: really not sure about this; one, this seems like a performance optimization that purposely; reaches through the abstraction to do Smart Things",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3186:998,perform,performance,998,https://hail.is,https://github.com/hail-is/hail/pull/3186,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"This change is split out from a larger refactoring effort on the various Backend; implementations. The goals of this effort are to provide query-level; configuration to the backend that's currently tied to the lifetime of a backend,; reduce code duplication and reduce state duplication. In this change, I'm removing blockmatrix persist/unpersist from the `Backend`; interface by adding `BlockMatrixCache: mutable.Map[String, BlockMatrix]` to; `ExecuteContext`. The various reader/writer implementations simply fetch the ; block matrix from this cache. For the spark backend, this is backed by a cache; whose lifetime is tied to the spark backend. Since block matrices are not; supported in the local and service backends, the cache is an empty map. Note that block matrix persist is broken in python (#14689)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14690:546,cache,cache,546,https://hail.is,https://github.com/hail-is/hail/pull/14690,3,['cache'],['cache']
Performance,"This commit introduces the SCode hierarchy. How are SCodes different; from PCodes? SCodes are what we want PCodes to be. They don't have; the methods `code / tcode`, which gives us a way to implement new; performance-improving types like SStackStruct with well-defined; boundaries around that functionality. Currently, the `asPCode` and `IEmitCode.typecast` methods break this; boundary, and I added these as a short-term mechanism to avoid another; very spicy meatball. This commit is entirely reorganizational, with no semantic changes; to the compiler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9729:205,perform,performance-improving,205,https://hail.is,https://github.com/hail-is/hail/pull/9729,1,['perform'],['performance-improving']
Performance,"This doesn't actually hugely improve performance:. Master:; ```; 2019-09-12 16:25:06,282: INFO: [1/1] Running import_bgen_force_count_all...; 2019-09-12 16:26:26,427: INFO: burn in: 80.14s; 2019-09-12 16:27:46,816: INFO: run 1: 80.39s; 2019-09-12 16:29:10,637: INFO: run 2: 83.82s; 2019-09-12 16:30:33,742: INFO: run 3: 83.11s; 2019-09-12 16:31:53,968: INFO: run 4: 80.22s; 2019-09-12 16:33:18,855: INFO: run 5: 84.89s; ```. PR:; ```; 2019-09-12 16:46:20,424: INFO: [1/1] Running import_bgen_force_count_all...; 2019-09-12 16:47:39,550: INFO: burn in: 79.12s; 2019-09-12 16:48:57,259: INFO: run 1: 77.71s; 2019-09-12 16:50:15,457: INFO: run 2: 78.20s; 2019-09-12 16:51:32,472: INFO: run 3: 77.01s; 2019-09-12 16:52:49,160: INFO: run 4: 76.68s; 2019-09-12 16:54:05,504: INFO: run 5: 76.34s; ```. However, it does make the generated much easier to profile, and; would make much more of difference if we support BGEN 1.3 with ZSTD.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7059:37,perform,performance,37,https://hail.is,https://github.com/hail-is/hail/pull/7059,1,['perform'],['performance']
Performance,"This doesn't optimize anything w.r.t. calculating let values. This is just to ensure that a stray `Let` that hasn't been moved already in an earlier optimization pass doesn't break up array deforestation since the values in those cases aren't calculated in the per-element scope. I think I don't quite understand what you're getting at with those examples---as far as I can tell, the let is defined on the outside and the value is therefore only ever computed once? I haven't changed that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5667#issuecomment-475419212:13,optimiz,optimize,13,https://hail.is,https://github.com/hail-is/hail/pull/5667#issuecomment-475419212,2,['optimiz'],"['optimization', 'optimize']"
Performance,This encapsulates the type dispatch necessary to load and store; annotations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2396:49,load,load,49,https://hail.is,https://github.com/hail-is/hail/pull/2396,1,['load'],['load']
Performance,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215:217,perform,performance,217,https://hail.is,https://github.com/hail-is/hail/pull/10215,2,['perform'],['performance']
Performance,"This fixes a bug in joins where:; * The join key is a prefix of the left key; * Values of the join key in the left table span multiple partitions. For example, say the left table is `{[(0, 0), (0, 1)], [(0, 2), (1, 0)]}`, where the two lists of tuples are the two partitions, and the key is the entire tuple, and the join key is just the first field. Then in the old code; ```; val loweredLeft = lower(left).strictify(); val leftKeyToRightKeyMap = left.typ.keyType.fieldNames.zip(right.typ.keyType.fieldNames).toMap; val newRightPartitioner = loweredLeft.partitioner.coarsen(commonKeyLength); .rename(leftKeyToRightKeyMap); val loweredRight = lower(right).repartitionNoShuffle(newRightPartitioner); ```; the left partitioner is coarsened to `[(0), (0)], [(0), (1)]` in the third line, and the `repartitionNoShuffle` in the fourth line fails because rows with key `(0)` are split across both partitions. One possible fix would be to strictify the coarsened partitioner, which in this case would force the left table to one partition. But this seems dangerous performance-wise. Instead, this PR makes the lowered behavior match the old behavior, which is to ""repartition"" the right to the invalid partitioner `[(0), (0)], [(0), (1)]`, meaning rows with key `(0)` get duplicated into both partitions. This is exactly the intended semantics of `alignAndZipPartitions` as described in the code:; > The partitioner of the result will be the left partitioner. Each partition will be computed by 'joiner', with corresponding partition of 'this' as first iterator, and with all rows of 'that' whose 'joinKey' might match something in partition as the second iterator. This behavior is implemented with a flag `allowDuplication` on `repartitionNoShuffle`. This simply omits the assertion on the new partitioner that guarantees each incoming row ends up in at most one result partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11556:1058,perform,performance-wise,1058,https://hail.is,https://github.com/hail-is/hail/pull/11556,1,['perform'],['performance-wise']
Performance,"This fixes one mistake (right join does not set fields in the right table to missing), and makes the join table to be more precise. Keys are arrays of columns/fields of the table. They need to be present (they must be of same length and type for a join to be performed). The values are what are considered. The description also doesn't give a clear idea that left/right join is really about returning all rows from left/right table, and then joining the right/left table's fields depending on matches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8467:259,perform,performed,259,https://hail.is,https://github.com/hail-is/hail/pull/8467,1,['perform'],['performed']
Performance,"This fixes two bugs:; 1. The container logs weren't being cached. This made the logs ""disappear"" for previous tasks while the job was still running. FYI @konradjk . 2. My job got stuck in ""running"" even though the job was deleted from the worker because writing the status to GCS timed out and we didn't actually mark the job complete. I'm not sure if we should always try to retry writing the status rather than failing on non-transient errors. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8054:58,cache,cached,58,https://hail.is,https://github.com/hail-is/hail/pull/8054,1,['cache'],['cached']
Performance,"This flips the pca algorithm, so that we're locally collecting a Krylov space basis on the column side (presumably the smaller side). This leads to a few simplifications, and allows for an optimization in the `compute_loadings=False` case. Once we have a complete TSQR implementation, we can optimize the `compute_loadings=True` case as well. With that, there should be no memory limitation to the number of rows (no local value is O(rows)).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10264:189,optimiz,optimization,189,https://hail.is,https://github.com/hail-is/hail/pull/10264,2,['optimiz'],"['optimization', 'optimize']"
Performance,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5078:307,Perform,Performance,307,https://hail.is,https://github.com/hail-is/hail/pull/5078,2,"['Perform', 'optimiz']","['Performance', 'optimization']"
Performance,"This has no discernable effect on latency but it makes the LIR much easier to read because, for example, `IfX IFNE L1 L2 ...` will have `L1` (in printed LIR and JVM bytecode) immediately after it rather than `L2`. The labels also tend to appear in sequential order, which I find helps me navigate the LIR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13779:34,latency,latency,34,https://hail.is,https://github.com/hail-is/hail/pull/13779,1,['latency'],['latency']
Performance,"This installs our fully-pinned requirements deep in the docker image and then installs the hail wheel without dependencies on top. This will be a lot more consistent (and docker cache friendly) than the current approach, which will install all dependencies with the wheel and possibly upgrade some of them. Also did the same to the hail-pip-installed images used for testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12433:178,cache,cache,178,https://hail.is,https://github.com/hail-is/hail/pull/12433,1,['cache'],['cache']
Performance,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10693:546,cache,caches,546,https://hail.is,https://github.com/hail-is/hail/pull/10693,1,['cache'],['caches']
Performance,"This is a minor architectural change (cc'ing @cseed @tpoterba) that I hope will improve maintainability of `batch`. It foreshadows the DAG functionality. There may be shared data structures between the server and the client. At the very least, the client sends structured data to the server (e.g. a pod spec and metadata about the job). Often, the server parses this data into an object or series of objects which contain methods for performing the server's job (e.g. `batch/server/job.py`). I think this architecture is more or less a different way of defining the API (see `batch/api.py`). I think defining the API via data objects is appealing because; - it centralizes serialization and deserialization for each data structure in one class,; - it enables sharing (via object composition) of that basic data structure between potentially complex client and server objects that implement algorithms on that data structure (I want to do this with the forthcoming DAG stuff), and; - the client has objects representing its ideas (i.e. ""a job"") and those objects can have `__str__`'s and `__repr__`'s facilitating debugging of the client. Moreover, this change pushes the use of k8s' swagger models everywhere possible. This means it's harder for us to make code mistakes because pylint will notice when we, for example, misspell a parameter to a k8s model.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4804:434,perform,performing,434,https://hail.is,https://github.com/hail-is/hail/pull/4804,1,['perform'],['performing']
Performance,"This is a mitigation and does not solve the issue where we have existing duplicated billing project names. I'm not entirely sure adding BINARY in front of the search value is correct, but I was going off of this post. Apparently, if you use `BINARY name = %s` instead of `name = BINARY %s`, then you pay a performance hit because you cannot use the index any longer. https://stackoverflow.com/questions/5629111/how-can-i-make-sql-case-sensitive-string-comparison-on-mysql",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12275:306,perform,performance,306,https://hail.is,https://github.com/hail-is/hail/pull/12275,1,['perform'],['performance']
Performance,"This is a multi-stage overhaul of our Kubernetes load balancers / service discovery. This involves moving off of NGINX onto Envoy, but more importantly involves better control of what namespaces and services are active in our cluster at a given point in time. TL;DR Switching from NGINX to Envoy with CI acting as the ""control plane"" for our internal networking allows us to more easily dynamically configure our Kubernetes networking and achieve proper connection pooling/load-balancing over TLS, which translates to less resource consumption and lower request latencies. ## Motivation; This is primarily a performance-motivated change, and one largely based on our (ab)use of NGINX in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:49,load,load,49,https://hail.is,https://github.com/hail-is/hail/pull/12095,3,"['load', 'perform']","['load', 'load-balancing', 'performance-motivated']"
Performance,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9129:222,load,load,222,https://hail.is,https://github.com/hail-is/hail/pull/9129,2,['load'],['load']
Performance,"This is a performance improvement (and also returns explosion after the right number of iterations which is more logical). It is only necessary to check the first element because I'm confident NaN appears nowhere or everywhere in deltaB (and certainly if it appears somewhere in one iteration, it spreads to everywhere in the next).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5050:10,perform,performance,10,https://hail.is,https://github.com/hail-is/hail/pull/5050,1,['perform'],['performance']
Performance,"This is a simple refactoring of `lir.Emit` to directly use the core visitor based interface of ASM, rather than the higher level `tree` interface. This should have a small performance benefit, as we aren't building the in-memory tree representation only to immediately walk it with a visitor. But I also find this version of `Emit` slightly cleaner. For reference, you can find the documentation for ASM 5.1 [here](https://javadoc.io/doc/org.ow2.asm/asm/5.1/index.html).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9511:172,perform,performance,172,https://hail.is,https://github.com/hail-is/hail/pull/9511,1,['perform'],['performance']
Performance,"This is a temporary fix for the sporadic copy failures. The problem is that this code cancels a task managed by the online bounded pool, and the pool treats that cancellation as an exception that it propagates up. I need to think through the details of the bounded gather with respect to cancellation, and that's going to take a few days. We could put this back when that's done, but honestly, it doesn't seem like an important optimization (given how rarely this failure comes up), so I'll probably just leave it out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10260:428,optimiz,optimization,428,https://hail.is,https://github.com/hail-is/hail/pull/10260,1,['optimiz'],['optimization']
Performance,"This is a total overhaul of our docker images. Though very verbose, I tried to stick to these main tenets:. - Any docker image has exactly 1 layer in it (all the way down to ubuntu) that installs pip dependencies. This primarily aims to protect the cache for this particularly large layer and also avoids a later layer silently upgrading the version of a dependency installed in an earlier layer. This pairs nicely with the following goal; - We only ever use 1 version of a dependency across the monorepo. Liberal use of pip's [constraint files](https://pip.pypa.io/en/stable/user_guide/#constraints-files) to ensure that the dependencies for a service must be compatible with dependencies from hail. The `install-dev-dependencies` target which install all our pinned requirements files would tell you if there's any incompatible versions of transitive dependencies across the repo; - The image graph is shallow and images don't contain more than they need. In order to have a single layer with requirements and hail code on top, I moved the service images to just be based on hail-ubuntu. This shortens the critical path and therefore reduces total image building time by reducing the number of times our image data needs to be downloaded and re-uploaded to the registry. I also removed a lot of unnecessary cruft like gcloud in places it wasn't used anymore, some unused/unnecessary pip requirements, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578:249,cache,cache,249,https://hail.is,https://github.com/hail-is/hail/pull/12578,1,['cache'],['cache']
Performance,This is all working. I added back the randomization as its omission made performance substantially worse. I'm estimating it's going to take about 3 hours and adds ~50 GB to the database.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12028#issuecomment-1185067057:73,perform,performance,73,https://hail.is,https://github.com/hail-is/hail/pull/12028#issuecomment-1185067057,1,['perform'],['performance']
Performance,"This is almost all coming from the Python change. the gqFromPL change has a negligible effect on performance, but still seems good to include.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6980#issuecomment-527418027:97,perform,performance,97,https://hail.is,https://github.com/hail-is/hail/pull/6980#issuecomment-527418027,1,['perform'],['performance']
Performance,"This is also my first migration PR so please take a very critical eye! I ran a load test on my namespace and noticed that while previously it looked like we only get deadlocks in MJC, the deadlock that this uncovered might be between MJC and MJS/SJ. Happy to provide details about the following deadlock to decide whether it's something we'd want to lump in here or tackle all at once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1035565057:79,load,load,79,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1035565057,1,['load'],['load']
Performance,"This is an initial implementation of the Scala Region as a reference to a C++ off-heap object. The C++ Region allocates ""small"" blocks out of 64KB chunks, and ""large"" blocks using; malloc() directly. There's a clear_but_keep_mem() which reuses all the 64KB chunks,; and the largest few individual allocations. The usefulness of this strategy is TBD. Currently all allocations are done with a JNI call to the C++, but fields of the object are; directly accessible so it's theoretically possible to try to write optimized Scala code; for the case of a small allocation which can fit in the current chunk. The other changes are mostly consequences of using absolute addresses rather than; offset-in-contiguous-buffer, and the change in the semantics of appendFoo() when a; Region's memory is in non-contiguous chunks - things which need to be located together,; such as the length of a string and its contents, now have to be within memory from a; single allocate() call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718:510,optimiz,optimized,510,https://hail.is,https://github.com/hail-is/hail/pull/3718,1,['optimiz'],['optimized']
Performance,This is causing unbearable performance slowdowns when big literals are added. Randomly assigned @chrisvittal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4907:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/issues/4907,1,['perform'],['performance']
Performance,"This is currently just dead code, although it could conceivably be useful in the future; I want to remove this for now as it's pretty simple to add back in at a later date (follows the pattern of Serialize/Deserialize Aggs pretty much exactly) and makes for less code to keep refactoring as we optimize the aggregator stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6813:294,optimiz,optimize,294,https://hail.is,https://github.com/hail-is/hail/pull/6813,1,['optimiz'],['optimize']
Performance,"This is due to FSs in `hailtop.fs` never getting closed. Unfortunately we exposed functions on a module, not a context manager. Options include; 1. adding a `hailtop.fs.close` method; 2. Using a new `RouterFS` on every `hailtop.fs` method; 3. Have users instantiate a `RouterFS` as a context manager and use that. Among these options I prefer 2 and 3. I think using a standalone function instead of properly allocating a context manager can be a convenience/performance tradeoff. 3 could use a bit of thought though as it adds more user-facing API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14280#issuecomment-1936521020:458,perform,performance,458,https://hail.is,https://github.com/hail-is/hail/issues/14280#issuecomment-1936521020,1,['perform'],['performance']
Performance,"This is failing because AD and PL arrays are (properly, I would argue) being dropped by buildGenotypeExtractor because their elements aren't required. This won't work unless import_vcf sets requiredness on import. Here is a possible diff (which is probably subsumed by your next PR) which fixes some (but maybe not all) of the failing tests:. ```; diff --git a/src/main/scala/is/hail/io/vcf/LoadVCF.scala b/src/main/scala/is/hail/io/vcf/LoadVCF.scala; index 77f6c72..fadc936 100644; --- a/src/main/scala/is/hail/io/vcf/LoadVCF.scala; +++ b/src/main/scala/is/hail/io/vcf/LoadVCF.scala; @@ -109,7 +109,7 @@ object LoadVCF {; (line.getType == VCFHeaderLineType.Flag && line.getCount == 0))); Field(id, baseType, i, attrs); else; - Field(id, TArray(baseType), i, attrs); + Field(id, TArray(!baseType), i, attrs); }; ; def headerSignature[T <: VCFCompoundHeaderLine](lines: java.util.Collection[T],; @@ -124,10 +124,10 @@ object LoadVCF {; callFields: Set[String] = Set.empty[String]): (TStruct, Int) = {; val canonicalFields = Array(; ""GT"" -> TCall(),; - ""AD"" -> TArray(TInt32()),; + ""AD"" -> TArray(!TInt32()),; ""DP"" -> TInt32(),; ""GQ"" -> TInt32(),; - ""PL"" -> TArray(TInt32())); + ""PL"" -> TArray(!TInt32())); ; val raw = headerSignature(lines, callFields); ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2416#issuecomment-343640495:391,Load,LoadVCF,391,https://hail.is,https://github.com/hail-is/hail/pull/2416#issuecomment-343640495,6,['Load'],['LoadVCF']
Performance,"This is fine: [x.txt](https://github.com/hail-is/hail/files/12231007/x.txt). ```; In [11]: import hail as hl; ...: gwas = hl.read_table(""gwas_filtered.ht""); ...: loci_to_gene = hl.import_table(""x.txt"",impute=True); ...: locus = hl.locus(loci_to_gene.chromosome, loci_to_gene.locus, ""GRCh38""); ...: loci_to_gene = loci_to_gene.annotate(locus=locus); ...: loci_to_gene = loci_to_gene.key_by(""locus""); ...: loci_to_gene = loci_to_gene.select(""gene""); ...: loci_to_gene = loci_to_gene.filter(loci_to_gene.locus.position == 51749536); ...: loci_to_gene = loci_to_gene.checkpoint('/tmp/foo.ht', overwrite=True); ...: print(gwas.collect()); ...: print(loci_to_gene.collect()); ...: gwas.annotate(gene=loci_to_gene[gwas.locus].gene).collect(); 2023-08-01 10:54:33.166 Hail: INFO: Reading table to impute column types; 2023-08-01 10:54:33.864 Hail: INFO: Finished type imputation; Loading field '' as type int32 (imputed); Loading field 'chromosome' as type str (imputed); Loading field 'locus' as type int32 (imputed); Loading field 'gene' as type str (imputed); 2023-08-01 10:54:34.137 Hail: INFO: Coerced sorted dataset; 2023-08-01 10:54:35.270 Hail: INFO: wrote table with 1 row in 1 partition to /tmp/foo.ht; [Struct(locus=Locus(contig=chr8, position=51749536, reference_genome=GRCh38), alleles=['G', 'T'])]; [Struct(locus=Locus(contig=chr8, position=51749536, reference_genome=GRCh38), gene='PXDNL')]; Out[11]: [Struct(locus=Locus(contig=chr8, position=51749536, reference_genome=GRCh38), alleles=['G', 'T'], gene='PXDNL')]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13339#issuecomment-1660499132:872,Load,Loading,872,https://hail.is,https://github.com/hail-is/hail/issues/13339#issuecomment-1660499132,4,['Load'],['Loading']
Performance,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:310,load,load,310,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082,1,['load'],['load']
Performance,"This is half code cleanup and half guardrail. The key assertion here is: if the request authenticates using a cookie and is attempting a state-changing HTTP method, it should pass a CSRF check. I tested this with the following:; 1. dev deployed and loaded the billing projects page; 2. Deleted the hidden csrf input from one of the forms; 3. Submitted the form; 4. Got a 401",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13604:249,load,loaded,249,https://hail.is,https://github.com/hail-is/hail/pull/13604,1,['load'],['loaded']
Performance,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8757:786,perform,performs,786,https://hail.is,https://github.com/hail-is/hail/pull/8757,1,['perform'],['performs']
Performance,"This is indeed tricky, but I think there are other options here. For example, we could expand the spec so that a job specifies *either* a job_id or update_relative_job_id, and same for parents. This way the relative vs absolute job id is baked into the schema of the spec, and not inferred from the sign of the job id. Though similar in concept, I think that would be much less confusing. However,. > We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd like to try this first. I feel like if we get a really solid API and it has a couple of superfluous requests in some edge cases, we will be able to come up with good performance shortcuts that don't muddle the normal path. Since the Query Driver currently lives the full life of the batch and is likely to stay that way for a while, it will satisfy these conditions without making any extra requests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685:757,perform,performance,757,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685,2,['perform'],['performance']
Performance,This is pseudo-stacked on #12490 in that I made a change here to serialize `_async_execute_many` but in #12490 I remove it altogether which takes care of the complications of multiple concurrent Query Drivers in the same batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12464#issuecomment-1324194896:184,concurren,concurrent,184,https://hail.is,https://github.com/hail-is/hail/pull/12464#issuecomment-1324194896,1,['concurren'],['concurrent']
Performance,"This is ready for final review. Since Konrad is blocked by it, I suggest, unless there are correctness failures or major performance issues, we merge it and I will address comments in follow up PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575711955:121,perform,performance,121,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575711955,1,['perform'],['performance']
Performance,"This is running into the split_multi issue. it *would* be running into this error in the RVD iterator, but we don't perform this check too many places anymore nowadays and it's not hitting this check anywhere after the split happens.; ```; if (localType.kRowOrd.gt(prevK.value, rv)) {; kUR.set(prevK.value); val prevKeyString = kUR.toString(). prevK.setSelect(localType.rowType, localType.kFieldIdx, rv); kUR.set(prevK.value); val currKeyString = kUR.toString(); fatal(; s""""""RVD error! Keys found out of order:; | Current key: $currKeyString; | Previous key: $prevKeyString; |This error can occur after a split_multi if the dataset; |contains both multiallelic variants and duplicated loci.; """""".stripMargin); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292:116,perform,perform,116,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292,1,['perform'],['perform']
Performance,This is still happening. I think particularly under heavy batch load it might take a while for this to propagate. https://ci.azure.hail.is/batches/34523/jobs/102,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11480:64,load,load,64,https://hail.is,https://github.com/hail-is/hail/pull/11480,1,['load'],['load']
Performance,This is sufficiently large to permit the transmission of the Hail JAR; which is about 38 MB. I will use this to test and eventually normally; use the Gradle build cache server.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7801:163,cache,cache,163,https://hail.is,https://github.com/hail-is/hail/pull/7801,1,['cache'],['cache']
Performance,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:985,perform,perform,985,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881,2,['perform'],['perform']
Performance,"This is the beginning of a series of changes to support export of VDS to VCF 4.5, the version of VCF that contains the standardized form of our work that culminated in SVCR/VDS. Reference blocks were standardized with a LEN rather than an END. So, now, by default, add LEN to all VDS reads and drop END in favor of LEN on all VDS writes. Our optimizer will be able to take care of pruning away the dead field in pipelines that don't use it. We make sure that all VDS creation (other than the combiner), such as read_vds and from_merged_representation, contains both LEN and END preserving user code that depends on the presence of the END field. Furthermore, this change contains necessary combiner updates to prefer LEN over END, and to use LEN in the combiner itself.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14675:342,optimiz,optimizer,342,https://hail.is,https://github.com/hail-is/hail/pull/14675,1,['optimiz'],['optimizer']
Performance,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3461:321,load,loaded,321,https://hail.is,https://github.com/hail-is/hail/pull/3461,2,['load'],"['loaded', 'loading']"
Performance,"This is the first chunk of the C++ support, giving Scala ref-counted pointers to C++ objects. On its own, this doesn't do anything very useful. But we need this infrastructure to support; off-heap Region. And a later PR will support compilation/loading of Scala-generated C++; functions (which also needs this infrastructure).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3595:245,load,loading,245,https://hail.is,https://github.com/hail-is/hail/pull/3595,1,['load'],['loading']
Performance,"This is the first step to removing batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376:933,cache,cached,933,https://hail.is,https://github.com/hail-is/hail/pull/10376,1,['cache'],['cached']
Performance,"This is the plan for the new Hail CI (tentatively: Hephaestus aka h8s [but Hodor is also in the running, see CI software name in Zulip for the real big questions of our time]). # Expected Repo Structure; Every repository to be tested has at least two files: `hail-ci-build-image` and `hail-ci-build.sh`. The former contains a docker image in a publicly accessible repository. The latter is a shell script that exits with 0 if this branch passes the tests, otherwise it exists with a non-zero code. The logs of this shell script will be shared publicly via the GH PR Status. This script will be executed in the image referenced by `hail-ci-build-image`. # Dockerfile.pr-builder; I carefully wrote a docker file to cache as much gradle crap as possible. # gitHash in Gradle; I pushed `gitHash`'s definition into the `doLast` blocks of the gradle steps that actually need it. `doLast` is only run when the task is actually requested. This allows me to run `downloadDependencies` without creating a dependency on the entire `.git` directory (which changes with each commit, thus invalidating the cache'd docker image).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4066:713,cache,cache,713,https://hail.is,https://github.com/hail-is/hail/pull/4066,2,['cache'],['cache']
Performance,This is the start of a series of changes that will culminate in the generation of static ordering methods between PTypes. The next step here is to get reference genomes into static container classes as well. We need this change first because; reference genomes may use the filesystem to load a fasta.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9044#issuecomment-652536769:287,load,load,287,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652536769,1,['load'],['load']
Performance,"This is ungodly slow. ```python; In [4]: %%timeit; ...: with hdfs_read('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:; 100 loops, best of 3: 8.51 ms per loop. In [5]: %%timeit; ...: with open('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:. The slowest run took 8.48 times longer than the fastest. This could mean that an intermediate result is being cached.; 100000 loops, best of 3: 12.7 µs per loop; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1645#issuecomment-292253230:380,cache,cached,380,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-292253230,1,['cache'],['cached']
Performance,"This is used in lowering `MatrixAnnotateColsTable`, and will allow; us to optimize away unnecessary keying shuffles that happen in column; annotation table manipulations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5860:74,optimiz,optimize,74,https://hail.is,https://github.com/hail-is/hail/pull/5860,1,['optimiz'],['optimize']
Performance,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024:1139,perform,perform,1139,https://hail.is,https://github.com/hail-is/hail/pull/7024,2,"['multi-thread', 'perform']","['multi-threaded', 'perform']"
Performance,"This isn't _really_ a bug. It's a combination of two things:. 1. Parquet can't write partitions larger than 2**31 (2.1G or so); 2. gzipped VCF can't be split, so the entire thing was loaded as 1 partition",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806#issuecomment-301426712:183,load,loaded,183,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301426712,1,['load'],['loaded']
Performance,"This isn't as easy as I had hoped. We have to sort out how to either let the container directly create overlay mounts or figure out how to get fuse-overlay working. I think for fuse-overlay, we might need to modify the VM image to include fuse-overlay. ```; + set +x; Using GOOGLE_APPLICATION_CREDENTIALS; + export TMPDIR=/io/; + TMPDIR=/io/; + retry buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; + buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; STEP 1/2: FROM us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04; Trying to pull us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04...; Getting image source signatures; Copying blob sha256:ca1778b6935686ad781c27472c4668fc61ec3aeb85494f72deb1921892b9d39e; Copying config sha256:88bd6891718934e63638d9ca0ecee018e69b638270fe04990a310e5c78ab4a92; Writing manifest to image destination; Storing signatures; time=\""2023-05-26T14:52:12Z\"" level=error msg=\""Unmounting /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged: invalid argument\""; Error: mounting new container: mounting build container \""45e0ed631d22b6e1de7945266efcf0b802aa3b919d6b6ebd529ded6fedc11cf9\"": creating overlay mount to /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged, mount_data=\""lowerdir=/var/lib/containers/storage/overlay/l/ZCKOX3GV2VWHWT4DMPLYJGMJWL,upperdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/diff,workdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/work,nodev,fsync=0,volatile\"": using mount program /usr/bin/fus",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692:456,cache,cache-from,456,https://hail.is,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692,8,['cache'],"['cache', 'cache-from', 'cache-to']"
Performance,"This isn't exactly meant to be merged as-is, but more to start a discussion. This set of changes (along with the PR before) turn the following IR:. ```; (TableCount; (MatrixColsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False True ""{\""name\"":\""MatrixNativeRead; ```. Into the lowered. ```; (TableCount; (TableKeyBy (s) False; (TableParallelize None; (Let __cols_and_globals; (TableGetGlobals; (TableZipUnchecked; (TableMapGlobals; (TableMapGlobals; (TableRead ""/Users/tpoterba/data/profile.mt/rows"" True Table{global:Struct{},key:[locus,alleles],row:Struct{locus:Locus(GRCh37),alleles:Array[String]}}); (Literal Struct{} <literal value>)); (InsertFields; (Ref global); (__cols; (GetField rows; (TableCollect; (TableRead ""/Users/tpoterba/data/profile.mt/cols"" False Table{global:Struct{},key:[],row:Struct{s:String}})))))); (TableRead ""/Users/tpoterba/data/profile.mt/entries"" True Table{global:Struct{},key:[],row:Struct{`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[Struct{}]}}))); (MakeStruct; (rows; (ArrayMap elt; (ArraySort True; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref __cols_and_globals))); (I32 1)); (Let __cols_element; (ArrayRef; (GetField __cols; (Ref __cols_and_globals)); (Ref i)); (MakeStruct; (_1; (SelectFields (s); (Ref __cols_element))); (_2; (Ref __cols_element))))); (True)); (GetField _2; (Ref elt)))); (global; (SelectFields (); (Ref __cols_and_globals)))))))); ```. And finally optimize this IR to:. ```; (I64 2535); ```. 😃",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5043#issuecomment-449876531:1568,optimiz,optimize,1568,https://hail.is,https://github.com/hail-is/hail/pull/5043#issuecomment-449876531,1,['optimiz'],['optimize']
Performance,"This issue could have been an RFC, but that felt too heavy. I can move this to a formal RFC if desired, but otherwise feedback and/or questions welcome in the discussion here. # Idea; For any key type, create an encoding to variable-length byte arrays, which preserves the key ordering. That way, algorithms and data structures which use key comparisons can be written monomorphically, with `memcmp` as the only comparison function needed. Idea inspired by [Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust](https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/) blog post. But while they've optimized for vectorized encoding (which we currently can't do), I've preferred simplicity and smaller encodings. # Design; Type encoders can emit three kinds of output to a byte array buffer:; - byte - simply add a byte to the result, first padding an incomplete byte if necessary; - bit - add a bit to the result, possibly leaving an incomplete byte. We must know statically how many bits are used in the byte.; - pad - add `0`s to pad the last incomplete byte. This is safe (prefix-free) because the number of used bits is a (statically known) constant. We use this to ensure the number of used bits is known statically.; 	; Types:; - missingness; - treat as a type constructor `optional<T>`, i.e. base types don't encode missingness. Emits a single bit in the encoding. Can invert this bit to control whether missing values come first or last in the ordering. If missing, nothing is emitted after.; - sort-order; - treat reversing the default ordering as a type constructor `reverse<T>`; - simply inverts the encoding bitwise; - primitive types; - same as in datafusion, encoding has same size as original type; - signed integers - flip the sign bit; - floating point numbers - if sign bit is set, invert all bits, otherwise only flip the sign bit; - arrays; - before each element and after last element, emit continuation bit (0 if no more elements); - pad be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14396:637,optimiz,optimized,637,https://hail.is,https://github.com/hail-is/hail/issues/14396,1,['optimiz'],['optimized']
Performance,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3707:51,load,loader,51,https://hail.is,https://github.com/hail-is/hail/pull/3707,3,['load'],"['load', 'loader']"
Performance,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361:1694,perform,performance,1694,https://hail.is,https://github.com/hail-is/hail/pull/8361,1,['perform'],['performance']
Performance,"This looks good. Long-term, we'll have Jon's HCS as an optimized gt-only VDS, yes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/736#issuecomment-244489292:55,optimiz,optimized,55,https://hail.is,https://github.com/hail-is/hail/pull/736#issuecomment-244489292,1,['optimiz'],['optimized']
Performance,"This means we don't need out Emit params to take a region as arg to load, which was super messy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12249:68,load,load,68,https://hail.is,https://github.com/hail-is/hail/pull/12249,1,['load'],['load']
Performance,"This needs tests as well as I think I need to fix indexing (so I don't blow memory on the full genome), but I wanted to share what I've been up to with y'all. Also, caitlin can use this branch to run an analysis if it comes to that. I would also appreciate some feedback on the approach. It would be much more ideal to just make joins of variant tables against BGENs smarter, but I think the infrastructure necessary for that is big. cc: @cseed. List of changes:. - added `_variants_per_file` limits the loaded variants to variants at the given array of indexes (0-indexed, same order as on disk). - ~added `row_fields` which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:504,load,loaded,504,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['load'],['loaded']
Performance,"This node encapsulates a pattern we use in several places, and lets us generate IR that we can optimize more effectively.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5986:95,optimiz,optimize,95,https://hail.is,https://github.com/hail-is/hail/pull/5986,1,['optimiz'],['optimize']
Performance,"This option controls parsing and loading of the `rsid` and `varid`; BGEN row fields. When (in a future PR) the reader does not decompress; the genotypes (nor decode them), these row field imports become; a substantial portion of the time necessary to load just the row; keys. broken out from: https://github.com/hail-is/hail/pull/3727",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3779:33,load,loading,33,https://hail.is,https://github.com/hail-is/hail/pull/3779,2,['load'],"['load', 'loading']"
Performance,"This patch and the contents of #13970 was sufficient to successfully run `make -C hail install SPARK_VERSION=3.4.0` (but is incompatible with Spark 3.3). ```diff; diff --git a/hail/build.gradle b/hail/build.gradle; index 1b65904484..d1feb0e578 100644; --- a/hail/build.gradle; +++ b/hail/build.gradle; @@ -40,7 +40,7 @@ tasks.withType(JavaCompile) {; }; ; project.ext {; - breezeVersion = ""1.1""; + breezeVersion = ""2.1.0""; ; sparkVersion = System.getProperty(""spark.version"", ""3.3.0""); ; diff --git a/hail/src/main/scala/is/hail/HailContext.scala b/hail/src/main/scala/is/hail/HailContext.scala; index 4e4063378b..4d2f9056a5 100644; --- a/hail/src/main/scala/is/hail/HailContext.scala; +++ b/hail/src/main/scala/is/hail/HailContext.scala; @@ -113,10 +113,10 @@ object HailContext {; ; {; import breeze.linalg._; - import breeze.linalg.operators.{BinaryRegistry, OpMulMatrix}; + import breeze.linalg.operators.{BinaryRegistry, HasOps, OpMulMatrix}; ; implicitly[BinaryRegistry[DenseMatrix[Double], Vector[Double], OpMulMatrix.type, DenseVector[Double]]].register(; - DenseMatrix.implOpMulMatrix_DMD_DVD_eq_DVD); + HasOps.impl_OpMulMatrix_DMD_DVD_eq_DVD); }; ; theContext = new HailContext(backend, branchingFactor, optimizerIterations); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13971#issuecomment-1792870445:1214,optimiz,optimizerIterations,1214,https://hail.is,https://github.com/hail-is/hail/issues/13971#issuecomment-1792870445,1,['optimiz'],['optimizerIterations']
Performance,This performs the operations described in the `NOTE` at the end of `vcf_combiner.py`. The only thing that I feel may be confusing is that we recompute the `DP` info fields as the sum of the `DP` entry fields. We do this as `densify` (currently a placeholder) may populate a previously missing entry.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5280:5,perform,performs,5,https://hail.is,https://github.com/hail-is/hail/pull/5280,1,['perform'],['performs']
Performance,This permits shuffle-free load of files where; multiallelics are split across partitions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/787:26,load,load,26,https://hail.is,https://github.com/hail-is/hail/pull/787,2,['load'],['load']
Performance,"This prevents decorators like `@rest_authenticated_users_only` from making an additional request to `auth` (which includes a database query for the `userinfo` endpoint) on every API request. I also realized that auth wasn't getting scraped by prometheus so I added the `grafanak8sapp` label to fix that. auth's `userinfo` endpoint should probably also have a cache, but it felt worth it to also implement it this way to avoid constant communication with the auth service from batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12122:359,cache,cache,359,https://hail.is,https://github.com/hail-is/hail/pull/12122,1,['cache'],['cache']
Performance,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3332:108,optimiz,optimizations,108,https://hail.is,https://github.com/hail-is/hail/pull/3332,1,['optimiz'],['optimizations']
Performance,"This provides necessary performance improvements when creating; an array of empty structs, which should have o(1) cost, not; O(n) cost.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3773:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/pull/3773,1,['perform'],['performance']
Performance,This quote from the 2.12.0 collections docs is as depressing and still seemingly misguided:. > (Since version 2.12.0) mutable.Stack is an inelegant and potentially poorly-performing wrapper around List. Use a List assigned to a var instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2185#issuecomment-326970456:171,perform,performing,171,https://hail.is,https://github.com/hail-is/hail/pull/2185#issuecomment-326970456,1,['perform'],['performing']
Performance,"This reduced the number of logs in ""worker.log"" by over 80% from 515K lines to 77K for an hour with some load. https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%0AlogName:%22worker%22%0ANOT%20labels.namespace%3D%22default%22%0ANOT%20%22crun%20process%22%20AND%20NOT%20%22crun%20run%20process%22%20AND%20NOT%20%22marking%20complete%22%20AND%20NOT%20%22initializing%22%20AND%20NOT%20%22running%20input%22%20AND%20NOT%20%22input:%22%20AND%20NOT%20%22running%20main%22%20AND%20NOT%20%22main:%22%20AND%20NOT%20%22running%20output%22%20AND%20NOT%20%22output:%22%20AND%20NOT%20%22cleaning%20up%22%20AND%20NOT%20%22downloading%20JAR%22%20AND%20NOT%20%22running%20jvm%20process%22%20AND%20NOT%20%22uploading%20log%22%20and%20NOT%20%22Obtained%20writer%22%20AND%20NOT%20%22finished%20normally%22%20AND%20NOT%20%22was%20cancelled%22%20AND%20NOT%20%22user%20exception%20encountered%22%20AND%20NOT%20%22:%20execute%22%20AND%20NOT%20%22JVM-%22%20AND%20NOT%20%22healthcheck%22%20AND%20NOT%20%22%2Fapi%2Fv1alpha%2Fbatches%2Fjobs%2Fcreate%22;timeRange=2022-06-06T17:00:59.759Z%2F2022-06-06T18:00:59.759Z;cursorTimestamp=2022-06-06T17:53:15.865608694Z?project=hail-vdc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11898:105,load,load,105,https://hail.is,https://github.com/hail-is/hail/pull/11898,1,['load'],['load']
Performance,This removes the name conflict with the `key` attribute of a table. I also changed the test `count` to `_force_count` to be sure they aren't ever optimized.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4257:146,optimiz,optimized,146,https://hail.is,https://github.com/hail-is/hail/pull/4257,1,['optimiz'],['optimized']
Performance,"This represents a small redesign of ValueWriter as well. Add deserializers, subclasses of ValueReader and have ReadValue use them. A ValueReader deserializes a single hail value from an input stream. This change also alters the semantics of ValueWriters. Both readers and writers no longer manage their own I/O resources. It is the responsibility of 'callers' to do so instead. However programmers must be careful as we need to create input/output buffers to perform native serialization/deserialization. For InputBuffers in particular, the underlying stream may be left in an unusable state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12948:459,perform,perform,459,https://hail.is,https://github.com/hail-is/hail/pull/12948,1,['perform'],['perform']
Performance,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:658,cache,cache-dir,658,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882,1,['cache'],['cache-dir']
Performance,"This should always force the temporary table to never be materialized for `billing_project_users`. > STRAIGHT_JOIN is similar to JOIN, except that the left table is always read before the right table. This can be used for those (few) cases for which the join optimizer processes the tables in a suboptimal order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12057:259,optimiz,optimizer,259,https://hail.is,https://github.com/hail-is/hail/pull/12057,1,['optimiz'],['optimizer']
Performance,This should be a big performance boost.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4769:21,perform,performance,21,https://hail.is,https://github.com/hail-is/hail/pull/4769,1,['perform'],['performance']
Performance,"This should be functioning (or very close) in GCP but is basically unimplemented in Azure. Azure needs a secret store like how we use Google Secret Manager that workers can access when they start up to load certificates. Azure Key Vault seems reasonable and can be created through terraform. The structure should mirror that in GCP, where we need a client that can upload the certs to Azure in `create_certs` and download them in the azure CloudWorkerAPI. I would leave this unimplemented in TerraAzure until an overall secrets story is established.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14581#issuecomment-2243030552:202,load,load,202,https://hail.is,https://github.com/hail-is/hail/pull/14581#issuecomment-2243030552,1,['load'],['load']
Performance,This should be working. I'll add the optimization to not double schedule jobs in another PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7833#issuecomment-572811460:37,optimiz,optimization,37,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-572811460,1,['optimiz'],['optimization']
Performance,"This should have almost no effect on performance, since we already scan the thing for partitioning",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2239#issuecomment-334155210:37,perform,performance,37,https://hail.is,https://github.com/hail-is/hail/pull/2239#issuecomment-334155210,1,['perform'],['performance']
Performance,"This should include performance experiments we use to drive design decisions (e.g., Array vs. Vector) so we know what assumptions change when we upgrade Scala/Spark/JVM or underlying abstractions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/19#issuecomment-152269229:20,perform,performance,20,https://hail.is,https://github.com/hail-is/hail/issues/19#issuecomment-152269229,1,['perform'],['performance']
Performance,This should lighten the database load a bit by avoiding a couple joins and a CTE.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12651:33,load,load,33,https://hail.is,https://github.com/hail-is/hail/pull/12651,1,['load'],['load']
Performance,"This should make computing the loadings better, since it uses a checkpointed variants table instead of accidentally recomputing the incoming MatrixTable. Also made a change to avoid accidentally clobbering a field name if someone had a field named `idx`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10201:31,load,loadings,31,https://hail.is,https://github.com/hail-is/hail/pull/10201,1,['load'],['loadings']
Performance,This should play well with [the optimizer](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/Relational.scala#L189-L192),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2285:32,optimiz,optimizer,32,https://hail.is,https://github.com/hail-is/hail/pull/2285,1,['optimiz'],['optimizer']
Performance,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7825:100,cache,cache,100,https://hail.is,https://github.com/hail-is/hail/pull/7825,1,['cache'],['cache']
Performance,This shouldn't be too hard. The only issue I can forsee is needing to convert the `Code[Byte]` from `loadByte` to `Code[Int]`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811#issuecomment-1989182606:101,load,loadByte,101,https://hail.is,https://github.com/hail-is/hail/issues/13811#issuecomment-1989182606,1,['load'],['loadByte']
Performance,This simplifies the code and means that region bookeeping for the prune; queue is unnecessary.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12076:73,queue,queue,73,https://hail.is,https://github.com/hail-is/hail/pull/12076,1,['queue'],['queue']
Performance,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6547:805,latency,latency,805,https://hail.is,https://github.com/hail-is/hail/issues/6547,1,['latency'],['latency']
Performance,"This was a good one :). I haven’t measured the performance difference, but clearly the bug defeated the purpose of using a union-find data structure in the first place, which was to reduce the complexity of `unify` from quadratic to linear. While I was here, I made a separate simplification. Now that the sets are being unioned as intended, each set contains exactly one block that doesn’t start with a `GotoX`, and that block is the final target of all blocks in the set. That observation allows a simplification when computing the `rootFinalTarget` map.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9475:47,perform,performance,47,https://hail.is,https://github.com/hail-is/hail/pull/9475,1,['perform'],['performance']
Performance,"This was identified as a cost center during my compiler investigations. The effect is not measurable on the standard benchmark [1]. More investigation is needed. As we improve the performance of the expression language, I suspect this will constitute a more significant fraction of execution time. [1]:; ```; filtergenotypes -c ' g.dp > 400 ||; (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) ||; (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) ||; (g.isHet && ( (g.ad[0] + g.ad[1]) / g.dp < 0.9 || g.ad[1] / g.dp < 0.20 || g.pl[0] < 20 ))' --keep; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1292:180,perform,performance,180,https://hail.is,https://github.com/hail-is/hail/pull/1292,1,['perform'],['performance']
Performance,This was preventing use of cache when building the notebook leader image. The `$*` variable is for use with [Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables). Still note sure why the build is failing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5226:27,cache,cache,27,https://hail.is,https://github.com/hail-is/hail/pull/5226,1,['cache'],['cache']
Performance,"This was unused, but was a performance nightmare -- the reconstruction of a single type triggered a full reallocation of the nested structure. ![image](https://user-images.githubusercontent.com/10562794/127706520-a3202b3f-b478-407d-b64f-496a4f08f68b.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10734:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/pull/10734,1,['perform'],['performance']
Performance,"This week, subsequent PRs will address:; - Implement `stop` which frees memory.; - Test many concurrent clients.; - Implement TableKeyBy and TableOrderBy with this (hidden by a hail context flag).; - Package this as a k8s service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-604091281:93,concurren,concurrent,93,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-604091281,1,['concurren'],['concurrent']
Performance,"This will be fixed in the next release.; If you can't wait until then and you're comfortable patching this yourself, replace the contents of the affected file with the following:; ```yaml; dataproc:; init_notebook.py: gs://hail-common/hailctl/dataproc/0.2.129/init_notebook.py; vep-GRCh37.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh37.sh; vep-GRCh38.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh38.sh; wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl; pip_dependencies: aiodns==2.0.0|aiohttp==3.9.3|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.2.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.30.1|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.4|boto3==1.34.55|botocore==1.34.55|cachetools==5.3.3|certifi==2024.2.2|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==42.0.5|decorator==4.4.2|deprecated==1.2.14|dill==0.3.8|frozenlist==1.4.1|google-auth==2.28.1|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.3|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.5|msal==1.27.0|msal-extensions==1.1.0|msrest==0.7.1|multidict==6.0.5|nest-asyncio==1.6.0|numpy==1.26.4|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.2.1|parsimonious==0.10.0|pillow==10.2.0|plotly==5.19.0|portalocker==2.8.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.9.0.post0|python-json-logger==2.0.7|pytz==2024.1|pyyaml==6.0.1|regex==2023.12.25|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.10.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity==8.2.3|tornado==6.4|typer==0.9.0|typing-extensions==4.10.0|tzdata==2024.1|urllib3==1.26.18|uvloop==0.19.0;sys_platform!=""win32""|wrapt==1.16.0|xyzservices==2023.10.1|yarl==1.9.4|; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651:801,cache,cachetools,801,https://hail.is,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651,1,['cache'],['cachetools']
Performance,This will hopefully make it harder to accidentally build in debug mode when doing performance testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14306:82,perform,performance,82,https://hail.is,https://github.com/hail-is/hail/pull/14306,1,['perform'],['performance']
Performance,"This will make `TableHead` ~and `TableTail`~ lowered implementation performant enough to actually use. EDIT: I was doing something wrong with `TableTail`, decided to just leave it for a follow up PR instead of having this sit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10719:68,perform,performant,68,https://hail.is,https://github.com/hail-is/hail/pull/10719,1,['perform'],['performant']
Performance,"This will prevent optimization around the filter intervals. Given the prevalence of filter intervals in the biggest, baddest pipelines, this is a concern.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209:18,optimiz,optimization,18,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209,1,['optimiz'],['optimization']
Performance,"This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1006,cache,cachedRsid,1006,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437,2,['cache'],['cachedRsid']
Performance,"Tim, would you have any recommendations?. ```sh; Gradle suite > Gradle test > is.hail.expr.ir.IRSuite.regressionTestUnifyBug FAILED; scala.MatchError: +Void (of class is.hail.expr.types.physical.PVoid$); at is.hail.expr.types.physical.PType.setRequired(PType.scala:206); at is.hail.expr.ir.InferPType$.apply(InferPType.scala:209); at is.hail.expr.ir.IR$class.inferSetPType(IR.scala:33); at is.hail.expr.ir.ArrayMap.inferSetPType(IR.scala:232); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:359); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:358); ```. This occurs because the IR is the following:. ```sh; ; //a; Literal(array<interval<locus<GRCh37>>>,WrappedArray([20:10277621-20:11898992))); // name; __iruid_11; //body; ApplySpecial(Interval,ArrayBuffer(MakeStruct(ArrayBuffer((locus,ApplySpecial(start,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), MakeStruct(ArrayBuffer((locus,ApplySpecial(end,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), True(), False())); ```. So I set this IR in the match to return PVoid(). However, in ArrayMap there is a requiredeness setter (which is inspired/copied from InferType's corresponding match:. ```scala; coerce[PStreamable](a.pType2).copyStreamable(body.pType2.setRequired(false)); ```. Which causes a nonsensical operation on PVoid, which is required: true (override final val). As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of its`Seq[IR]`'s are different. I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-512957123:1784,optimiz,optimization,1784,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-512957123,1,['optimiz'],['optimization']
Performance,"Timing is not clear, gonna try an LRU cache. I will close until I have time to work on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3450#issuecomment-385008102:38,cache,cache,38,https://hail.is,https://github.com/hail-is/hail/pull/3450#issuecomment-385008102,2,['cache'],['cache']
Performance,"Timing on laptop of read, method, count:. count(genotypes=True): 4.17 s; count: 1.84s; linreg: 7.1 s; score: 51 s; lrt: 92 s; wald: 93 s; firth: 300 s. ```; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True); %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True) # 4.17 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count() # 1.84 s. %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 7.1 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 51 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 92 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 93 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 300 s; ```. ![logreg performance](https://cloud.githubusercontent.com/assets/3201642/23095382/18e310de-f5d7-11e6-9b0b-909b6107286b.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507:1062,perform,performance,1062,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507,1,['perform'],['performance']
Performance,"Timing:. ```; mysql> SELECT job_id, spec, cores_mcpu FROM jobs WHERE batch_id = 10 AND state = 'Ready' AND always_run = 0 AND `cancelled` = 0 LIMIT 1;; +---------+------------------------------------------------------------------+------------+; | job_id | spec | cores_mcpu |; +---------+------------------------------------------------------------------+------------+; | 1606431 | [[[""batch-pods"", ""konradk-gsa-key"", ""/gsa-key"", 1]], null, 1, 1] | 1000 |; +---------+------------------------------------------------------------------+------------+; 1 row in set (1.22 sec). mysql> SELECT job_id, spec, cores_mcpu FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled) WHERE batch_id = 10 AND state = 'Ready' AND always_run = 0 AND `cancelled` = 0 LIMIT 1;; +---------+------------------------------------------------------------------+------------+; | job_id | spec | cores_mcpu |; +---------+------------------------------------------------------------------+------------+; | 1606431 | [[[""batch-pods"", ""konradk-gsa-key"", ""/gsa-key"", 1]], null, 1, 1] | 1000 |; +---------+------------------------------------------------------------------+------------+; 1 row in set (0.00 sec); ```. Explain verifies that the original query was performing a full batch scan.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8093:1240,perform,performing,1240,https://hail.is,https://github.com/hail-is/hail/pull/8093,1,['perform'],['performing']
Performance,"Tiny tiny tiny point about the API docs: query_variants says, ""Performs aggregation queries over variants and variant annotations, and returns python object(s) and type(s)."" But should end after ""object(s)"". It would've been faster for me to clone the repo, make this changes and submit a pull request. . Sorry.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1522:63,Perform,Performs,63,https://hail.is,https://github.com/hail-is/hail/issues/1522,1,['Perform'],['Performs']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Package Info:; Name: hail; Version: 0.2.93; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages; Requires: dill, bokeh, scipy, azure-storage-blob, janus, parsimonious, botocore, google-cloud-storage, tabulate, Jinja2, python-json-logger, plotly, avro, azure-identity, PyJWT, orjson, tqdm, aiohttp-session, google-auth, nest-asyncio, uvloop, humanize, hurry.filesize, decorator, requests, Deprecated, aiohttp, asyncinit, numpy, pyspark, sortedcontainers, boto3, pandas. -----------------------------------------------------------------------------. Importing hail via the IPython console in Spyder causes the following error:. Python 3.8.12 (default, Oct 12 2021, 06:23:56) ; IPython 8.2.0 -- An enhanced Interactive Python. In [1]: `import hail`. > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py"", line 36, in run; > task = asyncio.ensure_future(main); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/asyncio/tasks.py"", line 684, in ensure_future; > raise TypeError('An asyncio.Future, a coroutine or an awaitable is '; > TypeError: An asyncio.Future, a coroutine or an awaitable is required; > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Us",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758:346,Scalab,Scalable,346,https://hail.is,https://github.com/hail-is/hail/issues/11758,1,['Scalab'],['Scalable']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747:525,load,load,525,https://hail.is,https://github.com/hail-is/hail/issues/6747,1,['load'],['load']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:736,Load,Loading,736,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['Load'],['Loading']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2/devel hash eb1e04205793. ### What you did:; I'm trying to take PC loadings generated using Hail in one sample and project another sample into that PC space to generate scores for them. I've been using @danking's tricks developed for my PRS pipeline to speed things along and am now trying to feed the relevant inputs into gnomad Hail's [pc_project()](https://github.com/macarthur-lab/gnomad_hail/blob/master/utils/generic.py#L164) function. I've written out the function in script format for debugging purposes. Full code is below. . ```import hail as hl; import pickle; import time. generate_pcloadings_table = True; pcloadings_table_location = 'gs://ukbb_prs/sibdiff/keytables/ukb-pca-locus-allele-keyed.kt'; generate_contig_row_dict = True; contig_row_dict_location = 'gs://ukbb_prs/sibdiff/keytables/contig_row_dict-UKB'; output_location = 'gs://ukbb_prs/sibdiff/UKB_sibloadings.txt'; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; bgen_files = 'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}_v3.bgen'. # large block size because we read very little data (due to filtering & ignoring genotypes); hl.init(branching_factor=10, min_block_size=2000). ### set up the pcloadings table; if (generate_pcloadings_table):; pcloadings = hl.import_table('gs://phenotype_31063/ukb31063.gwas.pca_loadings.tsv.gz', impute=True); pcloadings = pcloadings.annotate(locus=hl.parse_locus(hl.str(pcloadings.chr) + "":"" + hl.str(pcloadings.pos)),; alleles=[pcloadings.ref,pcloadings.alt]).key_by('locus','alleles'). pcloadings.write(pcloadings_table_location, overwrite=True). pcloadings = hl.read_table(pcloadings_table_location). ### determine the file locations of the pca ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:323,load,loadings,323,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['load'],['loadings']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-ccaf3640241f. ### What you did:. ```; ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-17-671d2e9c22c8> in <module>(); 1 #ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ----> 2 ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True). /home/hail/hail.zip/hail/table.py in export(self, output, types_file, header, parallel); 994 """"""; 995 ; --> 996 self._jt.export(output, types_file, header, Env.hail().utils.ExportType.getExportType(parallel)); 997 ; 998 def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 186 import pyspark; 187 try:; --> 188 return f(*args, **kwargs); 189 except py4j.protocol.Py4JJavaError as e:; 190 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 321 raise Py4JE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033:474,cache,cache,474,https://hail.is,https://github.com/hail-is/hail/issues/4033,1,['cache'],['cache']
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2763:835,load,loadInt,835,https://hail.is,https://github.com/hail-is/hail/issues/2763,3,['load'],"['loadInt', 'loadLength']"
Performance,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Sorry for the awful formatting!! I'm not familiar with how to properly format for GitHub, and clearly some of my code got picked up as formatting.... . EDIT: Fixed the formatting :). ### Hail version: 0.2 / devel. ### What you did: ; ```; import. hail as hl. def flip_text(base):; """"""; :param StringExpression base: Expression of a single base; :return: StringExpression of flipped base; :rtype: StringExpression; """"""; return hl.cond(base == 'A', 'T',; hl.cond(base == 'T', 'A',; hl.cond(base == 'C', 'G',; hl.cond(base == 'G', 'C', base)))). # load ldpred sumstats file; sumstats = hl.import_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.txt', delimiter='\s+', impute=True). # create locus and alleles columns and key by locus; sumstats = (sumstats.annotate(locus=hl.parse_locus(sumstats.chrom[6:] + "":"" + hl.str(sumstats.pos)),; alleles=[sumstats.nt1,sumstats.nt2]); .key_by('locus'); .order_by('locus')). # repartition sumstats to save time; sumstats = sumstats.repartition(100). # write the sumstats table; sumstats.write('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt', overwrite=True). # read the sumstats table; sumstats = hl.read_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt'). # remove leading zeros from contigs; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}. # import variants; variants = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{22}_v3.bgen',; [],; sample_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # merge sumstats on bgen matrixtable; variants = variants.annotate_rows(ss = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:780,load,load,780,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['load'],['load']
Performance,"Total minutes for test_hail_python went up to 162 compared to last deploy at 152. Total minutes for local was 294 compared to last deploy at 312. No change in service backend runtime (I think service backend is bottlenecked on test parallelism in default and non-preemptible cores in PR, so maybe a less interesting number). Maybe the doubling of the block size is having a negative effect? Anyway, let's benchmark on something more realistic than the Hail tests and assess.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1536802594:211,bottleneck,bottlenecked,211,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1536802594,1,['bottleneck'],['bottlenecked']
Performance,Tracks latency and status code metrics on endpoints so we can see those too in the batch grafana dashboard.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10225:7,latency,latency,7,https://hail.is,https://github.com/hail-is/hail/pull/10225,1,['latency'],['latency']
Performance,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1185:1333,concurren,concurrent,1333,https://hail.is,https://github.com/hail-is/hail/issues/1185,2,['concurren'],['concurrent']
Performance,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:9,load,load,9,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['load'],['load']
Performance,Try using [`hl.experimental.pc_project`](https://hail.is/docs/0.2/experimental/index.html#hail.experimental.pc_project). You can also compute this yourself as follows. Note this assumes biallelic SNPs.; ```python3; # the loadings_table is the third return value from hwe_normalized_pca; mt = mt.annotate_rows(loadings = loadings_table[mt.row_key]); mt = mt.filter_rows(hl.is_defined(mt.loadings)); mt = hl.variant_qc(mt); n_variants = mt.count_rows(); gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.AF[1]) / hl.sqrt(n_variants * 2 * mt.AF[1] * (1 - mt.AF[1])); mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.loadings * gt_norm)).cols(); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-1298562226:309,load,loadings,309,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-1298562226,3,['load'],['loadings']
Performance,Tune write rows split files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5509:0,Tune,Tune,0,https://hail.is,https://github.com/hail-is/hail/pull/5509,1,['Tune'],['Tune']
Performance,Two big changes. Catch any errors and release the semaphore. Restart failed workers in the concurrent worker pool.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6804:91,concurren,concurrent,91,https://hail.is,https://github.com/hail-is/hail/pull/6804,1,['concurren'],['concurrent']
Performance,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:602,load,load,602,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512,1,['load'],['load']
Performance,"Ugh, discovered a problem with race conditions surrounding the `test.cpp` build path. Can cause failures with a naked `make -jN test` on a clean directory. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754:31,race condition,race conditions,31,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754,1,['race condition'],['race conditions']
Performance,Unassigning to get it off of my CI queue. Feel free to reassign when you want me to look at it again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13957#issuecomment-1795092510:35,queue,queue,35,https://hail.is,https://github.com/hail-is/hail/pull/13957#issuecomment-1795092510,1,['queue'],['queue']
Performance,"Under ideal conditions, I observed this code complete 12 jobs per second in a 1000 job; batch. However, it's usually closer to 6 jobs per second. There's still a deadlock in; `mark_job_complete` which hamstrings performance. I haven't thought carefully about why my metric; (jobs per second) is much lower than our usual metric (MJC per second). I will eventually squash; the `mark_job_complete` deadlock, but I haven't the time currently. The two main issues were the `batches` field `cancelled` and the `instances` field; `free_cores_mcpu`. We frequently locked the entire batch row just to prevent a batch from being; cancelled while other work was done. This effectively serialized all operation on jobs in the same; batch. We had the dual issue with free_cores_mcpu: we lock the entire instance record everywhere to; prevent the instance changing state while we update job information. This serialized updates to; free_cores_mcpu which serialized job operation for any job on the same instance. We also had some unnecessarily lock-heavy code inside `add_attempt`. This block of code is trying to; insert a new attempt record and update the associated instance's free cores. I also fixed some out of date values and bad syntax in `estimated-current.sql`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985:212,perform,performance,212,https://hail.is,https://github.com/hail-is/hail/pull/10985,1,['perform'],['performance']
Performance,"Unearthing this old issue. I agree with John's assessment. We should:. 1. Fix the docs for `n` to indicate it is the maximum number of returned strings, not the maximum number of splits.; 2. Deprecate the `n` parameter. Do this with a `warnings.warn` when `n` is not `None`.; 3. Introduce a new parameter `maxsplit` which is the maximum number of splits to perform. Document that `maxsplit` is equal to the old `n` parameters minus one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9383#issuecomment-1775409513:357,perform,perform,357,https://hail.is,https://github.com/hail-is/hail/issues/9383#issuecomment-1775409513,1,['perform'],['perform']
Performance,"Unused and incorrect -- if this method were used, it would not optimize; any IRs in the DAG, which may contain relational or value IRs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5834:63,optimiz,optimize,63,https://hail.is,https://github.com/hail-is/hail/pull/5834,1,['optimiz'],['optimize']
Performance,"Updated db.py to require user to specify region as shown below, so that data will be accessed from the requester pays bucket in the region specified by the user, available regions are `'us'` and `'eu'`. . `db = hl.experimental.DB(region='us')`; `mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics')`. Entries in the `annotation_db.json` file were modified to the following format:. ```; ""dataset_name"": { ""description"": ""some description here"",; ""key_properties"": [],; ""url"": ""https://www.someurlhere.com"",; ""versions"": [{""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""one_version""},; {""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""another_version""}]}; ```. The `annotation_db.json` file is now used by the `load_dataset()` function in `datasets.py` as well, any dataset in the JSON file should now be able to be loaded this way. Made changes to the following:; - `DB` class now requires a `region` parameter.; - `Dataset.from_name_and_json()` has had a `custom_config` parameter added that indicates whether or not the user has supplied their own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and rai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:920,load,loaded,920,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['load'],['loaded']
Performance,"Updated to break up struct construction when building the decoder. Tweaks the size estimates so the method size is 2-4K on some large examples. @tpoterba @konradjk I retimed loading a small subset of fields with this change. I'm not quite sure why things changed so much, but I now see about 4.5x improvement on read, which is more in line with (and even better than) what I was hoping:. ```; In [2]: t = hl.read_table('sites.ht', _row_fields=['locus', 'alleles', 'AN_AFR']). In [3]: %%timeit; ...: t._force_count(); ...: ; 3.43 s ± 54.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). In [4]: t = hl.read_table('sites.ht'). In [5]: %%timeit; ...: t._force_count(); ...: ; 15.3 s ± 45.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3669#issuecomment-392413464:174,load,loading,174,https://hail.is,https://github.com/hail-is/hail/pull/3669#issuecomment-392413464,1,['load'],['loading']
Performance,"Updates the requirements on [astroid](https://github.com/PyCQA/astroid) to permit the latest version.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/astroid/blob/main/ChangeLog"">astroid's changelog</a>.</em></p>; <blockquote>; <h1>What's New in astroid 2.10.0?</h1>; <p>Release date: 2022-02-27</p>; <ul>; <li>; <p>Fixed inference of <code>self</code> in binary operations in which <code>self</code>; is part of a list or tuple.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4826"">PyCQA/pylint#4826</a></p>; </li>; <li>; <p>Fixed builtin inference on <code>property</code> calls not calling the <code>postinit</code> of the new node, which; resulted in instance arguments missing on these nodes.</p>; </li>; <li>; <p>Fixed a crash on <code>Super.getattr</code> when the attribute was previously uninferable due to a cache; limit size. This limit can be hit when the inheritance pattern of a class (and therefore of the; <code>__init__</code> attribute) is very large.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5679"">PyCQA/pylint#5679</a></p>; </li>; <li>; <p>Inlcude names of keyword-only arguments in <code>astroid.scoped_nodes.Lambda.argnames</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5771"">PyCQA/pylint#5771</a></p>; </li>; <li>; <p>Fixed a crash inferring on a <code>NewType</code> named with an f-string.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5770"">PyCQA/pylint#5770</a></p>; </li>; <li>; <p>Add support for <a href=""https://github.com/python-attrs/attrs/releases/tag/21.3.0"">attrs v21.3.0</a> which; added a new <code>attrs</code> module alongside the existing <code>attr</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1330"">#1330</a></p>; </li>; <li>; <p>Use the <code>end_lineno</code> attribute for the <code>NodeNG.tolineno</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:902,cache,cache,902,https://hail.is,https://github.com/hail-is/hail/pull/11463,2,['cache'],['cache']
Performance,"Updates the requirements on [janus](https://github.com/aio-libs/janus) to permit the latest version.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/releases"">janus's releases</a>.</em></p>; <blockquote>; <h2>janus 1.0.0 release</h2>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Janus is marked as stable, no API changes was made for years</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/janus/blob/master/CHANGES.rst"">janus's changelog</a>.</em></p>; <blockquote>; <h2>1.0.0 (2021-12-17)</h2>; <ul>; <li>Drop Python 3.6 support</li>; </ul>; <h2>0.7.0 (2021-11-24)</h2>; <ul>; <li>Add SyncQueue and AsyncQueue Protocols to provide type hints for sync and async queues <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/374"">#374</a></li>; </ul>; <h2>0.6.2 (2021-10-24)</h2>; <ul>; <li>Fix Python 3.10 compatibility <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/358"">#358</a></li>; </ul>; <h2>0.6.1 (2020-10-26)</h2>; <ul>; <li>; <p>Raise RuntimeError on queue.join() after queue closing. <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/295"">#295</a></p>; </li>; <li>; <p>Replace <code>timeout</code> type from <code>Optional[int]</code> to <code>Optional[float]</code> <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/267"">#267</a></p>; </li>; </ul>; <h2>0.6.0 (2020-10-10)</h2>; <ul>; <li>; <p>Drop Python 3.5, the minimal supported version is Python 3.6</p>; </li>; <li>; <p>Support Python 3.9</p>; </li>; <li>; <p>Refomat with <code>black</code></p>; </li>; </ul>; <h2>0.5.0 (2020-04-23)</h2>; <ul>; <li>Remove explicit loop arguments and forbid creating queues outside event loops <a href=""https://github-redirect.dependabot.com/aio-libs/janus/issues/246"">#246</a></li>; </ul>; <h2>0.4.0 (2018-07-28)</h2>; <ul>; <li>; <p>Add <code>py.typed</code> macro <a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11466:815,queue,queues,815,https://hail.is,https://github.com/hail-is/hail/pull/11466,1,['queue'],['queues']
Performance,"Updates the requirements on [scipy](https://github.com/scipy/scipy) to permit the latest version.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/scipy/scipy/releases"">scipy's releases</a>.</em></p>; <blockquote>; <h1>SciPy 1.8.0 Release Notes</h1>; <p>SciPy <code>1.8.0</code> is the culmination of <code>6</code> months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.8.x branch, and on adding new features on the master branch.</p>; <p>This release requires Python <code>3.8+</code> and <code>NumPy 1.17.3</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A sparse array API has been added for early testing and feedback; this; work is ongoing, and users should expect minor API refinements over; the next few releases.</li>; <li>The sparse SVD library PROPACK is now vendored with SciPy, and an interface; is exposed via <code>scipy.sparse.svds</code> with <code>solver='PROPACK'</code>. It is currently; default-off due to potential issues on Windows that we aim to; resolve in the next release, but can be optionally enabled at runtime for; friendly testing with an environment variable setting of <code>USE_PROPACK=1</code>.</li>; <li>A new <code>scipy.stats.sampling</code> submodule that leverages the <code>UNU.RAN</code> C; library to sample from arbitrary univariate non-uniform c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11538:687,optimiz,optimizations,687,https://hail.is,https://github.com/hail-is/hail/pull/11538,1,['optimiz'],['optimizations']
Performance,Use NodeJS to compile docs for faster load times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/800:38,load,load,38,https://hail.is,https://github.com/hail-is/hail/pull/800,1,['load'],['load']
Performance,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242:49,cache,cache,49,https://hail.is,https://github.com/hail-is/hail/pull/5242,5,"['cache', 'optimiz', 'perform', 'response time']","['cache', 'optimize', 'performance', 'response time']"
Performance,Use class loader from asm4s instead of System,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3707:10,load,loader,10,https://hail.is,https://github.com/hail-is/hail/pull/3707,1,['load'],['loader']
Performance,Use set_message in CI and Batch to give users feedback after performing POST operations. I added set_message while working on notebook. Any message set will show up at the top of the next page loaded styled according to the message type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7160:61,perform,performing,61,https://hail.is,https://github.com/hail-is/hail/pull/7160,2,"['load', 'perform']","['loaded', 'performing']"
Performance,Use this to fully optimize `hl.read_matrix_table().count_cols()`. Stacked on #5041,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5043:18,optimiz,optimize,18,https://hail.is,https://github.com/hail-is/hail/pull/5043,1,['optimiz'],['optimize']
Performance,Used streamed buffered aggregate to optimize in lowering ofTableKeyByAndAggregate,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11395:36,optimiz,optimize,36,https://hail.is,https://github.com/hail-is/hail/pull/11395,1,['optimiz'],['optimize']
Performance,"Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3002,cache,cached,3002,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34309,cache,cached,34309,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"Using index expressions to represent abstract transformations on tensors. E.g. broadcasts from scalars and vectors to matrices would be represented by. ```; O(i, j) <- A(); O(i, j) <- A(i); O(i, j) <- A(j); ```; where A is the input tensor and O is the output tensor. Similarly, transpose and identity look like. ```; O(i, j) <- A(j, i); O(i, j) <- A(i, j); ```. Since the `O(i,j)` is the same in all of these, we only need to look at the ""in"" index expressions for broadcasts to distinguish what operation needs to be performed. There's also a Simplify rule to cut out identity broadcasts. ## Workaround; This method of index expressions treats scalars and vectors as 0-dimensional and 1-dimensional tensors, respectively. As such, their shapes in the IR are 0 and 1-dimensional. However, BlockMatrix (Py API and Scala backend) assumes that all BlockMatrices are 2-dimensional, with the potential to have dimensions of length 1. To satisfy the current user API while maintaining a tensor-like mindset in the IR, the BlockMatrixType now has a `isRowVector` flag to help the front-end BlockMatrix discern whether a 1-tensor IR should be treated as a row or column vector. As BlockMatrix generalizes to n-tensors this flag can be removed. The IR should not rely on this flag for any execution logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5338:519,perform,performed,519,https://hail.is,https://github.com/hail-is/hail/pull/5338,1,['perform'],['performed']
Performance,"Various changes:. 1. Locking is now done with a carefully restricted use of flock(), and the makefiles use; perl's rename command to get atomic rename, so they don't need to take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:436,cache,cache,436,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,2,['cache'],['cache']
Performance,"Version:; Apache Spark version 2.4.3; Hail version 0.2.19-c6ec8b76eb26. When exporting a table, the following error occurs when running on a yarn cluster. It does not occur when running locally. Any suggestions on this?. ```; Container exited with a non-zero exit code 127. Error file: prelaunch.err.; Last 4096 bytes of prelaunch.err :; Last 4096 bytes of stderr :; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/291/__spark_libs__4347827829503170766.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.0-292/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/09/06 15:54:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/09/06 15:54:30 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; /usr/java/default/bin/java: symbol lookup error: /data01/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616_0098/container_e2451_1565788829616_0098_01_000011/tmp/jniloader3452911880890326608netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. ```; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008:960,load,load,960,https://hail.is,https://github.com/hail-is/hail/issues/7008,2,['load'],"['load', 'loaded']"
Performance,"WIP pull request. - [x] Write tests to replicate existing errors. - [ ] Write test to replicate stalled request. - [ ] Resolve stalled request. However, I think we should consider writing a more complete solution to this. As far as I can tell, our use of threads is fragile; following Flask recommendations w.r.t reliance on production-ready WSGI server seems a good idea. Happy to take that on. I'd also like to move away from Flask for API stuff. While not likely to be a bottleneck for many moons, there are solutions rumored to be far faster (Falcon, esp using Cpython).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5065:474,bottleneck,bottleneck,474,https://hail.is,https://github.com/hail-is/hail/pull/5065,1,['bottleneck'],['bottleneck']
Performance,"We are interested in the performance of `hl.sample_qc` on a hard calls (`GT` only) dataset. In particular, are there computations that dominate when the data is small but are obscured when the input data is large? This is relevant to the analysis of 100k whole genomes (~600k variants).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4525:25,perform,performance,25,https://hail.is,https://github.com/hail-is/hail/issues/4525,1,['perform'],['performance']
Performance,"We are not permitted more than 10,000 parts. Moreover a tiny copy part size; seems to have negative consequences on throughput.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10938:116,throughput,throughput,116,https://hail.is,https://github.com/hail-is/hail/pull/10938,1,['throughput'],['throughput']
Performance,We are now optimizing after the lift.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4483#issuecomment-442165921:11,optimiz,optimizing,11,https://hail.is,https://github.com/hail-is/hail/issues/4483#issuecomment-442165921,1,['optimiz'],['optimizing']
Performance,"We are trying to setup hail `0.2.72` on spark `3.1.2` version. However, we are also facing similar error. * java version: `OpenJDK 64-Bit Server VM, 1.8.0_242`; * scala version: `2.12.10`; * py4j: `0.10.9`; * Python: `3.7.10`. <details>; <summary>Stacktrace</summary>. ```; Py4JJavaError: An error occurred while calling o126.exists.; : java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException; 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2532); 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2497); 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593); 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.exists(FS.scala:183); 	at is.hail.io.fs.FS.exists$(FS.scala:181); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.Ca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:1001,Cache,Cache,1001,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['Cache'],['Cache']
Performance,We can also just perform the checks in python code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/626#issuecomment-317568168:17,perform,perform,17,https://hail.is,https://github.com/hail-is/hail/issues/626#issuecomment-317568168,1,['perform'],['perform']
Performance,"We can make the virtual types line up with an explicit `SelectFields` to pick out the prefix used in the intervals from the key. We would just want to ensure that we can generate code (using the ""view"" struct ptype) that can perform the comparisons without copying the key.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7515#issuecomment-554446633:225,perform,perform,225,https://hail.is,https://github.com/hail-is/hail/pull/7515#issuecomment-554446633,1,['perform'],['perform']
Performance,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelle’s Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesn’t work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation · Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O · GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:312,perform,performance,312,https://hail.is,https://github.com/hail-is/hail/issues/5111,2,['perform'],['performance']
Performance,"We currently have a `@monitor_endpoint` decorator that we use to wrap aiohttp endpoints and produce prometheus metrics, but the same result can be achieved with fewer lines of code by using essentially the same wrapper as an aiohttp middleware. . Separately, I fixed the wrapper to correctly catch and report on endpoints that raise exceptions, as well as keeping track of the number of concurrent connections.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10600:387,concurren,concurrent,387,https://hail.is,https://github.com/hail-is/hail/pull/10600,1,['concurren'],['concurrent']
Performance,"We do the parsing. We don't do file checking, but this will come later in a query optimizer",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/131#issuecomment-235240333:82,optimiz,optimizer,82,https://hail.is,https://github.com/hail-is/hail/issues/131#issuecomment-235240333,1,['optimiz'],['optimizer']
Performance,"We don't want this in PR namespaces when running `test_ci`. ```; retry buildctl-daemonless.sh build --frontend dockerfile.v0 --local context=/io/repo/ --local dockerfile=/home/user --output 'type=image,""name=gcr.io/hail-vdc/ci-intermediate:kbqu6modc66u,gcr.io/hail-vdc/service-base:cache"",push=true' --export-cache type=inline --import-cache type=registry,ref=gcr.io/hail-vdc/service-base:cache --trace=/home/user/trace; cat /home/user/trace; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907:282,cache,cache,282,https://hail.is,https://github.com/hail-is/hail/pull/11907,4,['cache'],['cache']
Performance,"We get a good number of cache hits, which definitely help, but we still compile so many classes. gs://cdv-hail/logs/hail-joint-caller-oops-20191115-1801.log.xz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7528#issuecomment-554517326:24,cache,cache,24,https://hail.is,https://github.com/hail-is/hail/pull/7528#issuecomment-554517326,1,['cache'],['cache']
Performance,"We had a couple PRs fail because the database reached its [max connections](https://portal.azure.com/#@haildev.onmicrosoft.com/resource/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/haildev/providers/Microsoft.DBforMySQL/servers/db-393222c4/metrics). We should probably be resilient to this, but I figured we should be able to handle our normal PR load. I also checked GCP and their default is 4k. Azure sets its cap at 1250. I haven't applied any terraform since you've made your changes. Is that safe to do?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11329:368,load,load,368,https://hail.is,https://github.com/hail-is/hail/pull/11329,1,['load'],['load']
Performance,We have all of 1KG as vcfs in the `gs://hail-1kg/raw` bucket:; ```; (hail) 1 dking@wmb16-359 # gsutil du -sh gs://hail-1kg/raw/ALL.chr\*raw.HC.vcf.bgz; 260.94 GiB gs://hail-1kg/raw/ALL.chr*raw.HC.vcf.bgz; ```; Some subset of those chromosomes should satisfy any performance testing needs we have.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-454044971:262,perform,performance,262,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454044971,1,['perform'],['performance']
Performance,"We have persist / cache, but no unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1852:18,cache,cache,18,https://hail.is,https://github.com/hail-is/hail/issues/1852,1,['cache'],['cache']
Performance,"We keep an in-memory cache on the driver for which job ranges correspond to which job spec token. Currently, we cache the open range [N, None) which corresponds to the last bunch in the batch. This is fine when the batch is all submitted at once, but can become stale if there are updates to the batch. Here, we just use the range [N, n_jobs), which even if the number of jobs in the batch changes should still be correct. Ultimately, I think we should just add the end id to the batch_bunches table instead of doing this subquery and join against the batches table. This is a bit of a hack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12146:21,cache,cache,21,https://hail.is,https://github.com/hail-is/hail/pull/12146,2,['cache'],['cache']
Performance,"We may want to use NumPy ndarray as local matrix now, to avoid interface duplication and grab all its functionality, even if there's a performance hit in moving between Python and Java (worst case, we go through disk). I'm going to close this while we strategize, will PR the BlockMatrix updates separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825:135,perform,performance,135,https://hail.is,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825,1,['perform'],['performance']
Performance,"We need to configure Grafana with an SMTP server so that it can send us alert emails. I'd like to have alerts for things like disks getting dangerously close to filling up, abnormally high latency, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6697:189,latency,latency,189,https://hail.is,https://github.com/hail-is/hail/issues/6697,1,['latency'],['latency']
Performance,"We needed to await cancelled tasks to handle the error that was raised inside the task. Otherwise, I think what would happen is the unhandled cancelled error would cause an exception to be thrown when the main thread finally returned (file finished downloading) because errors in the cancelled tasks weren't ignored. For reference, the original error was this:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. https://stackoverflow.com/questions/57089430/asyncio-task-cancel-is-is-synchronous. It's possible I'm wrong and this code doesn't do anything. If that's the case, then I need additional help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534#issuecomment-853098749:1023,concurren,concurrent,1023,https://hail.is,https://github.com/hail-is/hail/pull/10534#issuecomment-853098749,1,['concurren'],['concurrent']
Performance,"We opened this because we were worried that if a PR becomes unapproved it will block the queue for other approved PRs. This does not happen. When a PR is unapproved, the next refresh will realize that no approved PRs are running, and it will kick off another build. This is not instantaneous (as we might like) but it happens within the refresh duration, which seems fine. cc: @cseed, I think our current operational state is fine and the marginal benefit of eagerly starting a new job is not worth the effort.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4605#issuecomment-433531464:89,queue,queue,89,https://hail.is,https://github.com/hail-is/hail/issues/4605#issuecomment-433531464,1,['queue'],['queue']
Performance,"We recently put in a couple PRs that improve performance on these searches so thought I would update here. They were mostly changes to things upstream of the portion of code we have been focusing on and change how data is initially read in, but the biggest performance gain we got was adding `hl._set_flags(use_new_shuffle='1')`. A lot of the focus was around how we handle searches in multiple data types which has been out of the scope of this work so far, so for the search we've been profiling here its only came down to like 80 seconds, but figured its worth sharing. Hopefully this does not cause to catastrophic of a merge conflict for you guys. https://github.com/broadinstitute/seqr/pull/3873; https://github.com/broadinstitute/seqr/pull/3876",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1945174111:45,perform,performance,45,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1945174111,2,['perform'],['performance']
Performance,"We run this query to get fair share values per user:. ```; SELECT user, CAST(COALESCE(SUM(n_cancelled_running_jobs), 0) AS SIGNED) AS n_cancelled_running_jobs; FROM user_inst_coll_resources; GROUP BY user; HAVING n_cancelled_running_jobs > 0;; ```. This query will return 0 even though there could be attempts still running. Plus these queries only look at running batches. ```; async for batch in self.db.select_and_fetchall(; '''; SELECT id; FROM batches; WHERE user = %s AND `state` = 'running' AND cancelled = 1;; ''',; (user,),; timer_description=f'in cancel_cancelled_running_jobs: get {user} cancelled batches'):; async for record in self.db.select_and_fetchall(; '''; SELECT jobs.job_id, attempts.attempt_id, attempts.instance_name; FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled); STRAIGHT_JOIN attempts; ON attempts.batch_id = jobs.batch_id AND attempts.job_id = jobs.job_id; WHERE jobs.batch_id = %s AND state = 'Running' AND always_run = 0 AND cancelled = 0; LIMIT %s;; ''',; (batch['id'], remaining.value),; timer_description=f'in cancel_cancelled_running_jobs: get {user} batch {batch[""id""]} running cancelled jobs'):; record['batch_id'] = batch['id']; yield record; ``` . I'll think about whether I can combine these queries over the attempts. However, it seemed clearer to me to look for two different things as the other queries are optimized by looking at the batch and job state where the orphaned query doesn't care about fair share or the batch / job state..",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10071#issuecomment-783583573:1366,optimiz,optimized,1366,https://hail.is,https://github.com/hail-is/hail/pull/10071#issuecomment-783583573,2,['optimiz'],['optimized']
Performance,"We should already have measures in place (like firewall rules) that prevent untrusted code from reaching the Batch Worker server, but this provides an extra layer of protection through which we can enforce that only the Batch front end and the Batch Driver can use the endpoints on the Batch Worker. This, along with #14581 are the final pieces to ensure that every endpoint in our system, both internal and external, uses HTTPS and performs the appropriate auth checks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14622:433,perform,performs,433,https://hail.is,https://github.com/hail-is/hail/pull/14622,1,['perform'],['performs']
Performance,We should be able to check something in netlib to determine if it loaded the natives or the reference library.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5040:66,load,loaded,66,https://hail.is,https://github.com/hail-is/hail/issues/5040,1,['load'],['loaded']
Performance,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:147,cache,cache,147,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073,14,['cache'],"['cache', 'cache-from', 'cache-not-image-itself']"
Performance,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:84,optimiz,optimized,84,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838,2,['optimiz'],"['optimized', 'optimized-os']"
Performance,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/714:9,load,loading,9,https://hail.is,https://github.com/hail-is/hail/issues/714,3,['load'],"['loadable', 'loading']"
Performance,"We use Q twice if someone wants to compute the loadings, so we want to save it to avoid redoing full QR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10223:47,load,loadings,47,https://hail.is,https://github.com/hail-is/hail/pull/10223,1,['load'],['loadings']
Performance,"We use `nest_asyncio` to allow synchronous blocking on a coroutine execution inside an async context. For example, if a user is using the synchronous interface of hail inside a jupyter cell (which runs an event loop). `nest_asyncio` achieves this by patching the `asyncio` event loop to make it reentrant, and with that there are footguns. This allows us to do things like create 100 tasks that all concurrently invoke `run_until_complete`, each of which will add stack frames to the event loop stack that can pile up and trigger a cryptic `RecursionError`. But internally we never *need* to make concurrent calls to `run_until_complete`, and more broadly we should never have `async / sync / async` inside hail code. This change exposes an asynchronous `validate_file` so that asynchronous methods in `hailtop` can use it directly instead of inserting a synchronous layer (`async_to_blocking`) between them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14576:399,concurren,concurrently,399,https://hail.is,https://github.com/hail-is/hail/pull/14576,2,['concurren'],"['concurrent', 'concurrently']"
Performance,"We use a readiness probe with a generous timeout to ensure the browser; loads the relevant files into a cache before any users see the website. This; slows deployment because we wait about 30s before sending any user traffic (in; the meantime, users will see 502s). On the bright side, after start-up, users will; always have a face experience, even if the pod was restarted since someone last; visited the site. I also codified some of the file update steps and clarified the README wrt Duncan's; GitHub repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8899:72,load,loads,72,https://hail.is,https://github.com/hail-is/hail/pull/8899,2,"['cache', 'load']","['cache', 'loads']"
Performance,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3392:375,Load,LoadVCF,375,https://hail.is,https://github.com/hail-is/hail/pull/3392,1,['Load'],['LoadVCF']
Performance,"We want to track Hail's performance with every release for a number of reasons, including but not limited to: ; - Measure how we are doing in delivering value to scientists; - Measure the effect of changes, test our intuition and learn how to improve the product. ; - Compare our solution with others; - Catch unexpected regressions. As of the time of writing, benchmarks are run rarely and have rotted somewhat. There's a bit of work required to get them going again. There's also some work in getting them running in CI and capturing the results. Very roughly, I think work can broken down as follows:; - [ ] get benchmarks passing; - [ ] organise trials with learnings from https://www.zora.uzh.ch/id/eprint/170445/1/emse_smb_cloud.pdf; - [x] run bechmarks in ci on deploy and store the results somewhere appropriate, fail if there's something really awful ; - [ ] visualise results on some appropriate cadance for trends. Might be nice to have a graphic on our github page. . I think many of these can be done in parallel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14221:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/issues/14221,1,['perform'],['performance']
Performance,"We'll use these functions for eigen-decomposition of kernel in linear mixed model, and elsewhere. The Breeze method symEig calls to LAPACK dsyev, which performs poorly relative to dsyevd and dsyevr. See:; http://www.netlib.org/lapack/lawnspdf/lawn183.pdf. http://scicomp.stackexchange.com/questions/11827/flop-counts-for-lapack-symmetric-eigenvalue-routines-dsyev-dsyevd-dsyevx-and-d. I added interfaces to [dsyevd](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html) and [dsyevr](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga2ad9f4a91cddbf67fe41b621bd158f5c.html) and used a version of symEigSpeedTest in symEigDSuite to test performance on my mid 2015 MacBook Pro (2.8 GHz Intel Core i7). Here's a plot from dimension 500 to 5500:; [symEig6k.pdf](https://github.com/hail-is/hail/files/512569/symEig6k.pdf). Extrapolating to 10k:; [symEig10k.pdf](https://github.com/hail-is/hail/files/512571/symEig10k.pdf). And to 25k for kicks (though by then we're at 5G of RAM...):; [symEig25k.pdf](https://github.com/hail-is/hail/files/512580/symEig25k.pdf). So at least on a [Wishart matrix](https://en.wikipedia.org/wiki/Wishart_distribution), symEigD (dsyevd) is best, with symEigR (dsyevr) close behind, then svd (dgesdd) about 2.5x worse than symEigD, and then symEig (dsyev) about 10x worse than symEigD. ```; dim svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/906#issuecomment-251835211:152,perform,performs,152,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211,2,['perform'],"['performance', 'performs']"
Performance,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:390,load,loaded,390,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345,3,['load'],"['load', 'loaded']"
Performance,"We're about 50% slower than numpy for large matmul, but closer to parity on IO:. ```; ('running:', 'write A'); ('write A', [0.4459199905395508, 0.4475231170654297, 0.4485299587249756]); ('running:', 'write B'); ('write B', [4.432413816452026, 4.644207954406738, 4.646740913391113]); ('running:', 'mul'); ('mul', [29.40219783782959, 28.894094944000244, 28.81586503982544]); ```. ```; import timeit; import numpy as np. def time(name, f, number=1, repeat=3):; print('running:', name); d = timeit.repeat(f, number=number, repeat=repeat); print(name, d). def writeA():; np.save('A.npy', np.random.rand(5000, 8000)). def writeB():; np.save('B.npy', np.random.rand(8000, 50000)). def mul():; A = np.load('A.npy'); B = np.load('B.npy'); np.save('product.npy', np.dot(A, B)). time('write A', writeA); time('write B', writeB); time('mul', mul); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2526#issuecomment-349848923:693,load,load,693,https://hail.is,https://github.com/hail-is/hail/pull/2526#issuecomment-349848923,2,['load'],['load']
Performance,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. Stacked on #8084, don't review until that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8085:52,optimiz,optimization,52,https://hail.is,https://github.com/hail-is/hail/pull/8085,1,['optimiz'],['optimization']
Performance,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. cc @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8067:52,optimiz,optimization,52,https://hail.is,https://github.com/hail-is/hail/pull/8067,1,['optimiz'],['optimization']
Performance,"We, unfortunately, have no satisfactory performance measurement, target, or monitoring story. Currently, when someone makes a change that risks changing the performance of Hail, we first do local timings on large files (I have a 1GB and a 30GB file). If those are satisfactory, we additionally run some timings using a cluster on larger files. Generally, we are only comparing against latest master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009:40,perform,performance,40,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009,2,['perform'],['performance']
Performance,"Weird. Did you do something like this?; ```; val statePV = new PCanonicalBaseStructSettable(stateType, state.off); statePV.loadField(ndarrayFieldNumber); .consume(cb, srvb.setMissing(), srvb.addIRIntermediate(_, deepCopy = true)); ```; Anyways, happy to approve if you don't want to mess with it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-669397111:123,load,loadField,123,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669397111,1,['load'],['loadField']
Performance,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:180,load,loading,180,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668,1,['load'],['loading']
Performance,What do we use instead of Stype.loadFrom?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10488#issuecomment-844405222:32,load,loadFrom,32,https://hail.is,https://github.com/hail-is/hail/pull/10488#issuecomment-844405222,1,['load'],['loadFrom']
Performance,"When I use python 2.7.5, it still can't work: ; ```; [root@tele-1 ~]# pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; ****Python 2.7.5 (default, Nov 6 2016, 00:28:07)**** ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 18:18:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 18:18:30 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:710,load,load,710,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,1,['load'],['load']
Performance,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:349,load,loading,349,https://hail.is,https://github.com/hail-is/hail/issues/1683,2,['load'],['loading']
Performance,"When loading a plink binary file, if the delimiter is incorrect it currently fails with the following cryptic error:. caught scala.MatchError: [Ljava.lang.String;@459f703f (of class [Ljava.lang.String;). This was a file with spaces (where the default is tab). As space seems to be the default for plink2, which was used to make these files, it may be worth allowing either by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/709:5,load,loading,5,https://hail.is,https://github.com/hail-is/hail/issues/709,1,['load'],['loading']
Performance,"When python loads a file it runs everything at the top level. That means running statements like `import aiohttp`, `MY_CONSTANT = 5`, and `def foo: …`. So importing files can define functions, so they can be used later, but those functions are not run (how could they be?). By putting the imports inside the functions themselves, we defer the import of aiohttp from when the deploy_config is imported to when the functions that depend on `aiohttp` are actually used. If they are never used, `aiohttp` will never be imported! This is why it made `hailctl dev config list` much faster, since it doesn't need to use those functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11293#issuecomment-1027295448:12,load,loads,12,https://hail.is,https://github.com/hail-is/hail/pull/11293#issuecomment-1027295448,1,['load'],['loads']
Performance,"When the number of fields in `x` is really huge, this optimization creates huge chains of `Let` bindings which cause stack size issues in downstream IR analysis passes. (we can remove this cap once our passes will no longer run into stack size issues on very deep IRs.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7726:54,optimiz,optimization,54,https://hail.is,https://github.com/hail-is/hail/pull/7726,1,['optimiz'],['optimization']
Performance,"While implementing IBD, I added two type-classes to make implementing a fuzzy-comparable slightly easier. I'm not sure this is the best approach. Unfortunately, we didn't heavily discuss this choice because IBD was moving into master rather quickly after the performance issues were resolved. The files are: [AbsoluteFuzzyComparable](https://github.com/hail-is/hail/blob/master/src/main/scala/org/broadinstitute/hail/utils/AbsoluteFuzzyComparable.scala) and [RelativeFuzzyComparable](https://github.com/hail-is/hail/blob/master/src/main/scala/org/broadinstitute/hail/utils/RelativeFuzzyComparable.scala). Absolute means `(x - y) < ε`. Relative means `(x - y) < max(abs(x), abs(y)) * ε` (i.e. `D_==`). Any type for which these type classes has been defined may be compared. For example:. ``` scala; (x: T, y: T) => AbsoluteFuzzyComparable.absoluteEq(epsilon, x, y); ```. I gave a type-class generator for `Map[K, V]` where `V` is comparable, so arbitrarily nested maps may be compared in this way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/876:259,perform,performance,259,https://hail.is,https://github.com/hail-is/hail/issues/876,1,['perform'],['performance']
Performance,"Why does that seem sub-optimal? That seems right for readiness. If it can serve the root, it's probably ready to go. For liveness, ideally the page doesn't introduce too much load because it gets hit somewhat frequently. None of this really matters until we have traffic anyway. If we anticipate a surge, we can make sure these aren't overloading the system before the surge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546443748:175,load,load,175,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546443748,1,['load'],['load']
Performance,"Why would performance be different than just `nan`? Compatibility is understandable (though you could just warn and implicitly convert to `nan` - not a great solution, but it's an option). And I guess `nan` is fine, but then perhaps having the `sparsify_*` methods to be able to convert to `nan` instead of 0 should be supported. You guys had all this logic about why `nan` was different from missing though and I really liked it, so it seems like that should carry forward here",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6097#issuecomment-492707284:10,perform,performance,10,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492707284,1,['perform'],['performance']
Performance,"Will accept this now to prevent perfect from being enemy of good. I do think some of the points raised should be considered (indent or replace result line, whether to check n_cancelled before re-queueing a job, whether to drop batch.state).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6341#issuecomment-502415859:195,queue,queueing,195,https://hail.is,https://github.com/hail-is/hail/pull/6341#issuecomment-502415859,1,['queue'],['queueing']
Performance,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3579:378,perform,performance,378,https://hail.is,https://github.com/hail-is/hail/pull/3579,5,"['bottleneck', 'perform', 'queue']","['bottleneck', 'performance', 'queue']"
Performance,"With good optimization too:. ```; hl.eval(hl.utils.range_table(10).annotate(x=[1,2,3]).explode('x').annotate(y=10).index_globals()); ```. ```; 2018-12-05 18:33:33 root: INFO: optimize: before:; (TableGetGlobals; (TableMapRows; (TableExplode x; (TableMapRows; (TableRange 10 12); (InsertFields; (Ref row); (x; (Literal Array[Int32] ""[1,2,3]""))))); (InsertFields; (Ref row); (y; (I32 10))))); 2018-12-05 18:33:33 root: INFO: optimize: after:; (MakeStruct); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4902:10,optimiz,optimization,10,https://hail.is,https://github.com/hail-is/hail/pull/4902,3,['optimiz'],"['optimization', 'optimize']"
Performance,"With the relation optimizer, metadata is being split into two pieces. signatures and other static metadata should be stored separately from query data: global annotations, sample IDs and sample annotations (the latter possibly as Parquet).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1140#issuecomment-299362173:18,optimiz,optimizer,18,https://hail.is,https://github.com/hail-is/hail/issues/1140#issuecomment-299362173,1,['optimiz'],['optimizer']
Performance,"With this change, only one test in `testTakeBy` fails. This is the generated code for the lead up to the invocation of the SeqOp method.; ```; /// THIS PR; public __m81wrapped(Lis/hail/annotations/Region;JJ)V; L0; LDC 0; LSTORE 6; LDC 0; LSTORE 8; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 8; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // we fail here; ISTORE 10; ILOAD 10; ISTORE 11; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // or here; ISTORE 12; ALOAD 0; ALOAD 0; ALOAD 1; ILOAD 11; INVOKEVIRTUAL __C44CompiledWithAggs.__m82str (Lis/hail/annotations/Region;I)J; LDC 9999; ILOAD 12; ISUB; INVOKEVIRTUAL __C44CompiledWithAggs.__m67take_by_seqop (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46Compiled",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:391,load,loadInt,391,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408,2,['load'],['loadInt']
Performance,"Without `--cache-repo`, Kaniko uses `{image_name}/cache` (e.g. `gcr.io/hail-vdc/batch-worker/cache`). That works particularly poorly for all the test images which are `gcr.io/hail-vdc/ci-intermediate` and therefore don't share a cache with the default images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10396:11,cache,cache-repo,11,https://hail.is,https://github.com/hail-is/hail/pull/10396,4,['cache'],"['cache', 'cache-repo']"
Performance,"Woah, so fast! . I think this is going to have poor performance until you implement the batched readers writers that look like. ```java; public void write(byte b[], int off, int len) throws IOException; public int read(byte b[], int off, int len) throws IOException. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10766#issuecomment-895516349:52,perform,performance,52,https://hail.is,https://github.com/hail-is/hail/pull/10766#issuecomment-895516349,1,['perform'],['performance']
Performance,"Wonder what your thoughts are on this. I think that we have too deep an image hierarchy and that it's only stalling our time-to-test in CI. I propose here getting rid of the run-tests image and just putting plink in the hail-run image. I also remove the dependency on service-base and just install hailtop in the hail-run image -- I don't think anything hail/hail should depend on services code. This change actually revealed an accidental dependency where there was a gear (services) test in the hail/python tests. I moved that over to the batch suite as batch is the primary user of that class and we don't have a gear suite atm. Ultimately, I think we should have a pretty flat docker image hierarchy. Each of our Dockerfiles does a decent job of following the docker mantra of dependencies first, code later. But if you consider the fact that we stack images on top of each other then you can see that in reality that doesn't translate to our images! We instead get an interleaving of hail code and dependencies such that it's very easy to break the cache for images like this one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12396:1054,cache,cache,1054,https://hail.is,https://github.com/hail-is/hail/pull/12396,1,['cache'],['cache']
Performance,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5850:401,cache,cache,401,https://hail.is,https://github.com/hail-is/hail/pull/5850,2,"['cache', 'load']","['cache', 'load']"
Performance,"Works for me. ```; $ gcloud version ; Google Cloud SDK 259.0.0; beta 2019.05.17; bq 2.0.46; core 2019.08.16; gsutil 4.42; $ gcloud dataproc clusters create cdv --master-machine-type n1-standard-1 --worker-machine-type n1-standard-1 --max-idle 10m --zone us-central1-b; Waiting on operation [projects/broad-mpg-gnomad/regions/global/operations/22271968-933b-3629-9d06-02f6a3039ddd].; Waiting for cluster creation operation...⠛ ; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done. ; Created [https://dataproc.googleapis.com/v1/projects/broad-mpg-gnomad/regions/global/clusters/cdv] Cluster placed in zone [us-central1-b]. $ gcloud dataproc clusters list; NAME WORKER_COUNT PREEMPTIBLE_WORKER_COUNT STATUS ZONE SCHEDULED_DELETE; cdv 2 RUNNING us-central1-b enabled; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6879#issuecomment-525345763:554,perform,performance,554,https://hail.is,https://github.com/hail-is/hail/pull/6879#issuecomment-525345763,3,['perform'],['performance']
Performance,"Would it be possible to add an option to annotate with --tab and --pick_allele, as below:; This would be super helpful!. <hail.vep.perl>; <hail.vep.location>; --format vcf; --tab; --pick_allele; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.cache_dir>/homo_sapiens/81_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa; --minimal; --assembly GRCh37; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/489:240,cache,cache,240,https://hail.is,https://github.com/hail-is/hail/issues/489,1,['cache'],['cache']
Performance,Would previously pull out address and load length into local variables even if we just need to pass along pointer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8236#issuecomment-594546138:38,load,load,38,https://hail.is,https://github.com/hail-is/hail/pull/8236#issuecomment-594546138,1,['load'],['load']
Performance,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8662:6,load,loading,6,https://hail.is,https://github.com/hail-is/hail/pull/8662,2,['load'],"['loaded', 'loading']"
Performance,XmYSkP5TA/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:34+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMeth,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:3914,concurren,concurrent,3914,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"YAML has an arbitrary extension mechanism. pyYAML defines a python extension that lets you create arbitrary python objets. This is clearly a huge security vulnerability. Apparently, pyYAML, by default, enables this extension (rather than just parsing vanilla YAML, 🤦‍♀️). `safe_load` loads vanilla YAML without the gaping security hole. I was getting warnings about this when testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5825:284,load,loads,284,https://hail.is,https://github.com/hail-is/hail/pull/5825,1,['load'],['loads']
Performance,"Ya I do. Haven't assigned that yet because I'm worried about db load, and I'm close enough to per-PR DBs anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13004#issuecomment-1540389363:64,load,load,64,https://hail.is,https://github.com/hail-is/hail/pull/13004#issuecomment-1540389363,1,['load'],['load']
Performance,"Ya I was wondering too which is why I tried this out. It was helpful in observing how much the number of files is the bottleneck for the copy tool. Takes between 15-20 seconds to copy down all 5k files in the repo while jobs that copy more MB but in fewer files are [much better](https://ci.hail.is/batches/6235246/jobs/52). I don't exactly prefer how, but it feels silly to spend 25% of the time of `auth_image` downloading the entire repo when you need just the auth directory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12371#issuecomment-1289877495:118,bottleneck,bottleneck,118,https://hail.is,https://github.com/hail-is/hail/pull/12371#issuecomment-1289877495,1,['bottleneck'],['bottleneck']
Performance,"Ya you're right, I was over-optimizing trying to share credentials between jobs of the same user. Just kept it as 1:1 jobs to credentials and it got a lot simpler. I changed the key to the job id because using the name of the identity would cause collisions between jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14125#issuecomment-1881179738:28,optimiz,optimizing,28,https://hail.is,https://github.com/hail-is/hail/pull/14125#issuecomment-1881179738,2,['optimiz'],['optimizing']
Performance,"Ya, all these options seem like optimizing for well in the future, so whichever you think is the most straightforward I support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1274953590:32,optimiz,optimizing,32,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274953590,1,['optimiz'],['optimizing']
Performance,"Yeah, I guess I should benchmark, since you've observed some strange brittleness in the linreg performance before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11070#issuecomment-972121769:95,perform,performance,95,https://hail.is,https://github.com/hail-is/hail/pull/11070#issuecomment-972121769,1,['perform'],['performance']
Performance,"Yeah, I misjudged, clearly the determinism issue. I cached for my test case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171:52,cache,cached,52,https://hail.is,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171,2,['cache'],['cached']
Performance,"Yeah, I'm not sure I like this change, for exactly the reason @tpoterba mentions: should we join with `&&` or `||`? This is more natural with files since the only thing a list can mean is to load them all (which is already supported in the syntax). What was the particular use case driving this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1208#issuecomment-271012002:191,load,load,191,https://hail.is,https://github.com/hail-is/hail/pull/1208#issuecomment-271012002,1,['load'],['load']
Performance,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:74,load,load,74,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615,1,['load'],['load']
Performance,"Yeah, that seems reasonable but we'll need some infrastructure work (including optimization) to make that reasonable. Hmm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4027#issuecomment-408648644:79,optimiz,optimization,79,https://hail.is,https://github.com/hail-is/hail/issues/4027#issuecomment-408648644,1,['optimiz'],['optimization']
Performance,"Yeah, that's it. Not surprised. ```; 2018-04-29 12:41:16 BLAS: WARN: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; 2018-04-29 12:41:16 BLAS: WARN: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS; ```. I'm on linux. I'll try to investigate what's going on tonight.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385283210:79,load,load,79,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385283210,2,['load'],['load']
Performance,Yep! Loaded it up and it worked without error. Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015#issuecomment-373027311:5,Load,Loaded,5,https://hail.is,https://github.com/hail-is/hail/issues/3015#issuecomment-373027311,1,['Load'],['Loaded']
Performance,"Yes totally. You'd just call ParsedLine.getValue. getKey would return an; empty array, since you passed nothing in. On Jul 13, 2016 12:21 AM, ""cseed"" notifications@github.com wrote:. > I haven't looked at this yet, I have a new use case in seqr for the table; > code. I need to be able to load data as RDD[Annotation] without pulling; > out a sample/variant key (or pull out a totally custom key). Can I do that; > easily with the new interface?; > ; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/462#issuecomment-232252890,; > or mute the thread; > https://github.com/notifications/unsubscribe/AKEs6uAzeg--b5QgvgWITjuIpY51afyEks5qVGesgaJpZM4JGtqM; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232253067:289,load,load,289,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232253067,1,['load'],['load']
Performance,"Yes, definitely! I'll assign you as a reviewer, though I'd like to also do a quick design review with Cotton to make sure I'm not doing anything crazy. I don't think there are any major redesigns I have in mind, barring feedback from review. I think I'll make it possible to control the default split size using a flag / env var, but other than that it works and seems to improve performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10512#issuecomment-852084745:380,perform,performance,380,https://hail.is,https://github.com/hail-is/hail/pull/10512#issuecomment-852084745,2,['perform'],['performance']
Performance,"Yes, functions would be great but that is a harder project. I'm skeptical the IR size is a huge bottleneck right now, although I could be wrong. The compiler and serializing many copies of the same bytecode is definitely known to be slow, and this fixes that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426#issuecomment-467104633:96,bottleneck,bottleneck,96,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467104633,1,['bottleneck'],['bottleneck']
Performance,"Yes, when I do :; ```; pyspark; sc.textFile('/hail/test/BRCA1.raw_indel.vcf') ; ```; The following information is shown:; ```; [root@tele-1 ~]# pyspark; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 19:16:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 19:16:02 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:548,load,load,548,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['load'],['load']
Performance,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:16,concurren,concurrent,16,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989,6,['concurren'],['concurrent']
Performance,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7641:974,perform,performance,974,https://hail.is,https://github.com/hail-is/hail/pull/7641,3,"['optimiz', 'perform']","['optimizing-innodb-transaction-management', 'performance', 'performance-ro-txn']"
Performance,"You can see the log of the test here:; http://hail-ci:8080/job/Hail%20-%20Test%20All%20Branches/52/console. It is ""Console Output"" on the left hand side on the build page. Cotton. On Fri, Apr 15, 2016 at 10:27 AM, Tim Poterba notifications@github.com; wrote:. > The 'details' page for Jenkins won't load. This branch passed on my local; > computer.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/pull/291#issuecomment-210482932",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/291#issuecomment-210483887:299,load,load,299,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210483887,1,['load'],['load']
Performance,"You should add the reference to the VSM metadata on write. On read, if the reference field is missing, print a warning and default to the the GRCh37. You should then verify the global reference matches the loaded reference. Finally, do you have the data for h38? You should add that in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1789#issuecomment-302184683:206,load,loaded,206,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302184683,1,['load'],['loaded']
Performance,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:47,queue,queue,47,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101,1,['queue'],['queue']
Performance,You were right about the cache'ing. Thanks @jbloom22 !,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5305:25,cache,cache,25,https://hail.is,https://github.com/hail-is/hail/pull/5305,1,['cache'],['cache']
Performance,You will use the principal components space that you learned from the unrelated samples. You can project the withheld samples by summing over all variants and multiplying the variant loadings by the withheld samples' GTs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-1284296019:183,load,loadings,183,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-1284296019,2,['load'],['loadings']
Performance,"Yup, I did that, and I also tried a version where I did `state.get` to get the emit code, then `toI.consume`ed that, and used that to get the pvalue I loaded the field of, and that also didn't work. I'm going to pick not to mess with it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-669402324:151,load,loaded,151,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669402324,1,['load'],['loaded']
Performance,[QOB] Parameterize code cache by backend (don't cache on service),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12618:24,cache,cache,24,https://hail.is,https://github.com/hail-is/hail/pull/12618,2,['cache'],['cache']
Performance,"[SIG Node]</li>; <li>Kubelet: add '--logging-format' flag to support structured logging (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91532"">kubernetes/kubernetes#91532</a>, <a href=""https://github.com/afrouzMashaykhi""><code>@​afrouzMashaykhi</code></a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Node]</li>; <li>Kubernetes is now built with golang 1.15.0-rc.1.; <ul>; <li>The deprecated, legacy behavior of treating the CommonName field on X.509 serving certificates as a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/93264"">kubernetes/kubernetes#93264</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li>; </ul>; </li>; <li>Promote Immutable Secrets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:11074,Scalab,Scalability,11074,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Scalab'],['Scalability']
Performance,"[VCF version 4.5](https://samtools.github.io/hts-specs/VCFv4.5.pdf) contains the changes we developed as part of our work developing the [Scalable Variant Call Representation](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1). As the developers and drivers of these changes, we should fully support v4.5 via import to VDS and export from VDS. Current checklist. May be extended over time:. - [x] Prefer `LEN` over `END` for reference blocks. (Begins in #14675); - [x] Update the combiner to convert to `LEN` from INFO `END` (Part of #14675).; - [x] Update `to_dense_mt` to use `LEN` (we think it may be more efficient).; - [ ] Add VDS to VCF export. (#14743); - [ ] Add Sparse VCF to VDS import. (#14743); - [x] ~'Official' non-ref genotype `<*>` support?~ (not part of this issue); - [ ] Make sure that we output well formed VCF 4.5, this includes things like VCF 4.4's phased haploid calls (this will also require updates to our parser)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14655:138,Scalab,Scalable,138,https://hail.is,https://github.com/hail-is/hail/issues/14655,1,['Scalab'],['Scalable']
Performance,[`orjson`](https://github.com/ijl/orjson#serialize) is a fast JSON serialize/deserialize library. I found that it; improved the performance of the copy tool substantially. I suspect we will see a low-level improvement across all; services. We can't use aiohttp's normal json library overrides because aiohttp stubbornly refuses to support a JSON interface that doesn't (unnecessarily) return strings (which are then decoded to utf-8 bytes anyway). cc: @daniel-goldstein,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10982:128,perform,performance,128,https://hail.is,https://github.com/hail-is/hail/pull/10982,1,['perform'],['performance']
Performance,[aiotools][batch][memory] Fix the secret cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11040:41,cache,cache,41,https://hail.is,https://github.com/hail-is/hail/pull/11040,1,['cache'],['cache']
Performance,[asyncfs] improvements to give better performance on S3,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752:38,perform,performance,38,https://hail.is,https://github.com/hail-is/hail/pull/10752,1,['perform'],['performance']
Performance,[auth] Cache user sessions in memory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12122:7,Cache,Cache,7,https://hail.is,https://github.com/hail-is/hail/pull/12122,2,['Cache'],['Cache']
Performance,[auth] Dont load session_id in the UserData dict,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13618:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/pull/13618,1,['load'],['load']
Performance,[azure] Optimize file status operations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13368:8,Optimiz,Optimize,8,https://hail.is,https://github.com/hail-is/hail/pull/13368,1,['Optimiz'],['Optimize']
Performance,[batch.aioclient] Optimize batch.wait for QoB,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13177:18,Optimiz,Optimize,18,https://hail.is,https://github.com/hail-is/hail/pull/13177,1,['Optimiz'],['Optimize']
Performance,[batch2] Fix queue starvation problem,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7336:13,queue,queue,13,https://hail.is,https://github.com/hail-is/hail/pull/7336,1,['queue'],['queue']
Performance,[batch2] optimize scheduler query,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7634:9,optimiz,optimize,9,https://hail.is,https://github.com/hail-is/hail/pull/7634,1,['optimiz'],['optimize']
Performance,[batch2] scalable job search/batch page,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7635:9,scalab,scalable,9,https://hail.is,https://github.com/hail-is/hail/pull/7635,1,['scalab'],['scalable']
Performance,[batch] Add image cache and remove old images,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8445:18,cache,cache,18,https://hail.is,https://github.com/hail-is/hail/pull/8445,1,['cache'],['cache']
Performance,[batch] Add k8s secret/sa cache and retry transient errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7850:26,cache,cache,26,https://hail.is,https://github.com/hail-is/hail/pull/7850,1,['cache'],['cache']
Performance,"[batch] Add kubernetes secret, service account cache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7825:47,cache,cache,47,https://hail.is,https://github.com/hail-is/hail/pull/7825,1,['cache'],['cache']
Performance,[batch] Cache tokens for job bunches on the driver to avoid db queries,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12023:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12023,1,['Cache'],['Cache']
Performance,[batch] Create pool scheduling loops after loading existing instances,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11766:43,load,loading,43,https://hail.is,https://github.com/hail-is/hail/pull/11766,1,['load'],['loading']
Performance,[batch] Dont include open-ended job ranges in token cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12146:52,cache,cache,52,https://hail.is,https://github.com/hail-is/hail/pull/12146,1,['cache'],['cache']
Performance,[batch] Have Batch Worker perform auth checks on requests from Batch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14622:26,perform,perform,26,https://hail.is,https://github.com/hail-is/hail/pull/14622,1,['perform'],['perform']
Performance,[batch] Improve SQL performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5945:20,perform,performance,20,https://hail.is,https://github.com/hail-is/hail/issues/5945,1,['perform'],['performance']
Performance,[batch] Miscellaneous performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5991:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/5991,1,['perform'],['performance']
Performance,[batch] Mitigate possible corrupt containerd image cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12758:51,cache,cache,51,https://hail.is,https://github.com/hail-is/hail/pull/12758,1,['cache'],['cache']
Performance,[batch] Mitigate siwei bug by removing cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399:39,cache,cache,39,https://hail.is,https://github.com/hail-is/hail/pull/13399,1,['cache'],['cache']
Performance,[batch] Optimize SQL query generated for listing jobs / batches / job groups,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14410:8,Optimiz,Optimize,8,https://hail.is,https://github.com/hail-is/hail/issues/14410,1,['Optimiz'],['Optimize']
Performance,[batch] Orphaned attempts loop performs full scan of instances table,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460:31,perform,performs,31,https://hail.is,https://github.com/hail-is/hail/issues/14460,1,['perform'],['performs']
Performance,[batch] Remove unused file cache from worker,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9860:27,cache,cache,27,https://hail.is,https://github.com/hail-is/hail/pull/9860,1,['cache'],['cache']
Performance,[batch] Use a simulated job queue to estimate the ready cores in the control loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12212:28,queue,queue,28,https://hail.is,https://github.com/hail-is/hail/pull/12212,1,['queue'],['queue']
Performance,[batch] active_instances_only name-token cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346:41,cache,cache,41,https://hail.is,https://github.com/hail-is/hail/pull/11346,1,['cache'],['cache']
Performance,[batch] add start jobs queue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6485:23,queue,queue,23,https://hail.is,https://github.com/hail-is/hail/pull/6485,1,['queue'],['queue']
Performance,[batch] always use throttler to delete pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6807:19,throttle,throttler,19,https://hail.is,https://github.com/hail-is/hail/pull/6807,1,['throttle'],['throttler']
Performance,[batch] cache files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9087:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/pull/9087,1,['cache'],['cache']
Performance,[batch] cache input files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/pull/9095,1,['cache'],['cache']
Performance,[batch] cache more hail images on the worker VMs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13913:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/issues/13913,1,['cache'],['cache']
Performance,[batch] create load test to stress the batch-driver at a maximum sche…,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11226:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/pull/11226,1,['load'],['load']
Performance,[batch] easy performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6487:13,perform,performance,13,https://hail.is,https://github.com/hail-is/hail/pull/6487,1,['perform'],['performance']
Performance,[batch] easy performance improvements for create_batch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6455:13,perform,performance,13,https://hail.is,https://github.com/hail-is/hail/pull/6455,1,['perform'],['performance']
Performance,[batch] fix get_jobs performance issue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5981:21,perform,performance,21,https://hail.is,https://github.com/hail-is/hail/pull/5981,1,['perform'],['performance']
Performance,[batch] fix revert input file cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9267:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/9267,1,['cache'],['cache']
Performance,[batch] fix search bar -- load js directory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6472:26,load,load,26,https://hail.is,https://github.com/hail-is/hail/pull/6472,1,['load'],['load']
Performance,[batch] fix throttler bugs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6959:12,throttle,throttler,12,https://hail.is,https://github.com/hail-is/hail/pull/6959,1,['throttle'],['throttler']
Performance,[batch] input file cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9074:19,cache,cache,19,https://hail.is,https://github.com/hail-is/hail/pull/9074,1,['cache'],['cache']
Performance,[batch] k8s cache fails for unclear reasons,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13909:12,cache,cache,12,https://hail.is,https://github.com/hail-is/hail/issues/13909,1,['cache'],['cache']
Performance,[batch] limit concurrent pods to 3k,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6802:14,concurren,concurrent,14,https://hail.is,https://github.com/hail-is/hail/pull/6802,1,['concurren'],['concurrent']
Performance,[batch] load credentials from file once instead of thrice,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11453:8,load,load,8,https://hail.is,https://github.com/hail-is/hail/pull/11453,1,['load'],['load']
Performance,[batch] make tests resilient to concurrent batches,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10928:32,concurren,concurrent,32,https://hail.is,https://github.com/hail-is/hail/pull/10928,1,['concurren'],['concurrent']
Performance,[batch] more concurrency in refresh loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6801:13,concurren,concurrency,13,https://hail.is,https://github.com/hail-is/hail/pull/6801,1,['concurren'],['concurrency']
Performance,[batch] more reslience in pod throttler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6804:30,throttle,throttler,30,https://hail.is,https://github.com/hail-is/hail/pull/6804,1,['throttle'],['throttler']
Performance,[batch] revert some of file cache changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9266:28,cache,cache,28,https://hail.is,https://github.com/hail-is/hail/pull/9266,1,['cache'],['cache']
Performance,"[batch] separate eviction from missing pod, check queue before rescheduling",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6799:50,queue,queue,50,https://hail.is,https://github.com/hail-is/hail/pull/6799,1,['queue'],['queue']
Performance,[batch] throttle instance pool on too many free cores,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8020:8,throttle,throttle,8,https://hail.is,https://github.com/hail-is/hail/pull/8020,1,['throttle'],['throttle']
Performance,[batch] throttle number of pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6675:8,throttle,throttle,8,https://hail.is,https://github.com/hail-is/hail/pull/6675,1,['throttle'],['throttle']
Performance,[batch] throttle pod creation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6694:8,throttle,throttle,8,https://hail.is,https://github.com/hail-is/hail/pull/6694,1,['throttle'],['throttle']
Performance,[batch] use safe load in batch client,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5825:17,load,load,17,https://hail.is,https://github.com/hail-is/hail/pull/5825,1,['load'],['load']
Performance,[benchmark] Fix `make_ndarray_bench` to avoid optimizations that obvi…,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10717:46,optimiz,optimizations,46,https://hail.is,https://github.com/hail-is/hail/pull/10717,1,['optimiz'],['optimizations']
Performance,[bugfix] Fix PCA loadings containing extra fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5691:17,load,loadings,17,https://hail.is,https://github.com/hail-is/hail/pull/5691,1,['load'],['loadings']
Performance,[bugfix] Optimize optional variantsTable IR in import_bgen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5730:9,Optimiz,Optimize,9,https://hail.is,https://github.com/hail-is/hail/pull/5730,1,['Optimiz'],['Optimize']
Performance,[bugfix] fix LoadVCF warning logic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6096:13,Load,LoadVCF,13,https://hail.is,https://github.com/hail-is/hail/pull/6096,1,['Load'],['LoadVCF']
Performance,[ci2] concurrency improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5921:6,concurren,concurrency,6,https://hail.is,https://github.com/hail-is/hail/pull/5921,1,['concurren'],['concurrency']
Performance,[ci] 8 concurrent builds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7728:7,concurren,concurrent,7,https://hail.is,https://github.com/hail-is/hail/pull/7728,1,['concurren'],['concurrent']
Performance,[ci] Bring max number of concurrent PRs down to three,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11762:25,concurren,concurrent,25,https://hail.is,https://github.com/hail-is/hail/pull/11762,1,['concurren'],['concurrent']
Performance,[ci] Dont try to json.loads the namespace string,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12631:22,load,loads,22,https://hail.is,https://github.com/hail-is/hail/pull/12631,1,['load'],['loads']
Performance,[ci] Fix bug where we pushed test images to the main image cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907:59,cache,cache,59,https://hail.is,https://github.com/hail-is/hail/pull/11907,1,['cache'],['cache']
Performance,[ci] Reduce max number of concurrent PRs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11426:26,concurren,concurrent,26,https://hail.is,https://github.com/hail-is/hail/pull/11426,1,['concurren'],['concurrent']
Performance,[ci] Use image and PR-specific cache tags for image builds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999:31,cache,cache,31,https://hail.is,https://github.com/hail-is/hail/pull/11999,1,['cache'],['cache']
Performance,[ci] all builds share the same cache repo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10396:31,cache,cache,31,https://hail.is,https://github.com/hail-is/hail/pull/10396,1,['cache'],['cache']
Performance,[ci] increase max concurrent PRs to 3,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11506:18,concurren,concurrent,18,https://hail.is,https://github.com/hail-is/hail/pull/11506,1,['concurren'],['concurrent']
Performance,[ci] reduce PR test latency to ten minutes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14003:20,latency,latency,20,https://hail.is,https://github.com/hail-is/hail/issues/14003,1,['latency'],['latency']
Performance,[compiler] Add partition planning optimizer,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12747:34,optimiz,optimizer,34,https://hail.is,https://github.com/hail-is/hail/pull/12747,2,['optimiz'],['optimizer']
Performance,[compiler] Chunk cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10618:17,cache,cache,17,https://hail.is,https://github.com/hail-is/hail/pull/10618,1,['cache'],['cache']
Performance,"[compiler] Fix bad performance in parseFloat{32, 64}",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10507:19,perform,performance,19,https://hail.is,https://github.com/hail-is/hail/pull/10507,1,['perform'],['performance']
Performance,[compiler] Lots of compiler and code size performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10746:42,perform,performance,42,https://hail.is,https://github.com/hail-is/hail/pull/10746,1,['perform'],['performance']
Performance,[compiler] Minor Requiredness Performance Enchancements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13991:30,Perform,Performance,30,https://hail.is,https://github.com/hail-is/hail/pull/13991,1,['Perform'],['Performance']
Performance,[compiler] Print after each pass rather than before/after optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12603:58,optimiz,optimize,58,https://hail.is,https://github.com/hail-is/hail/pull/12603,1,['optimiz'],['optimize']
Performance,[compiler] Remove SType.loadFrom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10488:24,load,loadFrom,24,https://hail.is,https://github.com/hail-is/hail/pull/10488,1,['load'],['loadFrom']
Performance,[compiler] Stream iota optimizations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10787:23,optimiz,optimizations,23,https://hail.is,https://github.com/hail-is/hail/pull/10787,1,['optimiz'],['optimizations']
Performance,[compiler] explicitly describe performance issue of BMAgg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12657:31,perform,performance,31,https://hail.is,https://github.com/hail-is/hail/pull/12657,1,['perform'],['performance']
Performance,[compiler] explicitly provide type infos for array loads,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11331:51,load,loads,51,https://hail.is,https://github.com/hail-is/hail/pull/11331,1,['load'],['loads']
Performance,[docker] Use updated cache tags in Makefiles,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12015:21,cache,cache,21,https://hail.is,https://github.com/hail-is/hail/pull/12015,1,['cache'],['cache']
Performance,[fs] Improve Azure read performance by using buffered copies,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12488:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/pull/12488,1,['perform'],['performance']
Performance,[gear] Dont cache the db ssl context in a global,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13652:12,cache,cache,12,https://hail.is,https://github.com/hail-is/hail/pull/13652,1,['cache'],['cache']
Performance,[git][docker] ignore mypy cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9270:26,cache,cache,26,https://hail.is,https://github.com/hail-is/hail/pull/9270,1,['cache'],['cache']
Performance,[gke] optimize utilization autoscaling and shrink nodes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11985:6,optimiz,optimize,6,https://hail.is,https://github.com/hail-is/hail/pull/11985,1,['optimiz'],['optimize']
Performance,[hail/ptypes] Die should perform deepInnerRequired,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7843:25,perform,perform,25,https://hail.is,https://github.com/hail-is/hail/pull/7843,1,['perform'],['perform']
Performance,[hail/ptypes] PStruct: make loadString an instance method,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754:28,load,loadString,28,https://hail.is,https://github.com/hail-is/hail/issues/7754,1,['load'],['loadString']
Performance,"[hail/ptypes] rename loadField, loadElement to loadFieldAddress, loadElementAddress",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7829:21,load,loadField,21,https://hail.is,https://github.com/hail-is/hail/issues/7829,4,['load'],"['loadElement', 'loadElementAddress', 'loadField', 'loadFieldAddress']"
Performance,[hail] All calls to optimize are tracked with ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7541:20,optimiz,optimize,20,https://hail.is,https://github.com/hail-is/hail/pull/7541,1,['optimiz'],['optimize']
Performance,[hail] Do not cache in interactive describe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7245:14,cache,cache,14,https://hail.is,https://github.com/hail-is/hail/pull/7245,1,['cache'],['cache']
Performance,[hail] Fix EXTREME performance regression in column scans.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7771:19,perform,performance,19,https://hail.is,https://github.com/hail-is/hail/pull/7771,1,['perform'],['performance']
Performance,[hail] Fix `mt.entry.take` performance regression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7764:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/pull/7764,1,['perform'],['performance']
Performance,[hail] Fix memory leak in BM and make cache size configurable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9501:38,cache,cache,38,https://hail.is,https://github.com/hail-is/hail/pull/9501,1,['cache'],['cache']
Performance,[hail] Fix performance problems in aggregators by binding arguments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6969:11,perform,performance,11,https://hail.is,https://github.com/hail-is/hail/pull/6969,1,['perform'],['performance']
Performance,[hail] Fix up usages of `Type.physicalType` in `LoadVCF`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7693:48,Load,LoadVCF,48,https://hail.is,https://github.com/hail-is/hail/pull/7693,1,['Load'],['LoadVCF']
Performance,[hail] Improve performance of IR copying,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7355:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/7355,1,['perform'],['performance']
Performance,[hail] Improve split_multi_hts performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6980:31,perform,performance,31,https://hail.is,https://github.com/hail-is/hail/pull/6980,1,['perform'],['performance']
Performance,[hail] Introduce better TableFilterIntervals optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7171:45,optimiz,optimization,45,https://hail.is,https://github.com/hail-is/hail/pull/7171,1,['optimiz'],['optimization']
Performance,[hail] LoadPlink ptype,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6421:7,Load,LoadPlink,7,https://hail.is,https://github.com/hail-is/hail/pull/6421,1,['Load'],['LoadPlink']
Performance,[hail] Log the size of the associative combiner queue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6755:48,queue,queue,48,https://hail.is,https://github.com/hail-is/hail/pull/6755,1,['queue'],['queue']
Performance,[hail] Table.union with almost-sorted could perform as well as unkeyed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8110:44,perform,perform,44,https://hail.is,https://github.com/hail-is/hail/issues/8110,1,['perform'],['perform']
Performance,[hail] [bugfix] Fix correctness bug in Table.order_by optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6757:54,optimiz,optimization,54,https://hail.is,https://github.com/hail-is/hail/pull/6757,1,['optimiz'],['optimization']
Performance,[hail] [high-prio] Don't optimize in ApplyIR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6353:25,optimiz,optimize,25,https://hail.is,https://github.com/hail-is/hail/pull/6353,1,['optimiz'],['optimize']
Performance,[hail] cache encoder and decoder functions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7528:7,cache,cache,7,https://hail.is,https://github.com/hail-is/hail/pull/7528,1,['cache'],['cache']
Performance,"[hail] clean up BlockMatrixMap IRs, delegate optimization of broadcasting for scalars to Simplify pass",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7566:45,optimiz,optimization,45,https://hail.is,https://github.com/hail-is/hail/pull/7566,1,['optimiz'],['optimization']
Performance,[hail] improve import_gtf performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8887:26,perform,performance,26,https://hail.is,https://github.com/hail-is/hail/pull/8887,1,['perform'],['performance']
Performance,[hail] improve performance of import_plink,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6984:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/6984,1,['perform'],['performance']
Performance,[hail] optimize flatten,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:7,optimiz,optimize,7,https://hail.is,https://github.com/hail-is/hail/pull/7719,1,['optimiz'],['optimize']
Performance,[hail] optimizer breaks correctness of scans,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6408:7,optimiz,optimizer,7,https://hail.is,https://github.com/hail-is/hail/issues/6408,1,['optimiz'],['optimizer']
Performance,[hail] remove load/store/append from Region methods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7092:14,load,load,14,https://hail.is,https://github.com/hail-is/hail/pull/7092,1,['load'],['load']
Performance,[hail][bugfix] Fix optimizer bug in PruneDeadFields for MatrixUnionRows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7965:19,optimiz,optimizer,19,https://hail.is,https://github.com/hail-is/hail/pull/7965,1,['optimiz'],['optimizer']
Performance,[hail][bugfix] Resolve performance bug in variant_qc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7989:23,perform,performance,23,https://hail.is,https://github.com/hail-is/hail/pull/7989,1,['perform'],['performance']
Performance,[hail][bugfix] Table/MatrixTable read should grab references and load into Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6921:65,load,load,65,https://hail.is,https://github.com/hail-is/hail/pull/6921,1,['load'],['load']
Performance,[hail][bugfix]: fix out of bounds read in LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7182:42,Load,LoadVCF,42,https://hail.is,https://github.com/hail-is/hail/pull/7182,1,['Load'],['LoadVCF']
Performance,[hail][performance] Deduplicate inlined IRs in `annotate(**thing)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6506:7,perform,performance,7,https://hail.is,https://github.com/hail-is/hail/pull/6506,1,['perform'],['performance']
Performance,[hail][performance] Fix literals lazy decoding,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6605:7,perform,performance,7,https://hail.is,https://github.com/hail-is/hail/pull/6605,1,['perform'],['performance']
Performance,[hailctl batch client] Add ability to cache batch status.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9510:38,cache,cache,38,https://hail.is,https://github.com/hail-is/hail/pull/9510,1,['cache'],['cache']
Performance,[hailctl] only load deploy metadata when needed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6513:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/pull/6513,1,['load'],['load']
Performance,[hailtop-auth] allow tokens to be loaded from user-specified file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9585:34,load,loaded,34,https://hail.is,https://github.com/hail-is/hail/pull/9585,1,['load'],['loaded']
Performance,[hailtop] Prevent event loop RecursionError during highly concurrent file uploads,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14576:58,concurren,concurrent,58,https://hail.is,https://github.com/hail-is/hail/pull/14576,1,['concurren'],['concurrent']
Performance,[hailtop] substantially improve the performance of copy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11079:36,perform,performance,36,https://hail.is,https://github.com/hail-is/hail/pull/11079,1,['perform'],['performance']
Performance,[hailtop][auth] log file when successfully loading tokens,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9132:43,load,loading,43,https://hail.is,https://github.com/hail-is/hail/pull/9132,1,['load'],['loading']
Performance,[k8s] Steps towards optimizing k8s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11900:20,optimiz,optimizing,20,https://hail.is,https://github.com/hail-is/hail/pull/11900,1,['optimiz'],['optimizing']
Performance,"[memory,query] Cache job function/context/outputs in memory service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10315:15,Cache,Cache,15,https://hail.is,https://github.com/hail-is/hail/pull/10315,1,['Cache'],['Cache']
Performance,[memory] add simple read-only cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9349:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/9349,2,['cache'],['cache']
Performance,[performance] Unkey table before collectAsDict,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5860:1,perform,performance,1,https://hail.is,https://github.com/hail-is/hail/pull/5860,1,['perform'],['performance']
Performance,[performance] fix split_multi_hts to only map split rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6554:1,perform,performance,1,https://hail.is,https://github.com/hail-is/hail/issues/6554,1,['perform'],['performance']
Performance,[qob] Invalidate batch cache on error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12519:23,cache,cache,23,https://hail.is,https://github.com/hail-is/hail/pull/12519,1,['cache'],['cache']
Performance,[qob] Support loading RGs from FASTA files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:14,load,loading,14,https://hail.is,https://github.com/hail-is/hail/pull/12736,1,['load'],['loading']
Performance,[qob] replace JVMJobs with DockerJobs which start a JVM using a JAR and a pre-warmed cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13675:85,cache,cache,85,https://hail.is,https://github.com/hail-is/hail/issues/13675,1,['cache'],['cache']
Performance,[query-service] concurrently use multiple Hail Query JAR versions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:16,concurren,concurrently,16,https://hail.is,https://github.com/hail-is/hail/pull/10279,3,['concurren'],['concurrently']
Performance,[query/shuffler] somewhat tune branching factor to dataset & log phases,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11784:26,tune,tune,26,https://hail.is,https://github.com/hail-is/hail/pull/11784,1,['tune'],['tune']
Performance,[query] Add CastToArray optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9265:24,optimiz,optimization,24,https://hail.is,https://github.com/hail-is/hail/pull/9265,1,['optimiz'],['optimization']
Performance,[query] Add a persist to improve performance of blanczos_pca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9498:33,perform,performance,33,https://hail.is,https://github.com/hail-is/hail/pull/9498,1,['perform'],['performance']
Performance,[query] Cache header lines instead of recomputing per row field in `import_matrix_table`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12038:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12038,1,['Cache'],['Cache']
Performance,[query] Cache table coercers per driver JVM to avoid scans on every q…,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12099:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12099,1,['Cache'],['Cache']
Performance,[query] Cache unkeyed interval instead of allocating for each partition,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11845:8,Cache,Cache,8,https://hail.is,https://github.com/hail-is/hail/pull/11845,1,['Cache'],['Cache']
Performance,[query] Call-Cache `CollectDistributedArray` (rfc-0000),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12954:13,Cache,Cache,13,https://hail.is,https://github.com/hail-is/hail/pull/12954,1,['Cache'],['Cache']
Performance,[query] Clean up PNDArray / Improve loadElement performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9883:36,load,loadElement,36,https://hail.is,https://github.com/hail-is/hail/pull/9883,2,"['load', 'perform']","['loadElement', 'performance']"
Performance,[query] Clear the bdist build cache before building the wheel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12008:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/12008,1,['cache'],['cache']
Performance,[query] Don't broadcast unnecessarily in LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9350:41,Load,LoadVCF,41,https://hail.is,https://github.com/hail-is/hail/pull/9350,1,['Load'],['LoadVCF']
Performance,[query] Dont compile code cache function three times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13796:26,cache,cache,26,https://hail.is,https://github.com/hail-is/hail/pull/13796,1,['cache'],['cache']
Performance,[query] Fix PCanonicalIntervalValue loadStart and loadEnd,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9032:36,load,loadStart,36,https://hail.is,https://github.com/hail-is/hail/pull/9032,2,['load'],"['loadEnd', 'loadStart']"
Performance,[query] Fix performance of parse_locus_interval,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11216:12,perform,performance,12,https://hail.is,https://github.com/hail-is/hail/pull/11216,1,['perform'],['performance']
Performance,[query] Fix subtle hamming performance regression/bug,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8497:27,perform,performance,27,https://hail.is,https://github.com/hail-is/hail/pull/8497,1,['perform'],['performance']
Performance,"[query] Hail uses more than 2.7 gigabytes of RAM to load ~100,000 export VCF results",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13533:52,load,load,52,https://hail.is,https://github.com/hail-is/hail/issues/13533,1,['load'],['load']
Performance,[query] Hana / SEQR need support optimizing Hail Query code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:33,optimiz,optimizing,33,https://hail.is,https://github.com/hail-is/hail/issues/13882,1,['optimiz'],['optimizing']
Performance,[query] Improve GVCF/VCF import performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8535:32,perform,performance,32,https://hail.is,https://github.com/hail-is/hail/pull/8535,1,['perform'],['performance']
Performance,[query] Improve `hl.Struct` performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10968:28,perform,performance,28,https://hail.is,https://github.com/hail-is/hail/pull/10968,1,['perform'],['performance']
Performance,[query] Load each missing byte once during struct decoding.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14406:8,Load,Load,8,https://hail.is,https://github.com/hail-is/hail/pull/14406,1,['Load'],['Load']
Performance,"[query] NDArray based PCA Performance Fix. Discard R, persist Q",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10223:26,Perform,Performance,26,https://hail.is,https://github.com/hail-is/hail/pull/10223,1,['Perform'],['Performance']
Performance,[query] Optimize GoogleFS listStatus operation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390:8,Optimiz,Optimize,8,https://hail.is,https://github.com/hail-is/hail/pull/13390,1,['Optimiz'],['Optimize']
Performance,[query] Parameterize Hail Query Class Loading by ClassLoader,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11212:38,Load,Loading,38,https://hail.is,https://github.com/hail-is/hail/pull/11212,1,['Load'],['Loading']
Performance,[query] Perform a shufflerectomy (remove the shuffle service and code),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10744:8,Perform,Perform,8,https://hail.is,https://github.com/hail-is/hail/pull/10744,1,['Perform'],['Perform']
Performance,[query] Refactor LoweringPipeline so that optimization is a pass,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9030:42,optimiz,optimization,42,https://hail.is,https://github.com/hail-is/hail/pull/9030,1,['optimiz'],['optimization']
Performance,"[query] Remove Region.{load,store}Primitive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10094:23,load,load,23,https://hail.is,https://github.com/hail-is/hail/pull/10094,1,['load'],['load']
Performance,[query] Remove optimize overload,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9800:15,optimiz,optimize,15,https://hail.is,https://github.com/hail-is/hail/pull/9800,1,['optimiz'],['optimize']
Performance,[query] Some new optimizations for StreamLen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10334:17,optimiz,optimizations,17,https://hail.is,https://github.com/hail-is/hail/pull/10334,1,['optimiz'],['optimizations']
Performance,[query] Use `loadFromNested` rather than matching on fundamental type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9989:13,load,loadFromNested,13,https://hail.is,https://github.com/hail-is/hail/pull/9989,1,['load'],['loadFromNested']
Performance,[query] Use a larger buffer size in scala FS to reduce request load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12071:63,load,load,63,https://hail.is,https://github.com/hail-is/hail/pull/12071,1,['load'],['load']
Performance,[query] VDS combiner will not load a successfully completed plan causing combiner to be rerun when a successful run has been completed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14079:30,load,load,30,https://hail.is,https://github.com/hail-is/hail/issues/14079,1,['load'],['load']
Performance,[query] Work around concurrency issues in FASTAReader,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9435:20,concurren,concurrency,20,https://hail.is,https://github.com/hail-is/hail/pull/9435,1,['concurren'],['concurrency']
Performance,[query] add functionality to `hardy_weinberg_test` to perform one-sided test of excess heterozygosity,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10989:54,perform,perform,54,https://hail.is,https://github.com/hail-is/hail/pull/10989,1,['perform'],['perform']
Performance,[query] blanczos_pca dont do extra loading work,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10201:35,load,loading,35,https://hail.is,https://github.com/hail-is/hail/pull/10201,1,['load'],['loading']
Performance,[query] cache RVD specs to avoid lots of file reads,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12086:8,cache,cache,8,https://hail.is,https://github.com/hail-is/hail/pull/12086,1,['cache'],['cache']
Performance,[query] eliminate optimization that can blow RAM,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13619:18,optimiz,optimization,18,https://hail.is,https://github.com/hail-is/hail/pull/13619,1,['optimiz'],['optimization']
Performance,"[query] farewell to `Region.{load,store}IRIntermediate`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10021:29,load,load,29,https://hail.is,https://github.com/hail-is/hail/pull/10021,1,['load'],['load']
Performance,[query] fix loading of reference genomes when GCS object versions are used,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10175:12,load,loading,12,https://hail.is,https://github.com/hail-is/hail/pull/10175,1,['load'],['loading']
Performance,[query] fix performance bug in lir.SimplifyControl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9475:12,perform,performance,12,https://hail.is,https://github.com/hail-is/hail/pull/9475,1,['perform'],['performance']
Performance,[query] load each missingness byte once when decoding a struct,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811:8,load,load,8,https://hail.is,https://github.com/hail-is/hail/issues/13811,1,['load'],['load']
Performance,[query] minor performance improvement,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13044:14,perform,performance,14,https://hail.is,https://github.com/hail-is/hail/pull/13044,1,['perform'],['performance']
Performance,[query] optimize in CompileAndEvalutate consistently,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9236:8,optimiz,optimize,8,https://hail.is,https://github.com/hail-is/hail/pull/9236,1,['optimiz'],['optimize']
Performance,[query] pca simplification/optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10264:27,optimiz,optimization,27,https://hail.is,https://github.com/hail-is/hail/pull/10264,2,['optimiz'],['optimization']
Performance,[query] substantially reduce single core latency for force-count,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13776:41,latency,latency,41,https://hail.is,https://github.com/hail-is/hail/pull/13776,1,['latency'],['latency']
Performance,[query] synchronize the code cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10309:29,cache,cache,29,https://hail.is,https://github.com/hail-is/hail/pull/10309,1,['cache'],['cache']
Performance,[query] use null (0) pointer as cached node value instead of -1L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12308:32,cache,cached,32,https://hail.is,https://github.com/hail-is/hail/pull/12308,1,['cache'],['cached']
Performance,[svcr] improve performance of both sample_qc methods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10727:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/10727,1,['perform'],['performance']
Performance,[various] Use load/dump over loads/dumps as appropriate,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9492:14,load,load,14,https://hail.is,https://github.com/hail-is/hail/pull/9492,2,['load'],"['load', 'loads']"
Performance,[vds/combiner] Fix attribute assignment on combiner plan load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12843:57,load,load,57,https://hail.is,https://github.com/hail-is/hail/pull/12843,1,['load'],['load']
Performance,[wip] perform simple CSE during python IR serialization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6688:6,perform,perform,6,https://hail.is,https://github.com/hail-is/hail/pull/6688,2,['perform'],['perform']
Performance,[www/docs] don't modify url on load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7385:31,load,load,31,https://hail.is,https://github.com/hail-is/hail/pull/7385,1,['load'],['load']
Performance,[www] cache bust 2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7056:6,cache,cache,6,https://hail.is,https://github.com/hail-is/hail/pull/7056,1,['cache'],['cache']
Performance,[www] cache bust css,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6997:6,cache,cache,6,https://hail.is,https://github.com/hail-is/hail/pull/6997,2,['cache'],['cache']
Performance,"\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}"")); (SelectFields (); (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C4.<init> instruction count: 3; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C4.apply instruction count: 401; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C4.apply instruction count: 16; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C5.<init> instruction count: 3; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C5.apply instruction count: 28; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C5.apply instruction count: 12; 2019-01-22 13:11:51 root: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:29201,optimiz,optimize,29201,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['optimiz'],['optimize']
Performance,"\; 1318 proto.END_COMMAND_PART; 1320 answer = self.gateway_client.send_command(command); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1324 for temp_arg in temp_args:; 1325 temp_arg._detach(). File /private/tmp/hail/hail/python/hail/backend/py4j_backend.py:35, in handle_java_exception.<locals>.deco(*args, **kwargs); 33 tpl = Env.jutils().handleForPython(e.java_exception); 34 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'). Java stack trace:; java.lang.RuntimeException: typ: inference failure:; 	at is.hail.expr.ir.IR.typ(IR.scala:38); 	at is.hail.expr.ir.IR.typ$(IR.scala:33); 	at is.hail.expr.ir.ToStream.typ(IR.scala:300); 	at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr_1$81(Parser.scala:1111); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2157); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.ut",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:3416,load,loader,3416,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['load'],['loader']
Performance,"]; 		at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25179,concurren,concurrent,25179,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux201,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38167,cache,cached,38167,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6250,cache,cached,6250,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"_From @cseed on August 26, 2015 14:20_; - fix representation for variant information; - support upstream deletion allele; - normalize on import: left-align, split complex. Should be done concurrently with:; https://github.com/cseed/k3/issues/5. _Copied from original issue: cseed/hail#9_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10:187,concurren,concurrently,187,https://hail.is,https://github.com/hail-is/hail/issues/10,1,['concurren'],['concurrently']
Performance,"_From @cseed on August 26, 2015 14:23_. Requires loading `.ped`/`.fam` files. _Copied from original issue: cseed/hail#10_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11:49,load,loading,49,https://hail.is,https://github.com/hail-is/hail/issues/11,1,['load'],['loading']
Performance,"_From @cseed on August 26, 2015 14:51_. Waiting on suitable machines (Intel spark cluster, cloud access, etc.); - measure size of stored data; - compressed vs uncompressed (gzip parquet, lz4 in SparkyVSM, etc.); - compute cost (or at least compute-hrs); - compare best-case (e.g. `gzip -cd file.vcf.gz | wc -l` vs `LoadVCF`). _Copied from original issue: cseed/hail#18_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/19:315,Load,LoadVCF,315,https://hail.is,https://github.com/hail-is/hail/issues/19,1,['Load'],['LoadVCF']
Performance,"_From @tpoterba on October 23, 2015 17:9_. Array data only includes a genotype call, so >80% of the memory and unpacking cpu for genotypes is wasted on missing data. We could implement multiple types of Genotype instances, if there is a way to do this without sequencing performance suffering. _Copied from original issue: cseed/hail#91_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/55:271,perform,performance,271,https://hail.is,https://github.com/hail-is/hail/issues/55,1,['perform'],['performance']
Performance,"__call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:3630,Cache,Cache,3630,https://hail.is,https://github.com/hail-is/hail/issues/10530,1,['Cache'],['Cache']
Performance,"_catch:. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda/lib/python3.7/site-packages/hail/table.py in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:2711,load,load,2711,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance,"_per_month); ```. This works out to 143 USD to run a 10,000 VM cluster 24 hours a day for 30 days. I suspect our average VM count in a month is closer to 10 which is within the free tier (340 MiB). I; might be wrong abou the connections per vm per aggregation interval, but this is straightforward to; monitor once we have the logs. For a sense of the cost landscape, these are all free:. 1. 1000 VMs.; 2. 500 VMs, with a sampling rate of 1.; 3. 200 VMs, with a sampling rate of 1, with an interval of 5 minutes.; 4. 10 VMs, with a sampling rate of 1, with an interval of 30 seconds. It's all linear, so if we need to halve the interval we can either change the sampling rate, reasses; our expected number of VM-hours, or adjust the service fee accordingly. We can also assess the landscape of fees necessary to cover costs (ignoring the free 50 GiB):. 1. 15 minute intervals, 0.5 sampling rate, 100 expected connections per vm per interval: 0.0000008; USD per core per hour. 2. 30 second intervals, 1.0 sampling rate, 100 expected connections per vm per interval: 0.00005 USD; per core per hour. 2. 5 second intervals, 1.0 sampling rate, 100 expected connections per vm per interval: 0.0003 USD; per core per hour. 2. 5 second intervals, 1.0 sampling rate, 1000 expected connections per vm per interval (1000 unique; connections per second honestly seems to me quite remarkable performance): 0.003 USD per core per; hour. ```; USD_per_core_per_hour = bytes_per_hour / vms / 1024. / 1024 / 1024 * 0.5 / 16. print(USD_per_core_per_hour); ```. ---. # Conclusion. I think we're safe to enable this with the parameters in this PR (15 minute intervals, 50%; sampling). We can assess unknown parameters, like connections per vm, and get comfortable looking at; these logs. Security constraints or observability demands may push us towards desiring more logs. If that; occurs, we can assess the need for a new fee. Regardless, this fee appears to be small relative to; the current cost of preemptible cores.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12883:4666,perform,performance,4666,https://hail.is,https://github.com/hail-is/hail/pull/12883,1,['perform'],['performance']
Performance,"_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:1142,Load,LoadVCF,1142,https://hail.is,https://github.com/hail-is/hail/issues/13346,1,['Load'],['LoadVCF']
Performance,"_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:1139,cache,cache,1139,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['cache'],['cache']
Performance,_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37958,cache,cached,37958,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1833,cache,cache-from,1833,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['cache'],['cache-from']
Performance,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5066:98,load,loadconda,98,https://hail.is,https://github.com/hail-is/hail/pull/5066,1,['load'],['loadconda']
Performance,"`Gen` expressions are generally OK because they are not executed until they're used in a `forall`. The issue with `DeNovoSuite` was that the `HailContext` is referenced by a function that produces a `Gen`, so it triggers `HailContext` initialization in order to produce the `Gen` itself (whereas the returned `Gen` doesn't actually perform any computation until its used in a `forall`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2262#issuecomment-332887472:332,perform,perform,332,https://hail.is,https://github.com/hail-is/hail/pull/2262#issuecomment-332887472,1,['perform'],['perform']
Performance,`NetworkAllocator.reserve` creates a bunch of network namespaces on startup and adds them to an `asyncio.Queue`. There's no reason to wait for all of them before accepting jobs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11251:105,Queue,Queue,105,https://hail.is,https://github.com/hail-is/hail/pull/11251,1,['Queue'],['Queue']
Performance,"`Table._select` got way too complicated (mostly my fault) when key changing moved from `TableMapRows` to `TableKeyBy`. Making `_select` a simple wrapper around `TableMapRows`, and moving the key logic to `key_by`, made both way simpler. I then realized the `key_by` code could be even simpler by adding some rules to the optimizer to clean up the case where all new keys are existing fields. I actually think some things had gotten broken in the old `_select` (performance wise). In particular, in `split_multi`, in the `split_rows` function with `rekey=false`, I think it's supposed to extend the key from `['locus']` to `['locus', 'alleles']`, but that wasn't happening. I changed `key_by` to no longer accept `key_by(None)` or `key_by([])`, both of which should now be `key_by()`, which is more consistent with the rest of our interface, but is a breaking change. Is it worth the disruption? Should I add a warning? Or just continue to accept both?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4455:321,optimiz,optimizer,321,https://hail.is,https://github.com/hail-is/hail/pull/4455,2,"['optimiz', 'perform']","['optimizer', 'performance']"
Performance,"``; # jstack 1433; ...; ""pool-1-thread-1"" #18 prio=5 os_prio=0 tid=0x00007f50c4f23000 nid=0x82c waiting on condition [0x00007f5084eeb000]; java.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:1099,concurren,concurrent,1099,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,```; Building dependency tree...; Reading state information...; [91mE: Unable to locate package python-setuptools; The command '/bin/sh -c apt-get install gcc python-setuptools && pip uninstall crcmod && pip install --no-cache-dir -U crcmod' returned a non-zero code: 100; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160:222,cache,cache-dir,222,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160,1,['cache'],['cache-dir']
Performance,"```; Error; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2272, in run; await self.jvm.execute(; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.Goo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:310,concurren,concurrent,310,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,5,['concurren'],['concurrent']
Performance,"```; In [1]: import hail as hl; In [2]: hl.hadoop_ls(""gs://bw2/bla""); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ca1bc15ebb3c> in <module>; ----> 1 hl.hadoop_ls(""gs://bw2/bla""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192); 	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186); 	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69); 	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22); 	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230); 	at scala.collection.TraversableLike$class.map(TraversableLike",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9600:581,load,loads,581,https://hail.is,https://github.com/hail-is/hail/issues/9600,1,['load'],['loads']
Performance,"```; In [4]: ht = hl.import_table('src/test/resources/variantAnnotations.alternateformat.tsv', min_partitions=100); 2019-01-20 11:36:31 Hail: INFO: Reading table with no type imputation; Loading column 'Chromosome:Position:Ref:Alt' as type 'str' (type not specified); Loading column 'Rand1' as type 'str' (type not specified); Loading column 'Rand2' as type 'str' (type not specified); Loading column 'Gene' as type 'str' (type not specified). In [5]: ht.n_partitions(); Out[5]: 1. In [6]: ht.repartition(100).n_partitions(); Out[6]: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5177#issuecomment-455882267:187,Load,Loading,187,https://hail.is,https://github.com/hail-is/hail/issues/5177#issuecomment-455882267,4,['Load'],['Loading']
Performance,"```; Lots of local cleanups.; Use `' for quoting inside error messages.; Added SuperCommand, ToplevelCommands. Use for annotatevariants and annotatesamples.; Try to make multiple instances of (essentially) same error message consistent.; Added option to LoadVCF, ImportVCF to skip genotypes. Use in importannotations.; Fixed some bugs in FatalException handling.; Moved Type.parse to trait Parsable.; Added Parser.parseAnnotationTypes.; Added `type_expr' non-terminal to Parser.; Prefer `import ...expr._' to `import ...expr' and `expr.Foo'.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/228:254,Load,LoadVCF,254,https://hail.is,https://github.com/hail-is/hail/pull/228,1,['Load'],['LoadVCF']
Performance,```; TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E reactor.core.Exceptions$ReactiveException: java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.Exceptions.propagate(Exceptions.java:392); E 	at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:97); E 	at reactor.core.publisher.Flux.blockLast(Flux.java:2519); E 	at com.azure.core.util.paging.ContinuablePagedByIteratorBase.requestPage(ContinuablePagedByIteratorBase.java:94); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable$ContinuablePagedByItemIterator.<init>(ContinuablePagedByItemIterable.java:50); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable.iterator(ContinuablePagedByItemIterable.java:37); E 	at com.azure.core.util.paging.ContinuablePagedIterable.iterator(ContinuablePagedIterable.java:106); E 	at java.lang.Iterable.forEach(Iterable.java:74); E 	at is.hail.io.fs.AzureStorageFS.delete(AzureStorageFS.scala:203); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1(ExecuteContext.scala:27); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1$adapted(ExecuteContext.scala:26); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.backend.OwningTempFileManager.cleanup(ExecuteContext.scala:26); E 	at is.hail.backend.ExecuteContext.close(ExecuteContext.scala:148); E 	at is.hail.utils.package$.using(package.scala:660); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.Ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:188,concurren,concurrent,188,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1311:337,load,loads,337,https://hail.is,https://github.com/hail-is/hail/issues/1311,1,['load'],['loads']
Performance,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8201:873,load,loads,873,https://hail.is,https://github.com/hail-is/hail/issues/8201,1,['load'],['loads']
Performance,"```; gsutil cat gs://hail-ci-0-1/deploy/ef349a51016f\*/job-log; ```. the last few lines:. ```; + make push-batch; docker build -t batch .; time=""2018-09-26T00:14:20Z"" level=error msg=""failed to dial gRPC: cannot connect to the Docker daemon. Is 'docker daemon' running on this host?: dial unix /var/run/docker.sock: connect: permission denied""; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=vhnl6wchhs00sgt8raa35j7m7&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; time=""2018-09-26T00:14:20Z"" level=error msg=""Can't add file /hail/repo/batch/batch/server.py to tar: io: read/write on closed pipe""; Makefile:14: recipe for target 'build-batch' failed; make: *** [build-batch] Error 1; ```. this is failing all deploys of hail, which is safe, but it prevents our users from getting updates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443:519,cache,cachefrom,519,https://hail.is,https://github.com/hail-is/hail/issues/4443,1,['cache'],['cachefrom']
Performance,"```; import hail as hl; ds = hl.balding_nichols_model(3, 100, 100); ds.annotate_globals(x=[1,2,3]); ```; The above script breaks on devel clusters.; ```; py4j.protocol.Py4JJavaError: An error occurred while calling o64.annotateGlobalExpr.; : java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:642); 	at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:174); 	at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:170); 	at is.hail.asm4s.package$.loadClass(package.scala:181); 	at is.hail.asm4s.FunctionBuilder$$anon$1.apply(FunctionBuilder.scala:312); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$12$$anonfun$apply$6.apply(Parser.scala:172); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2982:599,load,loadOrDefineClass,599,https://hail.is,https://github.com/hail-is/hail/issues/2982,2,['load'],"['loadClass', 'loadOrDefineClass']"
Performance,"```; is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:459,Load,LoadVCF,459,https://hail.is,https://github.com/hail-is/hail/issues/3015,2,['Load'],['LoadVCF']
Performance,"```; ❯ java -version. java version ""16.0.1"" 2021-04-20; Java(TM) SE Runtime Environment (build 16.0.1+9-24); Java HotSpot(TM) 64-Bit Server VM (build 16.0.1+9-24, mixed mode, sharing). ❯ python3 -m pip show pyspark. Name: pyspark; Version: 3.1.1; Summary: Apache Spark Python API; Home-page: https://github.com/apache/spark/tree/master/python; Author: Spark Developers; Author-email: dev@spark.apache.org; License: http://www.apache.org/licenses/LICENSE-2.0; Location: /Users/toby.manders/miniconda3/envs/hail-env/lib/python3.9/site-packages; Requires: py4j; Required-by: hail. ❯ python3 -m pip show hail. Name: hail; Version: 0.2.67; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /Users/toby.manders/miniconda3/envs/hail-env/lib/python3.9/site-packages; Requires: requests, pandas, hurry.filesize, nest-asyncio, gcsfs, scipy, fsspec, tabulate, bokeh, PyJWT, Deprecated, decorator, dill, numpy, aiohttp-session, pyspark, tqdm, google-cloud-storage, aiohttp, asyncinit, humanize, python-json-logger, parsimonious; Required-by:; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10524#issuecomment-848330358:644,Scalab,Scalable,644,https://hail.is,https://github.com/hail-is/hail/issues/10524#issuecomment-848330358,1,['Scalab'],['Scalable']
Performance,"```pycon; In [1]: import hail as hl. In [2]: hl.init(); 2022-03-11 14:49:23 WARN Utils:69 - Your hostname, metis resolves to a loopback address: 127.0.0.1; using 192.168.1.169 instead (on interface eth0); 2022-03-11 14:49:23 WARN Utils:69 - Set SPARK_LOCAL_IP if you need to bind to another address; 2022-03-11 14:49:23 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.2; SparkUI available at http://192.168.1.169:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.90-92e40ce648a8; LOGGING: writing to /home/cdv/src/hail/hail/hail-20220311-1449-0.2.90-92e40ce648a8.log. In [3]: mt = hl.import_vcf('src/test/resources/sample.vcf').filter_rows(False). In [4]: ht = mt._localize_entries('entries', 'columns'). In [5]: groups = ht.group_by(the_key=ht.key).aggregate(value=hl.agg.collect(ht.row_value)).collect(); 2022-03-11 14:50:08 Hail: INFO: Coerced sorted dataset; 2022-03-11 14:50:10 Hail: INFO: Ordering unsorted dataset with network shuffle1]. In [6]: len(groups); Out[6]: 346. In [7]: mt = mt.checkpoint('~/tmp/hail/sample.vcf.filtered.mt'); 2022-03-11 14:51:14 Hail: INFO: wrote matrix table with 0 rows and 100 columns in 0 partitions to ~/tmp/hail/sample.vcf.filtered.mt. In [8]: ht = mt._localize_entries('entries', 'columns'). In [9]: groups_native = ht.group_by(the_key=ht.key).aggregate(value=hl.agg.collect(ht.row_value)).collect(). In [10]: len(groups_native); Out[10]: 0; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11562:357,load,load,357,https://hail.is,https://github.com/hail-is/hail/issues/11562,1,['load'],['load']
Performance,"```python; import hail as hl; hl.init(); hl.utils.get_1kg('data'); mt = hl.read_matrix_table('data/1kg.mt'); mt.entries().show(10); df = mt.entries().to_pandas(); ```. ```; Hail version: 0.2.18-08ec699f0fd4; Error summary: HailException: optimization changed type!; before: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[Int32],DP:Int32,GQ:Int32,PL:Array[Int32]}}; after: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[+Int32],DP:Int32,GQ:Int32,PL:Array[+Int32]}}; ```. Randomly assigned @catoverdrive, cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6766:238,optimiz,optimization,238,https://hail.is,https://github.com/hail-is/hail/issues/6766,1,['optimiz'],['optimization']
Performance,"``scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1048,load,loadString,1048,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437,2,"['cache', 'load']","['cachedRsid', 'loadString']"
Performance,"`builder.loadFields` just loads the offset of the data array, but it needs to do a deep copy otherwise all of the array aggragators will start off writing to (and overwriting) the same arraybuilder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6819:9,load,loadFields,9,https://hail.is,https://github.com/hail-is/hail/pull/6819,2,['load'],"['loadFields', 'loads']"
Performance,`f` is a thunk so it is currently being evaluated thrice before inserted into the code cache. The `compiledFunction` variable was unused so I think this is what was originally intended.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13796:87,cache,cache,87,https://hail.is,https://github.com/hail-is/hail/pull/13796,1,['cache'],['cache']
Performance,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6880:1755,load,loading,1755,https://hail.is,https://github.com/hail-is/hail/issues/6880,1,['load'],['loading']
Performance,"`import_bgen` fails because there are no reference genomes on worker nodes. `import_bgen` needs to read the index file. Reading the index file means parsing a type. Parsing a locus type means looking up a reference genome. The error message comes from line 588 in `ReferenceGenome.scala` by way of line 70 of `IndexReader.scala`:; ```scala; val keyType = IRParser.parseType(metadata.keyType); ```. The root cause seems to be #5512, in which we [stop loading the genomes from resources](https://github.com/hail-is/hail/pull/5512/files#diff-16c24a9c4265932816e9e88806f5a2abL527).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5673:450,load,loading,450,https://hail.is,https://github.com/hail-is/hail/issues/5673,1,['load'],['loading']
Performance,"`ldscore` only has one `sparsify_row_intervals`, and it's used to immediately write and read. We could rework it to use `export_rectangles` and `rectangles_to_numpy` from #5516. Not sure what it would do to performance though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5522#issuecomment-469364826:207,perform,performance,207,https://hail.is,https://github.com/hail-is/hail/pull/5522#issuecomment-469364826,1,['perform'],['performance']
Performance,"`strides` shouldn't be a public field on `PNDArray` interface, just an internal detail of `PCanonicalNDArray`. . `dimensionLength` was just loading the shape, should never have been its own method. There's a lot more interface clean up to do, these two were just easy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9783:140,load,loading,140,https://hail.is,https://github.com/hail-is/hail/pull/9783,1,['load'],['loading']
Performance,"`tls.py` has many different functions with long names. This change reduces it to three functions:. - internal_server_ssl_context; - internal_client_ssl_context; - external_client_ssl_context. I also added `httpx.py` which contains the HTTPS-related functions that `tls.py` previously; contained. I also simplified the HTTPS-related functions to just:. - client_session; - blocking_client_session. I determine internal vs. external using the deploy config. ---. An [`ssl.SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) defines how a; network library (such as `aiohttp`) should perform SSL/TLS. Let's look at an example:. ```python3; server_ssl_context = ssl.create_default_context(; purpose=Purpose.CLIENT_AUTH,; cafile='/incoming.cacerts'); server_ssl_context.load_cert_chain(ssl_config['cert'],; keyfile=ssl_config['key'],; password=None); server_ssl_context.verify_mode = ssl.CERT_OPTIONAL; server_ssl_context.check_hostname = False; ```. The first function call states that we are a *server* performing *client; authentication* (`Purpose.CLIENT_AUTH`). We also state that anyone who sends requests to us will be; identified by a certificate that is trusted by our certificate database: `/incoming.cacerts` (which; is a file). `load_cert_chain` states where to find the certificates and secret key that prove who we are. The; certificate and secret key together are like a property title that proves someone owns a house. The; `password=None` means that our secret key has no password. Some keys are themselves locked by a; password. `verify_mode` means what do we expect our clients to have. `CERT_OPTIONAL` means anonymous clients; are OK. This is how servers normally operate (https://google.com does not care who you are). `check_hostname` means should we verify that the client certificate matches the client's; hostname. Since we allow anonymous clients, this must be `False`. ---. `test-address.py` is a gross hack. It will disappear in subsequent PRs. For now, I push",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9862:600,perform,perform,600,https://hail.is,https://github.com/hail-is/hail/pull/9862,1,['perform'],['perform']
Performance,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248:685,load,load,685,https://hail.is,https://github.com/hail-is/hail/pull/6248,2,['load'],"['load', 'load-main-class']"
Performance,"a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujson wasn't helping much. I can make a ujson-specific pr, but my goal was to test async library implementations in a simple applications, since we need a long term strategy for python web stuff that isn't Flask (or not just Flask)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:1796,cache,cache,1796,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051,4,"['cache', 'queue']","['cache', 'queue']"
Performance,a.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4592,Load,LoadVCF,4592,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,a.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11405,Load,LoadVCF,11405,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,a.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Back,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11830,Optimiz,Optimize,11830,https://hail.is,https://github.com/hail-is/hail/issues/8338,1,['Optimiz'],['Optimize']
Performance,a.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHan,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219136,concurren,concurrent,219136,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,a.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_392]; 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_392]; 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_392]; 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503) ~[?:1.8.0_392]; 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) ~[?:1.8.0_392]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:136) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:657) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$3(Worker.scala:135) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$2(Worker.scala:134) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:15227,concurren,concurrent,15227,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,5,['concurren'],['concurrent']
Performance,"a:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13180,Load,LoadVCF,13180,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,a:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$D,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215454,concurren,concurrent,215454,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"a:183); 	at is.hail.io.fs.FS.exists$(FS.scala:181); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.Spar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:2316,load,loadClass,2316,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['load'],['loadClass']
Performance,a:214); 	at java.lang.Thread.run(Thread.java:748)com.esotericsoftware.kryo.KryoException: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)sun.reflect.generics.reflectiveObjects.NotImplementedException: null; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.Obje,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:6232,concurren,concurrent,6232,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance,a:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:5925,concurren,concurrent,5925,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,"a:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary sea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:12493,concurren,concurrent,12493,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['concurren'],['concurrent']
Performance,a:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5680,concurren,concurrent,5680,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['concurren'],['concurrent']
Performance,a:475); 	at is.hail.methods.CalculateConcordance$.apply(CalculateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:7826,load,loadInt,7826,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadInt']
Performance,a:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.Transpo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218978,concurren,concurrent,218978,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,a:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.put(JsonResumableSession.java:68); 	at is,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5591,concurren,concurrent,5591,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,"a_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2688,Load,LoadVCF,2688,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"ab2e4""><code>e85d865</code></a> Simplify &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/7dc426c95a0c329d5514e6198d92080f1ffc1e5e""><code>7dc426c</code></a> Fix update() ordering to be more consistent with add() ordering (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/159"">#159</a>)</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/13d30bc654eb9e6be092282ca502967fcb7f0113""><code>13d30bc</code></a> Bump version to 2.2.2</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/4997d0e849f2275d1931772a5432163ecc20e0b0""><code>4997d0e</code></a> Refactor small slice optimization in SortedList.<strong>getitem</strong></li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/6ee5d57fc8d691fbab4972b853a60348d0f922ef""><code>6ee5d57</code></a> improve SortedList.<strong>getitem</strong>() performance for small slices</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/ac80254fb6a08045ced7d9704412878ff8000fa7""><code>ac80254</code></a> suppress warning in test of deprecated function (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/118"">#118</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/grantjenks/python-sortedcontainers/compare/v2.1.0...v2.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sortedcontainers&package-manager=pip&previous-version=2.1.0&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (depen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11476:3262,perform,performance,3262,https://hail.is,https://github.com/hail-is/hail/pull/11476,1,['perform'],['performance']
Performance,"able(10).collect(); Initializing Hail with default parameters...; /Users/dking/projects/hail/hail/python/hail/utils/java.py:54: UserWarning: When using the query service backend, use `await Env._async_hc()'; warnings.warn('When using the query service backend, use `await Env._async_hc()\''); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.81-edcde5c1b324; LOGGING: writing to /Users/dking/projects/hail/query/hail-20220113-1844-0.2.81-edcde5c1b324.log; Out[1]: ; [Struct(idx=0),; Struct(idx=1),; Struct(idx=2),; Struct(idx=3),; Struct(idx=4),; Struct(idx=5),; Struct(idx=6),; Struct(idx=7),; Struct(idx=8),; Struct(idx=9)]; ```. The very first time you execute this comand, it will run four batches to generate the four default; reference genomes and download those to your local machine. Those four reference genomes are cached; per-SHA on your local machine, so future Hail pipelines will have lower latency. If you have the ""standing working"" enabled, you should expect a latency of ~8s for the above job. I; think we can approximately halve this by re-using classloaders. Ask me more about this. After running this job a few times, the Batch UI looks like this:. ![Screen Shot 2022-01-14 at 11 59 41 AM](https://user-images.githubusercontent.com/106194/149554866-221e4243-1238-4d01-8944-0a6ed0b4c28c.png). When you call `collect`, the Python-side `ServiceBackend` writes a file to cloud storage containing; the temporary directory, billing project, and the IR. In addition to the `execute` command used for; `collect`, there are command for getting the table type and references; genomes. `ServiceBackend._rpc` defines this API on the Python-side and; `is.hail.backend.service.ServiceBackendSocketAPI2` defines this API on the Scala-side. After writing to cloud storage, the ServiceBackend submits the one-job driver batch to Hail; Batch. It then waits for the driver job to complete. When the driver job is finished, it reads the; outputs from google clo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:1886,latency,latency,1886,https://hail.is,https://github.com/hail-is/hail/pull/11194,1,['latency'],['latency']
Performance,"aceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2272, in run; await self.jvm.execute(; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.GoogleStorageFS$$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:1009,concurren,concurrent,1009,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['concurren'],['concurrent']
Performance,ache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.Callbac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216081,concurren,concurrent,216081,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13206,Load,LoadVCF,13206,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkB,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:5935,concurren,concurrent,5935,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:17855,concurren,concurrent,17855,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.D,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:10600,concurren,concurrent,10600,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,"ack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.Heartbe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:1785,load,loadClass,1785,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['load'],['loadClass']
Performance,"ackages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 57, in _read_logs; return {k: v for k, v in await future_logs}; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 52, in _read_log_from_gcs; pod_log = await self.app['log_store'].read_gs_file(LogStore.container_log_path(self.directory, task_name)); File ""/usr/local/lib/python3.6/dist-packages/batch/log_store.py"", line 40, in read_gs_file; return await self.gcs.read_gs_file(uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 27, in read_gs_file; return await self._wrapped_read_gs_file(self, uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 37, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7412:4643,concurren,concurrent,4643,https://hail.is,https://github.com/hail-is/hail/pull/7412,1,['concurren'],['concurrent']
Performance,ackend.scala:432); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4084,concurren,concurrent,4084,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,ackend.service.Worker$.$anonfun$main$3(Worker.scala:135) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$2(Worker.scala:134) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:7374,concurren,concurrent,7374,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['concurren'],['concurrent']
Performance,ackend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217930,Optimiz,OptimizePass,217930,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ackend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209909,Optimiz,OptimizePass,209909,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ackend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201957,Optimiz,OptimizePass,201957,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ackend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:197305,Optimiz,OptimizePass,197305,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21594,concurren,concurrent,21594,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,ackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:13880,concurren,concurrent,13880,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,"actChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBack",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213901,concurren,concurrent,213901,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"actually, let's just release now. We can make another release at the end of the week -- there are a couple of major performance improvements queued up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482:116,perform,performance,116,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482,2,"['perform', 'queue']","['performance', 'queued']"
Performance,"acy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the tru",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4554,load,load,4554,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,"ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:18.04; RUN touch foo; ```; To build I use this command (slightly different syntax from the buildctl syntax, but, AFAIK, uses the same backend):; ```; docker buildx \ ; build \; DIRECTORY_NAME_HERE \; --output 'type=image,""name=gcr.io/hail-vdc/dktest,gcr.io/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:2092,cache,cache,2092,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,4,['cache'],['cache']
Performance,"adBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 200 OK; < Content-Type: application/json; < Date: Wed, 05 Feb 2020 20:59:27 GMT; < Content-Length: 88; <; {; 	""service"": {; 		""namespace"": ""default"",; 		""name"": ""gateway""; 	},; 	""localEndpoints"": 1; }; ```; ```; }dking@gke-vdc-non-preemptible-pool-5-80798769-kp8n ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 503 Service Unavailable; < Conte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8045:1747,load,loadbalancer,1747,https://hail.is,https://github.com/hail-is/hail/pull/8045,1,['load'],['loadbalancer']
Performance,add instructions on how to load `code_style.xml`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/820:27,load,load,27,https://hail.is,https://github.com/hail-is/hail/pull/820,1,['load'],['load']
Performance,add isDeterministic flag to IRFunction to help with optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3430:52,optimiz,optimization,52,https://hail.is,https://github.com/hail-is/hail/pull/3430,1,['optimiz'],['optimization']
Performance,"ading a new JAR. You can interact at ipython like this:. ```; In [1]: import hail as hl; ...: hl.utils.range_table(10).collect(); Initializing Hail with default parameters...; /Users/dking/projects/hail/hail/python/hail/utils/java.py:54: UserWarning: When using the query service backend, use `await Env._async_hc()'; warnings.warn('When using the query service backend, use `await Env._async_hc()\''); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.81-edcde5c1b324; LOGGING: writing to /Users/dking/projects/hail/query/hail-20220113-1844-0.2.81-edcde5c1b324.log; Out[1]: ; [Struct(idx=0),; Struct(idx=1),; Struct(idx=2),; Struct(idx=3),; Struct(idx=4),; Struct(idx=5),; Struct(idx=6),; Struct(idx=7),; Struct(idx=8),; Struct(idx=9)]; ```. The very first time you execute this comand, it will run four batches to generate the four default; reference genomes and download those to your local machine. Those four reference genomes are cached; per-SHA on your local machine, so future Hail pipelines will have lower latency. If you have the ""standing working"" enabled, you should expect a latency of ~8s for the above job. I; think we can approximately halve this by re-using classloaders. Ask me more about this. After running this job a few times, the Batch UI looks like this:. ![Screen Shot 2022-01-14 at 11 59 41 AM](https://user-images.githubusercontent.com/106194/149554866-221e4243-1238-4d01-8944-0a6ed0b4c28c.png). When you call `collect`, the Python-side `ServiceBackend` writes a file to cloud storage containing; the temporary directory, billing project, and the IR. In addition to the `execute` command used for; `collect`, there are command for getting the table type and references; genomes. `ServiceBackend._rpc` defines this API on the Python-side and; `is.hail.backend.service.ServiceBackendSocketAPI2` defines this API on the Scala-side. After writing to cloud storage, the ServiceBackend submits the one-job driver batch to Hail; Batch. It the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:1733,cache,cached,1733,https://hail.is,https://github.com/hail-is/hail/pull/11194,2,"['cache', 'latency']","['cached', 'latency']"
Performance,"ae-afc6-326f710d9a89; 2023-09-24 01:58:24.305 GoogleStorageFS$: INFO: close: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtA",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4222,concurren,concurrent,4222,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,age.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3041:4892,concurren,concurrent,4892,https://hail.is,https://github.com/hail-is/hail/issues/3041,2,['concurren'],['concurrent']
Performance,ageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:22916,concurren,concurrent,22916,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,ageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 ________________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_spectra_4():; > spectra_helper(spec4). test/hail/methods/test_pca.py:229: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/methods/test_pca.py:172: in spectra_helper; hail_V = (np.array(scores.scores.collect()) / singulars).T; <decorator-gen-538>:2: in collect; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:9298,concurren,concurrent,9298,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"ah, OK. I'll un-block tomorrow, since I think there's already a queue of approved PRs and I have at least 2 that will need to go in this evening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5514#issuecomment-470285754:64,queue,queue,64,https://hail.is,https://github.com/hail-is/hail/pull/5514#issuecomment-470285754,1,['queue'],['queue']
Performance,aha! Looks like this caught a *separate* optimizer bug related to keying :(. Will fix this test and then fix that bug afterwars.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7071#issuecomment-534279756:41,optimiz,optimizer,41,https://hail.is,https://github.com/hail-is/hail/pull/7071#issuecomment-534279756,1,['optimiz'],['optimizer']
Performance,"ail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1939,cache,cached,1939,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Re,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5452,concurren,concurrent,5452,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,ail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188942,Optimiz,OptimizePass,188942,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181917,Optimiz,OptimizePass,181917,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:15652,Optimiz,OptimizePass,15652,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:8668,Optimiz,OptimizePass,8668,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4377,Optimiz,OptimizePass,4377,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181231,Optimiz,OptimizePass,181231,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188256,Optimiz,OptimizePass,188256,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14966,Optimiz,OptimizePass,14966,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:7982,Optimiz,OptimizePass,7982,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.lowering.LowerArrayAggsToRunAggsPass/is.hail.expr.ir.lowering.EmittableIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.525ms self 0.004ms children 0.521ms %children 99.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.Lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:214149,Optimiz,OptimizePass,214149,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.600ms self 0.007ms children 0.594ms %children 98.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:180561,Optimiz,Optimize,180561,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.601ms self 0.006ms children 0.595ms %children 99.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:187586,Optimiz,Optimize,187586,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.157ms self 0.010ms children 1.147ms %children 99.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14296,Optimiz,Optimize,14296,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:2044,concurren,concurrent,2044,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['concurren'],['concurrent']
Performance,ail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7270,Optimiz,Optimize,7270,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,"ail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5273,concurren,concurrent,5273,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"ail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:12369,concurren,concurrent,12369,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['concurren'],['concurrent']
Performance,"ailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:3546,Load,LoadVCF,3546,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,"ailKryoRegistrator \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; pyspark-shell '. from pyspark import SparkContext; sc=SparkContext.getOrCreate(). import hail as hl; hl.init(sc=sc); ```. - Error logs ; ```; 22/05/11 14:31:21 WARN Utils: Your hostname, spacerider.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en6); 22/05/11 14:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 22/05/11 14:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Input In [2], in <cell line: 6>(); 3 sc = spark._sc; 5 import hail as hl; ----> 6 hl.init(sc=sc). File <decorator-gen-1703>:2, in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory). File ~/miniforge3/envs/hail/lib/python3.9/site-packages/hail/typecheck/check.py:577, in _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827:1716,load,load,1716,https://hail.is,https://github.com/hail-is/hail/issues/11827,1,['load'],['load']
Performance,"aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3864,cache,cached,3864,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"akeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: before: IR size 8: \n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.148 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.160 root: INFO: Aggregate: useTreeAggregate=false\n2022-11-15 20:30:18.160 root: INFO: Aggregate: commutative=false\n2022-11-15 20:30:18.163 root: INFO: optimize optimize: compileLowerer, initial IR: before: IR size 70: \n(MakeTuple (0)\n (Let __iruid_455\n (MakeStruct)\n (Let __iruid_458\n (Let global\n (Ref __iruid_455)\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32)))))\n (Let __iruid_460\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_456\n __iruid_459\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct\n (__iruid_455 (Ref __iruid_455))\n (__iruid_458 (Ref __iruid_458)))\n (Let __iruid_458\n (GetField __iruid_458 (Ref __iruid_459))\n (Let __iruid_455\n (GetField __irui",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:2407,optimiz,optimize,2407,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,al(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6163,Load,LoadPlink,6163,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"alCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2542,Load,LoadVCF,2542,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,alFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188971,Optimiz,Optimize,188971,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,alFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181946,Optimiz,Optimize,181946,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,alFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:15681,Optimiz,Optimize,15681,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,alFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:8697,Optimiz,Optimize,8697,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,alFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4406,Optimiz,Optimize,4406,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,alLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:107778,Optimiz,OptimizePass,107778,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,alLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:93340,Optimiz,OptimizePass,93340,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,alLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:72161,Optimiz,OptimizePass,72161,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,alLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:57763,Optimiz,OptimizePass,57763,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,alLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:49235,Optimiz,OptimizePass,49235,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,alLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:40705,Optimiz,OptimizePass,40705,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:4459,concurren,concurrent,4459,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['concurren'],['concurrent']
Performance,ala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216357,concurren,concurrent,216357,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217779,concurren,concurrent,217779,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218275,concurren,concurrent,218275,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5623,optimiz,optimizeIR,5623,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['optimiz'],['optimizeIR']
Performance,ala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Ite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19034,Load,LoadVCF,19034,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"alhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5412,Latency,Latency,5412,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['Latency'],['Latency']
Performance,alizers.java:302); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:270); at org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:323); at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140); at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95); at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539); at is.hail.backend.spark.SparkBackend.broadcast(SparkBackend.scala:411); at is.hail.io.plink.MatrixPLINKReader.executeGeneric(LoadPlink.scala:390); at is.hail.io.plink.MatrixPLINKReader.lower(LoadPlink.scala:561); at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1062); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:4701,Load,LoadPlink,4701,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['Load'],['LoadPlink']
Performance,all our joins perform horribly if both sides don't have a reasonable number of partitions,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953#issuecomment-406016947:14,perform,perform,14,https://hail.is,https://github.com/hail-is/hail/issues/3953#issuecomment-406016947,1,['perform'],['perform']
Performance,allbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216654,concurren,concurrent,216654,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,allbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.Callback,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218076,concurren,concurrent,218076,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"also `loadElement` instead of `elementOffset`; although they're equivalent as of now because we store structs inline, it was sematincally wrong since we need the pointer to the actual element itself and not the pointer inside the array. (this would cause problems if, e.g., we had physical types that stored the struct as a pointer.). This (TAGG thing) got caught by some tests in #3431.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3434:6,load,loadElement,6,https://hail.is,https://github.com/hail-is/hail/pull/3434,1,['load'],['loadElement']
Performance,also remove load balancing from scorecard,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4447:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/pull/4447,1,['load'],['load']
Performance,"ameter so large groups only return their size rather than filling memory and killing the entire job. - Now computeGramianLargeN is used if n * m exceeds 8000 * 8000 (about 512MB of doubles) or maxSize * maxSize if maxSize is given and smaller than 8000. This seems a reasonable approach for now to prevent OOM without exposing additional memory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 200",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2248:1256,tune,tune,1256,https://hail.is,https://github.com/hail-is/hail/pull/2248,1,['tune'],['tune']
Performance,"ams that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4425,Latency,Latency,4425,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['Latency'],['Latency']
Performance,"analysis_type=VariantFiltration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:19663,perform,performanceLog,19663,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performanceLog']
Performance,"and give the user a warning that they might want to log in. Anonymous credentials will allow us to access public buckets without authenticating, which was not possible until now. I contemplated whether we should only suggest the login if they've received a 401 but I assume most everyone using this will want to be authenticated and we already log a warning as it is. This is reasonably testable, but I couldn't find where it should go. Does `test_fs` test credential-loading at all?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11207:468,load,loading,468,https://hail.is,https://github.com/hail-is/hail/pull/11207,1,['load'],['loading']
Performance,and.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailure,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:10904,concurren,concurrent,10904,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance,"andlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213253,concurren,concurrent,213253,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"anges include:; <ul>; <li>Add support for <code>sankey</code> links with arrows</li>; <li>Add <code>selections</code>, <code>newselection</code> and <code>activeselection</code> layout attributes to have persistent and editable selections over cartesian subplots</li>; <li>Add <code>unselected.line.color</code> and <code>unselected.line.opacity</code> options to <code>parcoords</code> trace</li>; <li>Display Plotly's new logo in the modebar</li>; </ul>; </li>; </ul>; <h2>[5.9.0] - 2022-06-23</h2>; <h3>Added</h3>; <ul>; <li><code>pattern_shape</code> options now available in <code>px.timeline()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3774"">#3774</a></li>; <li><code>facet_*</code> and <code>category_orders</code> now available in <code>px.pie()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3775"">#3775</a></li>; </ul>; <h3>Performance</h3>; <ul>; <li><code>px</code> methods no longer call <code>groupby</code> on the input dataframe when the result would be a single group, and no longer groups by a lambda, for significant speedups <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3765"">#3765</a> with thanks to <a href=""https://github.com/jvdd""><code>@​jvdd</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Allow non-string extras in <code>flaglist</code> attributes, to support upcoming changes to <code>ax.automargin</code> in plotly.js <a href=""https://github-redirect.dependabot.com/plotly/plotly.js/pull/6193"">plotly.js#6193</a>, <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3749"">#3749</a></li>; </ul>; <h2>[5.8.2] - 2022-06-10</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed a syntax error that caused rendering issues in Databricks notebooks and likely elsewhere. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3763"">#3763</a> with thanks to <a href=""https://github.com/fwetdb""><code>@​fwetdb</code></a></li>; </ul>; <h2>[5.8.1] - 2022-06-08</h2>; <p>(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12113:2383,Perform,Performance,2383,https://hail.is,https://github.com/hail-is/hail/pull/12113,1,['Perform'],['Performance']
Performance,annelHandlerContext.java:343); 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); 	Error: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.; 	This stopped SparkContext was created at:; 	; 	org.apache.spark.SparkContext.getOrCreate(SparkContext.scala); 	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	java.lang.reflect.Method.invoke(Method.java:498); 	sparklyr.Invoke.invoke(invoke.scala:139); 	sparklyr.StreamHandler.handleMethodCall(stream.scala:123); 	sparklyr.StreamHandler.read(stream.scala:66); 	sparklyr.BackendHandler.channelRead0(handler.scala:51); 	sparklyr.BackendHandler.channelRead0(handler.scala:4); 	io.netty.channel.SimpleChan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:6500,concurren,concurrent,6500,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,annelHandlerContext.java:343); 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:13118,concurren,concurrent,13118,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,annelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:22366,concurren,concurrent,22366,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:8279,concurren,concurrent,8279,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['concurren'],['concurrent']
Performance,anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:45); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:126); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:122); E 	at is.hail.backend.local.LocalBackend.lookupOrCompileCachedFunction(LocalBackend.scala:73); E 	at is.hail.expr.ir.Compile$.apply(Compile.scala:39); E 	at is.hail.expr.ir.CompileAndEvaluate$.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:8140,Optimiz,OptimizePass,8140,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['OptimizePass']
Performance,anonfun$apply$22.apply(ContextRDD.scala:308); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c8ca698; Error summary: NegativeArraySizeException: null,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3583:15570,concurren,concurrent,15570,https://hail.is,https://github.com/hail-is/hail/issues/3583,2,['concurren'],['concurrent']
Performance,"any different functions with long names. This change reduces it to three functions:. - internal_server_ssl_context; - internal_client_ssl_context; - external_client_ssl_context. I also added `httpx.py` which contains the HTTPS-related functions that `tls.py` previously; contained. I also simplified the HTTPS-related functions to just:. - client_session; - blocking_client_session. I determine internal vs. external using the deploy config. ---. An [`ssl.SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) defines how a; network library (such as `aiohttp`) should perform SSL/TLS. Let's look at an example:. ```python3; server_ssl_context = ssl.create_default_context(; purpose=Purpose.CLIENT_AUTH,; cafile='/incoming.cacerts'); server_ssl_context.load_cert_chain(ssl_config['cert'],; keyfile=ssl_config['key'],; password=None); server_ssl_context.verify_mode = ssl.CERT_OPTIONAL; server_ssl_context.check_hostname = False; ```. The first function call states that we are a *server* performing *client; authentication* (`Purpose.CLIENT_AUTH`). We also state that anyone who sends requests to us will be; identified by a certificate that is trusted by our certificate database: `/incoming.cacerts` (which; is a file). `load_cert_chain` states where to find the certificates and secret key that prove who we are. The; certificate and secret key together are like a property title that proves someone owns a house. The; `password=None` means that our secret key has no password. Some keys are themselves locked by a; password. `verify_mode` means what do we expect our clients to have. `CERT_OPTIONAL` means anonymous clients; are OK. This is how servers normally operate (https://google.com does not care who you are). `check_hostname` means should we verify that the client certificate matches the client's; hostname. Since we allow anonymous clients, this must be `False`. ---. `test-address.py` is a gross hack. It will disappear in subsequent PRs. For now, I pushed all the; gr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9862:1019,perform,performing,1019,https://hail.is,https://github.com/hail-is/hail/pull/9862,1,['perform'],['performing']
Performance,any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a062707668,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40478,cache,cached,40478,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,anylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35966,cache,cached,35966,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spark.storage.StorageLevel; import org.apache.{hadoop => hd}; import org.json4s.JValue; import org.json4s.JsonAST._; import org.json4s._; import org.json4s.jackson.JsonMethods; import org.json4s.jackson.JsonMethods._; import org.json4s.jackson.Serialization; import org.json4s.jackson.{JsonMethods, Serialization}; import org.json4s.{DefaultFormats, Formats}; import org.sparkproject.guava.util.concurrent.MoreExecutors; ```. We explicitly depend on; - `htsjdk`; - `breeze`; - `json4s`. That leaves:. ```; import org.apache.avro.SchemaBuilder; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.{GenericDatumWriter, GenericRecord, GenericRecordBuilder}; import org.apache.commons.io.IOUtils; import org.apache.commons.math3.distribution.ChiSquaredDistribution; import org.apache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spark.storage.StorageLevel; import org.apache.{hadoop => hd}; import org.sparkproject.guava.util.concurrent.MoreExecutors; ```. I could try to carefully figure out which one of `shadow` are needed for tests and which are not, but that seems annoying. Fix [here](https://github.com/hail-is/hail/pull/13740).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:5052,concurren,concurrent,5052,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,2,['concurren'],['concurrent']
Performance,apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9552,Load,LoadVCF,9552,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:20114,concurren,concurrent,20114,https://hail.is,https://github.com/hail-is/hail/issues/8106,2,['concurren'],['concurrent']
Performance,"application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:1624,perform,performance,1624,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,4,['perform'],['performance']
Performance,apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:96223,Optimiz,Optimize,96223,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:110661,Optimiz,Optimize,110661,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:75044,Optimiz,Optimize,75044,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:52118,Optimiz,Optimize,52118,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:60646,Optimiz,Optimize,60646,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:43590,Optimiz,Optimize,43590,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6908,Optimiz,OptimizePass,6908,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['OptimizePass']
Performance,apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBacken,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:189286,Optimiz,OptimizePass,189286,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBacken,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:182261,Optimiz,OptimizePass,182261,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBacken,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:15996,Optimiz,OptimizePass,15996,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBacken,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:9012,Optimiz,OptimizePass,9012,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBacken,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4721,Optimiz,OptimizePass,4721,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"ar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1608,concurren,concurrent,1608,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,"are also facing similar error. * java version: `OpenJDK 64-Bit Server VM, 1.8.0_242`; * scala version: `2.12.10`; * py4j: `0.10.9`; * Python: `3.7.10`. <details>; <summary>Stacktrace</summary>. ```; Py4JJavaError: An error occurred while calling o126.exists.; : java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException; 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2532); 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2497); 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593); 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.exists(FS.scala:183); 	at is.hail.io.fs.FS.exists$(FS.scala:181); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(Gate",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:1078,Cache,Cache,1078,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['Cache'],['Cache']
Performance,"are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:2526,load,loads,2526,https://hail.is,https://github.com/hail-is/hail/pull/11939,2,['load'],['loads']
Performance,"ark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 20 times, most recent failure: Lost task 3.19 in stage 8.0 (TID 54368) (leo-test-w-8.australia-southeast1-a.c.ourdna-browser.internal executor 14): java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(Contex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:4093,load,load,4093,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,ark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2491,Load,LoadVCF,2491,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,"array""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:4852,load,loadLength,4852,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadLength']
Performance,"artition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4483,concurren,concurrent,4483,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,"ary>; <p><em>Sourced from <a href=""https://github.com/googleapis/google-auth-library-python/releases"">google-auth's releases</a>.</em></p>; <blockquote>; <h2>v2.6.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2>v2.5.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h2>v2.4.1</h2>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2>v2.4.0</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c99350455d0f7fd3aab950ac47b43000c73dd312"">c993504</a>)</li>; <li>add api key support (<a href=""https://github-redirect.dependabot.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:1138,load,load,1138,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"ase-3.1.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/1e68ba86177504bb6404288610608b855eab93fa""><code>1e68ba8</code></a> release version 3.1.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/8efee35092404ba67ede8316566be4f430e7b61d""><code>8efee35</code></a> pre-commit updates latest release branch</li>; <li><a href=""https://github.com/pallets/jinja/commit/a24df26d54fa2ccbe9bdaa0bb9419075a00e2699""><code>a24df26</code></a> ignore new mypy finding</li>; <li><a href=""https://github.com/pallets/jinja/commit/9faee281ea75694e28c33e2878879b322359d411""><code>9faee28</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/jinja/commit/b802b5a6ad9deea082c16d9adb6417eda1a184d8""><code>b802b5a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1655"">#1655</a> from dvitek/dvitek/issue1654</li>; <li><a href=""https://github.com/pallets/jinja/commit/746bb95780c17687b27b6d1bf4df1216f0da972c""><code>746bb95</code></a> Fix race conditions in FileSystemBytecodeCache</li>; <li><a href=""https://github.com/pallets/jinja/commit/466a200ea40642b674db77588d13889abbad55f5""><code>466a200</code></a> update requirements</li>; <li><a href=""https://github.com/pallets/jinja/commit/990602f719b4086540287e95f601baefd830d790""><code>990602f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1647"">#1647</a> from Tom-Brouwer/202204/add-missing-overlay-options</li>; <li><a href=""https://github.com/pallets/jinja/commit/5d3d2414710c1439105d84efc58e4aba8e453cb3""><code>5d3d241</code></a> fix flake8-bugbear finding</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/3.0.3...3.1.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=3.0.3&new-version=3.1.2)](https://docs.github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12173:5795,race condition,race conditions,5795,https://hail.is,https://github.com/hail-is/hail/pull/12173,1,['race condition'],['race conditions']
Performance,"aseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;))); ```. # Pervasiveness. There is absolutely nothing about this bug that is whole-stage-codegen-specific, but I suspect the much larger single IRs compiled in whole stage code generation made it exponentially more likely for this corner case to occur. I imagine it would be possible to construct a failing pipeline with whole stage code generation turned off. # Testing. This is super hard to reproduce using small/public examples, and any unit tests to capture this *crazy edge case* are pretty much meaningless. John suggested we programmatically check the TypeInfo inference against some JVM reference, and I agree that's the best bet, but don't want to block this critical fix on that project. I fixed BALOAD for the same reason",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:6292,Load,LoadX,6292,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,ask 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1840); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1833); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1833); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8239:2715,concurren,concurrent,2715,https://hail.is,https://github.com/hail-is/hail/issues/8239,2,['concurren'],['concurrent']
Performance,"ask.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:915); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:12175,Load,LoadVCF,12175,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['Load'],['LoadVCF']
Performance,ass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 34.195ms self 6.525ms children 27.670ms %children 80.92%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 8.774ms self 0.014ms children 8.760ms %children 99.84%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:117338,Optimiz,OptimizePass,117338,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"aster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesn’t work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation · Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O · GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesn’t replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:1900,Perform,Performance,1900,https://hail.is,https://github.com/hail-is/hail/issues/5111,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"async def _read_output(self, ir: Optional[BaseIR], output_uri: str) -> bytes:; assert self._batch; ; try:; driver_output = await self._async_fs.open(output_uri); except FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: RuntimeException: Stream is already closed.; E ; E Java stack trace:; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:13705,concurren,concurrent,13705,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PCString; ```. Notice in particular:; ```; AS_VQSLOD=.,.;AS_YNG=.,.; ```; These fields are array fields containing missing values. By default, Hail errors when parsing these due to the inherent ambiguity of a single dot: is it a missing array or an array with one, missing, element. The error message should suggest that the user try using array_elements_required. The docs for `import_vcf` should provide enough information for the user to understand what this does. We should also consider making this the default. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:2431,concurren,concurrent,2431,https://hail.is,https://github.com/hail-is/hail/issues/13346,2,['concurren'],['concurrent']
Performance,at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAllKeepFirstError$3(package.scala:1054); E 	at is.hail.backend.local.LocalBackend.paralleli,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4204,concurren,concurrent,4204,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); 	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:237); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:235); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:235); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:9937,concurren,concurrent,9937,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,2,['concurren'],['concurrent']
Performance,at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.Mat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5611,Load,LoadPlink,5611,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,at scala.collection.AbstractTraversable.map(Traversable.scala:108); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty.scala:463); at is.hail.expr.ir.Pretty.$anonfun$sexprStyle$4(Pretty.scala:453); at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230); at is.hail.utils.richUtils.RichIterator$$anon$3.next(RichIterator.scala:67); at is.hail.utils.prettyPrint.Doc$.advance$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.fo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:2753,Optimiz,Optimize,2753,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['Optimize']
Performance,"ated (698.0K blocks / 410.0K chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:19.984 : INFO: RegionPool: REPORT_THRESHOLD: 2.0M allocated (1.0M blocks / 1010.0K chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:24.240 : INFO: RegionPool: REPORT_THRESHOLD: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:24.240 GoogleStorageFS$: INFO: createNoCompression: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:24.305 GoogleStorageFS$: INFO: close: gs://aou_tmp/tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:3582,cache,cache,3582,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['cache'],['cache']
Performance,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:2188,load,load,2188,https://hail.is,https://github.com/hail-is/hail/pull/9120,3,['load'],['load']
Performance,ationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.241ms self 0.241ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.267ms self 0.267ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:114202,Optimiz,OptimizePass,114202,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.295ms self 0.295ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.033ms self 0.033ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.250ms self 0.250ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:99764,Optimiz,OptimizePass,99764,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.348ms self 0.348ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.298ms self 0.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:78585,Optimiz,OptimizePass,78585,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.409ms self 0.409ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.398ms self 0.398ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:64187,Optimiz,OptimizePass,64187,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.450ms self 0.450ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.396ms self 0.396ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:55659,Optimiz,OptimizePass,55659,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.607ms self 0.607ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.447ms self 0.447ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:47131,Optimiz,OptimizePass,47131,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"ative (not AnyIO) cancellation with just the right timing, leaving the; next acquiring task waiting forever (<code>[#398](https://github.com/agronholm/anyio/issues/398) &lt;https://github.com/agronholm/anyio/issues/398&gt;</code>_)</li>; <li>Added workaround for bpo-46313_ to enable compatibility with OpenSSL 3.0</li>; </ul>; <p>.. _bpo-46313: <a href=""https://bugs.python.org/issue46313"">https://bugs.python.org/issue46313</a></p>; <p><strong>3.4.0</strong></p>; <ul>; <li>; <p>Added context propagation to/from worker threads in <code>to_thread.run_sync()</code>,; <code>from_thread.run()</code> and <code>from_thread.run_sync()</code>; (<code>[#363](https://github.com/agronholm/anyio/issues/363) &lt;https://github.com/agronholm/anyio/issues/363&gt;</code>_; partially based on a PR by Sebastián; Ramírez)</p>; <p><strong>NOTE</strong>: Requires Python 3.7 to work properly on asyncio!</p>; </li>; <li>; <p>Fixed race condition in <code>Lock</code> and <code>Semaphore</code> classes when a task waiting on <code>acquire()</code>; is cancelled while another task is waiting to acquire the same primitive; (<code>[#387](https://github.com/agronholm/anyio/issues/387) &lt;https://github.com/agronholm/anyio/issues/387&gt;</code>_)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/agronholm/anyio/commit/787cb0c2e53c2a3307873d202fbd49dc5eac4e96""><code>787cb0c</code></a> Pinned trio to &lt; 0.22 on AnyIO 3.x</li>; <li><a href=""https://github.com/agronholm/anyio/commit/7cc3cf8cdb58f3b6df6c4fe21e9daa11537c1844""><code>7cc3cf8</code></a> Updated pre-commit modules</li>; <li>See full diff in <a href=""https://github.com/agronholm/anyio/compare/3.6.1...3.6.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=anyio&package-manager=pip&previous-version=3.6.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12362:2859,race condition,race condition,2859,https://hail.is,https://github.com/hail-is/hail/pull/12362,1,['race condition'],['race condition']
Performance,"ator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.31-6060f9c971cc; Error summary: HailException: Hail only supports 8-bit probabilities, found 16. How can I solve it? Or why is it happening?. Thank you very much!. Kind regards,; Catarina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:19620,concurren,concurrent,19620,https://hail.is,https://github.com/hail-is/hail/issues/8545,2,['concurren'],['concurrent']
Performance,automated performance testing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/19:10,perform,performance,10,https://hail.is,https://github.com/hail-is/hail/issues/19,1,['perform'],['performance']
Performance,"ava:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21880,concurren,concurrent,21880,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,ava:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73); 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:962); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:14166,concurren,concurrent,14166,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,"ava:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraCl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:18640,load,load,18640,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['load']
Performance,ava:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5530,Load,LoadPlink,5530,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,ava:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:6722,concurren,concurrent,6722,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,ava:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:11918,concurren,concurrent,11918,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['concurren'],['concurrent']
Performance,"avro) to permit the latest version.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/apache/avro/releases"">avro's releases</a>.</em></p>; <blockquote>; <h2>Apache Avro 1.11.0</h2>; <p>The Apache Avro community is pleased to announce the release of Avro 1.11.0!</p>; <p>All signed release artifacts, signatures and verification instructions can; be found here: <a href=""https://avro.apache.org/releases.html"">https://avro.apache.org/releases.html</a></p>; <p>This release includes 120 Jira issues, including some interesting features:</p>; <p>Specification: AVRO-3212 Support documentation tags for FIXED types; C#: AVRO-2961 Support dotnet framework 5.0; C#: AVRO-3225 Prevent memory errors when deserializing untrusted data; C++: AVRO-2923 Logical type corrections; Java: AVRO-2863 Support Avro core on android; Javascript: AVRO-3131 Drop support for node.js 10; Perl: AVRO-3190 Fix error when reading from EOF; Python: AVRO-2906 Improved performance validating deep record data; Python: AVRO-2914 Drop Python 2 support; Python: AVRO-3004 Drop Python 3.5 support; Ruby: AVRO-3108 Drop Ruby 2.5 support</p>; <p>For the first time, the 1.11.0 release includes experimental support for; Rust. Work is continuing on this donated SDK, but we have not versioned and; published official artifacts for this release.</p>; <p>Python: The avro package fully supports Python 3. We will no longer publish a; separate avro-python3 package</p>; <p>And of course upgraded dependencies to latest versions, CVE fixes and more:; <a href=""https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0"">https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0</a></p>; <p>The link to all fixed JIRA issues and a brief summary can be found at:; <a href=""https://github.com/apache/avro/releases/tag/release-1.11.0"">https://github.com/apache/avro/releases/tag/release-1.11.0</a></p>; <p>In addition, language-specific release ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11475:1046,perform,performance,1046,https://hail.is,https://github.com/hail-is/hail/pull/11475,1,['perform'],['performance']
Performance,"b.com/ai/nanoid/blob/main/CHANGELOG.md"">nanoid's changelog</a>.</em></p>; <blockquote>; <h1>Change Log</h1>; <p>This project adheres to <a href=""http://semver.org/"">Semantic Versioning</a>.</p>; <h2>3.2</h2>; <ul>; <li>Added <code>--size</code> and <code>--alphabet</code> arguments to binary (by Vitaly Baev).</li>; </ul>; <h2>3.1.32</h2>; <ul>; <li>Reduced <code>async</code> exports size (by Artyom Arutyunyan).</li>; <li>Moved from Jest to uvu (by Vitaly Baev).</li>; </ul>; <h2>3.1.31</h2>; <ul>; <li>Fixed collision vulnerability on object in <code>size</code> (by Artyom Arutyunyan).</li>; </ul>; <h2>3.1.30</h2>; <ul>; <li>Reduced size for project with <code>brotli</code> compression (by Anton Khlynovskiy).</li>; </ul>; <h2>3.1.29</h2>; <ul>; <li>Reduced npm package size.</li>; </ul>; <h2>3.1.28</h2>; <ul>; <li>Reduced npm package size.</li>; </ul>; <h2>3.1.27</h2>; <ul>; <li>Cleaned <code>dependencies</code> from development tools.</li>; </ul>; <h2>3.1.26</h2>; <ul>; <li>Improved performance (by Eitan Har-Shoshanim).</li>; <li>Reduced npm package size.</li>; </ul>; <h2>3.1.25</h2>; <ul>; <li>Fixed <code>browserify</code> support.</li>; </ul>; <h2>3.1.24</h2>; <ul>; <li>Fixed <code>browserify</code> support (by Artur Paikin).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ai/nanoid/commit/23b136929a6d58f32e31b269534a3ce3f680a086""><code>23b1369</code></a> Release 3.2 version</li>; <li><a href=""https://github.com/ai/nanoid/commit/967788efce880960512f969a56f8f22f3fc20bae""><code>967788e</code></a> Remove TS test tools</li>; <li><a href=""https://github.com/ai/nanoid/commit/27eaa90cd207a7782bbcf17343092ae87dd62164""><code>27eaa90</code></a> Simplify new binary tool</li>; <li><a href=""https://github.com/ai/nanoid/commit/a9d91239931dc77506381874826d297aee71d6ef""><code>a9d9123</code></a> Update dependencies</li>; <li><a href=""https://github.com/ai/nanoid/commit/32b9bdaab1fbc28576b17de8516164ce0360f292""><code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11284:1147,perform,performance,1147,https://hail.is,https://github.com/hail-is/hail/pull/11284,2,['perform'],['performance']
Performance,"b.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c""><code>52c8ef9</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/83b20f0b4d32b2ff1183a9c2926afd37f3baf92b""><code>83b20f0</code></a> chore: update user creds for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/963"">#963</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/3c9feff3e9037a15bf07496623e3a810f117adcf""><code>3c9feff</code></a> chore(main): release 2.5.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/960"">#960</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a""><code>a8eb4c8</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/87706fd9561aeb651ef551f3576f236a73fad27a""><code>87706fd</code></a> chore: update user cred for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/957"">#957</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/5a09454703bd004d23355a6f660ec8579597d981""><code>5a09454</code></a> chore(main): release 2.4.1 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/955"">#955</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e""><code>c8b5cae</code></a> fix: urllib3 import (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:11252,load,load,11252,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"b_metadata/issues/300"">#300</a>: Removed compatibility shims for deprecated entry; point interfaces.</li>; </ul>; <h1>v4.13.0</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/396"">#396</a>: Added compatibility for <code>PathDistributions</code> originating; from Python 3.8 and 3.9.</li>; </ul>; <h1>v4.12.0</h1>; <ul>; <li>py-93259: Now raise <code>ValueError</code> when <code>None</code> or an empty; string are passed to <code>Distribution.from_name</code> (and other; callers).</li>; </ul>; <h1>v4.11.4</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/379"">#379</a>: In <code>PathDistribution._name_from_stem</code>, avoid including; parts of the extension in the result.</li>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/381"">#381</a>: In <code>PathDistribution._normalized_name</code>, ensure names; loaded from the stem of the filename are also normalized, ensuring; duplicate entry points by packages varying only by non-normalized; name are hidden.</li>; </ul>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12391:1518,load,loaded,1518,https://hail.is,https://github.com/hail-is/hail/pull/12391,1,['load'],['loaded']
Performance,"back (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purpos",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1410,concurren,concurrent,1410,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.092ms self 0.092ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:175520,Optimiz,OptimizePass,175520,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.201ms self 0.201ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:32203,Optimiz,OptimizePass,32203,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAllKeepFirstError$3(package.scala:1054); E 	at is.hail.backend.local.LocalBackend.parallelizeAndComputeWithIndex(LocalBackend.scala:146); E 	at is.hail.backend.BackendUtils.collectDArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4299,concurren,concurrent,4299,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,"backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25350,concurren,concurrent,25350,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"basically, it now looks at the partitioner for the MatrixValues of its children when it executes, and only shuffles if rvds are non-disjoint. It removes empty rvds and separates children into non-disjoint piles, shuffles those independently, and then concatenates them together in sorted order. (I'm not sure how this behaves relative to the old behavior (coerce everything together) when we have multiple shuffles that need to happen, but at least filtering out empty RVDs helps a lot in e.g. the split-multi case where none of the variants need to be moved.). I think the (a?) better optimization would be to add smarter partitioning so that we're adjusting partition bounds and not really shuffling under most circumstances, but I thought I'd PR this first since at least there's a warning in python about unioning non-disjoint datasets (where this will avoid the scan). I also added tests for MatrixUnionRows and pulled out one of the simplify rules; TableUnion does an unsorted union (I'm not sure this is actually correct behavior now if the tables have keys) and pulling the union outside of a MatrixRowsTable can cause the ordering to be wrong.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4043:586,optimiz,optimization,586,https://hail.is,https://github.com/hail-is/hail/pull/4043,1,['optimiz'],['optimization']
Performance,"been ""migrated"". If it hasn't then we insert new rows into the ""v3"" table with the deduped resource ID as the key. The exact trigger is . ```sql; DROP TRIGGER IF EXISTS aggregated_job_resources_v2_after_update $$; CREATE TRIGGER aggregated_job_resources_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this audit in chunks here as these tables are massive and a single audit query would take hours. The bounds of the audit for these chunks are on the order of `(batch_id, job_id)` rather than `(batch_id, job_id, resource_id)` which was used for the actual updates. This is because the resource_ids can di",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:2494,optimiz,optimized,2494,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"bility and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2578,perform,performance,2578,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,4,['perform'],['performance']
Performance,"bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>; <li><a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12809:2515,load,loads,2515,https://hail.is,https://github.com/hail-is/hail/pull/12809,2,['load'],['loads']
Performance,boost concurrent builds to 4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6040:6,concurren,concurrent,6,https://hail.is,https://github.com/hail-is/hail/pull/6040,1,['concurren'],['concurrent']
Performance,"borted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:3465,Load,LoadVCF,3465,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,"bot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:1709,cache,cache-semantics,1709,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['cache'],['cache-semantics']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:53515,concurren,concurrent,53515,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:76569,concurren,concurrent,76569,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:127113,concurren,concurrent,127113,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:84303,concurren,concurrent,84303,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:92037,concurren,concurrent,92037,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:61986,concurren,concurrent,61986,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:69130,concurren,concurrent,69130,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:109581,concurren,concurrent,109581,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:117020,concurren,concurrent,117020,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:99477,concurren,concurrent,99477,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:137931,concurren,concurrent,137931,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:191244,concurren,concurrent,191244,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:170389,concurren,concurrent,170389,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:159840,concurren,concurrent,159840,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:146710,concurren,concurrent,146710,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:181888,concurren,concurrent,181888,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"bump, this is in my queue right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4551#issuecomment-433526513:20,queue,queue,20,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-433526513,1,['queue'],['queue']
Performance,"but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:2099,load,load,2099,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['load'],['load']
Performance,"cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2098,cache,cached,2098,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,cala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:4374,concurren,concurrent,4374,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['concurren'],['concurrent']
Performance,"cala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1965,Load,LoadPlink,1965,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,cala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5456,Optimiz,Optimize,5456,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Optimiz'],['Optimize']
Performance,"can replicate:; ```; /Users/tpoterba/data/variants_out.txt', min_partitions=1000); 2019-03-26 09:13:47 Hail: INFO: Reading table with no type imputation; Loading column '19:60864:GCAGCCTCAGCACT:G' as type 'str' (type not specified). In [3]: ht.count(); [Stage 0:> (0 + 12) / 12]Out[3]: 848893. In [4]: ht.n_partitions(); Out[4]: 12; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5603#issuecomment-476625941:154,Load,Loading,154,https://hail.is,https://github.com/hail-is/hail/issues/5603#issuecomment-476625941,1,['Load'],['Loading']
Performance,can we merge this after #6969 goes in? That resolves a pretty big performance regression in stats/mean/corr/hist aggregators.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-526604943:66,perform,performance,66,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-526604943,1,['perform'],['performance']
Performance,"cate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:5041,load,loadElement,5041,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadElement']
Performance,"cc @mkveerapen @tpoterba . 1) Adds natural language documentation search bar with autocompletion; 2) Adds paginated search page to browse all search results; 3) Makes navbar scripts async, to allow inclusion of navbar scripts at top of file without impacting performance (since we include the navbar.html content in other templates we cannot easily locate those scripts at bottom of the includers' pages); 4) Increases hero content width on mobile.; 5) Hides sphinx search bar. http://34.207.246.132/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8955:259,perform,performance,259,https://hail.is,https://github.com/hail-is/hail/pull/8955,1,['perform'],['performance']
Performance,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:356,perform,performance,356,https://hail.is,https://github.com/hail-is/hail/issues/7988,2,['perform'],"['perform', 'performance']"
Performance,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2821:209,load,loads,209,https://hail.is,https://github.com/hail-is/hail/pull/2821,2,"['load', 'perform']","['loads', 'performs']"
Performance,"cc: @cseed @tpoterba. docker does not pull an image specified in --cache-from, so we need to; explicitly pull it before trying to use it as a cache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4356:67,cache,cache-from,67,https://hail.is,https://github.com/hail-is/hail/pull/4356,2,['cache'],"['cache', 'cache-from']"
Performance,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5016:53,cache,caches,53,https://hail.is,https://github.com/hail-is/hail/pull/5016,8,['cache'],"['cache', 'cache-from', 'caches']"
Performance,"cc: @daniel-goldstein, this is a tricky asyncio situation which you should also keep in mind. OK, there were two problems:. 1. A timeout of 5s appears to be now too short for Google Cloud Storage. I am not sure why but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:809,Queue,Queue,809,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['Queue'],['Queue']
Performance,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5421:398,perform,performing,398,https://hail.is,https://github.com/hail-is/hail/pull/5421,1,['perform'],['performing']
Performance,cc: @tpoterba. There's a Sphinx extension that uses Python type annotations to figure out; parameter types so we don't have to write it twice. I implement that here. I; wrestled for a while with the cyclical import structure until I realized that. ```; from . import module; ```. side steps a lot of the cyclical issues (pylint still dislikes it). I also added `sphinx.ext.intersphinx` which makes every reference to a Python; standard library thing work correctly. Now when I write; `concurrent.futures.ThreadPoolExecutor` it links right to the Python docs; page. It's really quite nice.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9198:485,concurren,concurrent,485,https://hail.is,https://github.com/hail-is/hail/pull/9198,1,['concurren'],['concurrent']
Performance,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8045:376,Load,LoadBalancer,376,https://hail.is,https://github.com/hail-is/hail/pull/8045,6,"['Load', 'load']","['LoadBalancer', 'loadBalancerIP', 'loadBalancers', 'loadbalancing']"
Performance,"ccessfully, the job is marked as scheduled.; 4. Once all requests complete, goto 1. On the worker, what happens inside `/api/v1alpha/batches/jobs/create`:; 1. Read metadata describing the job to schedule from the request body; 2. Using that information, load the full job spec from blob storage; 3. Spawn a task to run the job asynchronously; 4. Respond to the driver with a 200. The key point relevant to this issue is that the driver currently must wait for all the requests to workers in an iteration to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:1498,concurren,concurrent,1498,https://hail.is,https://github.com/hail-is/hail/issues/14456,1,['concurren'],['concurrent']
Performance,ce$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.pa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:3325,Optimiz,OptimizePass,3325,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['OptimizePass']
Performance,ce.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7825,concurren,concurrent,7825,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,ce.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142); E 	at is.hail.relocated.org.apache.htt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7539,concurren,concurrent,7539,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:7901,concurren,concurrent,7901,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['concurren'],['concurrent']
Performance,ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:5348,concurren,concurrent,5348,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:11371,concurren,concurrent,11371,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['concurren'],['concurrent']
Performance,cessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:11873,load,load,11873,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,"cessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). ```. I instead tried to run the same code in two separate jupyter notebooks, with the same code inside but different ways to initialize the hailcontext, one like this (works and exports):. ```; from hail import *; hc = HailContext(); ```; With startup messages looking like this:. ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 13:51:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 13:51:03 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 13:51:03 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN Utils: Your hostname, <my computer name> resolves to a l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:5924,load,load,5924,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['load'],['load']
Performance,"cestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:21171,Cache,CacheDir,21171,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Cache'],['CacheDir']
Performance,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:2820,load,loading,2820,https://hail.is,https://github.com/hail-is/hail/pull/3727,3,['load'],['loading']
Performance,"ch to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; meth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1909,load,loads,1909,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['loads']
Performance,chClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2584) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2917,concurren,concurrent,2917,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,ched contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35391,cache,cached,35391,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"ched gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2371,cache,cached,2371,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,ched oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-c,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38251,cache,cached,38251,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"ched python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2279,cache,cached,2279,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"ched requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2453,cache,cached,2453,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.140ms self 0.140ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.app,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:171533,Optimiz,OptimizePass,171533,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.187ms self 0.187ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.031ms self 0.031ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.app,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:28216,Optimiz,OptimizePass,28216,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183777,Optimiz,Optimize,183777,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190802,Optimiz,Optimize,190802,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:17512,Optimiz,Optimize,17512,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:10528,Optimiz,Optimize,10528,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,children 0.736ms %children 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.280ms self 0.280ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.140ms self 0.140ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:170425,Optimiz,OptimizePass,170425,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,children 1.252ms %children 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.383ms self 0.383ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.187ms self 0.187ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:27108,Optimiz,OptimizePass,27108,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readInt(InputBuffers.scala:409) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at __C16collect_distributed_array_table_coerce_sortedness.__m20INPLACE_DECODE_r_int32_TO_r_int32(Unknown Source) ~[?:?]; 	at __C16collect_dis",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:3099,concurren,concurrent,3099,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['concurren'],['concurrent']
Performance,"ck:; E Failed: Timeout >600.0s. /usr/lib/python3.9/concurrent/futures/thread.py:162: Failed; ---------------------------- Captured log teardown -----------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 163, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); OSError: [Errno 39] Directory not empty: '/tmp/JnQ2m'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 409, in rmtree; await rm_dir(pool, contents_tasks_by_dir.get(path, []), path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 387, in rm_dir; excs = [exc; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 389, in <listcomp>; for exc in [t.exception()]; File ""/usr/lib/python3.9/asyncio/futures.py"", line 214, in exception; raise exc; asyncio.exceptions.CancelledError; ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:13078,concurren,concurrent,13078,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,ckage.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_matrix_ir$2(SparkBackend.scala:689); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:69); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.annotations.RegionPool$.scoped(Reg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2700,Load,LoadPlink,2700,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"ckages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:1552,concurren,concurrent,1552,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"ckend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 0 total 4.078ms self 4.078ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 0 total 0.898ms self 0.898ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0 total 2.553ms self 0.011ms children 2.543ms %children 99.59%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 2.543ms self 2.543ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 0 total 8.675ms self 8.675ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0 total 4.506ms self 3.993ms children 0.513ms %children 11.40%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.513ms self 0.513ms children 0.000ms %children 0.00%; t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:3213,Optimiz,Optimize,3213,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"ckend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 1 total 0.414ms self 0.414ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 1 total 0.021ms self 0.021ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1 total 0.384ms self 0.005ms children 0.379ms %children 98.72%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 1 total 0.103ms self 0.103ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1 total 0.579ms self 0.273ms children 0.306ms %children 52.84%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.306ms self 0.306ms children 0.000ms %children 0.00%; t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:6356,Optimiz,Optimize,6356,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,ckend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99); 	at is.hail.relocated.com.google.cloud.storage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5529,concurren,concurrent,5529,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance,"ckend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2194/482268176.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCachea",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3490,concurren,concurrent,3490,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,"ckend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21795,concurren,concurrent,21795,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,ckend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73); 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:962); 	at java.io.Buf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:14081,concurren,concurrent,14081,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,ckend.scala:458); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7618,concurren,concurrent,7618,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,ckend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7332,concurren,concurrent,7332,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,"ckend.spark.SparkBackend#execute/is.hail.backend.spark.SparkBackend#_jvmLowerAndExecute total 0.053ms self 0.053ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply total 1.211s self 27.866ms children 1.183s %children 97.70%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR total 37.247ms self 0.358ms children 36.889ms %children 99.04%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.268ms self 0.268ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform total 36.599ms self 2.385ms children 34.214ms %children 93.48%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 0 total 4.078ms self 4.078ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 0 total 0.898ms self 0.898ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0 total 2.553ms self 0.011ms children 2.543ms %children 99.59%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:2119,Optimiz,Optimize,2119,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,ckend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:218343,Optimiz,Optimize,218343,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ckend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:210322,Optimiz,Optimize,210322,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ckend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:202370,Optimiz,Optimize,202370,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ckend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:197718,Optimiz,Optimize,197718,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"ckendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.120ms self 0.120ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 0 total 0.796ms self 0.796ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.111ms self 0.111ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 0 total 9.943ms self 9.943ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 1 total 0.414ms self 0.414ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 1 total 0.021ms self 0.021ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1 total 0.384ms self 0.005ms children 0.379ms %children 98.72%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:5235,Optimiz,Optimize,5235,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"ckendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 1 total 0.025ms self 0.025ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.151ms self 0.151ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 1 total 0.714ms self 0.714ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.124ms self 0.124ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable total 4.039ms self 0.016ms children 4.023ms %children 99.60%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Verify total 0.012ms self 0.012ms children 0.00",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:8378,Optimiz,Optimize,8378,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,cketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$paral,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7682,concurren,concurrent,7682,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,cketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75); E 	at is.hail.relocated.org.apache.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7396,concurren,concurrent,7396,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,"ckquote>; <h2><!-- raw HTML omitted -->2.9.13 (2022-06-27)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: /@fs/ dir traversal with escaped chars (fixes <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8498"">#8498</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8805"">#8805</a>) (<a href=""https://github.com/vitejs/vite/commit/e109d64"">e109d64</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8498"">#8498</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8805"">#8805</a></li>; <li>fix(wasm): support decoding data URL in Node &lt; v16 (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8668"">#8668</a>) (<a href=""https://github.com/vitejs/vite/commit/1afc1c2"">1afc1c2</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8668"">#8668</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.12 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>) (<a href=""https://github.com/vitejs/vite/commit/c0d6c60"">c0d6c60</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8534"">#8534</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.11 (2022-06-10)<!-- raw HTML omitted --></h2>; <ul>; <li>fix: respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>) (<a href=""https://github.com/vitejs/vite/commit/ab7dc1c"">ab7dc1c</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8481"">#8481</a></li>; <li>fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>) (<a href=""https://github.com/vitejs/vite/commit/01fa807"">01fa807",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:2357,optimiz,optimized,2357,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['optimiz'],['optimized']
Performance,closed until I can take another look at how broadcasts are used in the code cache.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10969#issuecomment-1032801893:76,cache,cache,76,https://hail.is,https://github.com/hail-is/hail/pull/10969#issuecomment-1032801893,1,['cache'],['cache']
Performance,"closing while I compare performance to filter(rows, cols) in new branch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2628#issuecomment-354845912:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/pull/2628#issuecomment-354845912,1,['perform'],['performance']
Performance,"code>1442e64</code></a> chore(main): release 2.6.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/965"">#965</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06""><code>f9f23f4</code></a> fix: revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/3c72365d8407bb097568919123cd7232c1a49f4f""><code>3c72365</code></a> chore: update user cred for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/966"">#966</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c""><code>52c8ef9</code></a> feat: ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/83b20f0b4d32b2ff1183a9c2926afd37f3baf92b""><code>83b20f0</code></a> chore: update user creds for system test (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/963"">#963</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/3c9feff3e9037a15bf07496623e3a810f117adcf""><code>3c9feff</code></a> chore(main): release 2.5.0 (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/960"">#960</a>)</li>; <li><a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a""><code>a8eb4c8</code></a> feat: ADC can load an impersonated service account credentials. (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:10343,load,load,10343,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,coerceOrCopy for one. ptype.loadCheapPCode sometimes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10488#issuecomment-844407350:28,load,loadCheapPCode,28,https://hail.is,https://github.com/hail-is/hail/pull/10488#issuecomment-844407350,1,['load'],['loadCheapPCode']
Performance,"collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:10774,load,loadClass,10774,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:251); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:7347,Cache,Cache,7347,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['Cache'],['Cache']
Performance,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:2933,concurren,concurrent,2933,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"com/googleapis/google-auth-library-python/issues/904"">#904</a>) (<a href=""https://www.github.com/googleapis/google-auth-library-python/commit/bd0ccc5fe77d55f7a19f5278d6b60587c393ee3c"">bd0ccc5</a>)</li>; </ul>; <h2>v2.3.2</h2>; <h3>Bug Fixes</h3>; <ul>; <li>add clock_skew_in_seconds to verify_token functions (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/894"">#894</a>) (<a href=""https://www.github.com/googleapis/google-auth-library-python/commit/8e95c1e458793593972b6b05a355aaeaecd31670"">8e95c1e</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/google-auth-library-python/blob/main/CHANGELOG.md"">google-auth's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.5.0...v2.6.0"">2.6.0</a> (2022-01-31)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.4.1...v2.5.0"">2.5.0</a> (2022-01-25)</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:5322,load,load,5322,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.Method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:3707,Cache,Cache,3707,https://hail.is,https://github.com/hail-is/hail/issues/10530,1,['Cache'],['Cache']
Performance,"conda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2649,Load,LoadVCF,2649,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,configure apache webserver to allow navbar to be loaded from github,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1024:49,load,loaded,49,https://hail.is,https://github.com/hail-is/hail/issues/1024,1,['load'],['loaded']
Performance,"consider:; ```ht.annotate(x=5)```. Here's the IR:. ```; (TableMapRows (idx) 1; (TableRange 5 5); (Let __uid_1; (InsertFields; (SelectFields (); (Ref Struct{idx:Int32} row)); (x; (I32 5))); (InsertFields; (MakeStruct; (idx; (GetField idx; (Ref Struct{idx:Int32} row)))); (x; (GetField x; (Ref Struct{x:Int32} __uid_1)))))); ```. Cotton added some optimizer rules, but they're not always sufficient. We need to fix this",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4001:346,optimiz,optimizer,346,https://hail.is,https://github.com/hail-is/hail/issues/4001,1,['optimiz'],['optimizer']
Performance,"cool, I'll proably rewrite the tests using that. the option for 1 already exists in `LoadMatrix.apply`; I just haven't exposed it to HailContext because there's a lot of stuff there already and I couldn't figure out what the name of the flag should be (currently, it's hasRowKeyLabel but I don't feel like that's super descriptive) but I could do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2440#issuecomment-345032622:85,Load,LoadMatrix,85,https://hail.is,https://github.com/hail-is/hail/pull/2440#issuecomment-345032622,1,['Load'],['LoadMatrix']
Performance,cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-no,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38555,cache,cached,38555,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"ct(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stag",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:5069,load,loads,5069,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['load'],['loads']
Performance,ct.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.Met,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3220,concurren,concurrent,3220,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,ct.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216435,concurren,concurrent,216435,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ct.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217857,concurren,concurrent,217857,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ct/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:2322,load,load,2322,https://hail.is,https://github.com/hail-is/hail/pull/4931,2,"['load', 'perform']","['load', 'performance']"
Performance,"cting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6941,cache,cached,6941,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"curl -XPOST localhost:5000/jobs/create -H 'Content-Type: application/json' -d '{ ; ""spec"" : {; ""containers"" : [; { ""name"" : ""foobarbaz""; , ""image"" : ""alpine:3.8""; , ""command"": [""/bin/sh"", ""-c"", ""echo hi""] }] } }'; ```; 3. ; ```; curl localhost:5000/jobs; ```; 4. the job never transitions to Complete and the server log shows:; ```; (hail-batch) # make run; BATCH_USE_KUBE_CONFIG=1 python -c 'import batch.server; batch.server.serve()'; INFO	| 2018-11-13 18:12:19,124 	| server.py 	| <module>:25 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-11-13 18:12:19,125 	| server.py 	| <module>:28 | instance_id = 63aeb0cd4fa840a9864cfd909ce7f682; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target kube_event_loop; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target polling_event_loop; INFO	| 2018-11-13 18:12:19,131 	| server.py 	| run_forever:391 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; INFO	| 2018-11-13 18:12:19,168 	| _internal.py 	| _log:88 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); INFO	| 2018-11-13 18:12:20,141 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:12:20,159 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:17:20,179 	| _internal.py 	| _log:88 | ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4773:1118,load,loading,1118,https://hail.is,https://github.com/hail-is/hail/issues/4773,1,['load'],['loading']
Performance,"cussion_r207422997>:; >>; >> > +}; >> +; >> +std::string strip_suffix(const std::string& s, const char* suffix) {; >> + size_t len = s.length();; >> + size_t n = strlen(suffix);; >> + if ((n > len) || (strncmp(&s[len-n], suffix, n) != 0)) return s;; >> + return std::string(s, 0, len-n);; >> +}; >> +; >> +std::string get_cxx_name() {; >> + char* p = ::getenv(""CXX"");; >> + if (p) return std::string(p);; >> + // We prefer clang because it has faster compile; >> + auto s = run_shell_get_first_line(""which clang"");; >> + if (strstr(s.c_str(), ""clang"")) return s;; >> + s = run_shell_get_first_line(""which g++"");; >>; >> I'm lazy and I want to save my thinking for stuff where I can really; >> provide value (like building a whole stage code generator, or designing a; >> new format to speed up the decoder!) If there are standards, I want to use; >> them so I don't have to think unless I have a strong reason to think they; >> won't work for me. It also makes communicating with others easier who are; >> expecting the same standards. So I'd just kick out a Makefile and call make; >> on it like I would for any project (and maybe print out some helpful info; >> when it fails like the make command, the path to the Makefile and maybe the; >> environment). I didn't realize it would be such a hard sell because I; >> thought I was proposing the easy path which was the least work. I like the; >> easy path! And if it means we can get this in and start building on it, all; >> the better.; >>; >> The cached Makefile isn't actually reused. ... It's occasionally useful; >> to run it by hand for debugging.; >>; >> Ah, thanks for clarifying. Definitely.; >>; >> —; >> You are receiving this because you modified the open/close state.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/hail-is/hail/pull/3973#discussion_r207422997>, or mute; >> the thread; >> <https://github.com/notifications/unsubscribe-auth/AJzExsJ8Hk-GuvDywoKPy8DF_SSBqgG-ks5uM66RgaJpZM4VbZpP>; >> .; >>; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:4246,cache,cached,4246,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,1,['cache'],['cached']
Performance,"cute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms self 0.367ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:12539,Optimiz,Optimize,12539,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,cute/is.hail.expr.ir.analyses.SemanticHash.apply/is.hail.expr.ir.NormalizeNames.apply total 25.511ms self 25.511ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.backend.spark.SparkBackend#_jvmLowerAndExecute total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply total 1m41.8s self 1m40.6s children 1.127s %children 1.11%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 1.106s self 33.060ms children 1.072s %children 97.01%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms ch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:2090,Optimiz,OptimizePass,2090,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,cutor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:146626,concurren,concurrent,146626,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"cutor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13244,Load,LoadVCF,13244,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,cutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216515,concurren,concurrent,216515,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,cutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(Bat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217937,concurren,concurrent,217937,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"d support for reading DX10 BC4 DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7603"">#7603</a> [<a href=""https://github.com/sambvfx""><code>@​sambvfx</code></a>]</li>; <li>Optimized ImageStat.Stat.count <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7599"">#7599</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Moved error from truetype() to FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7587"">#7587</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Correct PDF palette size when saving <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7555"">#7555</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed closing file pointer with olefile 0.47 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7594"">#7594</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>ruff: Minor optimizations of list comprehensions, x in set, etc. <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7524"">#7524</a> [<a href=""https://github.com/cclauss""><code>@​cclauss</code></a>]</li>; <li>Build Windows wheels using cibuildwheel <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7580"">#7580</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Raise ValueError when TrueType font size is zero or less <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7584"">#7584</a> [<a href=""https://github.com/akx""><code>@​akx</code></a>]</li>; <li>Install cibuildwheel from requirements file <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7581"">#7581</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:8813,optimiz,optimizations,8813,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['optimiz'],['optimizations']
Performance,"d#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms self 0.367ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.095ms self 0.095ms ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:12829,Optimiz,Optimize,12829,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"d. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_UR",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1726,load,loaded,1726,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,4,['load'],"['load', 'loaded']"
Performance,"d.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 0 total 8.675ms self 8.675ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0 total 4.506ms self 3.993ms children 0.513ms %children 11.40%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.513ms self 0.513ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.120ms self 0.120ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 0 total 0.796ms self 0.796ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.111ms self 0.111ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 0 total 9.943ms self 9.943ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:4388,Optimiz,Optimize,4388,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"d.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 1 total 0.103ms self 0.103ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1 total 0.579ms self 0.273ms children 0.306ms %children 52.84%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.306ms self 0.306ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 1 total 0.025ms self 0.025ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.151ms self 0.151ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 1 total 0.714ms self 0.714ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:7531,Optimiz,Optimize,7531,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,d.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:8560,load,loadField,8560,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadField']
Performance,d.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cma,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12940,Load,LoadMatrixParser,12940,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrixParser']
Performance,d.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196900,Optimiz,OptimizePass,196900,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,d.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209504,Optimiz,OptimizePass,209504,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,d.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217525,Optimiz,OptimizePass,217525,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,d.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201552,Optimiz,OptimizePass,201552,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"d6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1830,cache,cache-dir,1830,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,1,['cache'],['cache-dir']
Performance,"d</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5 - 2021-12-05</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/aee8a9fed45f84d227cf2cb7102656aa65a4890a""><code>aee8a9f</code></a> 3.6.7</li>; <li><a href=""https://github.com/ijl/orjson/commit/622cd7b1167262ffe458f6a2c15ec239f015d174""><code>622cd7b</code></a> Add special casing for deserializing empty objects, lists and strings</li>; <li><a href=""https://github.com/ijl/orjson/commit/5da14a00fed93",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:1790,perform,performance,1790,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['perform'],['performance']
Performance,"dAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:11670,Load,LoadVCF,11670,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,dConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5707,Optimiz,Optimize,5707,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,dEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:2992,Optimiz,OptimizePass,2992,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,dLeftJoinDistinct$1.apply(KeyedRVD.scala:152); E 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:149); E 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$24.apply(ContextRDD.scala:316); E 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$24.apply(ContextRDD.scala:316); E 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$10$$anonfun$apply$11.apply(ContextRDD.scala:218); E 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$10$$anonfun$apply$11.apply(ContextRDD.scala:218); E 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); E 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); E 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); E 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); E 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); E 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); E 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); E 	at is.hail.rvd.RVD$$anonfun$34.apply(RVD.scala:1220); E 	at is.hail.rvd.RVD$$anonfun$34.apply(RVD.scala:1219); E 	at is.hail.sparkextras.ContextRDD$$anonfun$crunJobWithIndex$1.apply(ContextRDD.scala:242); E 	at is.hail.sparkextras.ContextRDD$$anonfun$crunJobWithIndex$1.apply(ContextRDD.scala:240); E 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); E 	at org.apache.spark.scheduler.Task.run(Task.scala:121); E 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); E 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); E 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9867:9087,concurren,concurrent,9087,https://hail.is,https://github.com/hail-is/hail/issues/9867,2,['concurren'],['concurrent']
Performance,dRVD.scala:1014); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:3252,concurren,concurrent,3252,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['concurren'],['concurrent']
Performance,"d` an `OrderedRVD`, possibly with some non-empty key. This is consistent with the rule that the `rvd` must always have a stronger/longer key than the `TableType`.; * **small tweaks** - Now I start working through the `TableIR` nodes, rewriting them to remove explicit uses of `UnpartitionedRVD`. The general plan is to sandwich the rvd logic between `toOrderedRVD` and `toOldStyleRVD`. The first takes an `UnpartitionedRVD` to an `OrderedRVD` with empty key (and leaves `OrderedRVD`s alone), and the second takes an `OrderedRVD` to an `UnpartitionedRVD` if its key was empty, and leaves it alone otherwise. Once they're all rewritten this way, I redefine `toOldStyleRVD` to always return `OrderedRVD`, and `UnpartitionedRVD` is no longer used.; * **remove `TableUnkey`** - With `UnpartitionedRVD` going away, `TableUnkey` is no longer necessary, it's equivalent to keying by an empty key.; * **small tweaks** - these next two rewrite more `TableIR` nodes; * **Merge master** - the big one; * **tweak MatrixColsTable** - 1) Optimize `coerce` by checking if the requested key is empty, avoiding a scan in that case. 2) Optimize `sortedColsValue` by checking if the column key is empty, avoiding the sort in that case. 3) Simplify `colsRVD`, removing the case on the type of the `RVD`, just calling `coerce` and letting the previous optimizations avoid unnecessary work.; * **`distinctByKey` fix** - While looking over `TableIR` implementations, I noticed a bug in `distinctByKey`: you need to be sure no key is split across multiple partitions. To be sure the empty key edge case still works, I added a test to check that `strictify` on an empty-key partitioner will always collapse everything to one partition.; * **Flipped switch** - redifines `toOldStyleRVD` to just return the `OrderedRVD` unchanged, and asserts that `TableValue.rvd` is always an `OrderedRVD`.; * **rest of the `TableIR` tweaks** - added a factory method `OrderedRVD.unkeyed` to replace `UnpartitionedRVD.apply`.; * the rest are si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4319:1833,Optimiz,Optimize,1833,https://hail.is,https://github.com/hail-is/hail/pull/4319,1,['Optimiz'],['Optimize']
Performance,"d`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).write('/tmp/out.da', overwrite=True); ```. Block matrix performed the matrix multiply in 19.3s. DNDArray performed the; matrix multiply in 37.6s. Block Matrix:; ![Screen Shot 2020-05-26 at 1 37 51 PM](https://user-images.githubusercontent.com/106194/82932367-54630200-9f56-11ea-86f4-94726c36d727.png). DNDArray:; ![Screen Shot 2020-05-26 at 1 37 08 PM](https://user-images.githubusercontent.com/106194/829323",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:1984,Perform,Performance,1984,https://hail.is,https://github.com/hail-is/hail/pull/8864,1,['Perform'],['Performance']
Performance,"damentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalDict. A PCanonicalArray-backed implementation of PDict. # <a name=""parray""></a> PNDArray. An abstract class for multidimensional arrays (tensors) that have a row-major or column-major layout. ## Core Methods. ```scala; val shape: StaticallyKnownField[PTuple, Long]; val strides: StaticallyKnownField[PTuple, Long]; ```. - Defines the tensor shape. ```scala; def loadElementToIRIntermediate(indices: Array[Code[Long]], ndAddress: Code[Long], mb: MethodBuilder): Code[_]; ```. - Load the element's primitive representation, as indexed by `indices`, which specifies the element index at every dimension in the PNDArray's shape. ```scala; def linearizeIndicesRowMajor(indices: Array[Code[Long]], shapeArray: Array[Code[Long]], mb: MethodBuilder): Code[Long]; ```. - Get the off-heap index of the element (since NDArray elements are stored as a 1D series of bytes off-heap). ```scala; def unlinearizeIndexRowMajor(index: Code[Long], shapeArray: Array[Code[Long]], mb: MethodBuilder): (Code[Unit], Array[Code[Long]]); ```. - Generate the index path that represents the virtual, shape-dependent index into an arbitrary tensor. ```scala; def copyRowMajorToColumnMajor(rowMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:6612,load,loadElementToIRIntermediate,6612,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadElementToIRIntermediate']
Performance,dd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:3783,concurren,concurrent,3783,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['concurren'],['concurrent']
Performance,"ddition of a `save_path` and `vds_paths`. The `save_path` is used as; a filesystem or object storage location to save the state of the; combiner so that it may be resumed. The vds paths implement the long; awaited mixed hail and gvcf inputs, that was never completed for the VCF; combiner. End users should not call `VariantDatasetCombiner.__init__`; instead preferring the free function new_combiner. The; `VariantDatasetCombiner` API is fairly simple:. * @property finished; * save: serialize the current state to save_path; * step: does the next unit of work for the combiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added to the end of the list and then sorted. Since python's sort; function is stable, this will produce a stable output for a given set of; inputs and batch size. In order to preserve the gvcf sample ordering; from the paths, we work from the start of the gvcf list. Once all the gvcfs are combined, we combine the variant datasets in; reverse order (smallest number of samples to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10892:1383,load,load,1383,https://hail.is,https://github.com/hail-is/hail/pull/10892,1,['load'],['load']
Performance,"de></a> Prepare next development iteration</li>; <li><a href=""https://github.com/java-native-access/jna/commit/0d7499f105e4495bdea15fc21f5b1046e81ca822""><code>0d7499f</code></a> Release 5.12.0</li>; <li><a href=""https://github.com/java-native-access/jna/commit/fa86166d4f75ef4478de7ad9d7d6c0b6b6933ee0""><code>fa86166</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1445"">#1445</a> from matthiasblaesing/aix</li>; <li><a href=""https://github.com/java-native-access/jna/commit/4cca4405f7f6bc32d2a08495efb81c081b065279""><code>4cca440</code></a> Fix name mapping difference between AIX JDK 8 and Semeru JDK 18</li>; <li><a href=""https://github.com/java-native-access/jna/commit/f58b0f8f6b5c013adfe44a2cfb018ccb6ef6a688""><code>f58b0f8</code></a> Improve test stability on AIX (exclude tests that are expected to fail)</li>; <li><a href=""https://github.com/java-native-access/jna/commit/c1565fb89469cbcba67b1cc305e16d520779b270""><code>c1565fb</code></a> Handle race condition in PdhUtil#PdhEnumObjectItems (<a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1442"">#1442</a>)</li>; <li><a href=""https://github.com/java-native-access/jna/commit/99fcfa822db86b1f2ba5823dbf17efeb3d246ad5""><code>99fcfa8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1444"">#1444</a> from matthiasblaesing/update_libffi</li>; <li><a href=""https://github.com/java-native-access/jna/commit/9e473350a5ad5e04aab8b01e4018f973976e19f8""><code>9e47335</code></a> Update CHANGES.md</li>; <li>Additional commits viewable in <a href=""https://github.com/java-native-access/jna/compare/5.6.0...5.12.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=net.java.dev.jna:jna&package-manager=gradle&previous-version=5.6.0&new-version=5.12.1)](https://docs.github.com/en/github/ma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:7127,race condition,race condition,7127,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['race condition'],['race condition']
Performance,"de></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/720"">jpadilla/pyjwt#720</a></li>; <li>api_jwk: Add PyJWKSet.<strong>getitem</strong> by <a href=""https://github.com/woodruffw""><code>@​woodruffw</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/725"">jpadilla/pyjwt#725</a></li>; <li>Update usage.rst by <a href=""https://github.com/guneybilen""><code>@​guneybilen</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/727"">jpadilla/pyjwt#727</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/728"">jpadilla/pyjwt#728</a></li>; <li>fix: Update copyright information by <a href=""https://github.com/kkirsche""><code>@​kkirsche</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/729"">jpadilla/pyjwt#729</a></li>; <li>Docs: mention performance reasons for reusing RSAPrivateKey when encoding by <a href=""https://github.com/dmahr1""><code>@​dmahr1</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/734"">jpadilla/pyjwt#734</a></li>; <li>Fixed typo in usage.rst by <a href=""https://github.com/israelabraham""><code>@​israelabraham</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/738"">jpadilla/pyjwt#738</a></li>; <li>Add detached payload support for JWS encoding and decoding by <a href=""https://github.com/fviard""><code>@​fviard</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/723"">jpadilla/pyjwt#723</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/740"">jpadilla/pyjwt#740</a></li>; <li>Raise DeprecationWarning for jwt.decode(verify=...) by <a href=""https://github.com/akx""><code>@​akx</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11866:3669,perform,performance,3669,https://hail.is,https://github.com/hail-is/hail/pull/11866,1,['perform'],['performance']
Performance,"ded_gather`, allows it to be used recursively. In particular, suppose we had a semaphore of; 50. The outer `bounded_gather2` might need 20 slots to run its 20 paths in parallel. That leaves 30 slots of parallelism left over for its children. By passing the semaphore down, we let our children optimistically use some of that excess parallelism. 2. If we happen to have the `StatResult` for a particular object, we should never again look it up. In particular, getting the `StatResult` for every file in a directory can be done in O(1) requests. Getting the `StatResult` for each of those files individually (using their full paths) is necessarily O(N). If there was at least one glob and also there are no `suffix_components`, then we can use the `StatResult`s that we learned when checking the glog pattern. The latter point is perhaps a bit more clear with examples:. 1. `gs://foo/bar/baz`. Since there are no globs, we can make exactly one API request to list `gs://foo/bar/baz`. 2. `gs://foo/b*r/baz`. In this case, we must make one API request to list `gs://foo/`. This gives us a list of paths under that prefix. We check each path for conformance to the glob pattern `gs://foo/b*r`. For any path that matches, we must then list `<the matching path>/baz` which may itself be a directory containing files. Overall we make O(1) API requests to do the glob and then O(K) API requests to get the final `StatResult`s, where K is the number of paths matching the glob pattern. 3. `gs://foo/bar/b*z`. In this case, we must make one API request to list `gs://foo/bar/`. In `main`, we then throw away the `StatResult`s we got from that API request! Now we have to make O(K) requests to recover those `StatResult`s for all K paths that match the glob pattern. This PR just caches the `StatResult`s of the most recent globbing. If there is no suffix to later append, then we can just re-use the `StatResult`s we already have!. cc: @daniel-goldstein since you've reviewed this before. Might be of interest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13253:2059,cache,caches,2059,https://hail.is,https://github.com/hail-is/hail/pull/13253,1,['cache'],['caches']
Performance,default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4638,Load,LoadVCF,4638,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11451,Load,LoadVCF,11451,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Sm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1222,load,loading,1222,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['loading']
Performance,der.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2136:1394,Load,LoadVCF,1394,https://hail.is,https://github.com/hail-is/hail/issues/2136,2,['Load'],['LoadVCF']
Performance,deredRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.Co,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:8496,load,loadAddress,8496,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadAddress']
Performance,"difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bott",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1394,perform,performance,1394,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,1,['perform'],['performance']
Performance,"different than any physical type arguments passed to it. PLocus. - Concrete implementations (canonical/non). PCall. - Concrete implementations (canonical/non). PInterval. - Concrete implementations (canonical/non). ## Primitive Types. While long-term these may have canonical and non-canonical types, that is outside the scope of this design document. When non-canonical primitives are introduced they will follow the strucutre outlined for non-primitive types. PFloat32. - Represents a 4 byte float. PFloat64. - Represents an 8 byte float. PInt32. - Represents a 4 byte integer. PInt64. - Represents an 8 byte integer. PVoid. <br/>. # Common methods. ```scala; def constructAtAddress(mb: MethodBuilder, addr: Code[Long], region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Unit]; def constructAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:3396,Perform,Performs,3396,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['Perform'],['Performs']
Performance,ding control to the QoB Job.; 2024-11-05 02:43:37.206 ServiceBackendAPI$: INFO: BatchClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJson,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2834,concurren,concurrent,2834,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,"ding. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but canno",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1027,optimiz,optimized,1027,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,1,['optimiz'],['optimized']
Performance,dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$3(Worker.scala:135) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$2(Worker.scala:134) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:7276,concurren,concurrent,7276,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['concurren'],['concurrent']
Performance,"dler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 2.543ms self 2.543ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 0 total 8.675ms self 8.675ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0 total 4.506ms self 3.993ms children 0.513ms %children 11.40%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.513ms self 0.513ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.120ms self 0.120ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 0 total 0.796ms self 0.796ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.111ms self 0.111ms children 0.000ms %children 0.00%; timing is.hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:4076,Optimiz,Optimize,4076,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"dler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 1 total 0.103ms self 0.103ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1 total 0.579ms self 0.273ms children 0.306ms %children 52.84%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.306ms self 0.306ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 1 total 0.025ms self 0.025ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.151ms self 0.151ms children 0.000ms %children 0.00%; timing is.hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:7219,Optimiz,Optimize,7219,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2432:349,load,loadings,349,https://hail.is,https://github.com/hail-is/hail/pull/2432,2,['load'],['loadings']
Performance,don't check if agg states are loaded,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6791:30,load,loaded,30,https://hail.is,https://github.com/hail-is/hail/pull/6791,1,['load'],['loaded']
Performance,dpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215720,concurren,concurrent,215720,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"dre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(Compil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:2497,load,loads,2497,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['load'],['loads']
Performance,dren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:216333,Optimiz,OptimizePass,216333,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,dren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:208312,Optimiz,OptimizePass,208312,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,dren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:200360,Optimiz,OptimizePass,200360,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,dren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:195708,Optimiz,OptimizePass,195708,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,dren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compile,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199230,Optimiz,OptimizePass,199230,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,dropSamples plays well with optimizer,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2285:28,optimiz,optimizer,28,https://hail.is,https://github.com/hail-is/hail/pull/2285,1,['optimiz'],['optimizer']
Performance,"duler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6002,Load,LoadVCF,6002,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,dx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4368,concurren,concurrent,4368,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"e -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: IN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1433,Optimiz,Optimize,1433,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"e -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1572,Optimiz,Optimize,1572,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"e = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; m1 = g.sum(axis=1).cache(); m2 = (g**2).sum(axis=1).cache(). mean = m1 / n; stdev = ((m2-m1**2 / n) / (n-1)).sqrt(); g_std = ((g - mean) / stdev). g_std.write('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT_standardized.autosomes.bm', overwrite=True). I'll try this way of normalizing tomorrow to see if this is the root of the error and post back. Tagging @jbloom22 and @tpoterba 'cause why not :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6351:2020,cache,cache,2020,https://hail.is,https://github.com/hail-is/hail/issues/6351,2,['cache'],['cache']
Performance,e I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; IFNE L10; GOTO L11; L11; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 9; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 16; LDC 9999; ILOAD 16; ISUB; ISTORE 17; GOTO L12; L12; FRAME APPEND [T I]; ALOAD 0; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; LLOAD 14; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; ILOAD 17; INVOKEVIRTUAL __C46CompiledWithAggs.__m69take_by_seqop (ZJZI)V; RETURN; L10; FRAME CHOP 2; LDC 0; ISTORE 17; GOTO L12; L7; FRAME CHOP 3; LDC 0; LSTORE 14; GOTO L9; L13; LOCALVARIABLE get_tup_elem_o J L0 L13 6; LOCALVARIABLE bool Z L0 L13 8; LOCALVARIABLE get_tup_elem_o J L0 L13 9; LOCALVARIABLE bool Z L0 L13 11; LOCALVARIABLE invoke I L0 L13 12; LOCALVARIABLE local13 I L0 L13 13; LOCALVARIABLE mux J L0 L13 14; LOCALVARIABLE invoke I L0 L13 16; LOCALVARIABLE mux I L0 L13 17; MAXSTACK = 6; MAXLOCALS = 18; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:2203,load,loadInt,2203,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408,1,['load'],['loadInt']
Performance,"e a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to think about it some more. > It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, then remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time. Ah, good point!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:3149,race condition,race condition,3149,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,1,['race condition'],['race condition']
Performance,"e as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete imp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:976,load,loadAddress,976,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadAddress']
Performance,"e cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's ow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1335,Load,LoadSelfFirstURLClassLoader,1335,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['Load'],['LoadSelfFirstURLClassLoader']
Performance,"e copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the source, so stat'ing the sources and destinations are all overlapping. This avoids dependencies where I have to e.g. stat the input, decide what to do, and then perform a second action. I approached testing two ways: First, hand test common operations and errors (copy file, copy dir, overwrite, overwrite dir with file and vice versa, the various treat_dest_as settings, large files, detecting copy-and-files on input on Google Storage, etc.) Second, I enumerated essentially all single input transfers and recorded the results. These are then tested on every pair of file systems: file to file, file to gs, gs to file, etc. To make this run reasonably, I parallelize using xdist and the full tests (1364 tests) now take about 2 minutes. (If you restrict to the just the local filesystem, the tests just take a couple seconds.) The second test verifies that all cases run without unexpected errors, and that the behavior is file system independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:2157,perform,perform,2157,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['perform'],['perform']
Performance,"e fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was only ever used as unique name for the JAR. Instead, I just use the full JAR URL as a unique name for the JAR. If you need to defeat the cache, just create a new git commit before running `make -C query ipython`. If defeating the cache becomes a common problem, we can add a ""reload_jar"" parameter or similar to the job spec. Third, I renamed `push-jar` in `query/Makefile` to `upload-query-jar` to mirror the build.yaml step. Fourth, I embraced the use of `NAMEPSACE` in `query/Makefile` instead of relying on the minor hack that our laptop usernames match our namespace names. This does mean you need to always specify NAMESPACE when uploading a jar. Finally, a pleasant outcome of this change is the elimination of a bunch of conditional build.yaml logic in the service backend tests!. I think this will simplify the use of Hail Query by Australia et al. because I've isolated the use of hail-specific data to `query/Makefile`. If there's a way to access the relevant global-config variables from `query/Makefile`, I can also fix the `query/Makefile` to be deployment-independent. cc: @lgruen @illusional @tpoterba . [1] For our default namespace deployment, `gs://hail-query/jars/{GIT_REVISION}.jar`. In general, `{HAIL_QUERY_STORAGE_URI}{HAIL_QUERY_ACCEPTABLE_JAR_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:2365,cache,cache,2365,https://hail.is,https://github.com/hail-is/hail/pull/11645,1,['cache'],['cache']
Performance,"e non-empty). I also found all the function currying and comingling of fitting and testing really confusing. To be fair, the Scala code does this (and its really confusing). I think the current structure is easier to follow:. 1. Fit the null model.; 2. If wald, assume the beta for the genotypes is zero and use the rest of the parameters from the null model fit to compute the score (i.e. the gradient of the likelihood). Recall calculus: gradient near zero => value near the maximum. Return: this is the test.; 3. Otherwise, fit the full model starting at the null fit parameters.; 4. Test the ""goodness"" of this new & full fit. ---. Poisson regression is similar but with a different likelihood function and gradient thereof. Notice that I `key_cols_by()` to indicate to Hail that the order of the cols is irrelevant (the result is a locus-keyed table after all). This is necessary at least until #12753 merges. I think it's generally a good idea though: it indicates to Hail that the ordering of the columns is irrelevant, which is potentially useful information for the optimizer!. ---. Both logistic and Poisson regression can benefit from BLAS3 by running at least the score test for multiple variants at once. ---. I'll attach an image in the comments, but I spend ~6 seconds compiling this trivial model and ~140ms testing it. ```python3; import hail as hl; mt = hl.utils.range_matrix_table(1, 3); mt = mt.annotate_entries(x=hl.literal([1, 3, 10, 5])); ht = hl.poisson_regression_rows(; 'wald', y=hl.literal([0, 1, 1, 0])[mt.col_idx], x=mt.x[mt.col_idx], covariates=[1], max_iterations=2); ht.collect(); ```. I grabbed some [sample code from; scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html) for Poisson regression (doing a score test rather than a wald test) and timed it. It takes ~8ms. So we're 3 orders of magnitude including the compiler, and ~1.2 orders of magnitude off without the compiler. Digging in a bit:; - ~65ms for cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12793:1903,optimiz,optimizer,1903,https://hail.is,https://github.com/hail-is/hail/pull/12793,1,['optimiz'],['optimizer']
Performance,"e reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## W",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2272,CACHE,CACHE,2272,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['CACHE'],['CACHE']
Performance,"e that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/overview.html` redirect to `docs/0.1/overview.html`. Yes, the regex for rules two matches `/hail`, `/hail/`, `/hail/overview.html`, etc. and redirects, replacing the hail with `docs/0.1`. One last note: I used 301 Permanent Redirect for the `/hail` since that's a dead url. I used 307 Temporary Redirect for `/docs` since that will change when versions change. Here's a test interaction. I interleaved my c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4929:1513,perform,performed,1513,https://hail.is,https://github.com/hail-is/hail/pull/4929,1,['perform'],['performed']
Performance,"e to exceed `width`; * print `body` normally, as described in the previous paragraph, allowing nested `Group`s to print either flat or normally. This pretty-printer DSL has become fairly standard, with some common enhancements that I don't think we need. It was first described in [A prettier printer](https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf) by Wadler (though my implementation is completely different). This achieves stack safety by `Concat` taking an `Iterable`, so each contained `Doc` can be produced on demand. `render` pulls from these iterators, keeping in memory only things that might print to the current line, but where the format hasn't been decided yet. As soon as the formatting of a group is decided, as much of its body as possible is written to the `java.io.Writer`. A very quick and dirty performance comparison had the new pretty printer about 20% slower. That's paying for both the stack safety and the added smarts. And I think there's still room for optimization if it becomes necessary. Here is a snippet of the IR generated by `test_ld_score_regression`, first on master, then this PR:; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z); (Ref row)); None; (chi_squared; (Apply pow () Float64; (GetField Z; (Ref row)); (ApplyIR toFloat64 () Float64; (I32 2)))); (n; (GetField N; (Ref row))); (ld_score; (GetField L2; (GetField __uid_3; (Ref row)))); (locus; (Apply Locus () Locus(GRCh37); (GetField CHR; (GetField __uid_4; (Ref row))); (GetField BP; (GetField __uid_5; (Ref row))))); (alleles; (MakeArray Array[String]; (GetField A2; (Ref row)); (GetField A1; (Ref row)))); (phenotype; (Str ""50_irnt""))))); (InsertFields; (SelectFields (locus alleles chi_squared n ld_score phenotype); (SelectFields (SNP A1 A2 N Z chi_squared n ld_score locus alleles phenotype); (Ref row))); None)); ```; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z) (Ref row)); None; (chi_squared; (Apply pow () Float64; (GetField Z (Ref row)); (ApplyIR toFloat64 () Float64 (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9652:3721,optimiz,optimization,3721,https://hail.is,https://github.com/hail-is/hail/pull/9652,1,['optimiz'],['optimization']
Performance,e#apply/is.hail.expr.ir.TypeCheck.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.839ms self 0.009ms children 0.830ms %children 98.97%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.809ms self 0.073ms children 0.736ms %children 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:168849,Optimiz,OptimizePass,168849,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,e#apply/is.hail.expr.ir.TypeCheck.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.367ms self 0.010ms children 1.356ms %children 99.24%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.029ms self 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.303ms self 0.051ms children 1.252ms %children 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:25532,Optimiz,OptimizePass,25532,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,e-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached googl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34398,cache,cached,34398,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"e-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3962,cache,cached,3962,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"e-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5036,cache,cached,5036,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,e.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:3878,Cache,Cache,3878,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Cache'],['Cache']
Performance,"e.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateextern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:7058,concurren,concurrent,7058,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['concurren'],['concurrent']
Performance,e.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.126-ee77707f4fab; Error summary: HailException: cannot set missing field for required type +PFloat64; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:20672,concurren,concurrent,20672,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['concurren'],['concurrent']
Performance,e.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apach,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218475,concurren,concurrent,218475,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"e/releasenotes/10.2.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/10.2.0.html</a></p>; <h2>Changes</h2>; <ul>; <li>Add <code>keep_rgb</code> option when saving JPEG to prevent conversion of RGB colorspace <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7553"">#7553</a> [<a href=""https://github.com/bgilbert""><code>@​bgilbert</code></a>]</li>; <li>Trim negative glyph offsets in ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7672"">#7672</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Removed unnecessary &quot;pragma: no cover&quot; <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7668"">#7668</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Trim glyph size in ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7669"">#7669</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix loading IPTC images and update test <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7667"">#7667</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Allow uncompressed TIFF images to be saved in chunks <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7650"">#7650</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Concatenate multiple JPEG EXIF markers <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7496"">#7496</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Changed IPTC tile tuple to match other plugins <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7661"">#7661</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Do not assign new fp attribute when exiting context manager <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7566"">#7566</a> [<a href=""https://github.com/radarhere""><code>@​",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:1317,load,loading,1317,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['load'],['loading']
Performance,"e46c2abcb856217d204812ae8bd33f3dfc21e7""><code>6ce46c2</code></a> gcsfs: drop python 3.6 (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/445"">#445</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/fd67c7d1b6ca9db83a0deadd1557470c37b0836a""><code>fd67c7d</code></a> Update changelog, deps (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/443"">#443</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/2dc256b7c0df075d44f8d4a0f3400c1b926166ee""><code>2dc256b</code></a> Make tarball creation more reproducible (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/442"">#442</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/85e2ee2abfa429f3c10cc08dcda8ed30db8ab3b5""><code>85e2ee2</code></a> update deps, changelog (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/438"">#438</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/cbf751e83785d8b7e5d7ea879e5534e32d633ae0""><code>cbf751e</code></a> Don't touch cache on find with prefix (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/437"">#437</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/7b5aee98724c7ca44a73524bf448089ac4b79b75""><code>7b5aee9</code></a> for release (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/435"">#435</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/4de170703d3f245a2e4f5e5c7abcef3ad3ec33c7""><code>4de1707</code></a> fixup references to dask/gcsfs (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/434"">#434</a>)</li>; <li><a href=""https://github.com/fsspec/gcsfs/commit/8f7115216346648bacc57f5910048cab5735b9b3""><code>8f71152</code></a> Add support to additional 'fixed-key-metadata' (<a href=""https://github-redirect.dependabot.com/fsspec/gcsfs/issues/429"">#429</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/fsspec/gcsfs/compare/2021.04.0...2022.02.0"">compare view</a></li>; </ul>; </details",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11575:1617,cache,cache,1617,https://hail.is,https://github.com/hail-is/hail/pull/11575,1,['cache'],['cache']
Performance,e5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json BlockBlob Hot 4453 application/octet-stream 2023-06-09T12:44:22+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:34+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:3731,concurren,concurrent,3731,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520:2343,cache,cache,2343,https://hail.is,https://github.com/hail-is/hail/issues/1520,3,['cache'],['cache']
Performance,e=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.bam.list] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/scatter/temp_0001_of_1200/scattered.intervals] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=75 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false genotype_likelihoods_model=BOTH pcr_error_rate=1.0E-4 computeSLOD=false annotateNDA=false pair_hmm_implementation=ORIGINAL min_base_quality_score=17 max_deletion_fraction=0.05 min_indel_count_for_genotyping=5 min_indel_fraction_per_sample=0.25 indel_heterozygosity=1.25E-4 indelGapContinuationPenalty=10 indelGapOpenPenalty=45 indelHaplotypeSize=80 indelDebug=false ignoreSNPAlleles=false allReadsSP=false ignoreLaneInfo=false reference_sample_calls=(RodBinding name= source=UNBOUND) reference_sample_name=null sample_ploidy=2 min_quality_score=1 max_quality_score=40 site_quality_prior=20 min_power_threshold_for_calling=0.95 min_reference_depth=100 exclude_filtered_reference_sites,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:14403,perform,performanceLog,14403,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performanceLog']
Performance,"e></a> release: v2.9.13</li>; <li><a href=""https://github.com/vitejs/vite/commit/e109d64331d9fa57753832762c3573c3532a6947""><code>e109d64</code></a> fix: backport <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8804"">#8804</a>, /@fs/ dir traversal with escaped chars (fixes <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8498"">#8498</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8"">#8</a>...</li>; <li><a href=""https://github.com/vitejs/vite/commit/1afc1c2370e09998f800f9067491a25e9dd463a0""><code>1afc1c2</code></a> fix(wasm): support decoding data URL in Node &lt; v16 (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8668"">#8668</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/86a55d3cc0668eca79a55f5cf8b6034b9e3bf835""><code>86a55d3</code></a> release: v2.9.12</li>; <li><a href=""https://github.com/vitejs/vite/commit/c0d6c60b45d89e0995a5ea6bf74e9e3c023ae828""><code>c0d6c60</code></a> fix: backport outdated optimized dep removed from module graph (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8534"">#8534</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/078a7dcabc8ffc93a06c84063fba04e0e2157f3b""><code>078a7dc</code></a> release: v2.9.11</li>; <li><a href=""https://github.com/vitejs/vite/commit/01fa8070fab5faa590fbe312d2465897a0e6c6a2""><code>01fa807</code></a> fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ab7dc1c4405ce2814ccc38d5979b51ad2f37d4e6""><code>ab7dc1c</code></a> fix: backport respect server.headers in static middlewares (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8481"">#8481</a>)</li>; <li><a href=""https://github.com/vitejs/vite/commit/ced0374b867db3c01b91027",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:11961,optimiz,optimized,11961,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['optimiz'],['optimized']
Performance,eBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflect,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5381,Optimiz,Optimize,5381,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Optimiz'],['Optimize']
Performance,eChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:23867,concurren,concurrent,23867,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,eContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6047,Optimiz,Optimize,6047,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19700,concurren,concurrent,19700,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,2,['concurren'],['concurrent']
Performance,"e[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField(address: Long, fieldIdx: Int): Long; def storeField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Store field at a given memory address; - (This does not exist yet, but should I believe). ## <a name=""ptuple"">PCanonicalTuple</a>. An immutible, fixed-length collection of ordered values (of possibly different types). Number of elements known statically, and just like PCanonicalStruct, elements are stored inline, rather than behind a pointer to a collection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:9311,load,loadField,9311,https://hail.is,https://github.com/hail-is/hail/issues/7988,3,"['Load', 'load']","['Load', 'loadField']"
Performance,ead.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$an,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11340,Load,LoadVCF,11340,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
