quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`).; ### Minimal code sample (that we can copy&paste without having any data). ```python; analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False); ```. ```pytb; /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide; residuals = diff / np.sqrt(mu + mu**2 / theta); ```; `analytic_pearson[""X""]` ; Output:; ```py; array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,; -0.08709449, -0.16926342],; [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,; -0.09342913, -0.18157157],; [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,; 12.02085262, 6.06603257],; ...,; [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,; -0.03033674, -0.05896328],; [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,; -0.03077326, -0.05981169],; [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,; -0.03157547, -0.06137084]]); ```. ```py; adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]; adata.layers[""analytic_pearson_residuals""].sum(1); ```; Output:; `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2496:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. I have the following anndata; ```; AnnData object with n_obs × n_vars = 28752 × 22603; obs: 'batch', 'cluster1_psc', 'all_fib_types', 'psc_batch_temp', 'batch_combined', 'overall', 'cluster1_kpc', 'kpc_batch_temp', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'; var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'; uns: 'genome'; ```. When I run leiden clustering it shows that the parameters were added. ```computing PCA; on highly variable genes; with n_comps=30; finished (0:01:25); computing neighbors; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:04); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:18); running Leiden clustering; finished: found 23 clusters and added; 'NoBatchCorr', the cluster labels (adata.obs, categorical) (0:00:04); ```; But when I check my anndata, none present. As such if I try to generate a umap image I get the following error; ```; AnnData object with n_obs × n_vars = 28752 × 22603; obs: 'batch', 'cluster1_psc', 'all_fib_types', 'psc_batch_temp', 'batch_combined', 'overall', 'cluster1_kpc', 'kpc_batch_temp', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'Norm_Factor'; var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2330:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2330,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. It is hard to show the error without my data, but here it is what I did. I read in my 10 visium data (output from SpaceRanger 2.0.0) and everything seems fine. All I had to do was to change a file name (tissue_positions.csv to tissue_positions_list.csv) in my `outs/spatial` folder. ```python; healthy_A1 = sc.read_visium('../data/MGI3535_A1_010322NHK/outs/', count_file='raw_feature_bc_matrix.h5'); ```. I calculated qc metrics with this command.; ```python; sc.pp.calculate_qc_metrics(healthy_A1, percent_top=None, log1p=False, inplace=True); ```. But then, the problem occurs when I try to use scanpy.pl.spatial. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pl.spatial(healthy_A1, img_key = ""hires"", color = ""total_counts"") ; ```; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/ipykernel_1314047/3602867448.py in <module>; ----> 1 sc.pl.spatial(healthy_A1, img_key = ""hires"", color = ""ACTA2""). ~/.conda/envs/python_spatial/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs); 1009 show=False,; 1010 save=False,; -> 1011 **kwargs,; 1012 ); 1013 if not isinstance(axs, list):. ~/.conda/envs/python_spatial/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2391:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.; ### Comment; I was just going over the GSE92332 case study. On Section 2, the AnnData objects were created fine, but the concatenation method for AnnData object did not work out for me. I browsed through issues raised, there were issues on how to concatenate objects, but they did not seem to answer my question here. . ### Minimal code sample (that we can copy&paste without having any data). ```python; # Concatenate to main adata object; adata = adata.concatenate(adata_tmp, batch_key='sample_id'); ```. ```pytb; InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; anndata2ri 1.0.4; attr 20.2.0; backcall 0.2.0; cairo 1.19.1; certifi 2020.06.20; cffi 1.14.1; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; gprofiler 1.0.0; h5py 2.10.0; idna 2.10; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.34.0; louvain 0.6.1; markupsafe 1.1.1; matplotlib 3.3.1; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.4; pandas 1.1.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.6.1; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.24.0; r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. From [pl.embedding documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.embedding.html), for `palette`, it states; >If provided, values of adata.uns[""{var}_colors""] will be set. This implies that if not provided, then those values will not be set.; However, they are being set - either without specifying `palette` (default:None), or explicitly passing `palette=None`; Either it's a bug, or the documentation is not clear. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.read_h5ad('my_matrix.h5ad'); adata.uns = {} #confirming *_colors is not set already; sc.pl.embedding(adata, basis='X_umap', palette=None, color=['cell_type']); adata.uns['cell_type_colors']; ```. ```pytb; ['#1f77b4',; '#ff7f0e',; '#279e68',; '#d62728',; '#aa40fc',; '#8c564b',; '#e377c2',; '#b5bd61',; '#17becf']; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311:667,clear,clear,667,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311,1,['clear'],['clear']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello, . This may be a problem outside the realm of scanpy functionality, but I thought it best to bring up in case it is relevant or in case anyone here has seen something before while trying to use scanpy. It looks like I can having trouble importing a dependency of the sc.pp.regress() function. I don't think the data here is relevant, just something in my set up. I tried updating all the libraries so that everything is up to date. This problem just started occurring today (2/10/21) and had no issue yesterday, so I figure it was a change on the scanpy end that I didn't keep up with proprely. ```python; sc.pp.regress_out(merged_adata, ['pct_counts_mt', 'pct_counts_rp']); ```. yields the following error. ```pytb; /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/site-packages/statsmodels/tsa/filters/filtertools.py in <module>; 16 import scipy.fftpack as fft; 17 from scipy import signal; ---> 18 from scipy.signal.signaltools import _centered as trim_centered; 19 ; 20 from statsmodels.tools.validation import array_like, PandasWrapper. ImportError: cannot import name '_centered' from 'scipy.signal.signaltools' (/home/unix/jjeang/.local/lib/python3.8/site-packages/scipy/signal/signaltools.py); ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.8 umap==0.3.10 numpy==1.22.2 scipy==1.8.0 pandas==1.2.5 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.9.9 louvain==0.7.0 leidenalg==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2137:1589,learn,learn,1589,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2137,1,['learn'],['learn']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Since updating to scanpy 1.8 and pandas 1.3 I am getting an error with sc.tl.rank_genes_groups that my reference needs to be one of groupby=[cats], but as you can see in the traceback below, it appears identical. It has something to do with the data types, I've been dealing with this for a few weeks now and on some data sets I am eventually able to figure it out and get it to run, on some I am not. I think this is more of a pandas issue than scanpy, so I am wondering what version of pandas you recommend for anndata 0.7.6? Or if you know a simple workaround to make sure that the datatypes match? This has been a massive headache. I use this list(zip()) syntax in my code frequently, have never had an issue with datatypes inside of a zipped object... so this may be a simple pythonic question, but if there is a different method of accomplishing the same thing thing that doesn't introduce this error I would be happy to hear that as well. In the example below, in my adata.obs I have a column 'condition' that is a categorical variable of biological condition for differential expression, and so all cells belong to either group 0 or 1. I have tried seemingly every combination of having these, and the leiden clusters column be integers, strings, objects, before converting to the categorical dtype in line 7 below, always get the same error, and the reference = item always appears identical to the first item in the groupby = list. ### Minimal code sample (that we can copy&paste without having any data). ```python; cluster_method='leiden'; n_genes=1000; g1n='Control'; adata.obs['condition']=adata.obs['condition'].astype('category'); adata.obs[cluster_method]=adata.obs[cluster_method].astype('category'); pairs = list(zip(adata.obs['condition'], adata.obs[cl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:774,simpl,simple,774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['simpl'],['simple']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; AttributeError: module 'fa2.fa2util' has no attribute 'Node' for scanpy version 1.9.1. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.datasets.paul15(); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20); sc.tl.draw_graph(adata); ```. ```pytb; Traceback (most recent call last):; File ""C:\Users\wubaosheng\AppData\Local\Programs\Python\Python39\lib\site-packages\IPython\core\interactiveshell.py"", line 3398, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-9-e2440b0eca68>"", line 1, in <cell line: 1>; sc.tl.draw_graph(adata); File ""C:\Users\wubaosheng\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\tools\_draw_graph.py"", line 159, in draw_graph; positions = forceatlas2.forceatlas2(; File ""C:\Users\wubaosheng\AppData\Local\Programs\Python\Python39\lib\site-packages\fa2-0.3.5-py3.9-win-amd64.egg\fa2\forceatlas2.py"", line 162, in forceatlas2; nodes, edges = self.init(G, pos); File ""C:\Users\wubaosheng\AppData\Local\Programs\Python\Python39\lib\site-packages\fa2-0.3.5-py3.9-win-amd64.egg\fa2\forceatlas2.py"", line 103, in init; n = fa2util.Node(); AttributeError: module 'fa2.fa2util' has no attribute 'Node'; ```. #### Versions; '1.9.1' for scanpy; 3.9.7 for python. ![image](https://user-images.githubusercontent.com/50618480/176398309-d16c671f-e4bf-43b5-a23d-abf9bdbe2329.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2286:344,guid,guide,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2286,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; I do not know if it is the scanpy to be blamed. But two of my data both face this problem. When I run trajectory analysis upon. sc.tl.dpt(adata); sc.pl.draw_graph(adata, color=['seurat_clusters', 'dpt_pseudotime'], legend_loc='right margin',title = ['','pseudotime']). branches of image for dpt_pseudotime were all deep blue, which indicated close to zero. And. adata.obs['dpt_pseudotime']. also print number(0.07, 0.06, etc) that close to zero.; Therefore, I do not know if there is something wrong with my previous code, or there is something wrong with scanpy. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```; import numpy as np; import pandas as pd; import matplotlib.pyplot as pl; from matplotlib import rcParams; import scanpy as sc; sc.settings.verbosity = 3; sc.logging.print_versions(); adata = sc.read_h5ad(""/home/dell/at scanpy/pbmc3k.h5ad""); adata; adata.X=adata.X.astype('float64'); sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20); sc.tl.draw_graph(adata); adata.obs['seurat_clusters']= adata.obs['seurat_clusters'].astype('category'); sc.pl.draw_graph(adata, color='seurat_clusters', legend_loc='right margin',title = """"); sc.tl.diffmap(adata); sc.pp.neighbors(adata, n_neighbors=10, use_rep='X_diffmap'); sc.tl.draw_graph(adata); sc.pl.draw_graph(adata, color='seurat_clusters', legend_loc='on data',title = """"); sc.tl.paga(adata, groups='seurat_clusters'); sc.pl.paga(adata, color=['seurat_clusters'],title = """"); new_cluster_names = [; 'A', 'B',; 'C', 'D',; 'E', 'F',; G',",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2015:821,guid,guide,821,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2015,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); sc.external.pp.scrublet(adata, threshold='I am ignored'); ```. ```pytb; /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy.; view_to_actual(adata). Automatically set threshold at doublet score = 0.27; Detected doublet rate = 1.5%; Estimated detectable doublet fraction = 44.3%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1644:912,learn,learn,912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644,1,['learn'],['learn']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Data location: ; celltypist_adata can be found here under ""download"": https://www.celltypist.org/#video; hachoen_adata download: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE178341. ### Minimal code sample. ```python; # Your code here; celltypist_adata = sc.read_h5ad('/random-path/celltypist_allcells_raw.h5ad'); celltypist_myeloid_celltypes = ['DC1','DC2','Intermediate macrophages','Alveolar macrophages','Classical monocytes','Erythrophagocytic macrophages','Intestinal macrophages','Mast cells','Nonclassical monocytes','migDC','pDC']; celltypist_myeloid_adata = celltypist_adata[celltypist_adata.obs['cell_type'].isin(celltypist_myeloid_celltypes)]. hacohen_adata = sc.read_h5ad('/random-path/hacohen_allcells_raw.h5ad'); hacohen_myeloid_celltypes = ['AS-DC','DC IL22RA2','DC1','DC2','DC2 C1Q+','Granulocyte','Macrophage-like','Mast','Monocyte','mregDC','pDC']; hacohen_myeloid_adata = hacohen_adata[hacohen_adata.obs['cell_type'].isin(hacohen_myeloid_celltypes)]. hacohen_myeloid_adata.obs['dataset'] = 'CRC_hacohen'; celltypist_myeloid_adata.obs['dataset'] = 'celltypist'; combined_adata = hacohen_myeloid_adata.concatenate(celltypist_myeloid_adata,batch_key='dataset_ind'). ```. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-25-5f4cc5c2e544> in <module>; 1 hacohen_myeloid_adata.obs['dataset'] = 'CRC_hacohen'; 2 celltypist_myeloid_adata.obs['dataset'] = 'celltypist'; ----> 3 combined_adata = hacohen_myeloid_adata.concatenate(celltypist_myeloid_adata,batch_key='datas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2364:256,guid,guide,256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2364,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ### Description. I was installing the dev version of scanpy following the instructions [here](https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#working-with-git) and [here](https://scanpy.readthedocs.io/en/stable/installation.html#development-version). However two tests : `test_paga_plots[master_paga_continuous_obs-func2]` and `test_pca_chunked` seem to fail on master. . ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```bash; git clone https://github.com/scverse/scanpy.git; cd scanpy; git submodule update --init --recursive; conda create --name scanpy-dev python=3.8; conda activate scanpy-dev; pip install -e '.[dev,doc,test]'; pytest; ```. ```pytb; =========================================================================================== FAILURES ============================================================================================; _______________________________________________________________________ test_paga_plots[master_paga_continuous_obs-func2] _______________________________________________________________________. image_comparer = <function image_comparer.<locals>.make_comparer at 0x7fbbf032dc10>; pbmc = AnnData object with n_obs × n_vars = 700 × 765; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score...roups', 'paga', 'bulk_labels_sizes'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'; test_id = 'master_paga_continuous_obs', func = functools.partial(<function paga at 0x7fc009276e50>, color='cool_feature'). @needs_igraph; @pytest.mark.parametrize(; ""test_id,fun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2459:645,guid,guide,645,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2459,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ------it is easy for v1.9.1 out of memory.; I have the same data(200000x20000), the RAM of my compute is 64 GB, and the v1.8 can cope it easy, but the v1.9.1 is easy to occur the memoryError from the sc.read('./xxx.h5ad'). **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); sc.read('./xxx.h5ad'); ```python; # Your code here; ```. ```pytb; [Paste the error output produced by the above code here]; ```. #### Versions; v1.9.1; <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>; ![image](https://user-images.githubusercontent.com/50618480/170998448-db4300d9-54f0-4f03-b2be-7f6880006b29.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2266:475,guid,guide,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2266,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; sc.tl.rank_genes_groups(adata, ""origin"", method=""wilcoxon""); ```. ```pytb; [Paste the error output produced by the above code here]; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Input In [18], in <cell line: 1>(); ----> 1 sc.tl.rank_genes_groups(adata, ""origin"", method=""wilcoxon""); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). File ~/app/miniconda3/envs/bio/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds); 580 adata.uns[key_added] = {}; 581 adata.uns[key_added]['params'] = dict(; 582 groupby=groupby,; 583 reference=reference,; (...); 587 corr_method=corr_method,; 588 ); --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts); 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':; 593 logg.warning(; 594 ""It seems you use rank_genes_groups on the raw count data. ""; 595 ""Please logarithmize your data before calling rank_genes_groups.""; 596 ). File ~/app/miniconda3/envs/bio/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts); 82 def __init__(; 83 self,; 84 adata,; (...); 90 comp_pts=False,; 91 ):; ---> 93 if 'log1p' in adata.uns_keys() and adata.uns",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # update to h5py e.g. as: pip install h5py==3.0.0; import scanpy as sc. adata = sc.datasets.blobs(); sc.read(""foo.h5ad"").obs_names # names are bytes, not str; # Index([ b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', ..., dtype='object', length=640); ```; When using `h5py==2.10.0`, it works as expected (i.e. the index type is str). The same happens for `.var_names` (I haven't check further). I'd recommend updating the requirements.txt oto `h5py<=2.10.0` for now. Related h5py issue: https://github.com/h5py/h5py/issues/1732. Also, I've encoutered this bug when coming up with example (seems unrelated):; ```python; import scanpy as sc. adata = sc.datasets.paul15(). sc.read('data/paul15/paul15.h5'); ```; The last line raises:. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-24-3d2f3a02bf09> in <module>; ----> 1 sc.read('data/paul15/paul15.h5'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1480:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1480,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.read_excel(filename = '/Users/dan/Documents/Graduate_School/Bioinformatics_MS/BF550/Project/muscle_expression_.xlsx', sheet = 'expression'); ```. ```pytb; [/Users/dan/opt/anaconda3/lib/python3.9/site-packages/openpyxl/reader/workbook.py:88: UserWarning: File contains an invalid specification for expression. This will be removed; warn(msg). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); /var/folders/f1/7pq3lkn50cb7mtfkxrk9fnfh0000gn/T/ipykernel_3372/1224090119.py in <module>; ----> 1 adata = sc.read_excel(filename = '/Users/dan/Documents/Graduate_School/Bioinformatics_MS/BF550/Project/muscle_expression_.xlsx', sheet = 'expression'). ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_io/read.py in read_excel(filename, sheet, dtype); 73 from pandas import read_excel; 74 ; ---> 75 df = read_excel(fspath(filename), sheet); 76 X = df.values[:, 1:]; 77 row = dict(row_names=df.iloc[:, 0].values.astype(str)). ~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 309 stacklevel=stacklevel,; 310 ); --> 311 return func(*args, **kwargs); 312 ; 313 return wrapper. ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py in read_excel(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2371:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; ax = sc.pl.tracksplot(adata, markers, groupby = ""seurat_clusters"") ; adata.obs[""seurat_clusters""].dtype.name != 'category'; ```. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-33-097156e92bcf> in <module>; ----> 1 ax = sc.pl.tracksplot(adata, markers, groupby = ""seurat_clusters""). D:\python\empty for python\lib\site-packages\scanpy\plotting\_anndata.py in tracksplot(adata, var_names, groupby, use_raw, log, dendrogram, gene_symbols, var_group_positions, var_group_labels, layer, show, save, figsize, **kwds); 1304 if groupby not in adata.obs_keys() or adata.obs[groupby].dtype.name != 'category':; 1305 raise ValueError(; -> 1306 'groupby has to be a valid categorical observation. '; 1307 f'Given value: {groupby}, valid categorical observations: '; 1308 f'{[x for x in adata.obs_keys() if adata.obs[x].dtype.name == ""category""]}'. ValueError: groupby has to be a valid categorical observation. Given value: seurat_clusters, valid categorical observations: []. Ture; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1414:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; categories_order=['0','1','9','8','2','5','4','7','3','6']; sc.pl.tracksplot(adata,markers,groupby='leiden',vmax=3,categories_order=categories_order); ```. ```pytb; [Paste the error output produced by the above code here]; ```; ![image](https://user-images.githubusercontent.com/50618480/166931336-26e4431a-7bdb-41b4-a575-69aa5e9ef948.png); I can not get the order as ['0','1','9','8','2','5','4','7','3','6']; #### Versions; 1.9.1; <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2250:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2250,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import numpy as np; import pandas as pd; import scanpy as sc; from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),; obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1); sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)); a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)); a.obs['foo'].iloc[:16, :] = 0; a.obs['foo'].iloc[16:32, :] = 1; a.obs['foo'].iloc[32:, :] = 2; # wrong label-color mapping; sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2; sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns; a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]; sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4; sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5; # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in); sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)); ```; If you look closely (e.g. between fig. 2 and",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1591:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.experimental.pp.highly_variable_genes(; placenta, flavor=""pearson_residuals"", n_top_genes=2000, layer='raw', batch_key='sample'; ); ```. ```pytb; AttributeError: module 'scanpy' has no attribute 'experimental'; ```. #### Versions. <details>. scanpy==1.9.1 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==1.0.2 statsmodels==0.13.2 python-igraph==0.9.9 pynndescent==0.5.6. </details>. I have used sc.experimental for a time and it went well with version 1.7.2. ; Today I got this error, I thought it could be the version problem and I checked the experimental is in the main branch of version 1.9.1 on github, so I updated it to version 1.9.1. ; But it still gets the same error. Would this be due to the jupyterlab? (But I didn't change anything in the environment, it went well with this jupyter before). And I have the full set of files from the main branch like this; ```; /mnt/data/hong/anaconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/; ├── cli.py; ├── _compat.py; ├── datasets; ├── experimental; ├── external; ├── get; ├── __init__.py; ├── logging.py; ├── __main__.py; ├── _metadata.py; ├── metrics; ├── neighbors; ├── plotting; ├── preprocessing; ├── __pycache__; ├── queries; ├── readwrite.py; ├── _settings.py; ├── sim_models; ├── tools; └── _utils; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2378:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2378,2,"['guid', 'learn']","['guide', 'learn']"
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pl.highly_variable_genes(adata); ```. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj); ... FileNotFoundError: [Errno 2] No such file or directory: '/Users/name/.matplotlib/fontlist-v310.json.matplotlib-lock'. ```. #### Versions. <details>. ```; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; 0294638c8bf50491b025b096f3dba0a1 NA; absl NA; anyio NA; appnope 0.1.0; astunparse 1.6.3; attr 19.3.0; babel 2.9.0; backcall 0.2.0; brotli 1.0.9; certifi 2020.06.20; cffi 1.14.5; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; gast NA; get_version 2.2; google NA; h5py 2.10.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.3; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.36.0; markupsafe 1.1.1; matplotlib 3.2.1; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.0.7; numba 0.53.1; numexpr 2.7.3; numpy 1.19.0; opt_einsum v3.3.0; packaging 20.4; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.5; psutil 5.8.0; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pynndescent 0.5.2; py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1857:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1857,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. The *Edit on GitHub* button on the docs page for `scanpy.read_csv` [here](https://scanpy.readthedocs.io/en/stable/generated/scanpy.read_csv.html) forwards to a non-existing web page causing a 404 error. The same problem exists for `scanpy.read_h5ad`, `scanpy.read_excel` and `scanpy.read_hdf`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1967:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1967,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. pre_existing_clusters argument doesn't work. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```. ```pytb; KeyError Traceback (most recent call last); <ipython-input-207-326a4b3ee327> in <module>(); 1 sce.pp.hashsolo(adata, sample_to_column.keys(),; 2 number_of_noise_barcodes=1, pre_existing_clusters='leiden',; ----> 3 priors = [0.01, 0.6, 0.39],). /home/nicholas/repos/scanpy/scanpy/external/pp/_hashsolo.py in hashsolo(cell_hashing_adata, cell_hashing_columns, priors, pre_existing_clusters, clustering_data, resolutions, number_of_noise_barcodes, inplace); 249 unique_cluster_features = np.unique(cell_hashing_adata.obs[cluster_features]); 250 for cluster_feature in unique_cluster_features:; --> 251 cluster_feature_bool_vector = cell_hashing_adata.obs[cluster_features == cluster_feature]```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1482:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1482,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Found while investigating #1477. `scanpy.set_figure_parms` detects whether we're in an IPython session (or at least tries to #1841), and sets the default output format. The way it sets the format [is deprecated](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.set_matplotlib_formats), as it looks like the `matplotlib_inline` backend has moved to a separate package. . The new way to call this is: . ```python; import matplotlib_inline.backend_inline. if isinstance(ipython_format, str):; ipython_format = [ipython_format]; matplotlib_inline.backend_inline.set_matplotlib_formats(*ipython_format); ```. It may take some investigation to figure out how to do this in a backwards compatible way. I would also note the current backend format that we are using (`""png2x""`) is undocumented. It is equivalent to the documented format `""retina""`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1842:1045,undo,undocumented,1045,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1842,1,['undo'],['undocumented']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I clone the scanpy repository and I am on commit b69015e. I follow the instructions for a developmental install here:; https://scanpy.readthedocs.io/en/stable/installation.html#dev-install-instructions . ### Minimal code sample (that we can copy&paste without having any data). ```bash; pip install beni; beni pyproject.toml > environment.yml; conda env create -f environment.yml; ```. this is the error I get. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound:; - seaborn-split; ```. #### `environment.yml`. Here is the content of `environment.yml` which contains the strange package `seaborn-split`. So maybe the issue is upstream with beni?. <details>. ```; channels:; - conda-forge; dependencies:; - pip:; - flit; - bbknn; - scanpydoc>=0.7.4; - harmonypy; - magic-impute>=2.0; - cudf>=0.9; - cuml>=0.9; - cugraph>=0.9; - scanorama; - scrublet; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; - setuptools-scm; - black>=20.8b1; - docutils; - sphinx<4.2,>=4.1; - sphinx_rtd_theme>=0.3.1; - python-igraph; - leidenalg; - louvain!=0.6.2,>=0.6; - scikit-misc>=0.1.3; - pytest>=4.4; - pytest-nunit; - dask-core!=2.17.0; - fsspec; - zappy; - - zarr; - profimp; - flit-core; name: scanpy; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144:1310,learn,learn,1310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144,2,['learn'],['learn']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; **Hello Scanpy,; When I'm running sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'), it asks me to install scikit-misc, which is already installed. Please see the picture below.; Could you please help me to solve this issue?; Thanks!; Best,; YJ**; **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```; ![image](https://user-images.githubusercontent.com/75048821/145125005-64f8607e-9cb0-4740-8dca-7c80e35d30ef.png). ```pytb; [Paste the error output produced by the above code here]; ```. #### Versions. <details>3.8. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.2 pandas==1.3.4 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5; scvelo==0.2.4 scanpy==1.8.2 anndata==0.7.8 loompy==3.0.6 numpy==1.20.3 scipy==1.7.2 matplotlib==3.5.0 sklearn==1.0.1 pandas==1.3.4 ; cellrank==1.5.0 scanpy==1.8.2 anndata==0.7.8 numpy==1.20.3 numba==0.54.1 scipy==1.7.2 pandas==1.3.4 pygpcca==1.0.2 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 scvelo==0.2.4 pygam==0.8.0 matplotlib==3.5.0 seaborn==0.11.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073:528,guid,guide,528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073,3,"['guid', 'learn']","['guide', 'learn']"
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; module 'scanpy' has no attribute 'tl'. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```; adata_sc=diopy.input.read_h5(file ='/home/shpc_100862/xyh_desktop/2022918mouse_data/Reference/st_CTXmd.h5'); scanpy.tl.rank_genes_groups(adata_sc,groupby=""annotation"",use_raw=False); markers_df=pd.DataFrame(adata_sc.uns[""rank_genes_groups""][""names""]).iloc[0:100,:]; markers=list(np.unique(markers_df.melt().value.values)); ```pytb; [Paste the error output produced by the above code here]; ```; AttributeError Traceback (most recent call last); Input In [18], in <cell line: 4>(); 8 adata_st=diopy.input.read_h5(file = path); 9 adata_sc=diopy.input.read_h5(file ='/home/shpc_100862/xyh_desktop/2022918mouse_data/Reference/st_CTXmd.h5'); ---> 10 scanpy.tl.rank_genes_groups(adata_sc,groupby=""annotation"",use_raw=False); 11 markers_df=pd.DataFrame(adata_sc.uns[""rank_genes_groups""][""names""]).iloc[0:100,:]; 12 markers=list(np.unique(markers_df.melt().value.values)). AttributeError: module 'scanpy' has no attribute 'tl'; #### Versions; 1.9.1; <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; AttributeError: module 'scanpy' has no attribute 'logging'; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2343:296,guid,guide,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2343,1,['guid'],['guide']
Usability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. In the test of the flavor, the sorting over batches is correct with respect to how Seurat actually does it:. https://github.com/theislab/scanpy/blob/83f90141fd18943a1795772d3d39f4e9eefd65c3/scanpy/tests/test_highly_variable_genes.py#L138-L142. First sort by number of batches it's called a HVG and then second break ties by the median rank. In the actual function, it's the reverse:. https://github.com/theislab/scanpy/blob/83f90141fd18943a1795772d3d39f4e9eefd65c3/scanpy/preprocessing/_highly_variable_genes.py#L136-L143. I propose we put in the fix (which is currently being tested with high accuracy). The simplest solution would be to call it a new flavor. Ideally this would get in as soon as possible as this is a high volume function from what I understand.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2088:833,simpl,simplest,833,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2088,1,['simpl'],['simplest']
Usability,"- [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [x] External tools: Do you know an existing package that should go into `sc.external.*`?. Hi, thanks for developing such a fantastic framework for analyzing single-cell data, it really helps a lot in my research. I'm the developer of [Heatgraphy](https://github.com/Heatgraphy/heatgraphy), which is a python package to visualize multi-dimensional data using the x-layout system. You may think of it as python's version of complexHeatmap, but Heatgraphy can do much more. Personally, I think Heatgraphy can help visualize the AnnData intuitively. Here is [an example](https://heatgraphy.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html) of visualization of the PBMC3K dataset using Heatgraphy. The structure of AnnData fits the structure of this visualization quite well. We can plot the `.X` as the heatmap, and other attributes in `.obs`/`.var` or `.obsm`/`.varm` as side plots. ![](https://heatgraphy.readthedocs.io/en/latest/_images/sphx_glr_plot_pbmc3k_001_2_0x.png). Therefore, I propose adding a new API using Heatgraphy to help visualize AnnData. If adding to `sc.pl`, the API may look like this:. ```python; sc.pl.heatgraphy(adata, left=[(""cell_type"", ""color""), (""cell_type"", ""label"")], right=[(""gene_name"", ""label"")]); ```. If added as an extra class, the API can be more flexible and offer much more customization. It may look like this:. ```python; viz = AnnDataViz(adata); viz.add_left(key=""cell_type"", plot=""color"", cmap=""Set2"") # use a key from .obs and plot as color strip; viz.add_left(key=""cell_type"", plot=""label""); viz.render(); ```. It can also be applied to specific visualization for analysis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444:613,intuit,intuitively,613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444,1,['intuit'],['intuitively']
Usability,"- [yes] I have checked that this issue has not already been reported.; - [yes] I have confirmed this bug exists on the latest version of scanpy.; - [yes] I have confirmed this bug exists on the master branch of scanpy. Suggested fix: change line 44 in scanpy/scanpy/tools/_utils.py . https://github.com/theislab/scanpy/blob/83f90141fd18943a1795772d3d39f4e9eefd65c3/scanpy/tools/_utils.py#L44 . from `X = pca(adata.X)` to `X = pca(adata.X)[:, :n_pcs]`. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Minimal code ; import scanpy as sc; import anndata; import numpy as np; print('scanpy=', sc.__version__); print('anndata=', anndata.__version__); print('numpy=', np.__version__). np.random.seed(0); adata_raw = anndata.AnnData(np.random.randn(20,200)); print('adata shape', adata.shape). # neighbors without precomputed pca; adata = adata_raw.copy(); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=15); mat_neighbor10_pc15_without_pca = adata.obsp[""connectivities""].copy(). adata = adata_raw.copy(); sc.pp.pca(adata, n_comps=15); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=15); mat_neighbor10_pc15_with_pca = adata.obsp[""connectivities""].copy(). adata = adata_raw.copy(); sc.pp.pca(adata, n_comps=19) # max n_comps for 30 cells; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=19); mat_neighbor10_pc19_with_pca = adata.obsp[""connectivities""].copy(). print('\nResults for `sc.pp.neighbors(adata, n_neighbors=10, n_pcs=15)` '; 'with and without precomputed pca are different:'); print(np.allclose(mat_neighbor10_pc15_without_pca.toarray(), mat_neighbor10_pc15_with_pca.toarray())); print('\nResults of `sc.pp.neighbors(adata, n_neighbors=10, n_pcs=15)` without precomputed pca is same as '; 'results of `sc.pp.neighbors(adata, n_neighbors=10, n_pcs=19)` (19 is default `n_comps`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2093:485,guid,guide,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2093,1,['guid'],['guide']
Usability,"- [√] I have checked that this issue has not already been reported.; - [√] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; small bug report:; https://github.com/theislab/scanpy/blob/fb9e12f36b5c600913fa6243819d1575906c384e/scanpy/tools/_rank_genes_groups.py; line 538 wrong error raised when `use_raw=True`; ```python; if use_raw is None:; use_raw = adata.raw is not None; elif use_raw is True and adata.raw is not None:; raise ValueError(""Received `use_raw=True`, but `adata.raw` is empty.""); ```; Second `not` should be removed ,Corrected codes should be; ```python; if use_raw is None:; use_raw = adata.raw is not None; elif use_raw is True and adata.raw is None:; raise ValueError(""Received `use_raw=True`, but `adata.raw` is empty.""); ```; **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. #### Versions; this bug appeard in v1.8.0. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1929:879,guid,guide,879,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1929,1,['guid'],['guide']
Usability,"- [✔ ] I have checked that this issue has not already been reported.; - [✔ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python 3.8; # Your code here; ```sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon', use_raw=False). ```pytb; [Paste the error output produced by the above code here]; ```; ![image](https://user-images.githubusercontent.com/75048821/142975910-ee42c23e-976d-4980-a351-dcb53672b978.png). #### Versions. <details>. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.2 pandas==1.3.4 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5. </details>. ***************; Hello Scanpy,. Because the scRNA-seq data usually have mitochondrial gene contamination, it's reasonable to regress out mito genes by sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) and do scaling, and use this 'clear' data for determining the marker genes of each cluster by setting use_raw=False in sc.tl.rank_genes_groups(). However, I found that. 1. if using unregressed data by sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon', use_raw=True), the top marker genes have positive logFC, which is reasonable because these are top upregulated genes helping us to determine the annotations of clusters. ; ![image](https://user-images.githubusercontent.com/75048821/142977363-a7ce9cd6-5c2b-48f7-9e21-eccc66650f78.png). 2. the weird thing is, if using regressed data by sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon', use_raw=False), the logFC of top marker genes will become negative and even disappear, which means the downregulated genes and genes with unknown ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2057:259,guid,guide,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2057,2,"['guid', 'learn']","['guide', 'learn']"
Usability,"- [✔] I have checked that this issue has not already been reported.; - [✔ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ✔] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; I installed Scanpy, scVelo, CellRank, bbknn 2 months ago and never upgrade the packages. They were running very smoothly until I reimage my PC and reinstall Scanpy in anaconda today (Anaconda3-2021.05-Windows-x86_64, python3.8.12).; I tired `pip install scanpy[leiden]`. Tried `conda install seaborn scikit-learn statsmodels numba pytables`, `conda install -c conda-forge python-igraph leidenalg`. Tried installing `Java` and `visual C++ 2012-2022 redistributable`. Also tried rebuilding a new environment and reinstalled everything. Whatever I try, this bug still exists when I import Scanpy.; I guess it may be the incompatibility issue of packages. Some dependency packages which were upgraded by the developer in these months caused this incompatibility issue. Could you please help me with this bug?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import matplotlib.pyplot as pl; from matplotlib import rcParams; ```. ```pytb; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_7844/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:259,guid,guide,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,2,"['guid', 'learn']","['guide', 'learn']"
Usability,"- [✔] I have checked that this issue has not already been reported.; - [✔ ] I have confirmed this bug exists on the latest version of scanpy.; - [✔] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.; Hello Scanpy,; I'm using the DPT. Every function works fine except `sc.pl.dpt_timeseries(adata)`. I got no colors of the plot.; Could you please help me with this issue?; Thanks!; Best,; YJ; ![image](https://user-images.githubusercontent.com/75048821/154160553-19347b63-38e9-4f50-a4a3-f57780ed4545.png). ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.datasets.paul15(); adata; sc.pp.log1p(adata); sc.pp.neighbors(adata, n_neighbors=20, use_rep='X', method='umap'). sc.tl.diffmap(adata); sc.tl.dpt(adata, n_dcs=10, n_branchings=1). sc.pl.diffmap(adata, color=['dpt_pseudotime', 'dpt_groups', 'paul15_clusters']); sc.pl.dpt_groups_pseudotime(adata); sc.pl.dpt_timeseries(adata, color_map='viridis', show=True); ```. #### Versions. <details>. sc.pl.dpt_timeseries(adata, color_map='viridis', show=True). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2141:258,guid,guide,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2141,1,['guid'],['guide']
Usability,"- [✔] I have checked that this issue has not already been reported.; - [✔] I have confirmed this bug exists on the latest version of scanpy.; - [✔] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; It's very smooth to subset the adata by HVGs when doing `adata = adata[:, adata.var.highly_variable]` in the Scanpy pipeline. But when using the same coding to subeset a new raw adata, it generate errors. Could you please help me to check this issue?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; ACT_sub2 = sc.read('C:/Users/Park_Lab/Documents/ACT_sub2.h5ad') # Scanpy proceeded data; ACT_sub2; AnnData object with n_obs × n_vars = 2636 × 5000; obs: 'leiden', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_rpl', 'pct_counts_rpl', 'total_counts_rps', 'pct_counts_rps'; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells', 'mt', 'rpl', 'rps', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; layers: 'ambiguous', 'matrix', 'spliced', 'unspliced'; obsp: 'connectivities', 'distances'. adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/cellsorted_Apc_Cracd_Tumor_KPVDV.loom') # raw data; adata.var_names_make_unique(); adata; AnnData object with n_obs × n_vars = 13499 × 32285; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand'; layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'. adata.var['highly_variable']=ACT_sub2.var['highly_variable']; adata = adata[:, adata.var.highly_variable] # subset ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['guid'],['guide']
Usability,"- [✔] I have checked that this issue has not already been reported.; - [✔] I have confirmed this bug exists on the latest version of scanpy.; - [✔] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; This BUG is quite weird. It starts since we installed Anaconda3-v2021.11 on 3 individual Windows PCs. We run the same dataset by the same coding. However, it generates 3 different UMAPs. The coding is as below.; UMAP of Windows PC2 is consistent with our previous UMAPs done on PC1 and PC2 in November with Anaconda3-v2021.05.; Could you please help us with this issue?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/Tumor.loom'); adata.var_names_make_unique(); adata; sc.pl.highest_expr_genes(adata, n_top=20); sc.pp.filter_cells(adata, min_genes=100); sc.pp.filter_genes(adata, min_cells=25); adata.var['mt'] = adata.var_names.str.startswith('mt-'); adata.var['rpl'] = adata.var_names.str.startswith('Rpl'); adata.var['rps'] = adata.var_names.str.startswith('Rps'); adata; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt','rpl','rps'], percent_top=None, log1p=False, inplace=True); sc.pl.violin(adata, keys=['n_genes_by_counts', 'total_counts', 'pct_counts_mt','pct_counts_rpl','pct_counts_rps'], jitter=0.4, multi_panel=True); adata; sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='pct_counts_rpl'); sc.pl.scatter(adata, x='total_counts', y='pct_counts_rps'); sc.pl.scatter(adata, x='total_cou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:257,guid,guide,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['guid'],['guide']
Usability,- pkgutil-resolve-name=1.3.10; - platformdirs=4.2.2; - plotly=5.22.0; - plotly-orca=1.3.1; - pooch=1.8.2; - prettyprinter=0.18.0; - prometheus_client=0.20.0; - prompt-toolkit=3.0.47; - prompt_toolkit=3.0.47; - protobuf=4.25.3; - psutil=5.9.8; - pthread-stubs=0.4; - ptyprocess=0.7.0; - pure_eval=0.2.2; - py-cpuinfo=9.0.0; - pycparser=2.22; - pyfaidx=0.8.1.1; - pygments=2.18.0; - pymde=0.1.18; - pymongo=4.7.3; - pynndescent=0.5.12; - pyobjc-core=10.2; - pyobjc-framework-cocoa=10.2; - pyparsing=3.1.2; - pysocks=1.7.1; - pytables=3.9.2; - python=3.11.4; - python-dateutil=2.9.0; - python-fastjsonschema=2.19.1; - python-igraph=0.11.5; - python-json-logger=2.0.7; - python-kaleido=0.2.1; - python-tzdata=2024.1; - python_abi=3.11; - pytorch=2.2.2; - pytz=2024.1; - pyvcf3=1.0.3; - pyyaml=6.0.1; - pyzmq=26.0.3; - radian=0.6.12; - rchitect=0.4.6; - readline=8.2; - referencing=0.35.1; - requests=2.32.3; - rfc3339-validator=0.1.4; - rfc3986-validator=0.1.1; - rpds-py=0.18.1; - scanpy=1.10.1; - scikit-learn=1.5.0; - scipy=1.13.1; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.3; - session-info=1.0.0; - setuptools=70.0.0; - simplejson=3.19.2; - six=1.16.0; - snappy=1.2.0; - sniffio=1.3.1; - soupsieve=2.5; - stack_data=0.6.2; - statsmodels=0.14.2; - stdlib-list=0.10.0; - sympy=1.12; - tbb=2021.12.0; - tenacity=8.3.0; - terminado=0.18.1; - texttable=1.7.0; - threadpoolctl=3.5.0; - tinycss2=1.3.0; - tk=8.6.13; - tomli=2.0.1; - torchvision=0.17.2; - tornado=6.4.1; - tqdm=4.66.4; - traitlets=5.14.3; - types-python-dateutil=2.9.0.20240316; - typing-extensions=4.12.2; - typing_extensions=4.12.2; - typing_utils=0.1.0; - tzdata=2024a; - umap-learn=0.5.5; - uri-template=1.3.0; - urllib3=2.2.1; - wcwidth=0.2.13; - webcolors=24.6.0; - webencodings=0.5.1; - websocket-client=1.8.0; - wheel=0.43.0; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.5; - zipp=3.19.2; - zlib-ng=2.0.7; - zstd=1.5.6; - pip:; - absl-py==2.1.0; - astunparse==1.6,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:7089,learn,learn,7089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['learn'],['learn']
Usability,"----------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 103 ax.set_xscale(""log""); 104 show = settings.autoshow if show is None else show; --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save); 106 if show:; 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save); 337 show = settings.autoshow if show is None else show; 338 if save:; --> 339 savefig(writekey, dpi=dpi, ext=ext); 340 if show:; ...; -> 2456 fp = builtins.open(filename, ""w+b""); 2458 try:; 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'; Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```; scanpy==1.10.1 ; anndata==0.10.7 ; umap==0.5.5 ; numpy==1.26.4 ; scipy==1.13.0 ; pandas==2.2.2 ; scikit-learn==1.4.2 ; statsmodels==0.14.1 ; igraph==0.11.5 ; pynndescent==0.5.12; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3276:2902,learn,learn,2902,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276,1,['learn'],['learn']
Usability,"------------------; AxisError Traceback (most recent call last); <ipython-input-55-c0d016811ded> in <module>; ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py in regress_out(adata, keys, n_jobs, copy); 817 # split the adata.X matrix by columns in chunks of size n_chunk; 818 # (the last chunk could be of smaller size than the others); --> 819 chunk_list = np.array_split(adata.X, n_chunks, axis=1); 820 if variable_is_categorical:; 821 regressors_chunk = np.array_split(regressors, n_chunks, axis=1). <__array_function__ internals> in array_split(*args, **kwargs). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/lib/shape_base.py in array_split(ary, indices_or_sections, axis); 782 ; 783 sub_arys = []; --> 784 sary = _nx.swapaxes(ary, axis, 0); 785 for i in range(Nsections):; 786 st = div_points[i]. <__array_function__ internals> in swapaxes(*args, **kwargs). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in swapaxes(a, axis1, axis2); 595 ; 596 """"""; --> 597 return _wrapfunc(a, 'swapaxes', axis1, axis2); 598 ; 599 . ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. AxisError: axis1: axis 1 is out of bounds for array of dimension 0; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post3 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010:2461,learn,learn,2461,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010,1,['learn'],['learn']
Usability,"---. Hello there, I am relatively new at scanpy, and am getting the following error upon trying to plot the PCA components. Even though I am getting a plot, it is accompanied by this error. Could someone explain what could be the reason behind it? Thanks!. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here. # making a simple adata object; data = {'gene1':[1,2,3],; 'gene2':[3,2,2],; 'gene3':[1,4,1]}; df = pd.DataFrame(data); dft = df.T; adata = anndata.AnnData(X= df.iloc[0:,0:],; obs= df.iloc[0:,0:1],; var= dft.iloc[0:,0:1]). # computing principal component analysis; sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='gene2'). ```. ```pytb; [Paste the error output produced by the above code here]. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); [<ipython-input-10-876f2bccfb2c>](https://localhost:8080/#) in <module>; ----> 1 sc.pl.pca(adata, color='gene2'). 5 frames; [/usr/local/lib/python3.7/dist-packages/scanpy/plotting/_tools/scatterplots.py](https://localhost:8080/#) in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs); 870 if not annotate_var_explained:; 871 return embedding(; --> 872 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs; 873 ); 874 else:. [/usr/local/lib/python3.7/dist-packages/scanpy/plotting/_tools/scatterplots.py](https://localhost:8080/#) in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 451 elif colorbar_loc is not None:; 452 pl.colorbar(; --> 4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2332:371,simpl,simple,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2332,1,['simpl'],['simple']
Usability,"-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2197,clear,clear,2197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,2,['clear'],['clear']
Usability,"-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs); 126 ; 127 bytes_io = BytesIO(); --> 128 fig.canvas.print_figure(bytes_io, **kw); 129 data = bytes_io.getvalue(); 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs); 2054 orientation=orientation,; 2055 dryrun=True,; -> 2056 **kwargs); 2057 renderer = self.figure._cachedRenderer; 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs); 525 ; 526 else:; --> 527 FigureCanvasAgg.draw(self); 528 renderer = self.get_renderer(); 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self); 386 self.renderer = self.get_renderer(cleared=True); 387 with RendererAgg.lock:; --> 388 self.figure.draw(self.renderer); 389 # A GUI class may be need to update a window using this draw, so; 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer); 1707 self.patch.draw(renderer); 1708 mimage._draw_list_compositing_images(; -> 1709 renderer, self, artists, self.suppressComposite); 1710 ; 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 133 if not_composite or not has_images:; 134 for a in artists:; --> 135 a.draw(renderer); 136 else:; 137 # Composite any adjacent images together. ~/.local/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/787:1922,clear,cleared,1922,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787,1,['clear'],['cleared']
Usability,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7971,Clear,Clearly,7971,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['Clear'],['Clearly']
Usability,". in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. If this is the case, just step; > away for a while and do something else! But I think you won’t regret doing; > this. You’re learning good coding practices here that will come in handy in; > the future, I promise!; >; > Thank you for your contribution 🎉; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:2167,learn,learning,2167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,2,['learn'],['learning']
Usability,".0-1),; python3-pluggy (= 0.13.0-7.1),; python3-ply (= 3.11-5),; python3-py (= 1.10.0-1),; python3-pygments (= 2.7.1+dfsg-2.1),; python3-pyparsing (= 2.4.7-1),; python3-pytest (= 6.2.5-1),; python3-pytest-cov (= 3.0.0-1),; python3-pytoml (= 0.1.21-1),; python3-requests (= 2.25.1+dfsg-2),; python3-roman (= 3.3-1),; python3-scipy (= 1.7.1-2),; python3-seaborn (= 0.11.2-2),; python3-secretstorage (= 3.3.1-1),; python3-setuptools (= 58.2.0-1),; python3-setuptools-scm (= 6.0.1-1),; python3-sinfo (= 0.3.1-2),; python3-six (= 1.16.0-2),; python3-sklearn (= 0.23.2-5),; python3-sklearn-lib (= 0.23.2-5),; python3-skmisc (= 0.1.4+dfsg-1),; python3-slimit (= 0.8.1-4),; python3-statsmodels (= 0.12.2-2),; python3-statsmodels-lib (= 0.12.2-2),; python3-stdlib-list (= 0.8.0-4),; python3-tables (= 3.6.1-5),; python3-tables-lib (= 3.6.1-5),; python3-texttable (= 1.6.3-2),; python3-threadpoolctl (= 2.1.0-1),; python3-tk (= 3.9.8-1),; python3-toml (= 0.10.2-1),; python3-tqdm (= 4.57.0-2),; python3-tz (= 2021.3-1),; python3-urllib3 (= 1.26.5-1~exp1),; python3-wheel (= 0.34.2-1),; python3-zarr (= 2.10.2+ds-2),; python3-zipp (= 1.0.0-3),; python3.9 (= 3.9.8-2),; python3.9-minimal (= 3.9.8-2),; readline-common (= 8.1-2),; rpcsvc-proto (= 1.4.2-4),; sed (= 4.8-1),; sensible-utils (= 0.0.17),; sgml-base (= 1.30),; shared-mime-info (= 2.0-1),; sphinx-rtd-theme-common (= 1.0.0+dfsg-1),; systemd (= 249.6-2),; systemd-sysv (= 249.6-2),; sysvinit-utils (= 3.00-1),; tar (= 1.34+dfsg-1),; tk8.6-blt2.5 (= 2.5.3+dfsg-4.1),; ttf-bitstream-vera (= 1.10-8.1),; tzdata (= 2021e-1),; ucf (= 3.0043),; umap-learn (= 0.4.5+dfsg-2),; util-linux (= 2.37.2-4),; x11-common (= 1:7.7+23),; xkb-data (= 2.33-1),; xml-core (= 0.18+nmu1),; xz-utils (= 5.2.5-2),; zlib1g (= 1:1.2.11.dfsg-2); Environment:; DEB_BUILD_OPTIONS=""parallel=4""; LANG=""en_US.UTF-8""; LC_ALL=""C.UTF-8""; LC_COLLATE=""C.UTF-8""; LC_CTYPE=""en_US.UTF-8""; LC_MESSAGES=""en_US.UTF-8""; LD_LIBRARY_PATH=""/usr/lib/libeatmydata""; SOURCE_DATE_EPOCH=""1625592742""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:13854,learn,learn,13854,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,2,['learn'],['learn']
Usability,".0.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; dill 0.3.4; docrep 0.3.2; entrypoints 0.4; etils 0.8.0; flax 0.6.1; fsspec 2022.7.1; google NA; graphviz 0.20; h5py 3.7.0; idna 3.4; igraph 0.10.2; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.23; jaxlib 0.3.22; jedi 0.18.1; jinja2 2.11.3; jmespath 0.10.0; joblib 1.1.1; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.8.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; matplotlib_inline 0.1.6; ml_collections NA; mpl_toolkits NA; msgpack 1.0.3; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.56.3; numexpr 2.8.3; numpy 1.22.4; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.9; pyro 1.8.2; pytorch_lightning 1.7.7; pytz 2022.1; regex 2.5.116; requests 2.28.1; rich NA; scipy 1.7.3; scvi 0.18.0; session_info 1.0.0; setuptools 63.4.1; simplejson 3.17.6; six 1.16.0; sklearn 1.1.2; snappy NA; socks 1.7.1; sphinxcontrib NA; storemagic NA; tblib 1.7.0; tensorboard 2.9.1; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.11.0; toolz 0.11.2; torch 1.12.1; torchmetrics 0.10.0; torchvision 0.13.1; tornado 6.1; tqdm 4.64.1; traitlets 5.1.1; tree 0.1.7; typing_extensions NA; urllib3 1.26.12; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 23.2.0; zope NA; -----; IPython 7.31.1; jupyter_client 7.3.4; jupyter_core 4.11.1; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Jun 1 2022, 06:36:29) [Clang 12.0.0 ]; macOS-10.16-x86_64-i386-64bit; -----; Session information updated at 2022-10-22 15:12. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:3984,simpl,simplejson,3984,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['simpl'],['simplejson']
Usability,.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (3.0.4); Requirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.3.2); Requirement already satisfied: fonttools>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4313,learn,learn,4313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,2,['learn'],['learn']
Usability,.19.18 pypi_0 pypi; pyqtchart 5.12 pypi_0 pypi; pyqtwebengine 5.12.1 pypi_0 pypi; pyrsistent 0.17.3 py38h4d0b108_1 conda-forge; pysocks 1.7.1 py38h5347e94_2 conda-forge; pytables 3.6.1 py38h4e4ac5c_3 conda-forge; python 3.8.6 hcfdab8c_0_cpython conda-forge; python-dateutil 2.8.1 py_0 conda-forge; python-igraph 0.8.3 py38hcde0000_2 conda-forge; python-jsonrpc-server 0.4.0 pyh9f0ad1d_0 conda-forge; python-language-server 0.35.1 py_0 conda-forge; python.app 2 py38_10 ; python_abi 3.8 1_cp38 conda-forge; pytz 2020.1 pyh9f0ad1d_0 conda-forge; pyyaml 5.3.1 py38h94c058a_1 conda-forge; pyzmq 19.0.2 py38h2c785a9_2 conda-forge; qdarkstyle 2.8.1 pyh9f0ad1d_1 conda-forge; qt 5.12.9 h717870c_0 conda-forge; qtawesome 1.0.1 pyh9f0ad1d_0 conda-forge; qtconsole 4.7.7 pyh9f0ad1d_0 conda-forge; qtpy 1.9.0 py_0 conda-forge; readline 8.0 h0678c8f_2 conda-forge; requests 2.24.0 pyh9f0ad1d_0 conda-forge; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:7195,learn,learn,7195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,2,['learn'],['learn']
Usability,.45.2; pydantic 1.10.8; PyDispatcher 2.0.5; pydocstyle 6.3.0; pyerfa 2.0.0; pyflakes 3.0.1; Pygments 2.15.1; PyJWT 2.4.0; pylint 2.16.2; pylint-venv 2.3.0; pyls-spyder 0.4.0; pyodbc 4.0.34; pyOpenSSL 23.2.0; pyparsing 3.0.9; PyQt5-sip 12.11.0; pyrsistent 0.18.0; PySocks 1.7.1; pytest 7.4.0; python-dateutil 2.8.2; python-dotenv 0.21.0; python-json-logger 2.0.7; python-lsp-black 1.2.1; python-lsp-jsonrpc 1.0.0; python-lsp-server 1.7.2; python-slugify 5.0.2; python-snappy 0.6.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz-comms 2.3.0; PyWavelets 1.4.1; pyxdg 0.27; PyYAML 6.0; pyzmq 23.2.0; QDarkStyle 3.0.2; qstylizer 0.2.2; QtAwesome 1.2.2; qtconsole 5.4.2; QtPy 2.2.0; queuelib 1.5.0; regex 2022.7.9; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; responses 0.13.3; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rope 1.7.0; Rtree 1.0.1; ruamel.yaml 0.17.21; ruamel-yaml-conda 0.17.21; s3fs 2023.4.0; safetensors 0.3.2; scikit-image 0.20.0; scikit-learn 1.3.0; scikit-learn-intelex 20230426.111612; scipy 1.11.1; Scrapy 2.8.0; seaborn 0.12.2; SecretStorage 3.3.1; Send2Trash 1.8.0; service-identity 18.1.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 5.2.1; sniffio 1.2.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.4; Sphinx 5.0.2; sphinxcontrib-applehelp 1.0.2; sphinxcontrib-devhelp 1.0.2; sphinxcontrib-htmlhelp 2.0.0; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.3; sphinxcontrib-serializinghtml 1.1.5; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 1.4.39; stack-data 0.2.0; statsmodels 0.14.0; sympy 1.11.1; tables 3.8.0; tabulate 0.8.10; TBB 0.2; tblib 1.7.0; TELR 1.0; tenacity 8.2.2; terminado 0.17.1; text-unidecode 1.3; textdistance 4.2.1; texttable 1.7.0; threadpoolctl 2.2.0; three-merge 0.1.1; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.2.0; tokenizers 0.13.2; toml 0.10.2; tomlkit 0.11.1; toolz 0.12.0; tornado 6.3.2; tqdm 4.65.0; traitlets 5.7.1; transformers 4.32.1; Twisted 22.10.0; typing_extensions 4.7.1; tzdata 20,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:6518,learn,learn-intelex,6518,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['learn'],['learn-intelex']
Usability,".5.1; win32api NA; win32com NA; win32security NA; yaml 5.3.1; zmq 19.0.2; zope NA; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.6; notebook 6.1.4; -----; Python 3.8.5 (default, Sep 3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.19041-SP0; 4 logical CPU cores, Intel64 Family 6 Model 78 Stepping 3, GenuineIntel; -----; Session information updated at 2021-09-09 14:23. </details>\. Reading this stackoverflow (https://stackoverflow.com/questions/58518554/attributeerror-graph-object-has-no-attribute-node) I tried downgrading to networkx version 2.3 instead of 2.6.2 and it fixed this issue, but raised a separate incompatibility between networkx and matplotlib. ```pytb; AttributeError: module 'matplotlib.cbook' has no attribute 'iterable'; ```; ; which traces back to an issue in networkx rather than scanpy.; The following stackoverflow suggests that the older networkx2.3 requires matplotlib2.2.3 which is quite a large downgrade from my current matplatlib3.4.3 (current as of this date), so downgrading both networkx and matplotlib is not a great solution: https://stackoverflow.com/questions/63198347/attributeerror-module-matplotlib-cbook-has-no-attribute-iterable,. How i fixed it: ; After all that I went to line 860-862 of plotting/_tools/paga.py in my own scanpy installation and simply changed all the 'node' to 'nodes':; ```python; for count, n in enumerate(nx_g_solid.nodes()):; nx_g_solid.node[count]['label'] = str(node_labels[count]); nx_g_solid.node[count]['color'] = str(colors[count]); nx_g_solid.node[count]['viz'] = dict(; ```; to; ```python; for count, n in enumerate(nx_g_solid.nodes()):; nx_g_solid.nodes[count]['label'] = str(node_labels[count]); nx_g_solid.nodes[count]['color'] = str(colors[count]); nx_g_solid.nodes[count]['viz'] = dict(; ```. which apparently solved the issue; ```pytb; gexf=sc.pl.paga(paul15, labels=None, color='paul15_clusters', export_to_gexf=True); WARNING: exporting to write\paga_graph.gexf; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1997:3542,simpl,simply,3542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1997,1,['simpl'],['simply']
Usability,".get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); <ipython-input-20-26443e0aed95> in <module>; ----> 1 rnaseq1 = sc.read_10x_mtx(""GSE145328_RAW""). ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 479 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 480 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 481 adata = read(; 482 str(path),; 483 var_names=var_names,. ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 560 else:; 561 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 562 adata.var['feature_types'] = genes[2].values; 563 adata.obs_names = pd.read_csv(path / f'{prefix}barcodes.tsv.gz', header=None)[; 564 0. ~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3022 if self.columns.nlevels > 1:; 3023 return self._getitem_multilevel(key); -> 3024 indexer = self.columns.get_loc(key); 3025 if is_integer(indexer):; 3026 indexer = [indexer]. ~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3080 return self._engine.get_loc(casted_key); 3081 except KeyError as err:; -> 3082 raise KeyError(key) from err; 3083 ; 3084 if tolerance is not None:. KeyError: 2; ```. #### Versions; scanpy==1.8.0 anndata==0.7.6 umap==0.5.1 numpy==1.19.2 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.2. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916:2596,learn,learn,2596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916,1,['learn'],['learn']
Usability,".scale_array` function registered for a `dask.array.Array` type. Adding `@scale.register(da.Array)` [here](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#LL758C7-L758C7) would fix that, but considering `dask.array` is [only an optional dependency,](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#L32) there would have to be a conditional to wrap the function decoration. This brings me to the larger issue, is scanpy supposed to or working toward supporting dask arrays completely? I'm new to scanpy and I'm not sure if this is a bug report or an enhancement request. I could submit a pull request for this one issue but I'm curious if I'll run into many such issues as I dive in further and trying to figure out if there's existing momentum in this direction or whether I should be following some other parallelization strategy. I got here by following [AnnData's guide on using dask and zarr](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/%7Bread%2Cwrite%7D_dispatched.html) to try to parallelize processing of a large scRNA-seq file. I come from a microscopy data analysis and ML background where my image data is stored in S3 hosted zarr arrays (in an OME-NGFF schema), handled by using dask arrays wrapped in xarray DataArrays, and parallelized across compute using ray. It would be nice to use a similar stack (dropping in anndata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as ad; import dask.array as da; import scanpy as sc. # write data to zarr file; rel_zarr_path = 'data/pbmc3k_processed.zarr'; adata = sc.datasets.pbmc3k_processed(); adata.write_zarr(f'./{rel_zarr_path}', chunks=[adata.shape[0], 5]); zarr.consolidate_metadata(f'./{rel_zarr_path}'). # read data from zarr file with X as a dask array; def read_dask(store):; f = zarr.open(s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:1364,guid,guide,1364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['guid'],['guide']
Usability,".values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py in get(self, item, fastpath); 4113 ; 4114 if not isna(item):; -> 4115 loc = self.items.get_loc(item); 4116 else:; 4117 indexer = np.arange(len(self.items))[isna(self.items)]. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3078 return self._engine.get_loc(key); 3079 except KeyError:; -> 3080 return self._engine.get_loc(self._maybe_cast_indexer(key)); 3081 ; 3082 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2; ```. Versions of all packages:. `scanpy==1.4.5.post2 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.2 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1`. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:3859,learn,learn,3859,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['learn'],['learn']
Usability,0 conda-forge; pexpect 4.8.0 py36h9f0ad1d_1 conda-forge; pickleshare 0.7.5 py36h9f0ad1d_1001 conda-forge; pip 20.1.1 py36_1 ; plotly 4.0.0 py_0 plotly; prometheus_client 0.8.0 pyh9f0ad1d_0 conda-forge; prompt-toolkit 3.0.5 py_1 conda-forge; prompt_toolkit 3.0.5 1 conda-forge; pthread-stubs 0.4 h14c3975_1001 conda-forge; ptyprocess 0.6.0 py_1001 conda-forge; pygments 2.6.1 py_0 conda-forge; pyparsing 2.4.7 pyh9f0ad1d_0 conda-forge; pyqt 5.9.2 py36hcca6a23_4 conda-forge; pyrsistent 0.16.0 py36h8c4c3a4_0 conda-forge; pytables 3.6.1 py36h71ec239_0 ; python 3.6.10 h7579374_2 ; python-dateutil 2.8.1 py_0 conda-forge; python-igraph 0.8.2 pypi_0 pypi; python_abi 3.6 1_cp36m conda-forge; pytz 2020.1 py_0 ; pyzmq 19.0.1 py36h9947dbf_0 conda-forge; qt 5.9.7 h5867ecd_1 ; qtconsole 4.7.5 pyh9f0ad1d_0 conda-forge; qtpy 1.9.0 py_0 conda-forge; readline 8.0 h7b6447c_0 ; retrying 1.3.3 py_2 conda-forge; sam-algorithm 0.7.3 pypi_0 pypi; scanpy 1.5.1 pypi_0 pypi; scikit-learn 0.23.1 py36h423224d_0 ; scipy 1.5.0 py36h0b6359f_0 ; seaborn 0.10.1 py_0 ; send2trash 1.5.0 py_0 conda-forge; setuptools 47.3.1 py36_0 ; setuptools-scm 4.1.2 pypi_0 pypi; sip 4.19.8 py36hf484d3e_0 ; six 1.15.0 pyh9f0ad1d_0 conda-forge; snappy 1.1.8 he6710b0_0 ; sqlite 3.32.3 h62c20be_0 ; statsmodels 0.11.1 py36h7b6447c_0 ; tbb 2020.0.133 pypi_0 pypi; terminado 0.8.3 py36h9f0ad1d_1 conda-forge; testpath 0.4.4 py_0 conda-forge; texttable 1.6.2 pypi_0 pypi; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; tk 8.6.10 hbc83047_0 ; tornado 6.0.4 py36h8c4c3a4_1 conda-forge; tqdm 4.46.1 pypi_0 pypi; traitlets 4.3.3 py36h9f0ad1d_1 conda-forge; umap-learn 0.4.4 pypi_0 pypi; wcwidth 0.2.5 pyh9f0ad1d_0 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.34.2 py36_0 ; widgetsnbextension 3.5.1 py36_0 conda-forge; xorg-libxau 1.0.9 h14c3975_0 conda-forge; xorg-libxdmcp 1.1.3 h516909a_0 conda-forge; xz 5.2.5 h7b6447c_0 ; zeromq 4.3.2 he1b5a44_2 conda-forge; zipp 3.1.0 py_0 conda-forge; zlib 1.2.11 h7b6447c_3 ; zstd 1.4.4 h0b5b093_3 ; ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1293:10005,learn,learn,10005,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293,1,['learn'],['learn']
Usability,0.43.4; - pkgutil-resolve-name=1.3.10; - platformdirs=3.10.0; - plotly=5.16.1; - plotly-orca=3.4.2; - pooch=1.7.0; - prettyprinter=0.18.0; - prometheus_client=0.17.1; - prompt-toolkit=3.0.39; - prompt_toolkit=3.0.39; - psutil=5.9.5; - pthread-stubs=0.4; - ptyprocess=0.7.0; - pure_eval=0.2.2; - py-cpuinfo=9.0.0; - pycparser=2.21; - pyct=0.5.0; - pyfaidx=0.8.1.1; - pygments=2.16.1; - pymde=0.1.18; - pymongo=4.5.0; - pynndescent=0.5.11; - pyobjc-core=9.2; - pyobjc-framework-cocoa=9.2; - pyparsing=3.0.9; - pysocks=1.7.1; - pytables=3.8.0; - python=3.11.4; - python-dateutil=2.8.2; - python-fastjsonschema=2.18.0; - python-igraph=0.11.3; - python-json-logger=2.0.7; - python-kaleido=0.2.1; - python-tzdata=2023.3; - python_abi=3.11; - pytorch=2.0.1; - pytz=2023.3; - pyvcf3=1.0.3; - pyyaml=6.0.1; - pyzmq=25.1.1; - radian=0.6.7; - rchitect=0.4.1; - readline=8.2; - referencing=0.30.2; - requests=2.31.0; - rfc3339-validator=0.1.4; - rfc3986-validator=0.1.1; - rpds-py=0.9.2; - scanpy=1.10.1; - scikit-learn=1.3.0; - scipy=1.11.2; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.2; - session-info=1.0.0; - setuptools=68.1.2; - simplejson=3.19.2; - six=1.16.0; - snappy=1.1.10; - sniffio=1.3.0; - soupsieve=2.3.2.post1; - stack_data=0.6.2; - statsmodels=0.14.0; - stdlib-list=0.10.0; - svt-av1=1.6.0; - sympy=1.12; - tbb=2021.11.0; - tenacity=8.2.3; - terminado=0.17.1; - texttable=1.7.0; - threadpoolctl=3.2.0; - tinycss2=1.2.1; - tk=8.6.12; - tomli=2.0.1; - torchvision=0.15.2; - tornado=6.3.3; - traitlets=5.9.0; - typing_extensions=4.8.0; - typing_utils=0.1.0; - tzdata=2023c; - umap-learn=0.5.5; - uri-template=1.3.0; - wcwidth=0.2.6; - webcolors=1.13; - webencodings=0.5.1; - websocket-client=1.6.2; - wheel=0.41.2; - x264=1!164.3095; - x265=3.5; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopyth,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:13684,learn,learn,13684,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['learn'],['learn']
Usability,"0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.1.0; attrs 23.1.0; babel 2.13.1; certifi 2023.07.22; cffi 1.16.0; charset_normalizer 3.2.0; comm 0.2.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.10.0; idna 3.4; igraph 0.10.8; ipykernel 6.27.1; ipython_genutils 0.2.0; ipywidgets 8.1.1; isoduration NA; jedi 0.19.1; jinja2 3.1.2; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.20.0; jsonschema_specifications NA; jupyter_server 1.24.0; jupyterlab_server 2.25.2; kiwisolver 1.4.5; leidenalg 0.10.1; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.7.2; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; nbformat 5.9.2; numba 0.58.1; numpy 1.25.2; packaging 23.1; pandas 2.1.4; parso 0.8.3; patsy 0.5.6; pexpect 4.9.0; platformdirs 4.1.0; plotly 5.18.0; prometheus_client NA; prompt_toolkit 3.0.41; psutil 5.9.6; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.16.1; pyparsing 3.0.9; pytz 2023.3.post1; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.11.4; seaborn 0.13.1; send2trash NA; session_info 1.0.0; simplejson 3.19.1; six 1.16.0; sklearn 1.3.2; sniffio 1.3.0; sparse 0.14.0; stack_data 0.6.3; statsmodels 0.14.1; sympy 1.12; terminado 0.18.0; texttable 1.7.0; threadpoolctl 3.2.0; torch 2.1.2+cu121; torchgen NA; tornado 6.4; tqdm 4.66.1; traitlets 5.14.0; typing_extensions NA; uri_template NA; urllib3 2.0.4; wcwidth 0.2.12; webcolors 1.13; websocket 1.7.0; yaml 6.0.1; zmq 24.0.1; -----; IPython 8.18.1; jupyter_client 7.4.9; jupyter_core 5.5.0; jupyterlab 3.6.5; notebook 6.5.6; -----; Python 3.11.3 (main, Aug 25 2023, 17:24:38) [GCC 11.4.0]; Linux-5.15.0-91-generic-x86_64-with-glibc2.35; -----; Session information updated at 2024-02-07 10:05; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2842:2458,simpl,simplejson,2458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2842,1,['simpl'],['simplejson']
Usability,"1-dimensional'); 4162 else:. Exception: Data must be 1-dimensional. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-23-ccdbf8b7836c> in <module>; ----> 1 sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8) ## 200316 error fix later, also when I run the entire script from the start. ~/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save); 727 if issparse(X_col): X_col = X_col.toarray().flatten(); 728 new_gene_names.append(g); --> 729 df[g] = X_col; 730 df['hue'] = adata.obs[groups_key].astype(str).values; 731 if reference == 'rest':. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __setitem__(self, key, value); 3114 else:; 3115 # set column; -> 3116 self._set_item(key, value); 3117 ; 3118 def _setitem_slice(self, key, value):. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _set_item(self, key, value); 3188 """"""; 3189 ; -> 3190 self._ensure_valid_index(value); 3191 value = self._sanitize_column(key, value); 3192 NDFrame._set_item(self, key, value). ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value); 3170 value = Series(value); 3171 except:; -> 3172 raise ValueError('Cannot set a frame with no defined index '; 3173 'and a value that cannot be converted to a '; 3174 'Series'). ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.23.0 scikit-learn==0.21.3 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1; scvelo==0.1.25 scanpy==1.4.5.1 anndata==0.7.1 loompy==3.0.6 numpy==1.18.1 scipy==1.4.1 matplotlib==3.1.3 sklearn==0.21.3 pandas==0.23.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114:3455,learn,learn,3455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114,1,['learn'],['learn']
Usability,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487343093:83,intuit,intuitive,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487343093,2,['intuit'],['intuitive']
Usability,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output.; 2. Setting `svd_solver='arpack'` resolves that problem.; 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere.; 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again?. Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-433258438:85,learn,learn,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433258438,2,['learn'],['learn']
Usability,"1.],; [1., 0., 1., 0., 1., 0., 0., 0., 0.],; [1., 1., 0., 1., 1., 1., 0., 0., 0.],; [1., 0., 1., 0., 0., 1., 0., 0., 0.],; [1., 1., 1., 0., 0., 0., 1., 1., 0.],; [1., 0., 1., 1., 0., 0., 0., 1., 1.],; [1., 0., 0., 0., 1., 0., 0., 1., 0.],; [1., 0., 0., 0., 1., 1., 1., 0., 1.],; [1., 0., 0., 0., 0., 1., 0., 1., 0.]]); a = ad.AnnData(arr); a.uns = {'spatial': {'connectivities_key': 'spatial_connectivities', 'distances_key': 'spatial_distances', 'params': {'n_neighbors': 8, 'coord_type': None, 'radius': 1.5}}}; a.obsp['spatial_connectivities'] = conn; a.obsm['coords'] = arr.copy(); a.obsm['spatial'] = arr.copy(). sc.pl.embedding(a, 'coords', edges=True, neighbors_key = 'spatial'); print('coords', a.obsm['coords']); sc.pl.embedding(a, 'spatial'); print('spatial no edges', a.obsm['spatial']); sc.pl.embedding(a, 'spatial', edges=True, neighbors_key = 'spatial'); print('spatial edges', a.obsm['spatial']); ```. produces following output:. - expected (when plotting ""coords""); ![image](https://user-images.githubusercontent.com/13350159/101187374-9206bd00-3654-11eb-9b7c-38b5870ab815.png); ```; coords [[96 55]; [95 54]; [96 54]; [97 54]; [95 55]; [97 55]; [95 56]; [96 56]; [97 56]]; ```. - when plotting ""spatial"" without edges (obsm is not modified); ![image](https://user-images.githubusercontent.com/13350159/101187899-40aafd80-3655-11eb-9738-9899efba9d52.png); ```; spatial no edges [[96 55]; [95 54]; [96 54]; [97 54]; [95 55]; [97 55]; [95 56]; [96 56]; [97 56]]; ```. - when plotting ""spatial"" with edges (obsm is modified). ![image](https://user-images.githubusercontent.com/13350159/101187963-5ae4db80-3655-11eb-9866-6824831b18a9.png); ```; spatial edges [[ 96 255]; [ 95 254]; [ 96 254]; [ 97 254]; [ 95 255]; [ 97 255]; [ 95 0]; [ 96 0]; [ 97 0]]; ```. #### Versions. <details>. scanpy==1.6.1.dev70+g7f15d22d anndata==0.7.5 umap==0.4.6 numpy==1.19.4 scipy==1.5.3 pandas==1.1.4 scikit-learn==0.23.2 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.6.1 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1534:2880,learn,learn,2880,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1534,1,['learn'],['learn']
Usability,"110 ; -> 3111 key = _get_dendrogram_key(adata, dendrogram, groupby); 3112 ; 3113 dendro_info = adata.uns[key]. ~/.pyenv/versions/3.6.9/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 3195 ""tuning it is recommended to run `sc.tl.dendrogram` independently.""; 3196 ); -> 3197 dendrogram(adata, groupby, key_added=dendrogram_key); 3198 ; 3199 if 'dendrogram_info' not in adata.uns[dendrogram_key]:. ~/.pyenv/versions/3.6.9/lib/python3.6/site-packages/scanpy/tools/_dendrogram.py in dendrogram(adata, groupby, n_pcs, use_rep, var_names, use_raw, cor_method, linkage_method, optimal_ordering, key_added, inplace); 130 corr_matrix, method=linkage_method, optimal_ordering=optimal_ordering; 131 ); --> 132 dendro_info = sch.dendrogram(z_var, labels=categories, no_plot=True); 133 ; 134 # order of groupby categories. ~/.pyenv/versions/3.6.9/lib/python3.6/site-packages/scipy/cluster/hierarchy.py in dendrogram(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, no_plot, no_labels, leaf_font_size, leaf_rotation, leaf_label_func, show_contracted, link_color_func, ax, above_threshold_color); 3275 ""'bottom', or 'right'""); 3276 ; -> 3277 if labels and Z.shape[0] + 1 != len(labels):; 3278 raise ValueError(""Dimensions of Z and labels must be consistent.""); 3279 . ~/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pandas/core/indexes/base.py in __nonzero__(self); 2229 raise ValueError(""The truth value of a {0} is ambiguous. ""; 2230 ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""; -> 2231 .format(self.__class__.__name__)); 2232 ; 2233 __bool__ = __nonzero__. ValueError: The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().; ```. **VERSIONS:** scanpy==1.5.1 anndata==0.7.3 umap==0.4.6 numpy==1.16.4 scipy==1.5.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.8.2 leidenalg==0.8.1; Python version: 3.6.9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1313:4929,learn,learn,4929,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1313,1,['learn'],['learn']
Usability,2 pyhd3eb1b0_0 anaconda; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; pytables 3.8.0 py39h0da393b_2 conda-forge; python 3.9.16 hea58f1e_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge; python_abi 3.9 3_cp39 conda-forge; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyzmq 22.3.0 py39hc377ac9_2 anaconda; readline 8.2 h92ec313_1 conda-forge; requests 2.31.0 pyhd8ed1ab_0 conda-forge; scanpy 1.7.2 pyhdfd78af_0 bioconda; scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge; scipy 1.11.1 py39ha6b2cbd_0 conda-forge; seaborn 0.12.2 hd8ed1ab_0 conda-forge; seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge; setuptools_scm 7.1.0 hd8ed1ab_0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; snappy 1.1.10 h17c5cce_0 conda-forge; stack_data 0.2.0 pyhd3eb1b0_0 anaconda; statsmodels 0.14.0 py39h8a366b7_1 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; tbb 2021.9.0 hffc8910_0 conda-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.12 he1e0b03_0 conda-forge; tomli 2.0.1 pyhd8ed1ab_0 conda-forge; tornado 6.1 py39h1a28f6b_0 anaconda; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.1.1 pyhd3eb1b0_0 anaconda; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023c h71feb2d_0 conda-forge; umap-learn 0.5.3 py39h2804cbe_1 conda-forge; unicodedata2 15.0.0 py39h02fc5c5_0 conda-forge; urllib3 2.0.3 pyhd8ed1ab_1 conda-forge; wcwidth 0.2.5 pyhd3eb1b0_0 anaconda; wheel 0.40.0 pyhd8ed1ab_1 conda-forge; xorg-libxau 1.0.11 hb547adb_0 conda-forge; xorg-libxdmcp 1.1.3 h27ca646_0 conda-forge; xz 5.2.6 h57fd34a_0 conda-forge; zeromq 4.3.4 hc377ac9_0 anaconda; zipp 3.16.2 pyhd8ed1ab_0 conda-forge; zlib-ng 2.0.7 h1a8c8d9_0 conda-forge; zstd 1.5.2 h4f39d0f_7 conda-forge; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564:8115,learn,learn,8115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564,1,['learn'],['learn']
Usability,"2.8.2; debugpy 1.8.0; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.0; fastjsonschema NA; fqdn NA; h5py 3.9.0; idna 3.4; igraph 0.10.8; ipykernel 6.25.2; ipywidgets 8.1.1; isoduration NA; jedi 0.19.1; jinja2 3.1.2; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.19.1; jsonschema_specifications NA; jupyter_events 0.7.0; jupyter_server 2.7.3; jupyterlab_server 2.25.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.41.0; markupsafe 2.1.3; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; nbformat 5.9.2; numba 0.58.0; numpy 1.24.3; opt_einsum v3.3.0; overrides NA; packaging 23.1; pandas 2.0.3; parso 0.8.3; patsy 0.5.3; pickleshare 0.7.5; platformdirs 3.11.0; prometheus_client NA; prompt_toolkit 3.0.39; psutil 5.9.5; pure_eval 0.2.2; pyarrow 15.0.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 2.0.0; pygments 2.16.1; pyparsing 3.0.9; pythoncom NA; pythonjsonlogger NA; pytz 2023.3; pywin32_bootstrap NA; pywin32_system32 NA; pywintypes NA; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.10.1; seaborn 0.13.0; send2trash NA; session_info 1.0.0; simplejson 3.19.2; six 1.16.0; sklearn 1.3.1; sniffio 1.3.0; sparse 0.14.0; stack_data 0.6.3; statsmodels 0.14.0; sympy 1.12; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.0; toolz 0.12.0; torch 2.0.1+cpu; tornado 6.3.3; tqdm 4.66.1; traitlets 5.10.1; typing_extensions NA; uri_template NA; urllib3 2.0.6; vscode NA; wcwidth 0.2.8; webcolors 1.13; websocket 1.6.3; win32api NA; win32com NA; win32con NA; win32trace NA; winerror NA; yaml 6.0.1; zipp NA; zmq 25.1.1; -----; IPython 8.16.1; jupyter_client 8.3.1; jupyter_core 5.3.2; jupyterlab 4.0.7; notebook 7.0.5; -----; Python 3.11.9 (tags/v3.11.9:de54cf5, Apr 2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]; Windows-10-10.0.22621-SP0; -----; Session information updated at 2024-04-23 21:26. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:6772,simpl,simplejson,6772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['simpl'],['simplejson']
Usability,210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre-commit 3.7.0 pypi_0 pypi; profimp 0.1.0 pypi_0 pypi; pyarrow 15.0.2 pypi_0 pypi; pynndescent 0.5.12 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 7.4.4 pypi_0 pypi; pytest-cov 5.0.0 pypi_0 pypi; pytest-mock 3.14.0 pypi_0 pypi; pytest-nunit 1.0.7 pypi_0 pypi; pytest-xdist 3.5.0 pypi_0 pypi; python 3.12.2 hab00c5b_0_cpython conda-forge; python-dateutil 2.9.0.post0 pypi_0 pypi; pytz 2024.1 pypi_0 pypi; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h8228510_1 conda-forge; scanpy 1.10.0rc2.dev33+g9c8c095d pypi_0 pypi; scikit-image 0.22.0 pypi_0 pypi; scikit-learn 1.4.1.post1 pypi_0 pypi; scipy 1.13.0 pypi_0 pypi; seaborn 0.13.2 pypi_0 pypi; session-info 1.0.0 pypi_0 pypi; setuptools 69.2.0 pyhd8ed1ab_0 conda-forge; setuptools-scm 8.0.4 pypi_0 pypi; six 1.16.0 pypi_0 pypi; statsmodels 0.14.1 pypi_0 pypi; stdlib-list 0.10.0 pypi_0 pypi; texttable 1.7.0 pypi_0 pypi; threadpoolctl 3.4.0 pypi_0 pypi; tifffile 2024.2.12 pypi_0 pypi; tk 8.6.13 noxft_h4845f30_101 conda-forge; toolz 0.12.1 pypi_0 pypi; tqdm 4.66.2 pypi_0 pypi; typing-extensions 4.11.0 pypi_0 pypi; tzdata 2024.1 pypi_0 pypi; umap-learn 0.5.6 pypi_0 pypi; virtualenv 20.25.1 pypi_0 pypi; wheel 0.43.0 pyhd8ed1ab_1 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zarr 2.17.2 pypi_0 pypi; ```. </details>. <details>; <summary> My failing env </summary>. ```; # packages in environment at /mnt/workspace/mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-comp,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:28044,learn,learn,28044,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['learn'],['learn']
Usability,210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre-commit 3.7.0 pypi_0 pypi; profimp 0.1.0 pypi_0 pypi; pyarrow 15.0.2 pypi_0 pypi; pynndescent 0.5.12 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 8.1.1 pypi_0 pypi; pytest-cov 5.0.0 pypi_0 pypi; pytest-mock 3.14.0 pypi_0 pypi; pytest-nunit 1.0.7 pypi_0 pypi; pytest-xdist 3.5.0 pypi_0 pypi; python 3.12.2 hab00c5b_0_cpython conda-forge; python-dateutil 2.9.0.post0 pypi_0 pypi; pytz 2024.1 pypi_0 pypi; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h8228510_1 conda-forge; scanpy 1.10.0rc2.dev33+g9c8c095d pypi_0 pypi; scikit-image 0.22.0 pypi_0 pypi; scikit-learn 1.4.1.post1 pypi_0 pypi; scipy 1.13.0 pypi_0 pypi; seaborn 0.13.2 pypi_0 pypi; session-info 1.0.0 pypi_0 pypi; setuptools 69.2.0 pyhd8ed1ab_0 conda-forge; setuptools-scm 8.0.4 pypi_0 pypi; six 1.16.0 pypi_0 pypi; statsmodels 0.14.1 pypi_0 pypi; stdlib-list 0.10.0 pypi_0 pypi; texttable 1.7.0 pypi_0 pypi; threadpoolctl 3.4.0 pypi_0 pypi; tifffile 2024.2.12 pypi_0 pypi; tk 8.6.13 noxft_h4845f30_101 conda-forge; toolz 0.12.1 pypi_0 pypi; tqdm 4.66.2 pypi_0 pypi; typing-extensions 4.11.0 pypi_0 pypi; tzdata 2024.1 pypi_0 pypi; umap-learn 0.5.6 pypi_0 pypi; virtualenv 20.25.1 pypi_0 pypi; wheel 0.43.0 pyhd8ed1ab_1 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zarr 2.17.2 pypi_0 pypi; ```. </details>. Luke's environment: MacOS Ventura 13.4.1. Intel MacBook pro. <details>; <summary> Luke's failing env </summary>. ```; # packages in environment at /Users/luke.zappia/miniconda3/envs/scanpy-dev:; #; # Name Version Build Channel; anndata 0.10.6 pypi_0 pypi; array-api-compat 1.4.1 pyp,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:31409,learn,learn,31409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['learn'],['learn']
Usability,3.0; pyerfa 2.0.0.3; pyflakes 3.0.1; Pygments 2.16.1; PyJWT 2.8.0; pylint 2.17.5; pylint-venv 3.0.2; pyls-spyder 0.4.0; pynndescent 0.5.10; pyodbc 4.0.39; pyOpenSSL 23.2.0; pyparsing 3.1.1; PyQt5-sip 12.11.0; PySocks 1.7.1; pytest 7.4.2; python-dateutil 2.8.2; python-dotenv 1.0.0; python-json-logger 2.0.7; python-lsp-black 1.3.0; python-lsp-jsonrpc 1.1.2; python-lsp-server 1.7.2; python-slugify 8.0.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz_comms 3.0.0; PyWavelets 1.4.1; pyxdg 0.28; PyYAML 6.0.1; pyzmq 25.1.1; QDarkStyle 3.1; qstylizer 0.2.2; QtAwesome 1.2.3; qtconsole 5.4.4; QtPy 2.4.0; queuelib 1.6.2; referencing 0.30.2; regex 2023.10.3; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; reretry 0.11.8; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.6.0; tokenizers 0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:9020,learn,learn,9020,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['learn'],['learn']
Usability,"3.4; six | 1.16.0 | six | 1.16.0 | six | 1.16.0; sniffio | 1.2.0 | sniffio | 1.2.0 | sniffio | 1.2.0; sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0; statsmodels | 0.13.1 | statsmodels | 0.13.1 | statsmodels | 0.13.1; stdlib-list | 0.8.0 | stdlib-list | 0.8.0 | stdlib-list | 0.8.0; tables | 3.6.1 | tables | 3.6.1 | tables | 3.6.1; tblib | 1.7.0 | tblib | 1.7.0 | tblib | 1.7.0; terminado | 0.9.4 | terminado | 0.9.4 | terminado | 0.9.4; testpath | 0.5.0 | testpath | 0.5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Exc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:14616,learn,learn,14616,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['learn'],['learn']
Usability,"3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.con",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1825,learn,learn,1825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['learn'],['learn']
Usability,"3\lib\site-packages\numba\compiler.py in _compile_core(self); 372 if is_final_pipeline:; --> 373 raise e; 374 else:. ~\anaconda3\lib\site-packages\numba\compiler.py in _compile_core(self); 363 try:; --> 364 pm.run(self.state); 365 if self.state.cr is not None:. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in run(self, state); 346 patched_exception = self._patch_error(msg, e); --> 347 raise patched_exception; 348 . ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in run(self, state); 337 if isinstance(pass_inst, CompilerPass):; --> 338 self._runPass(idx, pass_inst, state); 339 else:. ~\anaconda3\lib\site-packages\numba\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in _runPass(self, index, pss, internal_state); 301 with SimpleTimer() as pass_time:; --> 302 mutated |= check(pss.run_pass, internal_state); 303 with SimpleTimer() as finalize_time:. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in check(func, compiler_state); 274 def check(func, compiler_state):; --> 275 mangled = func(compiler_state); 276 if mangled not in (True, False):. ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 406 # TODO: Pull this out into the pipeline; --> 407 NativeLowering().run_pass(state); 408 lowered = state['cr']. ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 348 metadata=metadata); --> 349 lower.lower(); 350 if not flags.no_cpython_wrapper:. ~\anaconda3\lib\site-packages\numba\lowering.py in lower(self); 231 # Materialize LLVM Module; --> 232 self.library.add_ir_module(self.module); 233 . ~\anaconda3\lib\site-packages\numba\targets\codegen.py in add_ir_module(self, ir_module); 200 ir = cgutils.normalize_ir_text(str(ir_module)); --> 201 ll_module = ll.parse_assembly(ir); 202 ll_module.name = ir_module.name. ~\anaconda3\lib\site-packages\ll",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:3792,Simpl,SimpleTimer,3792,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['Simpl'],['SimpleTimer']
Usability,3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi; tbb 2020.3 hfd86e86_0 ; tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:17893,learn,learn,17893,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['learn'],['learn']
Usability,"41 def plot(self, ax):; 1042 """"""Make the violin plot.""""""; -> 1043 self.draw_violins(ax); 1044 self.annotate_axes(ax); 1045 if self.orient == ""h"":. File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:761, in _ViolinPlotter.draw_violins(self, ax); 759 def draw_violins(self, ax):; 760 """"""Draw the violins onto `ax`.""""""; --> 761 fill_func = ax.fill_betweenx if self.orient == ""v"" else ax.fill_between; 762 for i, group_data in enumerate(self.plot_data):; 764 kws = dict(edgecolor=self.gray, linewidth=self.linewidth). AttributeError: 'numpy.ndarray' object has no attribute 'fill_betweenx'; ```. #### Option 2: group with two keys, passing first of two axes. ```python; import scanpy as sc; import matplotlib.pyplot as plt; adata = sc.datasets.pbmc3k(); adata.obs['group'] = adata.obs.index.to_series().str.startswith(""A"").astype(str); fig, axes = plt.subplots(1, 2); sc.pl.violin(adata2, keys=['CD8A', 'CD8B'], groupby=""group"", ax=axes[0]); ```. No traceback, but the second axis is simply not plotted. <img width=""388"" alt=""image"" src=""https://user-images.githubusercontent.com/84813314/153453540-76f48a6b-8d22-40bd-86fe-435d0878deb3.png"">. #### Option 3: group with two keys, passing one axis. ```python; import scanpy as sc; import matplotlib.pyplot as plt; adata = sc.datasets.pbmc3k(); adata.obs['group'] = adata.obs.index.to_series().str.startswith(""A"").astype(str); fig, ax = plt.subplots(); sc.pl.violin(adata, keys=['CD8A', 'CD8B'], groupby=""group"", ax=ax); ```. No traceback, even though this should error. Plots just the first of the two keys. <img width=""377"" alt=""image"" src=""https://user-images.githubusercontent.com/84813314/153453748-b402e8f7-9ac1-4fd8-b8ce-682cd25ea082.png"">. #### Versions. <details>. ```; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.4.0; anndata 0.7.8; asttokens NA; attr 21.2.0; backcall 0.2.0; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2136:3464,simpl,simply,3464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2136,1,['simpl'],['simply']
Usability,"435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html); - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors); - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors); - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>; <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build; --- | --- | --- | --- | --- | ---; [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow; [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519:1161,simpl,simply,1161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519,1,['simpl'],['simply']
Usability,"45f6> in <module>; ----> 1 adata_mnn, _, _ = sc.external.pp.mnn_correct(*adata_list, batch_key=""sample""). /projects/da_workspace/Users/amoussavi/Software/Anaconda_python3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 152 save_raw=save_raw,; 153 n_jobs=n_jobs,; --> 154 **kwargs,; 155 ); 156 return datas, mnn_list, angle_list. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,; 125 compute_angle=compute_angle, mnn_order=mnn_order,; --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs); 127 print('Packing AnnData object...'); 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 180 print(' Computing correction vectors...'); 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,; --> 182 new_batch_in, sigma); 183 if not same_set:; 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type; ```. #### Versions. <details>. scanpy==1.5.1 anndata==0.7.1 umap==0.4.6 numpy==1.19.1 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.1. numba==0.50.1; llvmlite==0.33.0+1.g022ab0f. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1367:3056,learn,learn,3056,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367,1,['learn'],['learn']
Usability,5d3f_0 ; r-utf8 1.1.4 r36h6115d3f_0 ; r-uuid 0.1_2 r36h6115d3f_4 ; r-viridislite 0.3.0 r36h6115d3f_0 ; r-whisker 0.3_2 r36h6115d3f_4 ; r-withr 2.1.2 r36h6115d3f_0 ; r-xfun 0.6 r36h6115d3f_0 ; r-xml2 1.2.0 r36h6115d3f_0 ; r-xtable 1.8_4 r36h6115d3f_0 ; r-xts 0.11_2 r36h6115d3f_0 ; r-yaml 2.2.0 r36h6115d3f_0 ; r-zoo 1.8_5 r36h6115d3f_0 ; referencing 0.33.0 pypi_0 pypi; requests 2.31.0 py311haa95532_1 ; rfc3339-validator 0.1.4 py311haa95532_0 ; rfc3986-validator 0.1.1 py311haa95532_0 ; rpds-py 0.18.0 pypi_0 pypi; scanpy 1.10.0 pypi_0 pypi; scikit-image 0.22.0 pypi_0 pypi; scikit-learn 1.4.1.post1 pypi_0 pypi; scikit-misc 0.3.1 pypi_0 pypi; scipy 1.12.0 pypi_0 pypi; seaborn 0.13.2 pypi_0 pypi; send2trash 1.8.2 py311haa95532_0 ; session-info 1.0.0 pypi_0 pypi; setuptools 68.2.2 py311haa95532_0 ; six 1.16.0 pyhd3eb1b0_1 ; sniffio 1.3.1 pypi_0 pypi; soupsieve 2.5 py311haa95532_0 ; sqlite 3.41.2 h2bbff1b_0 ; stack-data 0.6.3 pypi_0 pypi; stack_data 0.2.0 pyhd3eb1b0_0 ; statsmodels 0.14.1 pypi_0 pypi; stdlib-list 0.10.0 pypi_0 pypi; tenacity 8.2.3 pypi_0 pypi; terminado 0.18.1 pypi_0 pypi; texttable 1.7.0 pypi_0 pypi; threadpoolctl 3.4.0 pypi_0 pypi; tifffile 2024.2.12 pypi_0 pypi; tinycss2 1.2.1 py311haa95532_0 ; tk 8.6.12 h2bbff1b_0 ; tornado 6.4 pypi_0 pypi; tqdm 4.66.2 pypi_0 pypi; traitlets 5.14.2 pypi_0 pypi; types-python-dateutil 2.9.0.20240315 pypi_0 pypi; typing-extensions 4.9.0 py311haa95532_1 ; typing_extensions 4.9.0 py311haa95532_1 ; tzdata 2024.1 pypi_0 pypi; umap-learn 0.5.5 pypi_0 pypi; uri-template 1.3.0 pypi_0 pypi; urllib3 2.2.1 pypi_0 pypi; vc 14.2 h21ff451_1 ; vs2015_runtime 14.27.29016 h5e58377_2 ; wcwidth 0.2.13 pypi_0 pypi; webcolors 1.13 pypi_0 pypi; webencodings 0.5.1 pypi_0 pypi; websocket-client 1.7.0 pypi_0 pypi; wheel 0.41.2 py311haa95532_0 ; widgetsnbextension 4.0.10 pypi_0 pypi; win_inet_pton 1.1.0 py311haa95532_0 ; winpty 0.4.3 4 ; xz 5.4.6 h8cc25b3_0 ; yaml 0.2.5 he774522_0 ; zeromq 4.3.5 hd77b12b_0 ; zlib 1.2.13 h8cc25b3_0 ; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:14724,learn,learn,14724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['learn'],['learn']
Usability,6115d3f_0 ; r-spatial 7.3_11 r36h6115d3f_4 ; r-squarem 2017.10_1 r36h6115d3f_0 ; r-stringi 1.4.3 r36h6115d3f_0 ; r-stringr 1.4.0 r36h6115d3f_0 ; r-survival 2.44_1.1 r36h6115d3f_0 ; r-sys 3.2 r36h6115d3f_0 ; r-tibble 2.1.1 r36h6115d3f_0 ; r-tidyr 0.8.3 r36h6115d3f_0 ; r-tidyselect 0.2.5 r36h6115d3f_0 ; r-tidyverse 1.2.1 r36h6115d3f_0 ; r-timedate 3043.102 r36h6115d3f_0 ; r-tinytex 0.12 r36h6115d3f_0 ; r-ttr 0.23_4 r36h6115d3f_0 ; r-utf8 1.1.4 r36h6115d3f_0 ; r-uuid 0.1_2 r36h6115d3f_4 ; r-viridislite 0.3.0 r36h6115d3f_0 ; r-whisker 0.3_2 r36h6115d3f_4 ; r-withr 2.1.2 r36h6115d3f_0 ; r-xfun 0.6 r36h6115d3f_0 ; r-xml2 1.2.0 r36h6115d3f_0 ; r-xtable 1.8_4 r36h6115d3f_0 ; r-xts 0.11_2 r36h6115d3f_0 ; r-yaml 2.2.0 r36h6115d3f_0 ; r-zoo 1.8_5 r36h6115d3f_0 ; referencing 0.33.0 pypi_0 pypi; requests 2.31.0 py311haa95532_1 ; rfc3339-validator 0.1.4 py311haa95532_0 ; rfc3986-validator 0.1.1 py311haa95532_0 ; rpds-py 0.18.0 pypi_0 pypi; scanpy 1.10.0 pypi_0 pypi; scikit-image 0.22.0 pypi_0 pypi; scikit-learn 1.4.1.post1 pypi_0 pypi; scikit-misc 0.3.1 pypi_0 pypi; scipy 1.12.0 pypi_0 pypi; seaborn 0.13.2 pypi_0 pypi; send2trash 1.8.2 py311haa95532_0 ; session-info 1.0.0 pypi_0 pypi; setuptools 68.2.2 py311haa95532_0 ; six 1.16.0 pyhd3eb1b0_1 ; sniffio 1.3.1 pypi_0 pypi; soupsieve 2.5 py311haa95532_0 ; sqlite 3.41.2 h2bbff1b_0 ; stack-data 0.6.3 pypi_0 pypi; stack_data 0.2.0 pyhd3eb1b0_0 ; statsmodels 0.14.1 pypi_0 pypi; stdlib-list 0.10.0 pypi_0 pypi; tenacity 8.2.3 pypi_0 pypi; terminado 0.18.1 pypi_0 pypi; texttable 1.7.0 pypi_0 pypi; threadpoolctl 3.4.0 pypi_0 pypi; tifffile 2024.2.12 pypi_0 pypi; tinycss2 1.2.1 py311haa95532_0 ; tk 8.6.12 h2bbff1b_0 ; tornado 6.4 pypi_0 pypi; tqdm 4.66.2 pypi_0 pypi; traitlets 5.14.2 pypi_0 pypi; types-python-dateutil 2.9.0.20240315 pypi_0 pypi; typing-extensions 4.9.0 py311haa95532_1 ; typing_extensions 4.9.0 py311haa95532_1 ; tzdata 2024.1 pypi_0 pypi; umap-learn 0.5.5 pypi_0 pypi; uri-template 1.3.0 pypi_0 pypi; urllib3 2.2.1 pypi_0 pypi,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:13813,learn,learn,13813,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['learn'],['learn']
Usability,"68ed6b771b7377e89acb47cc7e5aa9f21a574/doc/api.rst), [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_range_finder.html#sklearn.utils.extmath.randomized_range_finder), and [seaborn](https://github.com/mwaskom/seaborn/blob/ba4bd0fa0a90b2bd00cb62c2b4a5e38013a73ac6/doc/api.rst) do. Now `make clean` also deletes all auto generated `rst` files. This also dramatically simplifies our `.gitignore`. ## Consolidation. I've also consolidated the managed api docs to a smaller number of files. Namely, instead of `api/*.rst` there's now just `api.rst`. Instead of `external/*.rst` there's just `external.rst`. ## Plotting functions API docs. In the current api docs, some plotting functions have a plot displayed inline instead of a one line description. This is nice because it provides a visual reference for what the plotting function does. It's less nice because:. * It takes up a huge amount of space; * It takes up a different amount of space for each function; * These plots are manually generated, and are often quite out of date. <details>; <summary> example </summary>. <img width=""899"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111894736-5657fd80-8a61-11eb-8077-97bcb5c54765.png"">. </details>. I think the reference docs should be handled in the same way for each of the modules. Example plots make more sense in the user guide/ tutorials and within each functions documentation (#1664). I'd like to go further with this by moving the plotting tutorial's content to the user guide, so we can have more in depth linking to different sections. Additional benefits here include plots being generated with each doc build and having links from functions in the guide to their api docs. ## TODO. - [x] [Redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html) (redirects need to be added on readthedocs so old links won't fail) **added**; - [x] Decide where module docs live, as doc-string, or as rst. **rst**",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1753:2665,guid,guide,2665,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753,3,['guid'],['guide']
Usability,"6a4308_0; - sip=4.19.8=py37hf484d3e_0; - six=1.15.0=py37h06a4308_0; - sqlite=3.33.0=h62c20be_0; - terminado=0.9.2=py37h06a4308_0; - testpath=0.4.4=pyhd3eb1b0_0; - tk=8.6.10=hbc83047_0; - tornado=6.1=py37h27cfd23_0; - traitlets=5.0.5=pyhd3eb1b0_0; - wcwidth=0.2.5=py_0; - webencodings=0.5.1=py37_1; - wheel=0.36.2=pyhd3eb1b0_0; - widgetsnbextension=3.5.1=py37_0; - xz=5.2.5=h7b6447c_0; - zeromq=4.3.3=he6710b0_3; - zipp=3.4.0=pyhd3eb1b0_0; - zlib=1.2.11=h7b6447c_3; - pip:; - anndata==0.7.5; - cached-property==1.5.2; - click==7.1.2; - cycler==0.10.0; - get-version==2.1; - h5py==3.1.0; - importlib-metadata==3.4.0; - joblib==1.0.0; - kiwisolver==1.3.1; - legacy-api-wrap==1.2; - leidenalg==0.8.3; - llvmlite==0.35.0; - loompy==3.0.6; - louvain==0.7.0; - matplotlib==3.3.4; - natsort==7.1.1; - networkx==2.5; - numba==0.52.0; - numexpr==2.7.2; - numpy==1.20.0; - numpy-groupies==0.9.13; - pandas==1.2.1; - patsy==0.5.1; - pillow==8.1.0; - python-igraph==0.8.3; - pytz==2021.1; - scanpy==1.6.1; - scikit-learn==0.24.1; - scipy==1.6.0; - scvelo==0.2.2; - seaborn==0.11.1; - setuptools-scm==5.0.1; - sinfo==0.3.1; - statsmodels==0.12.1; - stdlib-list==0.8.0; - tables==3.6.1; - texttable==1.6.3; - threadpoolctl==2.1.0; - tqdm==4.56.0; - typing-extensions==3.7.4.3; - umap-learn==0.4.6; ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. ; If you need access to the data and the container please contact me and I will make it available to you.; The data is already at the ICB cluster. Code:. ```; from os import path; import numpy as np; import matplotlib.pyplot as plt; import scanpy as sc; import scanpy.external as sce; from os import listdir; import pandas as pd; import seaborn as sb; import datetime, time; import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:4957,learn,learn,4957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['learn'],['learn']
Usability,"8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is; - a) only a question currently open (I am really not particularly an expert in R with limited bandwidth to check things there so waiting for their answer).; - b) Even if true, this does not affect our examples here. It comes into play when we try to further be as consistent with Seurat and their textual description as possible. Yes, this is confusing :) Hope I did not confuse something myself here, checked but consider it a to-the-best-of-my-current-knowledge guidance towards a working solution for you rather than peer-reviewed ground truth ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:2529,guid,guidance,2529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,2,['guid'],['guidance']
Usability,9hf0e4da2_0 ; scanpy 1.8.1 pypi_0 pypi; scikit-learn 1.1.2 py39h598ef33_0 conda-forge; scikit-misc 0.3.1 pypi_0 pypi; scipy 1.13.1 py39h3d5391c_0 conda-forge; scvi-tools 0.20.3 pyhd8ed1ab_0 conda-forge; seaborn 0.12.2 hd8ed1ab_0 conda-forge; seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge; send2trash 1.8.2 py39hca03da5_0 ; session-info 1.0.0 pyhd8ed1ab_0 conda-forge; setuptools 69.5.1 py39hca03da5_0 ; sinfo 0.3.4 pypi_0 pypi; sip 6.7.12 py39h313beb8_0 ; six 1.16.0 pyh6c4a22f_0 conda-forge; sniffio 1.3.0 py39hca03da5_0 ; soupsieve 2.5 py39hca03da5_0 ; sqlite 3.45.3 h80987f9_0 ; stack_data 0.2.0 pyhd3eb1b0_0 ; statsmodels 0.14.2 py39h161d348_0 conda-forge; stdlib-list 0.10.0 pyhd8ed1ab_0 conda-forge; tbb 2021.8.0 h48ca7d4_0 ; terminado 0.17.1 py39hca03da5_0 ; texttable 1.7.0 pyhd8ed1ab_0 conda-forge; threadpoolctl 3.5.0 pyhc1e730c_0 conda-forge; tinycss2 1.2.1 py39hca03da5_0 ; tk 8.6.14 h6ba3021_0 ; tomli 2.0.1 py39hca03da5_0 ; toolz 0.12.1 pyhd8ed1ab_0 conda-forge; torchmetrics 1.0.3 pyhd8ed1ab_0 conda-forge; tornado 6.4.1 py39h80987f9_0 ; tqdm 4.66.4 pyhd8ed1ab_0 conda-forge; traitlets 5.14.3 py39hca03da5_0 ; typing-extensions 4.12.2 hd8ed1ab_0 conda-forge; typing_extensions 4.12.2 pyha770c72_0 conda-forge; tzdata 2024a h04d1e81_0 ; umap-learn 0.5.6 pypi_0 pypi; unicodedata2 15.1.0 py39h0f82c59_0 conda-forge; urllib3 2.2.2 py39hca03da5_0 ; wcwidth 0.2.5 pyhd3eb1b0_0 ; webencodings 0.5.1 py39hca03da5_1 ; websocket-client 1.8.0 py39hca03da5_0 ; wheel 0.43.0 py39hca03da5_0 ; widgetsnbextension 4.0.10 py39hca03da5_0 ; xlrd 1.2.0 pyh9f0ad1d_1 conda-forge; xorg-libxau 1.0.11 hb547adb_0 conda-forge; xorg-libxdmcp 1.1.3 h27ca646_0 conda-forge; xz 5.4.6 h80987f9_1 ; yaml 0.2.5 h3422bc3_2 conda-forge; zeromq 4.3.5 h313beb8_0 ; zipp 3.19.2 pyhd8ed1ab_0 conda-forge; zlib 1.2.13 hfb2fe0b_6 conda-forge; zstd 1.5.6 hb46c0d2_0 conda-forge; ```; Im so sorry somethine else is wrong so scanpy.logging.print_versions() doesn't work at the moment. I'll fix it and update the post; </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:15688,learn,learn,15688,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['learn'],['learn']
Usability,::test_sparse_nanmean - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_sparse_vs_dense - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_deplete - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_npnanmean_vs_sparsemean - AttributeError: 'csr_matrix' object has no attribute 'A'; ```. ### Minimal code sample. ```python; pip install scipy==1.14.0rc1; pytest; ```. ### Error output. _No response_. ### Versions. <details>. ```; + anndata==0.11.0.dev116+g1bff5fb (from git+https://github.com/scverse/anndata@1bff5fbf0894185c0759b61d78c6df66d6dfeeba); + annoy==1.17.3; + anyio==4.4.0; + array-api-compat==1.7.1; + pillow==10.3.0; + platformdirs==4.2.2; + pluggy==1.5.0; + pre-commit==3.7.1; + profimp==0.1.0; + psutil==5.9.8; + pyarrow==16.1.0; + pygments==2.18.0; + pygsp==0.5.1; + pynndescent==0.5.12; + pyparsing==3.1.2; + pytest==8.2.1; + pytest-cov==5.0.0; + pytest-memray==1.6.0; + pytest-mock==3.14.0; + pytest-nunit==1.0.7; + pytest-xdist==3.6.1; + python-dateutil==2.9.0.post0; + pytz==2024.1; + pyyaml==6.0.1; + rich==13.7.1; + scanorama==1.7.4; + scanpy==1.10.2.dev25+gf5a62eee (from file:///home/vsts/work/1/s); + scikit-image==0.23.2; + scikit-learn==1.5.0; + scikit-misc==0.3.1; + scipy==1.14.0rc1; + scprep==1.1.0; + seaborn==0.13.2; + session-info==1.0.0; + setuptools==70.0.0; + setuptools-scm==8.1.0; + six==1.16.0; + sniffio==1.3.1; + sortedcontainers==2.4.0; + sparse==0.16.0a7; + statsmodels==0.14.2; + stdlib-list==0.10.0; + tasklogger==1.2.0; + tblib==3.0.0; + texttable==1.7.0; + textual==0.63.6; + threadpoolctl==3.5.0; + tifffile==2024.5.22; + toolz==0.12.1; + tornado==6.4; + tqdm==4.66.4; + typing-extensions==4.12.0; + tzdata==2024.1; + uc-micro-py==1.0.3; + umap-learn==0.5.6; + urllib3==2.2.1; + virtualenv==20.26.2; + wrapt==1.16.0; + zarr==2.18.2; + zict==3.0.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3083:2511,learn,learn,2511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3083,2,['learn'],['learn']
Usability,":; 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 339 (self.pipeline_name, pass_desc); 340 patched_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 ; 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 330 pass_inst = _pass_registry.get(pss).pass_inst; 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:; 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 289 mutated |= check(pss.run_initialization, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:; 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 262 ; 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):; 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 90 % (state.func_id.func_name,)):; 91 # Type inference; ---> 92 typemap, return_type, calltypes = type_inference_stage(; 93 state.typingctx,; 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors); 68 ; 69 infer.build_constraint(); ---> 70 infer.propagate(raise_errors=raise_errors); 71 typemap, restype, calltypes = infer.uni",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:6848,Simpl,SimpleTimer,6848,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['Simpl'],['SimpleTimer']
Usability,"; certifi 2021.10.08; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.4.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fastjsonschema NA; fsspec 2021.08.1; h5py 3.3.0; idna 3.2; igraph 0.10.2; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.1.2; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.23.3; jupyterlab_server 2.8.2; kiwisolver 1.3.1; leidenalg 0.9.0; llvmlite 0.37.0; markupsafe 2.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.2.0; nbclassic 0.4.8; nbformat 5.7.0; nbinom_ufunc NA; notebook_shim NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.11.0; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.4.1; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyexpat NA; pygments 2.10.0; pynndescent 0.5.8; pyparsing 3.0.4; pyrsistent NA; pytz 2021.3; pywt 1.1.1; requests 2.26.0; scipy 1.7.1; scrublet NA; seaborn 0.11.2; send2trash NA; session_info 1.0.0; settings NA; simplejson 3.17.6; six 1.16.0; skimage 0.18.3; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tblib 1.7.0; terminado 0.9.4; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.62.3; traitlets 5.1.0; typing_extensions NA; umap 0.5.3; urllib3 1.26.7; wcwidth 0.2.5; websocket 1.4.2; yaml 6.0; zmq 22.2.1; zope NA; -----; IPython 7.29.0; jupyter_client 6.1.12; jupyter_core 4.8.1; jupyterlab 3.2.1; notebook 6.4.5; -----; Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]; Linux-4.18.0-305.45.1.el8_4.x86_64-x86_64-with-glibc2.28; -----; Session information updated at 2022-12-04. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2377:6094,simpl,simplejson,6094,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2377,1,['simpl'],['simplejson']
Usability,"; charset_normalizer 2.0.7; cloudpickle 2.0.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2021.10.0; dateutil 2.8.0; debugpy 1.4.1; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; fasteners NA; fsspec 2021.10.1; gi 3.36.0; gio NA; glib NA; gobject NA; gtk NA; h5py 3.5.0; idna 2.8; igraph 0.9.9; ipykernel 6.3.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 2.10.1; joblib 1.0.1; json5 NA; jsonpointer 2.0; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.2; kiwisolver 1.3.2; leidenalg 0.8.8; llvmlite 0.37.0; louvain 0.7.1; markupsafe 1.1.0; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; netifaces 0.10.4; numba 0.54.0; numcodecs 0.9.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.2; parso 0.8.2; patsy 0.5.1; pexpect 4.6.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.4.1; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pynndescent 0.5.5; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.26.0; roifile 2021.6.6; scipy 1.7.1; seaborn 0.11.2; send2trash NA; shapely 1.7.1; simplejson 3.16.0; sitecustomize NA; six 1.14.0; sklearn 1.0.2; sniffio 1.2.0; sparse 0.13.0; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; terminado 0.11.1; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.11.1; toolz 0.11.1; torch 1.6.0; tornado 6.1; tqdm 4.62.2; traitlets 5.1.0; typing_extensions NA; umap 0.5.1; urllib3 1.26.6; wcwidth 0.2.5; websocket 1.2.1; yaml 6.0; zarr 2.10.2; zmq 22.2.1; zope NA; -----; IPython 7.27.0; jupyter_client 7.0.2; jupyter_core 4.7.1; jupyterlab 3.1.12; notebook 6.4.3; -----; Python 3.8.10 (default, Sep 28 2021, 16:10:42) [GCC 9.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.29; 8 logical CPU cores, x86_64. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2244:3386,simpl,simplejson,3386,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2244,1,['simpl'],['simplejson']
Usability,; packaging 24.0; pandas 1.5.3; pandocfilters 1.5.0; panel 1.3.8; param 2.0.2; parso 0.8.3; partd 1.4.1; patsy 0.5.6; pexpect 4.9.0; pickleshare 0.7.5; pillow 10.2.0; pip 24.0; pkgutil_resolve_name 1.3.10; platformdirs 4.2.0; pooch 1.8.1; prometheus_client 0.20.0; prompt-toolkit 3.0.42; protobuf 4.25.3; psutil 5.9.8; ptxcompiler 0.8.1; ptyprocess 0.7.0; pure-eval 0.2.2; pyarrow 14.0.2; pyarrow-hotfix 0.6; pycparser 2.21; pyct 0.5.0; pydantic 1.10.14; pyee 8.1.0; Pygments 2.17.2; pylibcugraph 24.2.0; pylibraft 24.2.0; pynndescent 0.5.11; pynvml 11.4.1; pyparsing 3.1.2; pyppeteer 1.0.2; pyproj 3.6.1; PySocks 1.7.1; python-dateutil 2.9.0; python-json-logger 2.0.7; pytometry 0.1.4; pytz 2024.1; pyviz_comms 3.0.1; PyWavelets 1.4.1; PyYAML 6.0.1; pyzmq 25.1.2; raft-dask 24.2.0; rapids_singlecell 0.9.6; readfcs 1.1.7; referencing 0.33.0; requests 2.31.0; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.7.1; rmm 24.2.0; rpds-py 0.18.0; Rtree 1.2.0; scanpy 1.10.0rc2; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; seaborn 0.13.2; Send2Trash 1.8.2; session-info 1.0.0; setuptools 69.1.1; shapely 2.0.3; simpervisor 1.0.0; single_cell_helper 0.0.1 ; six 1.16.0; sniffio 1.3.1; sortedcontainers 2.4.0; soupsieve 2.5; stack-data 0.6.2; statsmodels 0.14.1; stdlib-list 0.10.0; streamz 0.6.4; tblib 3.0.0; terminado 0.18.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tinycss2 1.2.1; tomli 2.0.1; toolz 0.12.1; tornado 6.4; tqdm 4.66.2; traitlets 5.14.1; treelite 4.0.0; types-python-dateutil 2.8.19.20240311; typing_extensions 4.10.0; typing-utils 0.1.0; uc-micro-py 1.0.3; ucx-py 0.36.0; umap-learn 0.5.5; unicodedata2 15.1.0; uri-template 1.3.0; urllib3 1.26.18; wcwidth 0.2.13; webcolors 1.13; webencodings 0.5.1; websocket-client 1.7.0; websockets 10.4; wget 3.2; wheel 0.42.0; widgetsnbextension 4.0.10; wrapt 1.16.0; xarray 2024.2.0; xgboost 2.0.3; xyzservices 2023.10.1; yarl 1.9.4; zarr 2.17.1; zict 3.0.0; zipp 3.17.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:5510,learn,learn,5510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,2,['learn'],['learn']
Usability,"; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/153:1260,learn,learn,1260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153,1,['learn'],['learn']
Usability,"; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; pynndescent 0.5.11 pyhca7485f_0 conda-forge; pyparsing 3.1.1 pyhd8ed1ab_0 conda-forge; python 3.11.7 hab00c5b_1_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.4 pyhd8ed1ab_0 conda-forge; python_abi 3.11 4_cp311 conda-forge; pytz 2023.3.post1 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; scanpy 1.9.6 pyhd8ed1ab_1 conda-forge; scikit-learn 1.3.2 py311hc009520_2 conda-forge; scipy 1.11.4 py311h64a7726_0 conda-forge; seaborn 0.13.1 hd8ed1ab_0 conda-forge; seaborn-base 0.13.1 pyhd8ed1ab_0 conda-forge; session-info 1.0.0 pyhd8ed1ab_0 conda-forge; setuptools 69.0.3 pyhd8ed1ab_0 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; statsmodels 0.14.1 py311h1f0f07a_0 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; tbb 2021.11.0 h00ab1b0_0 conda-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.13 noxft_h4845f30_101 conda-forge; tqdm 4.66.1 pyhd8ed1ab_0 conda-forge; tzdata 2023d h0c530f3_0 conda-forge; umap-learn 0.5.5 py311h38be061_0 conda-forge; wheel 0.42.0 pyhd8ed1ab_0 conda-forge; xorg-libxau 1.0.11 hd590300_0 conda-forge; xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zstd 1.5.5 hfc55251_0 conda-forge; ```. 2) I imported the scanpy, seaborn, pandas, numpy and matplotlib libraries. Then I called the `read_10x_mtx()` function. The code is given below. ```pycon; >>> import scanpy as sc; >>> import pandas as pd; >>> import numpy as np; >>> import matplotlib; >>> import seaborn as sns; >>> !ls -lh ./H004/; -rwxrwxrwx. 1 nikolay nikolay 49K Mar 25 2021 barcodes.tsv.gz; -rwxrwxrwx. 1 nikolay nikolay 424K Mar 25 2021 features.tsv.gz; -rwxrwxrwx. 1 nikolay nikolay 101M Mar 25 2021 matrix.mtx.gz; >>> adata = sc.read_10x_mtx(; ... './H004/', ; ... var_names='gene_symbols', ; ... cache=True); >>> adata.var_names_make_unique(); >>> print(adata.var); gene_ids feature_types; Gm26206 ENSMUSG00000064842 Gene Expression; Gm26206-1 ENSMUSG00",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:5295,learn,learn,5295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['learn'],['learn']
Usability,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8531,learn,learn,8531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,2,['learn'],['learn']
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1615:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1615,14,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Add pynndescent to `sc.logging.print_header()`. Close #1613,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1807:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1807,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Adress one point in #1664,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1810:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1810,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Fixes #1662,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1671:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1671,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. closes #1502 . ensure that `legend_loc=None` also removes continuous colorbars for scatterplots like umap,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. part of #1664; add inline examples for; - sc.pl.draw_graph; - sc.pl.heatmap; - sc.pl.dendrogram; - sc.pl.tsne; - sc.pl.diffmap,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1809:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1809,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Add violin plot examples according to https://github.com/theislab/scanpy/issues/1664,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1814:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1814,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Added links to dorothea/progeny in the ecosystem documentation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1767:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Inline documentation for the matrixplot. Addressing issue #1664,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1808:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1808,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1614:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614,2,['guid'],"['guide', 'guidelines']"
Usability,"<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; closses #1174 . hue key is now added to keys if hue is in kwds. before obs_tidy did not include this keyword, so this resulted in an ValueError. an alternative would be to add hue as a function parameter to `sc.pl.violin`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1822:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822,2,['guid'],"['guide', 'guidelines']"
Usability,<!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; ref #1664 sc.pl.pca_loadings,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1815:72,guid,guidelines,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1815,2,['guid'],"['guide', 'guidelines']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->. After I use function. ```py; adata = sc.read_visium(; './', count_file='V1_Human_Lymph_Node_filtered_feature_bc_matrix.h5',; genome=None, library_id=None, load_images=True,; ); ```. I use `sc.pp.calculate_qc_metrics(adata, qc_vars=[""mt""], inplace=True)` and got an error message:. ```pytb; RuntimeError Traceback (most recent call last); ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 744 try:; --> 745 yield; 746 except NumbaError as e:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 272 loc=self.loc, errcls_=defaulterrcls):; --> 273 self.lower_inst(inst); 274 self.post_block(block). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst); 485 if isinstance(inst, _class):; --> 486 func(self, inst); 487 return. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _lower_parfor_parallel(lowerer, parfor); 239 lowerer, parfor, typemap, typingctx, targetctx, flags, {},; --> 240 bool(alias_map), index_var_typ, parfor.races); 241 finally:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _create_gufunc_for_parfor_body(lowerer, parfor, typemap, typingctx, targetctx, flags, locals, has_aliases, index_var_typ, races); 1326 flags,; -> 1327 locals); 1328 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(typingctx, targetctx, func_ir, args, return_type, flags, locals, lifted, lifted_from, is_lifted_loop, library, pipeline_class); 666 return pipeline.compile_ir(func_ir=func_ir, lifted=lifted,; --> 667 lifted_from=lifted_from); 668 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(self, func_ir, lifted, lifted_from); 348 FixupArgs().run_pass(self.state); --> 349 return self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->. Hi,; I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; adata = sc.datasets.paul15(); sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.dpt(adata); sc.tl.paga(adata, groups='paul15_clusters'); sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-a9471349c389> in <module>; ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->. Ingest tries to search for the metric used when neighbors was called. When this information is not available it fails. Is there a workaround for this? . <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; adata = sc.datasets.pbmc68k_reduced(); adata_ref = sc.datasets.pbmc3k_processed(). var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. # add fake batch; adata_ref.obs['batch'] = pd.Categorical(np.random.choice(a=[0, 1, 2], size=adata_ref.shape[0])). sc.pp.pca(adata_ref); sc.external.pp.bbknn(adata_ref, batch_key='batch'); sc.tl.umap(adata_ref); sc.tl.ingest(adata, adata_ref, obs='louvain', embedding_method='umap'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; scanpy/scanpy/tools/_ingest.py in _init_neighbors(self, adata, neighbors_key); 283 dist_args = (); 284 ; --> 285 self._metric = neighbors['params']['metric']; 286 dist_func = named_distances[self._metric]; 287 . KeyError: 'metric'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.7.dev83+g5345a50.d20200506",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1201:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1201,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->. y-axis range of sc.pl.rank_genes_groups seems unnecessarily long:. ![image](https://user-images.githubusercontent.com/1140359/88754278-41bea200-d12c-11ea-9d8c-aeb013ea6a02.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc. sc.set_figure_params(dpi=72); adata = sc.datasets.paul15(); sc.pp.log1p(adata); sc.tl.rank_genes_groups(adata, groupby='paul15_clusters', method='t-test'); sc.pl.rank_genes_groups(adata, sharey=False); ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.5.2.dev52+g10937f5f anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1335:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1335,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; 'tuple' object has no attribute 'tocsr'. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; # neighborhood graph of cells (determine optimal number of PCs here); sc.pp.neighbors(adata, n_neighbors=15, n_pcs=30); # compute UMAP; sc.tl.umap(adata); # tSNE; tsne = TSNE( n_jobs=20 ); adata.obsm['X_tsne'] = tsne.fit_transform( adata.X ); adata.write( f_anndata_path ); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; computing neighbors; using 'X_pca' with n_pcs = 30; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-38-e2dd1fe70ab9> in <module>; 6 sc.settings.n_jobs = 15; 7 with parallel_backend('threading', n_jobs=20):; ----> 8 sc.pp.neighbors(adata, n_neighbors=15, n_pcs=30); 9 ; 10 #sc.settings.n_jobs = 15. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; (Python & GitHub novice here, apologies in advance.). Running through a tutorial using the 10xGenomics 3K PBMC dataset in Jupyter Notebook on Windows 10, caught an error at sc.pp.calculate_qc_metrics. Based on a quick look with my untrained eyes, this may not be a scanpy issue per se so much as an underlying data structure conflict issue in numba and/or llvmlite?. Trimmed down code I used to reach that point (the skipped steps, in ellipses, don't seem to be necessary, but I may still have a few extras there):. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; import pandas as pd; import numpy as np; import matplotlib.pyplot as plt. ... adata = sc.read_10x_mtx(""/PBMC_10X/""). ... adata_10x = sc.read_10x_mtx(""/PBMC_10X/""). ... sc.pp.calculate_qc_metrics(adata_10x, inplace = True); ```. That spat out:. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; RuntimeError Traceback (most recent call last); ~\anaconda3\lib\site-packages\numba\errors.py in new_error_context(fmt_, *args, **kwargs); 716 try:; --> 717 yield; 718 except NumbaError as e:. ~\anaconda3\lib\site-packages\numba\lowering.py in lower_block(self, block); 287 loc=self.loc, errcls_=defaulterrcls):; --> 288 self.lower_inst(inst); 289 self.post_block(block). ~\anaconda3\lib\site-packages\numba\lowering.py in lower_inst(self, inst); 475 if isinstance(inst, _class):; --> 476 func(self, inst); 477 return. ~\anaconda3\lib\site-packages\numba\npyufunc\parfor.py in _lower_parfor_parallel(lowerer, parfor); 240 lowerer, parfor, typemap, typingctx, targetctx, flags, {},; --> 241 bool(alias_map), index_var_typ, parfor.races); 242 numba.parfor.sequential_parfor_lowering = False. ~\anaconda3\lib\site-packages\numba\npyufunc\parfor.py in _create_gufunc_f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1136:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...AttributeError Traceback (most recent call last); <ipython-input-41-ed9365d2081e> in <module>; ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw); 637 else:; --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw); 639 if groupby is None:; 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 160 for k, l in zip(keys, lookup_keys):; 161 if not use_raw or k in adata.obs.columns:; --> 162 df[k] = adata.obs_vector(l, layer=layer); 163 else:; 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/942:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ... I am not sure it's bug or package error. I am getting Segmentation fault error with sc.pp.calculate_qc_metrics with my data. I tried running this on eg. data, and have the same error. I think this might be related to numba issue, but not sure. I did ran python debugger on the script, also attaching the output i got. ; [scanpylog.txt](https://github.com/theislab/scanpy/files/4567012/scanpylog.txt). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; #!/usr/bin/env python; import os, sys ; import scanpy as sc; import scanpy.external as sce; import scipy as sp; import numpy as np; import pandas as pd; os.getcwd(). #sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); #sc.logging.print_versions() #this also give segmentation fault error . #1102 external eg file ; #curl -o pbmc_1k_v2_filtered_feature_bc_matrix.h5 -O http://cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_1k_v2/pbmc_1k_v2_filtered_feature_bc_matrix.h5. #load file; ext_AD = sc.read_10x_h5('/home/pjb40/scratch/KimCarla_Timeseries_scRNAseq_lung_cancer_organoids_hbc03856_scrathDir/data/TimeSeries_epithelial_ScanpyNotebook/ext_data/pbmc_1k_v2_filtered_feature_bc_matrix.h5', gex_only = True). ext_AD.var_names_make_unique(). print(ext_AD). sc.pp.calculate_qc_metrics(ext_AD, inplace=True). print (ext_AD). ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""timeseriesScanpy.py"", line 65, in <module>; sc.pp.calculate_qc_metrics(ext_AD, inplace=True); File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 274, in calculate_qc_metrics; parallel=parallel; File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 102, in describe_obs; proportions = top_segment_proportions(X, percent_top, par",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ... I am running into an error when I try to create PAGA plots using the pl.paga() function. I get the error: ""AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike' "". After looking through matplotlib documentation is_numlike appears to be deprecated. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; import scanpy.external as sce; import pandas as pd; import numpy as np; import matplotlib as mpl; import matplotlib.pyplot as pl; from scipy.stats import mode; from collections import Counter; import loompy. sc.settings.verbosity = 3; sc.set_figure_params(color_map='viridis'); sc.logging.print_versions(). adata_sim = sc.tl.sim('krumsiek11'); adata_sim.var_names_make_unique(). sc.pp.neighbors(adata_sim, n_neighbors=7, n_pcs=20); sc.tl.louvain(adata_sim). sc.tl.paga(adata_sim); sc.pl.paga(adata_sim, color=['louvain'], edge_width_scale=0.2, threshold=0.2); ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-10-973f72fa2eb5> in <module>; ----> 1 sc.pl.paga(adata_sim, color=['louvain'], edge_width_scale=0.2, threshold=0.2). ~/.local/lib/python3.6/site-packages/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 541 single_component=single_component,; 542 arrowsize=arrowsize,; --> 543 pos=pos,; 54",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1094:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; .../usr/local/lib/python3.6/site-packages/joblib/externals/loky/backend/semaphore_tracker.py:198: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown; len(cache)). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...gene_trends = d.palantir.presults.compute_gene_trends(pr_res, ; ...: d.imp_df.iloc[:, 0:1000], ['RG']) ; ...: . ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...Segmentation fault (core dumped); ```; it made me out of the python environment.; #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; Scanpy version: 1.4.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1223:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1223,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ...; ![image](https://user-images.githubusercontent.com/39158851/82787725-a09c3c80-9e99-11ea-9a09-94e43c114185.png); with adata like this:; ![image](https://user-images.githubusercontent.com/39158851/82787817-d6412580-9e99-11ea-9fc6-2866402b668e.png). and adata.X:; ![image](https://user-images.githubusercontent.com/39158851/82787794-cc1f2700-9e99-11ea-9957-a1b37cbd7881.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; 1.5.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ...Trying to use `adata.write()` to save a results file - running into the same issue over and over. I tried uninstalling and reinstall both scanpy as well as h5py. . <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; results_file = 'NG2019_MCF10A2.h5ad'; adata.write(results_file); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; OSError: Unable to create link (name already exists). Above error raised while writing key 'genes' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'genes' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'raw/var' of <class 'h5py._hl.files.File'> from /.; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1275:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ...When run bbknn on adata which has been calculated the pca, umap, and leiden, the AttributeError shows 'tuple' object has no attribute 'tocsr'. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.umap(adata); ...; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:27); computing neighbors; using 'X_pca' with n_pcs = 50; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:24); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:01:27). %%time; sc.external.pp.bbknn(adata, batch_key='batch'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; computing batch balanced neighbors; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-9-9b24f504f73c> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', ""sc.external.pp.bbknn(adata, batch_key='batch')""). 6 frames; <decorator-gen-60> in time(self, line, cell, local_ns). <timed eval> in <module>(). /usr/local/lib/python3.6/dist-packages/bbknn/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 63 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 64 ; ---> 65 return distances, connectivities.tocsr(); 66 ; 67 def create_tree(data,approx,metric,use_faiss,n_trees):. AttributeError: 'tuple' object has no attribute 'tocsr'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.2 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1249:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1249,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ...when I did combat batch correction, it outputs errors. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; # ComBat batch correction; sc.pp.combat(adata, key='sample'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-5-350690ae55dc> in <module>; 1 # ComBat batch correction; ----> 2 sc.pp.combat(adata, key='sample'). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 266 denom = np.dot(dsq, np.ones((1, n_batches[j]))); 267 numer = np.array(bayesdata[batch_idxs] - np.dot(batch_design.loc[batch_idxs], gamma_star).T); --> 268 bayesdata[batch_idxs] = numer / denom; 269 ; 270 vpsq = np.sqrt(var_pooled).reshape((len(var_pooled), 1)). ValueError: operands could not be broadcast together with shapes (23259,18243) (23259,15479) ; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.11.1 python-igraph==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1170:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1170,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; After creating a fresh conda environment on Mac OS Mojave and starting to replicate the ""Analysis and visualization of spatial transcriptomics data"" tutorial, after running. ```python; adata = sc.datasets.visium_sge(); ```; I get ; <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; FileNotFoundError: [Errno 2] No such file or directory: '/Users/lisa/data/V1_Breast_Cancer_Block_A_Section_1'; ```; I figured it is because the intermediate folder ""/data"" is missing as well - . ```python; sample_dir.mkdir(exist_ok=True); ```; in ```_download_visium_dataset()``` cannot create it, it would need the flag ```parents=True``` to do so (https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir). #### Versions:. scanpy==1.4.7.dev52+g590d4230 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1184:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1184,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; After running ```sc.pp.combat(adata, key='sample')```, adata.X is full of NaNs and ```sc.pp.highly_variagle_genes(adata)``` fails. No issues (and no NaNs) if NOT running the combat correction. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.pp.combat(adata, key='sample'); sc.pp.highly_variable_genes(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; In [1]: sc.pp.combat(adata, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; As requested [here ](https://github.com/theislab/scanpy/issues/1172#issuecomment-616818311). It might be of interest to inform the user about the problem or set Combat to ignore that cell/sample...thats for the experts to decide.; In this case scenario, Combat will complete the analysis and yield no errors. However, obviously, subsequent call to ```sc.highly_variable_genes()``` will result in disaster. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1175:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1175,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Console outputs a long list of errors/warnings when running scanpy.pp.combat().; ; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; adataCombat = sc.read_h5ad(results_file); sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <cla",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; First of all, thank you for your great platform! . When I try to export a SPRING project I get the following error (it seems that the class NeighborsView is not defined; I have a 'neighbors' key in .uns): . <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.external.exporting.spring_project(adata, '/Users/mariusmessemaker/Documents/Project/mempel/SPRING', 'X_umap', subplot_name='Mempel', cell_groupings=['State', 'ImmGen', 'Biological replicate'], ; custom_color_tracks=None, total_counts_key='nCount_RNA', neighbors_key='neighbors', overwrite=False). AnnData object with n_obs × n_vars = 8757 × 20679 ; obs: 'SeqRun', 'Biological replicate', 'nCount_RNA', 'nCount_SCT', 'nFeature_RNA', 'nFeature_SCT', 'novelty', 'orig_ident', 'percent_mt', 'sc_leiden_res_48.75', 'State', 'ImmGen'; var: 'Selected', 'sct_detection_rate', 'sct_gmean', 'sct_residual_mean', 'sct_residual_variance', 'sct_variable', 'sct_variance'; uns: 'Biological replicate_colors', 'ImmGen_colors', 'State_colors', 'leiden', 'neighbors', 'state'; obsm: 'X_pca', 'X_umap'; varm: 'pca_feature_loadings'; layers: 'norm_data', 'scale_data'; obsp: 'connectivities', 'distances'; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-208-9f15be957dd9> in <module>; 1 sc.external.exporting.spring_project(adata, '/Users/mariusmessemaker/Documents/Project/mempel/SPRING', 'X_umap', subplot_name='Mempel', cell_groupings=['State', 'ImmGen', 'Biological replicate'], ; ----> 2 custom_color_tracks=None, total_counts_key='nCount_RNA', neighbors_key='neighbors', overwrite=False). ~/miniconda3/envs/py36-sc/lib/python3.6/site-packages/scanpy/external/exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1260:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1260,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Here is my use case: I run rank_genes_groups( ) with rankby_abs=True, as I want both upregulated and downregulated marker genes. Then I run filter_rank_genes_groups to set some thresholds, and all my downregulated genes disappear! . I see two possible solutions; 1) rankby_abs should be an argument for the filter_rank_genes_groups function as well; when rankby_abs=True, then min_fold_change should be interpreted as an absolute value threshold.; 2) filter_rank_genes_groups should follow the behavior of rank_genes_groups. This could be easily implemented if min_fold_change is always used as an absolute value threshold -- if there are only positive fold changes in the .uns['rank_genes_groups'] slot to begin with, only positive fold changes will be returned, and otherwise, both upregulated and downregulated genes will be returned.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1325:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1325,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Hey!; The color bar and legend don't seem to align well... might be because i'm subsetting the object. Here is what happens (reproducible example and my plot, which is even stranger with legend overlap):. <img width=""623"" alt=""Screenshot 2020-03-15 at 13 09 33"" src=""https://user-images.githubusercontent.com/13019956/76701118-73bff200-66be-11ea-9c99-ee4e0427bbfa.png"">; <img width=""580"" alt=""Screenshot 2020-03-15 at 13 02 52"" src=""https://user-images.githubusercontent.com/13019956/76701119-76224c00-66be-11ea-9b45-b94c1d9a57bd.png"">. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.pl.dotplot(adata[adata.obs.bulk_labels.isin(['Dendritic', 'CD14+ Monocyte', 'CD4+/CD25 T Reg']),], var_names=['HES4', 'TNFRSF4', 'SSU72'], groupby='bulk_labels', figsize=(8,8)). ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. Also, matplotlib is 3.1.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1103:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi I'm trying to run Louvain clustering but I'm getting a module not found error. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.tl.louvain(dge_E, flavor='vtraag', resolution=0.5; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ModuleNotFoundError Traceback (most recent call last); <ipython-input-43-d2a2f7b009fa> in <module>; ----> 1 sc.tl.louvain(dge_E, flavor='vtraag', resolution=0.5). c:\users\jamie\appdata\local\programs\python\python37\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy); 135 weights = None; 136 if flavor == 'vtraag':; --> 137 import louvain; 138 if partition_type is None:; 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi Scanpy team,; I am trying to analyse CTE-seq data. At the nomalization step of the protein data, the attibute normalize_geometric is not recognize. Could this be a version issue?; Thank you for your help!. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.pp.normalize_geometric(protein). <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...AttributeError Traceback (most recent call last); <ipython-input-80-db93ca6d0f1d> in <module>; ----> 1 sc.pp.normalize_geometric(protein). AttributeError: module 'scanpy.preprocessing' has no attribute 'normalize_geometric'. ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.7.dev30+g668b6776 anndata==0.7.1 umap==0.3.10 numpy==1.16.2 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.22.2.post1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1208:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1208,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi, I was trying to apply the SAM integration on a merged dataset of three example dataset from 10x homepage, in an attempt to compare the result to the PAGA and bbknn integrated umap.; I got a successful run on one of the PBMC dataset with no reprocessing. However in any other case I kept bump into an error:. TypeError: some keyword arguments unexpected. Here is the record. On the other hand, if I want to integrate bbknn with SAM, do I just apply bbknn after the run of SAM like this?. ////; import scanpy.external as sce. sam_obj = sce.tl.sam(adata); sc.pl.umap(sam_obj, color='Sample') . bbknn.bbknn(adata,batch_key='Sample'); #does this change the umap? or do I need to make another call of tl.umap?. sc.pl.umap(sam_obj, color='Sample') ; ////; i. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. import scanpy.external as sce; for adata in adatalist:; sam_obj = sce.tl.sam(adata); sce.pl.sam(adata,projection='X_umap'). <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ... Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.016564277393631113; Iteration: 1, Convergence: 0.01278454723440345; Computing the UMAP embedding...; Elapsed time: 50.534051179885864 seconds; Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.022868878389371346. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-17-4514ae92b370> in <module>; 1 import scanpy.external as sce; 2 for adata in adatalist:; ----> 3 sam_obj = sce.tl.sam(adata); 4 sce.pl.sam(adata,projection='X_umap'). ~/.local/lib/python3.7/site-packages/scanpy/external/tl/_sam.py in sam(adata, max_iter, num_norm_avg, k, distance, standardization, weight_pcs, sparse_pca, n_pcs, n_genes, projection, inplace, verbo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi,. I am new to python and have no idea what this could imply but thought I could report it. I am reading a file convert from `seurat` using `as.loom`. This file works fine if I open it in `scvelo` using `scv.read`, however, when starting from scanpy using; ```; adata = sc.read_loom(""seu.loom""); ```; I get the following:; ```; /Users/rmvl/miniconda3/envs/scanpy_env/lib/python3.7/site-packages/loompy/loom_layer.py:123: RuntimeWarning: invalid value encountered in not_equal; nonzeros = np.where(vals != 0); ```; I have tried closing all kernels, reinstalling loompy, scanpy, but nothing changed. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1074:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi,. not really a bug, more of a documentation issue: ; `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`).; To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):; ```python; foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9); ```; I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too...; Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)?. Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects?; `.raw` and the `.layer` dont have a lot of documentation. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/967:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967,4,"['clear', 'guid', 'learn', 'undo']","['clear', 'guide', 'learn', 'undo']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi. When I use `sc.pl.spatial` to visualize the same data with or without the H&E staining image in the background, the y-axis is flipped if I don't set the `img_key` parameter. Is this expected? ; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; from matplotlib import rcParams. adata_vis = sc.datasets.visium_sge('V1_Human_Lymph_Node'). rcParams[""figure.figsize""] = [8,8]. sc.pp.calculate_qc_metrics(adata_vis, inplace=True); sc.pl.spatial(adata_vis, img_key = ""hires"",color=['total_counts']); sc.pl.spatial(adata_vis,color=['total_counts'], size=100). ```; ![Screenshot 2020-02-18 at 16 30 56](https://user-images.githubusercontent.com/32264060/74756365-538e4600-526c-11ea-89f7-46409cb03d3d.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1059:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I am calculating custom connectivities using hsnw on rep 'X', I don't want to calculate PCA, I want to compute UMAP using these connectivities. ; sc.tl.umap falls back to pca in:; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_umap.py#L153; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L23. how to get sc.tl.umap to run on the precomputed 'X 'connectivities?; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; from scvelo.pp import neighbors; adata; #AnnData object with n_obs × n_vars = 4329 × 192; #obs: 'BARCODE', 'sample', 'detectable.features'; #var: 'gene_ids', 'feature_types'; #layers: 'normalized.counts'. neighbors(adata, n_neighbors = 20, use_rep = ""X"",knn = True,random_state = 0,method = 'hnsw',metric = ""euclidean"",metric_kwds = {""M"":20,""ef"":200,""ef_construction"":200},num_threads=1). adata.uns[""neighbors""]['params']; #{'n_neighbors': 20, 'method': 'hnsw', 'metric': 'euclidean', 'n_pcs': None}. sc.tl.umap(adata). #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.19.0 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1318:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I am looking for the expression of 'NCAM1'. It works when I am plotting my data (umap, violin plot, matrix plot) but I cant find it in the adata.var and I am not able to subset adata for this particular gene while it is working with the other genes. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; #what works:; marker_genes = ['NCAM1']; ax = sc.pl.violin(adata, marker_genes, groupby='timepoint'). # what doesnt work:; subset_NCAM = adata[:, 'NCAM1']. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ""None of [Index(['NCAM1'], dtype='object', name='index')] are in the [index]"". #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > 1.4.4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/999:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/999,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I am trying to integrate data using ingest. I followed exactly the same steps and scripts as described in the scanpy tutorial. However, I got the error message that ‘UMAP’ object has no attribute ‘_input_distance_func’ every time when I ran the command of sc.tl.ingest(adata, adata_ref, obs=‘louvain’). I have the same problem even with using the example pbmc3k_processed and pbmc3k_processed as in the tutorial. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(); var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]; sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref). # problem occurs here; sc.tl.ingest(adata, adata_ref, obs='louvain'); ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; AttributeError Traceback (most recent call last); <ipython-input-12-27e22cc8f823> in <module>(); ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). 3 frames; /usr/local/lib/python3.6/dist-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.2 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.0 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I am using scanpy with pyscenic. I am able to use tsne with rep ""X_pca"", but when I try to use a custom rep (X_aucell) to create a tsne plot, the kernel dies and Python also crashes. I can also use U-map with rep X_aucell. I have been able to use tsne with rep aucell on larger datasets in the past, so I have a hard time believing it's a memory issue. I'm really lost on what is causing this. I have restarted my computer and jupyter multiple times. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.tl.tsne(adata, use_rep='X_aucell'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; computing tSNE; using the 'MulticoreTSNE' package by Ulyanov (2017). Kernel Restarting ; The kernel appears to have died. It will restart automatically; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.23.1 statsmodels==0.11.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1291:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1291,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I have `pytorch` and `scanpy` installed inside a conda environment. When I want to import scanpy **after** torch, the import won't finish. The interesting part is that importing scanpy before torch is possible! For example, this code takes a long time and probably does not finish:. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import torch; import scanpy; ```. But the following example works:. ```python; import scanpy; import torch; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.18.5 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 pytorch==1.1.0 torchvision==0.3.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1286:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I have upgraded to scanpy 1.4.6 in a conda environment. Since then I cannot load the package into python, as it gives me the following error: `AttributeError: module 'cairo' has no attribute 'version_info'`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2349, in <module>; switch_backend(rcParams[""backend""]); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/__init__.py"", line 833, in __getitem__; plt.switch_backend(rcsetup._auto_backend_sentinel); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 204, in switch_backend; switch_backend(candidate); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, leve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I installed the package by `pip install scanpy`. When I imported it, there was such an error. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/sibcb1/mxqianlab1/common/anaconda/envs/cnmf_env/lib/python3.6/site-packages/scanpy/__init__.py"", line 36, in <module>; from . import tools as tl; File ""/sibcb1/mxqianlab1/common/anaconda/envs/cnmf_env/lib/python3.6/site-packages/scanpy/tools/__init__.py"", line 17, in <module>; from ._sim import sim; File ""/sibcb1/mxqianlab1/common/anaconda/envs/cnmf_env/lib/python3.6/site-packages/scanpy/tools/_sim.py"", line 23, in <module>; from .. import _utils, readwrite, logging as logg; File ""/sibcb1/mxqianlab1/common/anaconda/envs/cnmf_env/lib/python3.6/site-packages/scanpy/readwrite.py"", line 10, in <module>; import tables; File ""/sibcb1/mxqianlab1/common/anaconda/envs/cnmf_env/lib/python3.6/site-packages/tables/__init__.py"", line 93, in <module>; from .utilsextension import (; ImportError: libblosc.so.1: cannot open shared object file: No such file or directory. ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > version 1.5.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1284:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1284,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I ran filter_cells but still get a zero column. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_cells(adata, min_counts=300); sc.pp.filter_genes(adata, min_counts=1); sc.pp.filter_cells(adata,max_counts=15000). sc.pl.scatter(adata, x='nCount_RNA', y='percent.mt'); sc.pl.scatter(adata, x='nCount_RNA', y='nFeature_RNA'); sc.pl.highest_expr_genes(adata, n_top=20 ). print(np.any(adata.X.sum(axis=0) == 0)) # A gene's total UMI across all cells; print(np.any(adata.X.sum(axis=1) == 0)) # nUMI. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; True; False; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; Running scvelo 0.1.25 (python 3.7.3) on 2020-03-04 08:24.; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1083:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1083,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I was plotting the paga path, I got the error of TypeError: float() argument must be a string or a number, not 'csr_matrix'. I guess this might be related with the sparse format of adata.raw.X, because my codes works if I deleted adata.raw. What would be the solution? Thank you. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python; _, axs = pl.subplots(ncols=3, figsize=(6, 2.5), gridspec_kw={'wspace': 0.05, 'left': 0.12}); pl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2); for ipath, (descr, path) in enumerate(paths):; _, data = sc.pl.paga_path(; adata, path, gene_names,; show_node_names=False,; ax=axs[ipath],; ytick_fontsize=8,; left_margin=0.15,; n_avg=50,; annotations=['distance'],; show_yticks=True if ipath==0 else False,; show_colorbar=False,; color_map='Greys',; groups_key='clusters',; color_maps_annotations={'distance': 'viridis'},; title='{} path'.format(descr),; return_data=True,; show=False); data.to_csv('./write/paga_path_{}.csv'.format(descr)); pl.savefig('./figures/paga_path_panglao.pdf'); pl.show(); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); <ipython-input-8-86ecf06e6589> in <module>(); 18 title='{} path'.format(descr),; 19 return_data=True,; ---> 20 show=False); 21 data.to_csv('./write/paga_path_{}.csv'.format(descr)); 22 pl.savefig('./figures/paga_path_panglao.pdf'). 5 frames; <__array_function__ internals> in cumsum(*args, **kwargs). /usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 excep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I was using the sc.pl.rank_genes_groups_violinfunction and got the error:. ```; Exception Traceback (most recent call last); <ipython-input-195-8f87448845a3> in <module>; 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'); ----> 2 sc.pl.rank_genes_groups_violin(adata, groups='0', n_genes=1). ~/miniconda3/envs/scrna/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save); 727 if issparse(X_col): X_col = X_col.toarray().flatten(); 728 new_gene_names.append(g); --> 729 df[g] = X_col; 730 df['hue'] = adata.obs[groups_key].astype(str).values; 731 if reference == 'rest':. ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/frame.py in __setitem__(self, key, value); 2936 else:; 2937 # set column; -> 2938 self._set_item(key, value); 2939 ; 2940 def _setitem_slice(self, key, value):. ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/frame.py in _set_item(self, key, value); 2997 """"""; 2998 ; -> 2999 self._ensure_valid_index(value); 3000 value = self._sanitize_column(key, value); 3001 NDFrame._set_item(self, key, value). ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value); 3052 if not len(self.index) and is_list_like(value) and len(value):; 3053 try:; -> 3054 value = Series(value); 3055 except (ValueError, NotImplementedError, TypeError):; 3056 raise ValueError(. ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath); 303 data = data.copy(); 304 else:; --> 305 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True); 306 ; 307 data = SingleBlockManager(data, index, fastpath=True). ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1199:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1199,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I'm trying to run an enrichment analysis after filtering out certain genes via `sc.tl.filter_rank_genes_groups`, so I use `key='rank_genes_groups_filtered'` as an argument for `sc.queries.enrich`. Since the filtered values are replaced with `nan` I hoped they'd by ignored in the enrichment analysis, but it actually leads to an uninformative `AssertionError`. My suggestion here is simply to filter `nan` values from the gene list around here and 2 lines later: https://github.com/theislab/scanpy/blob/249fc572471683357b86b8bbf41d3284118bc8f8/scanpy/queries/_queries.py#L296. I can make a little PR if we agree with this simple fix. Note you can reproduce this very simply without an adata object (but of course the likely use case is with an adata object as outlined above):. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.queries.enrich([float('nan')]); ```; Output:; <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; AssertionError: query failed with error 500; ```. #### Versions:; ```; scanpy==1.4.5.post2 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.2.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.8.0; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1043:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043,5,"['clear', 'learn', 'simpl']","['clear', 'learn', 'simple', 'simply']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I'm using the ""scvelo"" https://scvelo.readthedocs.io/getting_started.html for scRNA data analysis. It underlying called ""scanpy"" function ""umap"" for calculating the coordinates. I tried the release version ""scanpy-1.4.4.post1-py_0"". It can not be imported to Python. Error message: ""ImportError: cannot import name '_Metric' from 'scanpy.neighbors' (/Users/shuzhe/anaconda3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py)"". I finally switch to the developing version for ""scanpy"". When I run ""umap"", it gives me the error message below. I'm wondering what the "".obsp"" is and how it is generated.; ; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; scv.tl.umap(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-391fc8667646> in <module>; ----> 1 scv.tl.umap(adata). ~/scanpy/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 125 start = logg.info('computing UMAP'); 126 ; --> 127 neighbors = NeighborsView(adata, neighbors_key); 128 ; 129 if ('params' not in neighbors. ~/scanpy/scanpy/_utils.py in __init__(self, adata, key); 667 self._dists_key = self._neighbors_dict['distances_key']; 668 ; --> 669 if self._conns_key in adata.obsp:; 670 self._connectivities = adata.obsp[self._conns_key]; 671 if self._dists_key in adata.obsp:. AttributeError: 'AnnData' object has no attribute 'obsp'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.7.dev26+gc255fa10 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.21.2 statsmodels==0.11.0 python-igraph==0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; I've had hard time in figuring this out. This is not a problem of scanpy directly but apparently is related to [scikit-learn 0.21 series](https://github.com/scikit-learn/scikit-learn/issues/14485) which is a dependency of latest scanpy version (1.4.6). Also related to [this comment in pytorch](https://github.com/pytorch/pytorch/issues/2575#issuecomment-523657178). My issue is that I'm using, in addition to scanpy, another library performing a dl_import with static TLS. ; So if I issue; ```python; import scanpy as sc; import graph_tool.all as gt; ```; I get. ```python; ImportError: dlopen: cannot load any more object with static TLS ; ```; error and I'm not able to use the second library. Reversing the order of the imports raises the same error and I'm not able to use `scanpy`. The issue is solved installing scikit-learn 0.20.4 (the last of 0.20 series). What are the exact scikit-learn 0.21.2 dependecies in scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121,6,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; In the `sc.tl.dendrogram` module, I noticed that the correlation matrix was directly inputted into the `scipy.cluster.hierarchy.linkage`. That results in calculating the distance between samples by the Euclidean(X1_cor_with_others, X2_cor_with_others). Shouldn't the distance be the pure correlation value here? Please correct me if I didn't understand it correctly. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; The one I would perfer; ```python; df = 1-adata.uns['dendrogram_sample']['correlation_matrix']; data_linkage = hierarchy.linkage(ssd.squareform(; df); ...; ```; The one currently in sc.tl.dendrogram; ```python; data_linkage = hierarchy.linkage(adata.uns['dendrogram_sample']['correlation_matrix']); ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1288:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1288,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; IndexError Traceback (most recent call last); <ipython-input-22-408c81d5d845> in <module>; 1 t1 = time.time(); ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]); 3 t2 = time.time(); 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 152 save_raw=save_raw,; 153 n_jobs=n_jobs,; --> 154 **kwargs,; 155 ); 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,; 125 compute_angle=compute_angle, mnn_order=mnn_order,; --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs); 127 print('Packing AnnData object...'); 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 180 print(' Computing correction vectors...'); 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,; --> 182 new_batch_in, sigma); 183 if not same_set:; 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/973:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; It seems the sc.tl.draw_graph() method fix the layout to 'fr' regardless of the input parameter? So I'm unable to use other method to produce the layout. . I'm using an example data from your tutorial. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import numpy as np; import pandas as pd; import matplotlib.pyplot as pl; from matplotlib import rcParams; import scanpy as sc. adata = sc.datasets.paul15(); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20); sc.tl.draw_graph(adata, layout='fa'); adata.obsm; ```. Although I set layout to 'fa', the output is still a 'fr' layout?. ```; AxisArrays with keys: X_pca, X_draw_graph_fr; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.1 anndata==0.7.3 umap==0.3.10 numpy==1.17.4 scipy==1.4.1 pandas==1.0.0 scikit-learn==0.21.3 statsmodels==0.11.1 python-igraph==0.8.0 leidenalg==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1299:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1299,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Loading data using `adata = sc.datasets.visium_sge('V1_Human_Lymph_Node')` with Anndata<0.7rc1 leads to error `'AnnData' object has no attribute 'is_view'`.; The reason is that the function name changed in version 0.7rc1 from `isview` -> `is_view`. I propose two possible solutions:; **Solution A**: Change requirements to `anndata>=0.7rc1`; **Solution B**: Add function to anndata:; ```python; def isview(self):; return self.is_view(); ```; I think solution B is preferable as it provides back-compatibility of anndata. ---; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; import scanpy as sc; adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-2-59eff31dcd22> in <module>; 1 get_ipython().system('pip install git+https://github.com/theislab/scanpy.git@spatial'); 2 import scanpy as sc; ----> 3 adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'). /opt/conda/lib/python3.7/site-packages/scanpy/datasets/__init__.py in visium_sge(sample_id); 368 ; 369 # read h5 file; --> 370 adata = read_10x_h5(files['counts']); 371 adata.var_names_make_unique(); 372 . /opt/conda/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only); 169 if gex_only:; 170 adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]; --> 171 if adata.is_view:; 172 return adata.copy(); 173 else:. AttributeError: 'AnnData' object has no attribute 'is_view'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; >",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Not able to install with conda and no info about the source of error.; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```bash; (scrna) $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Running Python 3.7 on Jupyter lab, with version 1.4.6 and also with 1.4.4.post1 (the version my friend can successfully run). When creating an AnnData object, and trying to run calculate_qc_metrics, I get the following error:. ```pytb; AssertionError: Sizes of partitioned, $174.6 do not match on /home/$USER/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py (397). ```. What I find most interesting, is that the adata.X object is a **CSR** matrix, whereas for my friend whose code works, adata.X is a **SparseCSRView**. Why is this the case? (same scanpy versions). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; batch='pool1'. adata = sc.read_mtx(folder.format(batch)+'matrix.mtx'); adata = adata.transpose(); adata.obs['barcodes'] = pd.read_csv(folder.format(batch)+'barcodes.tsv', sep = '\t', header = None).values; adata.var_names = pd.read_csv(folder.format(batch)+'features.tsv', sep = '\t', header = None)[0].values. sc.pp.calculate_qc_metrics(adata). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; **EDIT: Creating another environment with Python 3.6.1 fixes the issue... What could be wrong?**",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1143:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Running ```sc.external.pp.mnn_correct()``` outputs abundant Numba warnings and an Index error when reaches Step2 (of 11) during Computing correction vectors... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; adataMNN = sc.read_h5ad(results_file); sc.pp.highly_variable_genes(adataMNN, batch_key = 'sample'); var_select = adataMNN.var.highly_variable_nbatches > 1; var_genesMNN = var_select.index[var_select]; datasets = [adataMNN[adataMNN.obs['sample'] == sa].copy() for sa in adataMNN.obs['sample'].cat.categories]; sc.external.pp.mnn_correct(*datasets, var_subset=var_genesMNN, batch_key='sample'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: Code running in object mode won't allow parallel execution despite nogil=True.; @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""l2_norm"" failed type inference due to: Invalid use of Function(<function norm at 0x7f637ad4ca70>) with argument(s) of type(s): (axis=Literal[int](1), x=array(float32, 2d, A)); * parameterized; In definition 0:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; In definition 1:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; This error is usually caused by passing an argument of a type that is unsupported by the named function.; [1] During: resolving callee type: Function(<function norm at 0x7f637ad4ca70>); [2] During: typi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Running ```sc.pp.regress_out(adata, ['n_counts'])``` or in any other obs, it outputs error on the ""first guess"". I have used the function before with no problems. So I am not sure where the issue is now (I have updated to the latest version of Scanpy 2 days ago...but I dont know if it has anything to do with it). I have followed the steps in this [tutorial](https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb).; I must add that this ```adata``` object is a subset of all cells created by keeping only the cells that express the gene Crabp1, as follows:; ```adata = adata_full[adata_full.obs['Crabp1_cell'] == 'True']```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.pp.regress_out(adata, ['n_counts']); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; sc.pp.regress_out(adata, ['n_counts']); regressing out ['n_counts']; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:179: RuntimeWarning: invalid value encountered in true_divide; return np.sum(resid_dev * freq_weights * var_weights / scale); Traceback (most recent call last):. File ""<ipython-input-6-4693dee26417>"", line 1, in <module>; sc.pp.regress_out(adata, ['n_counts']). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 841, in regress_out; res = list(map(_regress_out_chunk, tasks)). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 867, in _regress_out_chunk; result = sm.GLM(data_chunk[:, col_index], regres, family=sm.families.Gaussian()).fit(). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1027, in fit; cov_kwds=cov_kwds, use_t=use_t, **kwargs). File ""/home/auesro/anaconda3/envs/Sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1171:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1171,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Since I update to scanpy==1.4.5.1 I am getting multiple plotting errors. `sc.pl.rank_genes_groups() and sc.pl.violin()` are still working fine but I am getting errors in the rank_genes functions like `sc.pl.rank_genes_groups_violin(`) and `sc.pl.tracksplot()`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ##still working fine; sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names. pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names', 'pvals']}).head(5); ##gives error; sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value); 3169 try:; -> 3170 value = Series(value); 3171 except:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath); 273 data = _sanitize_array(data, index, dtype, copy,; --> 274 raise_cast_failure=True); 275 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py in _sanitize_array(data, index, dtype, copy, raise_cast_failure); 4160 if isinstance(data, np.ndarray):; -> 4161 raise Exception('Data must be 1-dimensional'); 4162 else:. Exception: Data must be 1-dimensional. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-23-ccdbf8b7836c> in <module>; ----> 1 sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8) ## 200316 error fix later, also when I run ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; The bug is just like the title of issue, _AttributeError: module 'scanpy' has no attribute 'anndata'_, for I just wanna to load a h5ad file from Tabula-Muris dataset; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc. data = sc.anndata.read_h5ad(''tabula-muris-senis-facs-processed-official-annotations-Bladder.h5ad'); ```; <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):. File ""<ipython-input-91-67a79760d7cb>"", line 1, in <module>; sc.anndata.read_h5ad('tabula-muris-senis-facs-processed-official-annotations-Bladder.h5ad'). AttributeError: module 'scanpy' has no attribute 'anndata'; ```; #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.3 anndata==0.6.19 umap==0.3.10 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1322:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1322,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; The y-scale label on the right of the stacked_violin is missing in the version of 1.5.2.dev. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']; sc.pl.stacked_violin(pbmc,marker_genes,groupby='louvain',row_palette='muted',figsize=(7,4),log=False); sc.logging.print_versions(); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ![download](https://user-images.githubusercontent.com/30639029/87759504-434ea880-c7c3-11ea-9c3a-b59e027ffeb6.png). #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.2.dev123+g0cf308c anndata==0.7.3 umap==0.4.4 numpy==1.18.5 scipy==1.4.1 pandas==1.0.4 scikit-learn==0.23.1 statsmodels==0.11.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1321:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1321,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Trying for the first time the tutorial notebook and I stumble onto the error below. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Collecting git+https://github.com/theislab/scanpy.git@spatial; Cloning https://github.com/theislab/scanpy.git (to revision spatial) to /tmp/pip-req-build-1aajpyr3; Running command git clone -q https://github.com/theislab/scanpy.git /tmp/pip-req-build-1aajpyr3; WARNING: Did not find branch or tag 'spatial', assuming revision or ref.; Running command git checkout -q spatial; error: pathspec 'spatial' did not match any file(s) known to git.; ERROR: Command errored out with exit status 1: git checkout -q spatial Check the logs for full command output.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1104:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1104,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Trying to make a violin plot adding the seaborn hue argument will result in ValueError.; In adata, 'timepoint' and 'replicate' are categorical adata.obs containing floats and ints, respectively. 'timepoint' is the age of the embryo from which cells were isolated (9.5, 10.5, etc) and 'replicate' the number of the replica (1, 2, 3). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.pl.violin(adata, 'n_genes', jitter=0.4, groupby = 'timepoint', stripplot=False, hue='replicate'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; sc.pl.violin(adata, 'n_genes', jitter=0.4, groupby = 'timepoint', stripplot=False, hue='replicate'); Traceback (most recent call last):. File ""<ipython-input-5-756b321177a2>"", line 1, in <module>; sc.pl.violin(adata, 'n_genes', jitter=0.4, groupby = 'timepoint', stripplot=False, hue='replicate'). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 759, in violin; **kwds,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/seaborn/categorical.py"", line 2393, in violinplot; color, palette, saturation). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/seaborn/categorical.py"", line 559, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/seaborn/categorical.py"", line 152, in establish_variables; raise ValueError(err). ValueError: Could not interpret input 'replicate'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1174:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1174,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; When I tried to plot the expression of a particular gene on umap map by the tutorial, it always showed the following error:; ```; >>> sc.pl.umap(post_adata, color=['XKR4']); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 542, in umap; return embedding(adata, 'umap', **kwargs); File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 207, in embedding; use_raw=use_raw, gene_symbols=gene_symbols,; File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 865, in _get_color_values; values = adata.raw.obs_vector(value_to_plot); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 413, in obs_vector; idx = self._normalize_indices((slice(None), k)); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 364, in _normalize_indices; var = _normalize_index(var, self.var_names); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 155, in _normalize_index; return name_idx(index); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 142, in name_idx; .format(i)); IndexError: Key ""XKR4"" is not valid observation/variable name/index. ```; However, the gene XKR4 did exist in the var_names:; ```; >>> post_adata.var_names; Index(['XKR4', 'RP1', 'SOX17', 'MRPL15', 'LYPLA1', 'TCEA1', 'RGS20', 'ATP6V1H',; 'OPRK1', 'NPBWR1',; ...; '2700089I24RIK', 'RAB11FIP2', 'E330013P04RIK', 'NANOS1', 'EIF3A',; 'FAM45A', 'SFXN4', 'PRDX3', 'GRK5', 'CSF2RA'],; dtype='object', length=16249); ```. The anndata object looked as below and it was fine when I tried to show the louvain clusters:. ```; >>> post_adata; AnnData object with n_obs × n_vars = 88291 × 16249; obs: 'CellID', 'batch_indices', 'labels', 'local_means', 'local_vars'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1039:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; When I use `sc.pp.log1p(adata)` and then `sc.pp.log1p(adata, layer='other')` it warns me that the data has already been logged even though I am logging a layer as opposed to adata.X. Would be nice to flag logging for each layer instead of when anything is logged. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc. adata = sc.datasets.pbmc3k_processed(); adata.layers['other'] = adata.X; sc.pp.log1p(adata, layer='other'); sc.pp.log1p(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; WARNING: adata.X seems to be already log-transformed.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.2.dev5+ge5d246aa anndata==0.7.3 umap==0.3.10 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; When I use the `scanpy.datasets.ebi_expression_atlas`, I got an error `HTTP Error 500: `. My python version is 3.6.10, and I also reproduced it on google colab. ; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; adata = sc.datasets.ebi_expression_atlas(""E-MTAB-4888""); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; HTTPError Traceback (most recent call last); <ipython-input-6-0ae186d1a0d7> in <module>; ----> 1 adata = sc.datasets.ebi_expression_atlas(""E-MTAB-4888""). ~/anaconda3/envs/sc-py/lib/python3.6/site-packages/scanpy/datasets/_ebi_expression_atlas.py in ebi_expression_atlas(accession, filter_boring); 117 pass; 118 ; --> 119 download_experiment(accession); 120 ; 121 logg.info(f""Downloaded {accession} to {experiment_dir.absolute()}""). ~/anaconda3/envs/sc-py/lib/python3.6/site-packages/scanpy/datasets/_ebi_expression_atlas.py in download_experiment(accession); 41 ; 42 _download(; ---> 43 download_url + ""experiment-design"", experiment_dir / ""experimental_design.tsv"",; 44 ); 45 _download(. ~/anaconda3/envs/sc-py/lib/python3.6/site-packages/scanpy/readwrite.py in _download(url, path); 877 ; 878 try:; --> 879 urlretrieve(url, str(path), reporthook=update_to); 880 except Exception:; 881 # Make sure file doesn’t exist half-downloaded. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in urlretrieve(url, filename, reporthook, data); 246 url_type, path = splittype(url); 247 ; --> 248 with contextlib.closing(urlopen(url, data)) as fp:; 249 headers = fp.info(); 250 . ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context); 221 else:; 222 opener = _opener; --> 223 return opener.open(url, data, timeout); 224 ; 225 def install_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1221:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1221,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; When i use umap with the parameter init_pos='paga', I got a strange result.; ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.pl.umap(adata,color='louvain'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; no error; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; When multiple plots in one figure, if the legend of one figure is comprehensive, there is no space left for the legend to be shown. The scanpy just overlap the next plot on it. ![25AD7E00-E9A8-4CC6-A347-C4C76177F770](https://user-images.githubusercontent.com/16257776/86968887-e0ae3a80-c13a-11ea-805c-8620a9557087.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.pl.umap(adata, color=['sample_id','IMPACT_TMB']) ; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1312:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1312,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; When trying to get the obs_df or the var_df, the function throws an error when there is a single key. In this case it iterates through the letters in the given key and causes a KeyError. . <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; pbmc = sc.datasets.pbmc68k_reduced(); sc.get.obs_df(pbmc, keys=('HES4')); sc.get.var_df(pbmc, keys=('AAAGCCTGGCTAAC-1')); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-60-663347265b80> in <module>; ----> 1 sc.get.var_df(pbmc, keys=('AAAGCCTGGCTAAC-1')). ~/.conda/envs/scvelo_updated/lib/python3.8/site-packages/scanpy/get.py in var_df(adata, keys, varm_keys, layer); 223 not_found.append(key); 224 if len(not_found) > 0:; --> 225 raise KeyError(; 226 f""Could not find keys '{not_found}' in columns of `adata.var` or""; 227 "" in `adata.obs_names`."". KeyError: ""Could not find keys '['A', 'A', 'A', 'G', 'C', 'C', 'T', 'G', 'G', 'C', 'T', 'A', 'A', 'C', '-', '1']' in columns of `adata.var` or in `adata.obs_names`.""; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.1 anndata==0.7 umap==0.4.4 numpy==1.19.0 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1315:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1315,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; When using `sc.pl.scatter()` and providing an existing axis object, the legend doesn't always appear correctly and cannot be accessed. ; This doesn't seem to happen with a categorial coloring however, only with a continous colormap. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; This code works as expected:; ```python; sc.pp.calculate_qc_metrics(adata_raw, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); sc_fig, (sc_ax1, sc_ax2) = plt.subplots(1,2, figsize=(12,5)); sc.pl.scatter(adata_raw, 'total_counts','n_genes_by_counts', color='batch', size = 10, ax=sc_ax1, show=False, title=""all counts""); sc_ax1.get_legend().remove(); sc.pl.scatter(adata_raw[adata_raw.obs['total_counts']<1000],'total_counts','n_genes_by_counts', color='batch', size = 10, ax=sc_ax2, show=False, title=""< 1000 counts""); plt.show(); ```; It creates some metrics and stores them in `adata_raw.obs`, then plots these metrics for all counts and for counts < 1000 on the two axes created by `plt.subplots()`. The legend from the first axis is then removed.; This is an example of this output:; ![image](https://user-images.githubusercontent.com/50995210/83322157-ac9b4c00-a255-11ea-9861-8974b535cac3.png). Now the code that doesn't work:; ```python; sc_fig, (sc_ax1, sc_ax2) = plt.subplots(1,2, figsize=(12,5)); sc.pl.scatter(adata_raw, 'total_counts','n_genes_by_counts', color='pct_counts_mt', size = 10, ax=sc_ax1, show=False, title=""all counts""); #sc_ax1.get_legend().remove(); sc.pl.scatter(adata_raw[adata_raw.obs['total_counts']<1000],'total_counts','n_genes_by_counts', color='pct_counts_mt', size = 10, ax=sc_ax2, show=False, title=""< 1000 counts""); plt.show(); ```. Essentially the same thing but colored by the percentage of mitochondrial counts.; Only one legend seems to be drawn and this one is not looking as expected. Plus, I cannot remove the legend from the first plot. ; Th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1258:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; With the new `batch_key` option in `highly_variable_genes` downstream functions like PCA can fail silently with the old defaults. The same is true for `sc.pl.highly_variable_genes(adata)` which currently doesn't recognize the output key in `adata.var` is `highly_variable_intersection` rather than `highly_variable`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```; <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-125-322839e541fd> in <module>; ----> 1 sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 529 pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state); 530 X = adata_comp.X; --> 531 X_pca = pca_.fit_transform(X); 532 ; 533 if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/sklearn/decomposition/pca.py in fit_transform(self, X, y); 358 ; 359 """"""; --> 360 U, S, V = self._fit(X); 361 U = U[:, :self.n_components_]; 362 . ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit(self, X); 380 ; 381 X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,; --> 382 copy=self.copy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:; https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; # I have already read in an Ann data object from an h5ad existing file; sc.tl.pca(adata, n_comps=30, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=15); sc.tl.umap(adata). k = 15; communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k); adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities); adata.uns['PhenoGraph_Q'] = Q; adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'; adata.write_h5ad(path_to_h5ad_file) # works. # but if I run; sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'); rcParams['figure.figsize'] = 4,4; rcParams['axes.grid'] = True; sc.pl.rank_genes_groups(adata); pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works; adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-23-cb0bc3c267ae> in <module>; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; _ingest.py tries to import the UMAP function like so:; `from umap import UMAP`; I believe this is wrong, and it should be replaced with:; `from umap.umap_ import UMAP`. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ImportError: cannot import name 'UMAP' from 'umap' (/opt/anaconda3/lib/python3.7/site-packages/umap/__init__.py); ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 numpy==1.17.4 scipy==1.3.1 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1202:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; ```pytb; -----------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-22-408c81d5d845> in <module>; 1 t1 = time.time(); ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]); 3 t2 = time.time(); 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 152 save_raw=save_raw,; 153 n_jobs=n_jobs,; --> 154 **kwargs,; 155 ); 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,; 125 compute_angle=compute_angle, mnn_order=mnn_order,; --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs); 127 print('Packing AnnData object...'); 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 180 print(' Computing correction vectors...'); 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,; --> 182 new_batch_in, sigma); 183 if not same_set:; 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/974:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; `wx` appears to be a missing scanpy dependancy linked to matplotlib when installing on macOS. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; >>> import scanpy as sc; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2282, in <module>; switch_backend(rcParams[""backend""]); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/miniconda3/envs/path/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/backends/backend_wxagg.py"", line 1, in <module>; import wx; ModuleNotFoundError: No module named 'wx'; ```. The solution is simple, install `wxPython` https://pypi.org/project/wxPython/. However, it would be nice if scanpy could handle this OS-specific dependancy. #### Versions:; The latest scanpy version (1.5.1) installed via conda- of course I cannot print the versions since the scanpy import fails, other details;. ```; >>> import sys; print(sys.version); 3.7.6 | p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; here is the code for marker filter; I think the 3 condition need to be OR instead of AND; gene_names = gene_names[; (fraction_in_cluster_matrix > min_in_group_fraction) &; (fraction_out_cluster_matrix < max_out_group_fraction) &; (fold_change_matrix > min_fold_change); ]. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1213:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; sc.pl.tracksplot, produce a wrong highlight bar without brackets. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pbmc = sc.datasets.pbmc68k_reduced(); marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']; ax = sc.pl.tracksplot(pbmc,marker_genes, groupby='louvain',; var_group_positions=[(0,2),(4,5)],var_group_labels=['set1','set2']); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ![image](https://user-images.githubusercontent.com/30639029/83604801-9dedb700-a52b-11ea-9c32-fc35ea959d61.png). #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.4.7.dev136+g7f5c907 anndata==0.7.1 umap==0.4.1 numpy==1.18.3 scipy==1.4.1 pandas==1.0.3; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1265:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1265,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; scanpy.pp.pca fails if n_samples < 50 < n_features. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import numpy as np; import scanpy as sc; import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 100))); sc.pp.pca(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/scottgigante/.local/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 531, in pca; X_pca = pca_.fit_transform(X); File ""/usr/lib/python3.8/site-packages/sklearn/decomposition/_pca.py"", line 369, in fit_transform; U, S, V = self._fit(X); File ""/usr/lib/python3.8/site-packages/sklearn/decomposition/_pca.py"", line 418, in _fit; return self._fit_truncated(X, n_components, self._fit_svd_solver); File ""/usr/lib/python3.8/site-packages/sklearn/decomposition/_pca.py"", line 497, in _fit_truncated; raise ValueError(""n_components=%r must be between 1 and ""; ValueError: n_components=50 must be between 1 and min(n_samples, n_features)=40 with svd_solver='arpack'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.2.3.dev1409+g7ca201d.d20200112 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.0 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22 statsmodels==0.11.0rc1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1051:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; scanpy.pp.recipe_seurat and scanpy.pp.recipe_zheng17 indicate that they expect non log-transformed data. This leads both functions to do by default the highly variable gene (HVG) selection on non log-transformed data. This seems contrary to the scanpy and seurat clustering tutorials, which perform HVG selection after log-transform. It also seems contrary to the new function scanpy.pp.highly_variable_genes which expects log-transformed inputs. scanpy version : 1.5.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1251:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251,1,['clear'],['clear']
Usability,"<!-- Please give a clear and concise description of what the bug is: -->; trying to run louvain clustering but got the error:. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-fe1390cdc24a> in <module>; ----> 1 sc.tl.louvain(adata, resolution=1.0). ~/miniconda3/envs/scrna/lib/python3.7/site-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 136 partition_kwargs[""weights""] = weights; 137 logg.info(' using the ""louvain"" package of Traag (2017)'); --> 138 louvain.set_rng_seed(random_state); 139 part = louvain.find_partition(; 140 g, partition_type,. ~/miniconda3/envs/scrna/lib/python3.7/site-packages/louvain/functions.py in set_rng_seed(seed); 23 def set_rng_seed(seed):; 24 """""" Set seed for internal random number generator. """"""; ---> 25 _c_louvain._set_rng_seed(seed); 26 ; 27 def find_partition(graph, partition_type, initial_membership=None, weights=None, **kwargs):. AttributeError: module 'louvain._c_louvain' has no attribute '_set_rng_seed'; ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.tl.louvain(adata, resolution=1.0); ```. Python 3.7. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.2 numpy==1.18.3 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.6.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191:19,clear,clear,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"<!-- What kind of feature would you like to request? -->. It is a huge win for the GPU community to be able to use Louvain clustering and UMAP from RAPIDS. It would be very useful to see more of the tools providing options to use RAPIDS, such as PCA, T-SNE, and KNN. It would also be useful if the Scanpy API could support distributed versions of these algorithms, and perhaps even support end to end distributed workflows using Dask or Spark. . - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1178:555,simpl,simple,555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1178,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->. It would be very useful for the GPU data science and research community if Scanpy were able to perform end to end workflows on the GPU, using either Cupy, CuDF or both. An initial iteration of this feature could include simply swapping out the numpy imports for cupy. . - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177:278,simpl,simply,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177,2,['simpl'],"['simple', 'simply']"
Usability,"<!-- What kind of feature would you like to request? -->; - [ X] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. <!-- Please describe your wishes below: -->; It may be useful to adopt a PCA option similar to **`multiBatchPCA`** in the R batchelor package.; This is a useful approach where there are imbalances in batch size and PCA is conducted across a merged experiment.; It is pretty slow in R. From their documentation:. > ; > _""Our approach is to effectively weight the cells in each batch to mimic the situation where all batches; > have the same number of cells. This ensures that the low-dimensional space can distinguish subpopulations in smaller batches. Otherwise, batches with a large number of cells would dominate; > the PCA, i.e., the definition of the mean vector and covariance matrix. This may reduce resolution; > of unique subpopulations in smaller batches that differ in a different dimension to the subspace of; > the larger batches.""_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289:86,simpl,simple,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1077:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077,2,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ... How to do correlation between celltypes and age in scanpy?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ... I want to save h5ad object without cells in the red circle, how to do that in scanpy?. Thank you; ![image](https://user-images.githubusercontent.com/23288387/132060260-4700d085-595e-473f-ab09-6ff70ffde357.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1992:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Hi,. In my h5ad object in adata.X, which data I should use for downstream analysis? counts, normalized or scaled one?; Can I have all them in my h5ad object and how to switch between them?; In seurat there is option called active assay to assign.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1875:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Hi; I am wondering Why there is not expression_cutoff in stacked violin plot as in the dot plot?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1871:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1871,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; How to convert from Seurat visium to AnnData?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1882:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1882,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; How to generate two umaps for one gene split by a condition[one variable in obs] ?; so you can compare where gene expressed in cell types and how they differ in the two conditions. could we add option split by to umap scanpy function?; https://github.com/theislab/scanpy/issues/759,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1879:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1879,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; I need to filter the cells which expressed 'A' and 'B' genes >1; The below line should work for one gene but how to do it for two genes?; `adata = adata[adata[: , 'A'].X > 1, :] `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1870:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1870,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Is there a function in scanpy to compute and plot the correlation between the cell types frequency and the cells clinical information n stored in obs?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1826:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1826,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Sometimes we need to collapse the single cell ATAC-seq peak matrix to a ""gene activity matrix"", as same as in seurat, I wish the scanpy can also provide this function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1539:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1539,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; is there a scanpy method to do a correlation between cell types and continuous variables stored in .obs?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1855:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1855,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Hi, ; If this is frequency table per cohort; which test to use to identify significant cell type per group? ; ![image](https://user-images.githubusercontent.com/23288387/117582425-efa1a880-b0cf-11eb-8095-2f7ecacaf7b0.png). ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1831:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1831,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ x] Other?; s; <!-- Please describe your wishes below: -->; ...; In sc.pl.dotplot(), dot_ax.grid(False) is the default. Can an argument please be added to the function that allows the option of True: dot_ax.grid(True, linewidth = x) with linewidth as part of the argument?. Thank you!; Salwan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1371:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [X] Other?. <!-- Please describe your wishes below: -->. Would it be possible to project a query dataset onto an existing reference umap, such that the reference dataset is not altered? The readout would be the existing reference umap with perhaps the density of query cells overlaid on top. Perhaps also quantification of the percentage of query cells falling into specific clusters on the reference data set? Apologies if this already exists, I would be very interested to know how to do this using the anndata framework. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1847:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1847,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy?. The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):; - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. ; - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package.; - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R.; - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm adapting ScanPy for my compositional data analysis (CoDA) methodologies in the same vein as https://github.com/scverse/scanpy/issues/2475 . In this, I would like to perform Aitchison PCA but I'd also like to keep my Anndata object in counts form instead of creating another one. . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. # Add CLR to layers; adata.layers[""clr""] = adata.to_df().apply(clr, axis=1) # Not the fastest way but just to show example. sc.tl.pca(adata, svd_solver='arpack', layer=""clr"") # -> Return addata object where PCA is performed on CLR. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2476:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2476,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm in the process of redesigning my most used workflows from https://github.com/jolespin/soothsayer to wrap around ScanPy. I come from microbial ecology but do a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,3,"['guid', 'learn', 'simpl']","['guide', 'learning', 'simple']"
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ...; `sc.pl.stacked_violin` legend shows the group median by default, How can I show the group means instead? . Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2340:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2340,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; Hello Scanpy,; I'm merging millions of cells to run the Scanpy, and the steps for constructing UMAP kill me. Is it possible or is there any plan to make these functions `sc.tl.pca, sc.pp.neighbors, sc.tl.leiden, and sc.tl.umap` support multiprocessing?; Thanks!; Best,; Yuanjian",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2372:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2372,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; e.g. squares, hexagons cc @jwrth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. We started a [gitter](https://gitter.im/scvi-tools/development) at scvi-tools and surprisingly it has gotten more traction than our Discourse forum. Just putting it out there to gauge interest here. You could have a development room and a usage room. The pros are that it's more informal and less of a barrier of entry compared to github issues, cons include that it's not as searchable (though I'm a novice with it). I feel that you might be able to get better community answers if gitter became actively used.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1716:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1716,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [✔] Other?. <!-- Please describe your wishes below: -->; Hello Scanpy,; I'm wondering whether it is possible to show the downregulated marker genes by sc.pl.rank_genes_groups() or other functions, so that we can export the gene list for further GSEA analysis?; I know sc.pl.rank_genes_groups_dotplot can show the downregulated genes by change n_genes to negative numbers, but it didn't work in sc.pl.rank_genes_groups().; Thanks!; Best,; YJ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2052:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2052,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [X] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Is it possible to impliment [PyDESeq2](https://github.com/owkin/PyDESeq2) in `scanpy`; Thanks,; Shams",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2385:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2385,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [x] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I would like to add AutoGeneS (https://github.com/theislab/AutoGeneS) as an external package.; It's a bulk deconvolution tool that is based on anndata and scanpy.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2287:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2287,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ x ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I am using Delaunay triangulation to give a continuous and smooth aspect for plotting values at 2D discrete points. I think it would be useful to have it in scanpy/episcanpy for plotting spatial gene expressions, on top of the scatter plot that is currently used. This can also be used for 3D plotting (3D transcriptomics, such as STARMAP, or even 3D epigenomics in the future). One can have the option of slicing the 3D volume image with a user defined plane position, etc... I can provide my scripts and would be happy to contribute. Thank you very much.; Adem Saglam (DZNE-Bonn, AG Schultze)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1287:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1287,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Hi all!. My use case for scanpy is analysis of whole-body data from a weird marine annelid. We sort of have an idea of what to expect, but a lot of the analysis is exploratory, and my main job is helping canalize the knowledge that is available in the lab into making sense of the data. In this context, dotplots are our best friend, as it provides a very nice summary of gene expression over the whole (clustered) dataset. However, yesterday we noticed a confusing edge case: let’s say gene $g$ is expressed in the same number of cells in two clusters, 4 and 23. Cluster 4 has many, many more cells than 23, therefore on the dotplot it will look like $g$; is barely expressed in 4, but a great marker for 23. Of course, combining a dotplot with a feature plot helps you see that, but you get no sense of how many cells those are (more/less/the same). To alleviate this I am proposing an extension of dotplots: instead of circles, boxes, that have a height proportional to $log(#cells_{cluster})$, are filled proportionally to how many cells express gene $g$, and are colored according to the average expression. I think this works better than violinplots. Sadly I see no good way to multiplex this and plot multiple genes at once. I am really interested in feedback - maybe I am overlooking something super simple/basic?. ![image](https://user-images.githubusercontent.com/1651067/149312386-fbabade5-fdbe-4a72-a627-599bd103a9a9.png). the corresponding dotplot:. ![image](https://user-images.githubusercontent.com/1651067/149316899-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ... Currently, to compare gene expressions on the umap between different samples or conditions; I use the for loop and do something like this. for i in ['embryonic day1', 'postnatal day2']:; print(i); sc.pl.umap(; adata[adata.obs[""bulk.ident""] == i], size=20,; color=[""gene""],; frameon=False, ; ). but i would love to have a function for this that creates multiple umaps in parallel in one row with maybe 'ncol' fxn to decide how many in one row. could we please add split.by-like function function in the scanpy?. +addendum; Also, simply subplotting multiple umaps per condition could be somewhat missleading as scale for particular gene expression would very per samples. We would need a comprehensive fxn that provides uniform scale of expression for multiple samples.. #1879",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2333:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2333,2,['simpl'],"['simple', 'simply']"
Usability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; I am wondering if scanpy has an option to plot three dimension of information in one plot such as stacked barplot. I need to show cell types distribution in absent/present of medication per each cohort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1824:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Hey @fidelram !. I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:; ```; def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):; tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):; tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'); tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs); ```. Example output from:; ```; test = sc.datasets.pbmc68k_reduced(); sc.pp.pca(test); sc.pp.neighbors(test); sc.tl.umap(test); cluster_small_multiples(test, 'bulk_labels'); ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/955:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955,3,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python; import matplotlib.pyplot as plt; import scanpy as sc; import numpy as np. # given integrated object adata, clustered via the leiden algorithm and; # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID; count_series = adata.obs.groupby(['leiden', 'batch']).size(); new_df = count_series.to_frame(name = 'size').reset_index(); # convert from multi index to pivot; constitution = new_df.pivot(index='leiden', columns='batch')['size']; # convert to %batch (but could be modified to show different things instead; perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))); # keep track of the batch, cluster IDs so we can use them for plotting; clusters = adata.obs.leiden.cat.categories; batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots; # replace styling with scanpy defaults probably?; fig, ax = plt.subplots(); ax.grid(False); ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]); bottom = np.zeros(clusters.shape); for i, b in enumerate(batches):; ax.bar(clusters, perc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; It seems like there is an argument in `sc.pl.spatial` to return the Matplotlib figure used for plotting. However, from inspecting the source code, I think that this doesn't really do that; it seems more like that argument just makes the function return the Matplotlib Axes instead? For context, I am using `sc.pl.spatial` + `sq.pl.extract` to plot many features on subplots, but I would like to add a ""suptitle"" to the whole figure.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2316:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2316,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. Hello Scanpy team,. I was wondering should it be possible to rotate the gene symbols in the output of `scanpy.pl.rank_genes_groups`? I don't think anybody enjoys looking at them sideways :) I might be wrong of course... Maybe I'm missing something and you can swap axes in the sub-plots? . Thank you in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2146:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2146,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [✔] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Hello Scanpy,; I'm not sure whether Scanpy already has this function. For example, if we have `query` and `ref` data, we can use `ingest` to map the `query` onto the embedding of `ref`. Then, we can get similar UMAPs between these 2 data and do downstream analysis (like scVelo) based on these 2 UMAPs (coding below). In this way, because the UMAP is similar, we can have a more clear answer about how different these 2 date is.; ```python; ref = sc.read('ref.h5ad'); query = sc.read('query.h5ad'); var_names = query.var_names.intersection(ref.var_names); query = query[:, var_names]; ref = ref[:, var_names]; sc.tl.ingest(adata=query, adata_ref=ref, obs='leiden'); sc.pl.umap(query, color=['leiden'], legend_loc='on data', frameon=False, title='', use_raw=False) # this step will generate new obs['leiden'] and obsm['X_umap'] for query, which is a similar embedding with ref. adata = sc.read_loom(filename='queryraw.loom'); adata.obs['leiden']=query.obs['leiden'] # copy ingested leiden to raw data; adata.obsm['X_umap']=query.obsm['X_umap'] # copy ingested X_umap to raw data; # then do scVelo on this adata by using this embedding.; ```. However, `ingest` doesn't remove the batch effect. `BBKNN` does. After `BBKNN`, both `query` and `ref` will have new UMAPs stored at the same obsm['X_umap']. I'm wondering whether it is possible to split these 2 UMAPs? For example, store `query` UMAP in obsm['X_query_umap'] and `ref` UMAP in obsm['X_ref_umap'] so that we can copy each into a raw data. ![image](https://user-images.git",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2123:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2123,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [X] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; `sc.tl.score_genes_cell_cycle` calculates scores via `sc.tl.score_genes` but also assigns a categorical label of cell cycle phase. Given lists of marker genes of cell types, can a similar approach be used to potentially annotate putative cell types? Maybe it is too naive? My main concern is that cells that do not fit any cell type for which there are markers will be mis-assigned. I am aware that there are tons of automated cell type prediction tools for scRNA-seq, but not found anything directly supported by scanpy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1863:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1863,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. PyMDE is a nice visualization method that to me seems to effectively serve the same purpose as UMAP in analyses (discussion about appropriateness of these methods can be in another issue :) ). It's super fast because after running pynndescent it puts the graph on the GPU optionally (using pytorch). I would love to see this in scanpy. There might be a way to use the scanpy neighbors graph from `sc.pp.neighbors` directly in pymde as the function below is a wrapper of some internal classes. <img width=""2431"" alt=""Screen Shot 2022-02-24 at 12 38 25 PM"" src=""https://user-images.githubusercontent.com/10859440/155603698-7f0e975e-2f4b-4a95-97eb-f119522c2510.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Adding multiplex community detection from Leiden: https://leidenalg.readthedocs.io/en/stable/multiplex.html#layer-multiplex. It seems very straightforward and would be the most simple way to integrate two modalities on the graph. We would make great use of it in Squidpy (rna counts+image), but I think it should live in Scanpy becasue it could be useful for other multi-modal data. This is a duplicate of #1107 and it has been extensively discussed in #1117 . In the latter however, lots of thought went into normalization/processing which is superfluous for this case as it is only specific for CITE-seq data. Here we'd just want to allow users to get partitions out of multiple graphs. This could be done in two ways:; - adding arguments to existing `tl.leiden`, so that it accepts multiple graphs and multiple resolutions params per graph.; - creating a separate function `sc.tl.leiden_multiplex`.; Any thoughts on this @ivirshup @Koncopd ?. I think @WeilerP also had some thoughts along these lines. Have you ever tried this out? is there any other analysis tool you explored with a simlar purpose? Would be interested to hear your thoughts!; worth mentioning that another approach, the WNN from seurat, was also mentioned here: https://github.com/theislab/scanpy/pull/1117#issuecomment-777020580; although am not sure how much work that requries.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818,2,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; We are increasingly using ProjectR as a transfer learning technique from one dataset onto others. It would be great if this were part of the scanpy package, since most of the rest of what we do with single-cell uses scanpy. I'm going to start working on integration of the two but doubt I have the data science experience currently to submit a PR to this project for it. Are there any plans already in the works to pull in transfer-learning techniques such as this?. Relevant links:. - [Article in Bioinformatics](https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btaa183/5804979?redirectedFrom=PDF); - [Article in Cell](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30146-2.pdf); - [GitHub](https://github.com/genesofeve/projectR)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1205:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205,3,"['learn', 'simpl']","['learning', 'simple']"
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [x] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Hello! I would like to use [MetaCell](https://github.com/tanaylab/metacell.py) for some analyses. It (supposdely) operates on the KNN graph. The R documentation is more complete and can be seen [here](https://tanaylab.github.io/metacell/articles/a-basic_pbmc8k.html#building-the-balanced-cell-graph), where it shows how to run the individual libraries on metacell",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1016:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1016,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. For gEAR (Gene Expression Analysis Resource), we have had a few users request Similarity Weighted Nonnegative Embedding (SWNE) plots instead of tSNE or UMAP plots. Having this embedding option (scanpy.tl and scanpy.pl) would allow use to expand functionality in several of our tools without having to leave the scanpy environment in order to provide them. Repo: https://github.com/yanwu2014/swne; Paper: https://www.sciencedirect.com/science/article/pii/S240547121830440X?via%3Dihub; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2308:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2308,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; I would like to see how to use scanpy to compare the cell types distribution per cohort per different condition. Imagine you have different disease state who have different drug exposure, so you need to compare different cell types in each cohort per each condition or drug exposure. so it id three dimension: cell types[Bcells and Tcells], disease status[CKD vs DKD] and drug exposure[absent vs non absent]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1804:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1804,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1568:168,simpl,simple,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; In function read_10x_mtx there could be an option to search for non-gzipped files when reading v3 10x. Currently, I have files barcodes.tsv features.tsv matrix.mtx, but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:168,simpl,simple,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Is there an option to use umap transform (https://umap-learn.readthedocs.io/en/latest/transform.html) with scanpy? E.g. for embedding new data into the same UMAP space.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259:168,simpl,simple,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259,2,"['learn', 'simpl']","['learn', 'simple']"
Usability,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; When determining colours of categorical variables in uns they are based on alphabetical order (if I am not mistaken) - being represented just as an ordered list. Thus it is a bit inconvenient to change colours, especially if categories change during the analysis - the whole order changes and the mapping breaks. Would it be possible to use a dictionary of colours as values and categories as keys (with a default for any categories missing colours)?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340:168,simpl,simple,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Add color annotation to dotplot, e.g. same as row_colors and col_colors in seaborn heatmap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194:168,simpl,simple,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?; ...; It would be nice to have something like https://github.com/powellgenomicslab/Nebulosa to plot sparse genes.; On one hand, ordering with highest value on top (top plot) does not always work as what is below top layer is hidden and decreasing point size to combat this is not always good if different regions of UMAP are differently dense, thus creating white patches. On the other hand, random ordering (middle plot) can be hard to look at for sparse genes.; The gene on the plot is highly correlated with pattern from the bottom plot, but this is not so clear when plotting the gene alone. Sort order=True; <img width=""221"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/136395395-ec372b26-6552-4136-ae91-875713f700cb.png"">; Random cell ordering; <img width=""217"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/136395363-9e9fad24-e451-4d47-ac64-ff9b6b7b5774.png"">; Strongly correlated with; <img width=""194"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/136395551-1d6731aa-0749-425b-be68-f57344df0376.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2013:168,simpl,simple,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2013,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"<!-- What kind of feature would you like to request? -->; - [N] Additional function parameters / changed functionality / changed defaults?; - [N] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [Y] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [N] External tools: Do you know an existing package that should go into `sc.external.*`?; - [N] Other?. <!-- Please describe your wishes below: -->; Hello Scanpy,; As we know, Scanpy can calculate PCs and plot PCA plots by clustering cells with similar PCs around the nearby locations in the PCA space.; ```python; sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='leiden'); ```; ![image](https://user-images.githubusercontent.com/75048821/196794747-04e88d2d-05e8-4224-91d7-f403b5376e4f.png); Sometimes, we're trying to use PCA plots to show that how similar multiple samples are. For examples, we concatenate multiple scRNA-seq datasets, and do the PCA plot.; ```python; adata_concat = adata1.concatenate(adata2, adata3, adata4, batch_categories=['adata1', 'adata2', 'adata3', 'adata4']); sc.tl.pca(adata_concat, svd_solver='arpack'); sc.pl.pca(adata_concat, color='batch'); ```; We're expecting to see the PCA plots like this (one scRNA-seq dataset, one dot); ![image](https://user-images.githubusercontent.com/75048821/196796252-12c072d1-992e-4056-af09-10168f225a9a.png); but it generates PCA plots like this (every cell has a dot); ![image](https://user-images.githubusercontent.com/75048821/196802697-e751d475-2ac6-4cea-9af5-02340ef6d023.png). **We're wondering whether it is possible to plot a PCA plot for multiple datasets with one single dot representing one scRNA-seq dataset.**. Thanks!; Best,; Yuanjian",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2356:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2356,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?. <!-- Please describe your wishes below: -->; Sorry to file two issues in a row! I've been using Scanpy + scVI for my analysis recently, and am really enjoying it!. Currently, if use_rep is set in `sc.pp.neighbors`, then n_pcs is ignored. These seems like sane default behaviour to me - if we aren't using 'X_pca', then n_pcs doesn't really make sense. . This can actually be limiting, as I recently discovered. If you calculate a latent representation with scVI with `n_latent = 50`, you can't then do... ```python; sc.pp.neighbors(adata, use_rep='X_scvi', n_pcs = 25); ```. ...as the neighborhood graph is calculated on all 50 latent variables. If I want to use only 25 - say, as a point of comparison to see which best represents my data - I have to recalculate the latent representation with scVI with `n_latent = 25`. Basically, if you aren't using PCA, you have to calculate a full reduction for every number of dimensions you are interested in. . I don't think the fix would be that major. The source seems to be in the `_choose_representation` function, with the directly relevant snippet below:. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L48-L58. I think the only change needed would be to have the `n_pcs is not None` catch in all cases, not just when `use_rep = 'X_pca'`. Might also generalise the variable name to make it more clear it refers to any reduction, not just PCA. Admittedly, I only just started exploring the code base, so I'm not sure where else `_choose_representation` is called, or what other impacts this could have. . I have some blue sky time tomorrow, so I'll fork it then and see if I can whip up a pull request. Thanks for the excellent product!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1281:1540,clear,clear,1540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1281,1,['clear'],['clear']
Usability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Hi,; ""Scanorama outputs both corrected expression matrices and embeddings."" However, `scanpy.external.pp` only implemented `.scanorama_integrate` #1332 . Is there any plan to implement the `.scanorama_correct` method as well? Or let `.scanorama_integrate` output the correct matrices as an option. ; Thanks!; Hurley",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2002:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2002,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:; ```PYTHON; sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1); ```; or . ```; sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'); .add_dendrogram(width=0.4,color='grey'); .swap_axes(); .dot_size_legend(title='fraction', location='left'); ```. Any comments? ; (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/956:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?; - [X] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [X] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; t-SNE is an interative algorithm, and takes numerous iterations to converge, particularly on larger datasets. For example, if MulticoreTSNE is installed, it accepts `n_iter=30000` as opposed to the default `n_iter=1000`. It would be nice to have this parameter exposed. For larger datasets of say 200K cells, 1000 iterations isn't enough to fully converge to its final compact cluster shapes. . Alternatively, is it possible to pass in a kwargs to scanpy tools that wrap other algorithms, so that the advanced user can flexibly look up [additional MulticoreTSNE parameters](https://github.com/DmitryUlyanov/Multicore-TSNE/blob/62dedde52469f3a0aeb22fdd7bce2538f17f77ef/MulticoreTSNE/__init__.py#L55) to modify, without needing to exhaustively enumerate all parameters in the scanpy wrapper?. Finally, it would be even better to have the faster FFT-based tsne to generalize to millions of cells, the most recent re-implementation being https://github.com/pavlin-policar/openTSNE. In the mean time, one has to overwrite the `.X_tsne` attribute after running these other tools separately.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1150:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1150,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. ![](https://els-jbs-prod-cdn.jbs.elsevierhealth.com/cms/attachment/e9e581b0-24ce-4e5b-a848-11de803dba32/gr3_lrg.jpg). Hi! It would be great if Scanpy could create sublevels of annotation like the the attached figure 3D of the seurat v4 paper. This would make legends clearer in cases where you have different subtypes of T cells for example. For instance the `color` argument could take as an input a dictionary with the label categories.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2024:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2024,2,"['clear', 'simpl']","['clearer', 'simple']"
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. ## Description. ATM, the argument `delimiter` in `scvelo.readwrite.py::read` is not passed to `read_csv`. For more flexibility when reading from CSV files, it would be good to do so.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1968:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1968,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. ; ; 1. I would be great if the `reference` option in `sc.tl.rank_genes_groups()` would accept not only 'rest' or a single group identifier, but a list of identifiers (as is possible for the `groups` option). Currently, lists such as `reference=['g1','g2','g3']` are not accepted:. <details>. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Input In [72], in <cell line: 2>(); 1 # each WT sample individually vs all other WT samples; ----> 2 sc.tl.rank_genes_groups(remerged_WT, groupby='sample', groups=WT_samples, reference=['g1','g2','g3'], ; method='wilcoxon'); ; File D:\Programme\Anaconda\envs\squidpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py:570, in ; rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, ; tie_correct, layer, **kwds); 568 if isinstance(groups_order[0], int):; 569 groups_order = [str(n) for n in groups_order]; --> 570 if reference != 'rest' and reference not in set(groups_order):; 571 groups_order += [reference]; 572 if reference != 'rest' and reference not in adata.obs[groupby].cat.categories:. TypeError: unhashable type: 'list'; ```. </details>. 2. Also I would appreciate if `sc.tl.rank_genes_groups()` would allow testing against a reference which is (partly) included in the groups that are being tested. In other words, I would like to test all individual groups `['a','b','c','d']` against a fixed reference `['b','c','d']`. Cur",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2317:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2317,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Categorical legends can be removed from embedding plots by passing the argument `'none`' to `legend_loc` (e.g. `sc.pp.umap(adata, color=""CellType"", legend_loc='none')`. It would be useful if the drawn colorbar is omitted when plotting continuous data if `legend_loc='none'` is passed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2127:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2127,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Currently for most plots (where it makes sense) you can specify the column of .var which contains gene symbols. This doesn't seem to work for sc.pl.violin because. https://github.com/theislab/scanpy/blob/c488909a54e9ab1462186cca35b537426e4630db/scanpy/plotting/_anndata.py#L727. is not getting passed a `gene_symbols` argument when called from violin (that method does accept that argument). This should be an easy fix and make violin plot behave a bit more closely to the other plots. Happy to submit a PR.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1794:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1794,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Especially when we visualize large datasets with multiple categorical variables (e.g. patient, disease, cell type) using `sc.pl.dotplot`, and we use a sequence in the `groupby` argument (`e.g. sc.pl.dotplot(ad, 'genex', groupby=['individual', 'disease_status', 'cell type'])`), sometimes we end up with too few cells in some rows, in which summary statistics like fraction of nonzero expressors or mean expression are not very robust. To avoid that, I think it'd be cool to have a minimum observation cutoff in the function, where e.g. `min_cells=5` would show `groupby` combinations with at least 5 cells. Without this option, this sort of filtering becomes an annoying pandas exercise (which some might enjoy but possibly not everyone).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1829:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1829,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. The current `external.pp.scanorarma_integrate()` function has a `basis` argument which isn't actually used in the code (as far as I can tell). This is probably confusing to users (definitely was to me) so I would suggest removing it or at least mentioning in the docs that it isn't used. I can probably submit a PR if maintainers let me know what the preferred option is.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1745:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1745,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. When we visualize gene expression, range of colorbar can be different. ![image](https://user-images.githubusercontent.com/30337666/89102749-a6d10b00-d43e-11ea-99f7-0867f3c31d13.png). Usually, it's fine. But if we want to compare gene expression in control-treat experiment in single-cell level, sometimes this happens. ![image](https://user-images.githubusercontent.com/30337666/89102998-08927480-d441-11ea-9068-3d7f09ab26a3.png)![image](https://user-images.githubusercontent.com/30337666/89103003-1942ea80-d441-11ea-8a6f-091a6cd2f19b.png). It's hard for us to estimate up or down regulation of this gene in different group of cells because colors with same value in the two figures are not consistent. Plotting the two figures with same colorbar range can solve it. So, I want to know that are there parameters solving it in scanpy.pl.umap or some matplotlib methods that can be integrated with Scanpy?. Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1352:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1352,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ... Hi!. We're considering implementing some of the t-SNE recommendations in https://www.nature.com/articles/s41467-019-13056-x for our single-cell analysis. They use a different t-SNE implementation (https://github.com/KlugerLab/FIt-SNE), and before I ran off doing my own wrapping and plumbing to integrate with Scanpy I thought I'd check: have you considered integrating this yourselves?. Thanks!. Jon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/996:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/996,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Hi scanpy develovepers,. A rotation student asked me what is `sc.pl.umap` showing if `sc.tl.umap` was not computed beforehand. To which I don't have the answer since I have never done it. If you know the answer I'd like to know it, but most importantly, I think it would be nice to have an error message in the UMAP plotting function if UMAP has not been computed. Unless there were meaning and a reason to use `sc.pl.umap` without running `sc.tl.umap` previously, and it was designed that way purposely. I assume this would apply to other plotting functions too. Thanks!; Alejandro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1460:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1460,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; As more and more technologies allow multimodal characterization of single cells it could be useful to exploit some functionalities of scanpy's toolkit to perform, at least, some rough integrative analysis. Assuming we have to modalities on different layers (say RNA and ATAC), one could create two knn graphs for both layers and use `leidenalg.find_partition_multiplex` to perform a joint call of partitions handling the two (or more) graphs as a multiplex. I have tested myself this approach, described in [leidenalg documentation](https://leidenalg.readthedocs.io/en/latest/multiplex.html), it works and it is highly configurable. ; We can take care of the implementation of enhancement (as `leiden_multiplex()` function?), I just want to be sure that it is not already on the development roadmap and that it is ok to have it into scanpy and not as an external tool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Hey,; currently, when trying to use plotting functions that require categorical obs columns (for example `sc.pl.clustermap` `obs_keys` parameter), but one passes a boolean column key in `.obs,` scanpy will raise an error (or pandas does but the origin is in scanpy's codebase): `AttributeError: Can only use .cat accessor with a 'category' dtype`. Would it be possible to let the passed key be from a column of `dtype bool` as well? Are there any downsides? Happy to provide more detail if needed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2249:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2249,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Hi,. Thanks for developing scanpy, which is quite handy. I am just wondering would it be possible to add a parameter like `sort_by` in `tl.rank_genes_groups()`, so sorting by logfoldchanges could be feasible for each group. Or I will appreciate it if you can help me with alternatives. . I understand this might be a minor thing, so please feel free to say no :). Thanks,; Nan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2247:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2247,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Hi,; I'm wondering if it is possible to add a new feature to sc.pl.dotplot if it is not too much of work. Say I'm interested in just one gene, and I want to plot the expression across two conditions. I understand that currently this could be achieved by using groupby = ['var1', 'var2'], but it'll be only one column, and conditions will be coerced into var1_var2. Is it possible to add a feature to the plotting function and change this behavior? I want var1 to be the x axis and var2 to be the y axis. Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I find using the `adata.layers` really useful to compare normalisation strategies. I'd like to also be able to seamlessly run `sc.tl.pca` on data stored in different layers of the same `anndata` object. . At the moment my workaround is to set `adata.X` before PCA and changing the key to the `adata.obsm` element after:; ```python; adata.X = adata.layers[""mylayer""]; sc.tl.pca(adata); adata.obsm[""mylayer_pca""] = adata.obsm[""X_pca""]; ```; Ideally I'd like to just be able to set a `layer` argument in `sc.tl.pca`, as in the plotting functions. . Any plans on linking data layers to dimensionality reductions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I would like to use a precomputed distance or affinity matrix with many of the tools in scanpy. Currently this feature is unsupported. It should be a relatively small PR - essentially adding ""precomputed_key"" to `sc.pp.neighbors` etc that specifies a key in `obsp` to look for distances.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2453:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2453,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; It is a lovely tool, but it would be great to be able to not plot figure legends in, e.g. sc.pl.umap, so that all plots (regardless of what annotation is being colored) are exactly the same size. Perhaps there is already a way to do this, but I've yet to find it. . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1502:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; It would be great to manually specify the `.obsm` key generated by the `.tl` functions like UMAP, PCA, etc. `sc.tl.umap(adata, key_added=""X_custom_umap"")`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; It would be great to see alternative statistics to ""mean"" in [matrixplot](https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.matrixplot.html), e.g., median, variance come to mind.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1709:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1709,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Quite often I need to color UMAPs based on features that are not part of `adata.X` but `adata.obsm` for the reason that they are special. E.g. KO data with gRNAs versus endogenes/ target genes, or viral genes versus edogenes. Example use case: ; - Cluster cells based on endogenes; - UMAP and color by a bunch of viral genes. Clustering must not include these viral genes -> must be excluded from `X`. ; I don't want to store so many additional columns in `obs` and I need to have these features separated in their own matrix for downstream analysis, which is why I want to use `obsm`. Can we have sth. like this:; ```; sc.pl.umap(adata, color='viral_genes') # adata.obsm['viral_genes'] is a pandas.DataFrame ?; ```. It shouldn't be overcomplicated I think, since this only involves an additional check: if the elements in the color arg list are not found in `obs.columns` nor `var.columns`, then check the keys in `obsm` and use the entire dataframe behind this key.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1500:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1500,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/984:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Would be great if `sc.pp.scale` could support scaling within a user-defined category - for example scaling the cells generated using 3' vs 5' 10X technology can be used to account for this source of batch effect. If this is easy to implement of your side, I think this option will be useful for many applications.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2142:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2142,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Would it be possible to add a parameter 'annotate_var_explained' = True/False to the function scanpy.pp.pca to annotate the x- and y-axis of the PCA plot with the variance explained by the component? E.g. x-axis label = PC1 (29%) and y-axis label = PC2 (3%),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1445:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1445,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; hi scanpy team,. it is great to have rapids/gpu version of knn and umap that it has made my millions cell analysis much faster. however clustering is still very slow. i wonder if scanpy can support rapids leiden clustering. thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2358:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2358,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; would be really cool to have adjustText for automatic ordering of text in `sc.pl.embedding`. https://github.com/Phlya/adjustText. has anybody ever looked into it?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513,1,['simpl'],['simple']
Usability,<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. Allow using seaborn color map strings: https://twitter.com/michaelwaskom/status/1367553033454968834. ![image](https://user-images.githubusercontent.com/8238804/110413994-1ef16480-80e3-11eb-9d46-c7ba3ffe3f1f.png). I think this should be as easy as swapping out `mpl.cm.get_cmap` for `sns.color_palette`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1723:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1723,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ x] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ... 1. exposed a setting to prevent `sc.pp.<fn>` from saving with a ""prefix"" (default filename) if `type(save) == str`. Why?; I don't want my files having that prefix. . 2. please add PHATE and magic-impute to embedding options :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2450:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2450,1,['simpl'],['simple']
Usability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; How to use sc.pl.correlation_matrix to compute correlation between two different anndata? I want to compare urine""data1"" with biopsy cells""data2"". ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1760:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1760,1,['simpl'],['simple']
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1877:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1877,60,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. - Depends on #2814. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2777, closes #2807; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. TODO:. - [x] batched. <table>; <thead>; <tr>; <th scope=row>. `flavor=`. <th>. `""seurat""`. <th>. `""cell_ranger""`. <tbody>; <tr>; <th scope=row>. `n_top_genes=n`. <td>. - [x] &zwnj;. <td>. - [x] (https://github.com/dask/dask/issues/10853). <tr>; <th scope=row>. `{min,max}_{disp,mean}`. <td>. - [x] &zwnj;. <td>. - [x] &zwnj;. </table>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Only check the following box if you did not include release notes -->. ## TODO:. - [x] Fix tests; - [x] Figure out PCA test case with anndata 0.8.0; - [x] Add CI job; - [x] Rename CI job to be less similar to minimal dependencies, this will probably be `MinVer`; - [x] Bump anndata requirement back down to 0.7.3 (breaks dask tests); - Maybe 0.8 is low enough?; - [x] Bump pandas requirement back down to 1.5 (breaks grouped plots ordering). ## Some thoughts. * Sibling PR to: https://github.com/scverse/anndata/pull/1314; * Not completley sure what to do about plotting tests yet. Possible we just ignore any comparison failures, but ideally we could still know if these are broken.; * Metric consistency test failure is from https://github.com/scverse/scanpy/issues/2688; * Test updates in https://github.com/scverse/scanpy/pull/2705 (plus bumping one test a little lower) fixes it. <!-- Please check (“- [x]”) and fill in the following boxes -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ X] Closes # (New Feature); - [ X] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [X ] Release notes not necessary because:. I thought I would give a shot at contributing to this awesome tool! I added a function to Scanpy's plotting API that I use in my own research for creating stacked barplots for visualizing compositions of cell groups. An example is depicted below:. ![image](https://github.com/scverse/scanpy/assets/5004419/21885a72-e15f-4f94-b1e5-84c1de8ca954). Specifically, this function enables one to plot the fraction of each cell group (e.g., cluster) that are labelled with a specific categorical variable. For example, if the cell groups are clusters, then one might be interested in examining the fraction of each cluster that originates from each ""batch"" of cells or each patient sample. This function also enables ordering of the groups according to a specific value (e.g., a patient number or batch ID) or by agglomerative clustering. An example of ordering the groups based on agglomerative clustering is shown below:. ![image](https://github.com/scverse/scanpy/assets/5004419/bfde8173-4f0d-483f-b37e-849446b65153). The function supplies an argument to specify whether the dendrogram should or should not be plotted. Please let me know if this feature is of interest and if so, what else needs to be adjusted or fixed prior to merging. Thanks!. Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2873:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2873,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes # _no existing issue_; - [ ] Tests included or not required because: _No new tests_; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: _I did not write release notes_. Hi :). I am proposing a change that speeds up `filter_cells` (x1000 speedup) and `filter_genes` (x2 speedup) for CSR sparse matrices. On my personal machine for 1M cells, `sc.pp.filter_cells(adata, min_genes=xx)` runs in 1ms instead of 10s currently. The speedup should be even stronger on sparser modalities like ATAC. In spirit, this simply replaces `np.sum(X > 0, axis=axis)` with `X.getnnz(axis=axis)`, which is much more efficient. But the axis argument in `getnnz` in `csr_array` may be deprecated. I think it should still be fine with `csr_matrix`, but since I don't know for sure I manually implemented it for the CSR case as in https://github.com/scipy/scipy/issues/19405 . What do you think?. Regarding `getnnz`: Of course it would be nicer to be able to write `.getnnz(axis=axis)`, which extends beyond CSR to other sparse matrices. Can we assume that we're getting sparse matrices and not sparse arrays ?. Pinging @dschult from the Scipy issue liked above, who mentioned: . > I'm pretty sure that a reasonable and commonly occuring use-case would be enough to make the developers include this feature somehow. (edited because I confused `csr_array` and `csr_matrix`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2772:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2772,3,"['guid', 'simpl']","['guide', 'guidelines', 'simply']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #1263; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Implements something close to what is described here:. * https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554. Doesn't:. * Use ""current"", instead that's just `None`; * Include random, instead lets the user pass an array to order by, so random can be `rng.permutation(adata.n_obs)`; * Do broadcasting. Can either be added later or we can tell people to do this themselves. ## TODO:. - [ ] Check when input order array has repeated values; - [ ] Test sort_order argument deprecation; - [ ] Add support for `pd.Series` array values.; - [ ] Maybe `list`s?; - [ ] ""How to"" or modify existing advanced plotting tutorial; - [ ] Tests for; - [ ] Categorical ordering; - [ ] None is same as `np.arange(N)`; - [ ] direct overlap + ordering is equivalent to masking; - [ ] Continuous ordering; - [ ] ""ascending"" is like `np.argsort(values)` and vice versa; - [ ] ""ascending"" is like ""descending"" for inverted values; - [ ] Check masking for both; - [ ] Errors; - [ ] For incorrectly sized input array; - [ ] incorrect non-array input; - [ ] ""ascending"" is when the highest value goes on top, right?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2998:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2998,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2872:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2872,22,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Hi,; We are submitting PR for speed up of the clipping part of scaling function. ; | | Time(sec)|; | -----------| ----- |; | Original | 11.82 |; | Updated | 1.59 |; | Speedup | 7.433962264 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluste",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100,3,"['guid', 'simpl']","['guide', 'guidelines', 'simplefilter']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because: docs; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3001:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3001,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit.; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3313:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2779:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2779,4,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. @Intron7 . This adds benchmarks for the off axis for all parameters. The off axis peak memory is lower since we `.pop` that layer. If you think that’s confusing I could change it. Regarding big datasets for `_get_mean_var`, I already added that benchmark. You could maybe check if there’s anything else that should go into the `FastSuite`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3147:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3147,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Builds on and supersedes #2482,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2977:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2977,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] See https://github.com/scverse/scanpy/pull/3216/checks?check_run_id=29482422648; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3217:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3217,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Testing for #2969; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2976:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2976,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [X] #1549; - [X] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2770:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2770,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [X] Closes #1549; - [X] Tests included or not required because:; - Added regression test for subsetting var_names; - Added test for when groupby is None; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [X] Fixes #1867; - [X] Tests included or not required because: New tests included which catch the failure mode described in #1867. Current implementation fails these.; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: Added entry in release notes. Addresses issue #1867 with a fix as outlined by @jlause and tests which catch the failure mode detected and nicely demonstrated by @jlause.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2757:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2757,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Adresses #2088 and adresses #1733; <!-- Only check the following box if you did not include release notes -->; - [x] Tests included or not required because: They are required and some suggested; - [x] Release notes; - [x] Doc update - depending on feedback here; - [x] Doc update - guidance scanpy vs seurat. **Context**; As discussed in issues #2088 and #1733, `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,4,"['feedback', 'guid']","['feedback', 'guidance', 'guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1053; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1633; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. This adds relies on `_empty` to check which parameters have been specified, and only changes those",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3206:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3206,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #173; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Docs:; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/release-notes/index.html#version-1-10; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pp.scrublet.html; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pp.scrublet_simulate_doublets.html; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pl.scrublet_score_distribution.html. ### How to review this PR. I made tests quantitative before this PR, so note that the only change that modified tests is 42143d88a0d499130fac8e5ca60eef0c19163734. In that PR, I make it so there are no longer any duplicate simulated doublets being created. This is necessary to be able to support any neighborhood detection algorithm. I also feel like it makes more sense. This is the only algorithmic change to upstream. Please use your own judgement to check if this makes sense to you. ### TODO:. - [x] remove unused utils (plotting, preprocessing); - [ ] figure out what remaining utils to replace with ours; - [x] PCA/SVD: https://github.com/scverse/scanpy/blob/bf5f1f9343f5729df6f90f7c68363682022e0480/scanpy/preprocessing/_scrublet/__init__.py#L415-L417; - [x] mean_center, normalize_variance, zscore: small enough to be left alone I think; - [ ] get_knn_graph: no need to have multiple implementations here, but our current implementation automatically calculates connectivities, which this doesn’t need https://github.com/scverse/scanpy/pull/2723; - [ ] refactor so the class API ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2703:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2703,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1861; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3183:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3183,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1861; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3184:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3184,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1986; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3299:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2012; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. TODO:. - [ ] Figure out and fix this: https://github.com/scverse/scanpy/blob/4fb3dc7d11f1b067f0aea1008cd3e7fbc3e5d54c/scanpy/metrics/_gearys_c.py#L128-L131 (related to https://github.com/numba/numba/issues/6976?); - [x] Maybe remove these: https://github.com/scverse/scanpy/blob/4fb3dc7d11f1b067f0aea1008cd3e7fbc3e5d54c/scanpy/tools/_umap.py#L167-L171 https://github.com/scverse/scanpy/blob/4fb3dc7d11f1b067f0aea1008cd3e7fbc3e5d54c/scanpy/neighbors/_connectivity.py#L120-L123; - [x] release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2870:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2870,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2236; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3286:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2427; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2782:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2782,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2644; - [x] Tests included or not required because: dev workflow; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev workflow. Very simple, following https://docs.pypi.org/trusted-publishers/adding-a-publisher/. The change removes most of the technical parts of making a release including `twine check` which is just done by default by the GH action. The only parts I’m not 100% sure about removing are; - “When to make a pre-release” – I feel like “if UR unsure, make one of these” wasn’t helping here either, so maybe that should just be fleshed out as a section now we’re down a few sections; - “Check the file contents of the wheel” should probably go into “how to code review a PR that touches the build process”, and we don’t have any other guides on how to do code reviews, so …",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2720:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2720,4,"['guid', 'simpl']","['guide', 'guidelines', 'guides', 'simple']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2680; - [x] Tests included or not required because: this is a longterm fix - failing of this functionality would be captured by existing tests; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2739:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2739,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2688; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: just modifying tests. Fixes tests for metrics. Some notes on an in progress PR:. * Previously xfail tests didn't actually fail because nothing was asserted; * This behavior changes with version of numba.; * numba .56<= seems more reproducible, but differences are greater when they occur (e.g. calculating on sparse vs dense); * Ideally want per metric, per calculation tolerances; * Both threading options can differer; * ~~I think single threaded + `fastmath=False` is reproducible, but need to confirm~~ – still no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2705:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2705,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2730; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2734:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2734,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2741, Closes #2276; - [x] Tests included or not required because: Only docstring changes; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: Added. **Notes**. - [x] How does `sc.pp.combat` return `None`? A: when `inplace=True`: it never returns an `AnnData` object though. Hence removed `AnnData` from the return type list. - `sc.pp.normalize` has both `inplace` and `copy` so did not force to harmonize with others; - `sc.pp.pca` allows adata and array/sparsematrix input, so did not force to harmonize with others; - `sc.pp.filter_cells`, `sc.pp.filter_genes` , `sc.pp.subsample` acts on data in different manner (changing dims), so did not force to harmonize with others; - `sc.pp.log1p` , `sc.pp.sqrt` seem to be understandable enough without bloating this up",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2746; - [x] Tests included or not required because: this is a minor fix; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: the related code has not been released by scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2743:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2743,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2762; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process change. All changes were automatic, except for:. - removing unused imports or replacing them with `__all__`. Much more uncontroversial than AnnData as scanpy’s public modules were more well defined from the start. There were no ambiguous cases except for `sc.get` re-exporting `""_check_mask""`, `""_get_obs_rep""`, and `""_set_obs_rep""`. But since those aren’t documented, we can decide over their fate at a later date.; - Fixing circular imports like `sc.{pp,tl}.pca`. I only needed to create a `neighbors/_doc.py` file for shared neighbors/tools doc parts, and put the `pca` import in `tools` in `__getattr__` (supported since 3.7); - replacing some `with open(p) as f: x = json.loads(f.read())`s with `x = json.loads(Path(p).read_text())`. All in all surprisingly easy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2761:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2761,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2788; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2789:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2789,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2804; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2928:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2928,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2808; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3340:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2836; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev change. Changes:; - Removes import-time change to globals:; 	- `matplotlib.testing:setup` should be called before each (plotting) test; 	- `sc.set_figure_params(dpi=40, color_map=""viridis"")` seems to be overwritten. When calling it inline, it messes up the figure params; 	- `sc.pl.set_rcParams_defaults()` is redundant, `setup` from above does that.; - Use workaround from https://github.com/pytest-dev/pytest/issues/11759#issuecomment-1888888146",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2838:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2838,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2839; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. How to review:. 1. https://github.com/scverse/scanpy/compare/585f58c9e4dd82dd7809a831538c4e230b008818...60804430db089f1887085e537ad946b7f691a8b4; 2. 5710a1de4f091db607d76b790676b56f988638f8; 3. https://github.com/scverse/scanpy/compare/5710a1de4f091db607d76b790676b56f988638f8...2cbe106328bcb9585c13def47ac3e1bf9629e448,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2844:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2844,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2842; - [x] Tests included or not required because: it's docs; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2849:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2849,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2858 ; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: this is a minor quality of life change, and could be tacked onto any other release as opposed to creating a dedicated release",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2859:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2859,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2866; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: technical fix,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3125:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3125,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2878 ; - [x] Tests included or not required because: dev change; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: dev change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2879:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2879,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2902; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2905:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2905,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2910 ; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2921:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2921,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2929; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: Fixes a bug that wasn't in a release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2934:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2934,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2930; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: Fixing something that never made it to a release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2950:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2950,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2931; - [x] Tests included or not required because: doc change; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: No release yet,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2932:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2932,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2935; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: Code has not been in a release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2951:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2951,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2937; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3307:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2969; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Seems like this code is super performance sensitive: Having a Python implementation of `getrandbits` in 8572ecb1b38616f98f2af6462aa4fe5a3a8871ae resulted in a slowdown:. | Change | Before [0d4554b4] | After [1b2d9dd5] | Ratio | Benchmark (Parameter) |; |----------|----------------------|---------------------|---------|------------------------------------------|; | + | 15.2±0.03ms |	31.7±0.1ms |	2.09 | preprocessing.time_highly_variable_genes |,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3041:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3041,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2973 ; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Visium HD stores its coordinates in a `.parquet` file. This loads said file.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2993; - [x] Tests included or not required because: fixing dev install; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2994:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2994,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2993; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev change. Pytest 8.2 is released and should solve the problem. /edit: it does. It’s installed in the test jobs and works.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3034:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3034,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3010 ; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:. no need to backport github changes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3040:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3040,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3012 ; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. I added a sparse dataset to cover more code paths in most benchmarks. `regress_out` seems pretty slow with sparse data, maybe that should be tackled instead of hidden by disabling the benchmark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3031:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3031,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3027; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: they are, added. This PR fixes the bug reported in the linked issue. A new test which spots the erroneous computations has been added. I would use this chance to refactor the `_highly_variable_genes.py`, rather than using the 2-lines fix suggested in the first commit:; Doing the multi-batch hvg flagging differently for seurat_v3 and seurat/cell_ranger is what made this bug hard to spot in the first place I think.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3051; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. TODO:. - [x] release notes; - [x] some added text explaining things; - [x] run internet tests, implement caching for datasets. Optional:. 1. continue to not run the internet tests in CI. A side effect of this PR is that our tests get less flaky by not running the flaky `ebi_expression_atlas` doctest; 2. run internet tests in CI; 1. add caching to CI; 2. make sure the dataset functions don’t download already-downloaded data; 3. validate cached data instead; 4. run the internet tests (with caching) in CI. ## [Rendered](https://icb-scanpy--3060.com.readthedocs.build/en/3060/api/datasets.html)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3060:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3060,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3062; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3101:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3101,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3074; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: trivial change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3075:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3075,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3083; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Two different fixes:. 1. The `np.asarray` cases are when we e.g. had `spmat.sum(axis=1).A` (i.e. a dense matrix). I could also leave these as `.A`.; 2. the `.toarray()` cases are when we converted a sparse matrix to dense directly.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3084:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3084,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3087; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3089:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3089,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3114; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Other things done:. - Rename some variables and reorder functions so the diff between the metrics is minimal for a future unification; - Skip seurat v3 tests with numpy 2 because skmisc isn’t ready: https://github.com/has2k1/scikit-misc/issues/34. This will skip the seurat v3 tests for all but the minimum versions test job for now, but I think that’s OK?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3152; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3196:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3196,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3157; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3176:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3176,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3158; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3302:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3199; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3204:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3204,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3226; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3260; - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3275:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3282; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3283:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3310; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: real small change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3328:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3318; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3324:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #761, closes #2322, closes #2229, closes #2267; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Also. - documents the parameter better, see #1502; - remove duplicated tests for embedding plots with continuous variables: `legend_loc` does nothing there (yet)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3163:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3163,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A (infra change for other PRs touching `external`); - [x] Tests included or not required because: works if docs build; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: see above. Adds link targets for `scanpy.external.{pp,tl,pl,…}` like they exist for `scanpy.{pp,tl,pl,…}`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2716:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2716,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A (see https://github.com/scverse/scanpy/pull/2809#pullrequestreview-1864823464); - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev change. Adds a consistency test before #2809 is merged. When these tests were written originally, the `.csv` file extension was used for files that aren’t comma-separated. This fixes that too without changing too many files.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2851:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2851,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: Release documentation; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2726:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2726,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: dev fix; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev fix,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2845:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2845,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: dev process; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2707:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2707,4,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: dev process; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process. Filt resulted in the release workflow failing, as it tries to install the package’s runtime dependencies. Backporting the switch to hatch fixes that, now building only needs build deps",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2727:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2727,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: dev process; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process. This can help us debug long running tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2786:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2786,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: format change; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: format change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2718:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2718,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: manual test: check if logo displays in rendered docs; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: unreleased bug. Seems like GitHub renders .svgs even if the namespace isn’t there, but browsers don’t.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2749:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2749,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: release prep; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2724:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2724,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2728:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2728,6,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2852:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2852,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev changes. This PR contains more quantitative scrublet tests. It should be merged prior to #2703 to make sure our changes don’t change how scrublet works.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2776:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2776,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2775:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2775,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: docs change. Also bump scanpydoc so the `[source]` links for wrapped functions work. E.g. `sc.pp.filter_cells` now links to the correct code lines):. ```diff; -<a href=""https://github.com/scverse/scanpy/tree/419c1a45aef26b5531a5b9cf1ec430e5ae67ce97/python3.11/site-packages/legacy_api_wrap/__init__.py#L49-L193"">[source]</a>; +<a href=""https://github.com/scverse/scanpy/tree/2d5bda1e45525354b9b751aa572c0b08175450cf/scanpy/preprocessing/_simple.py#L49-L193"">[source]</a>; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2800:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2800,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: fixes tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2745:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2745,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: improves an earlier unreleased change. For some reason, PyNNDescent takes >2s to import: https://github.com/lmcinnes/pynndescent/issues/111. This change makes e.g. test collection fast again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2715:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2715,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: internal API,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2750:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2750,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: only minor changes. some small changes before #2809,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2810:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2810,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: small change. And for broken links, I found the working location manually. The only website that doesn’t have a working HTTPS version is http://vitessce.io (https://github.com/vitessce/vitessce/issues/1121), which is impressive given the late-90s-look of some of those journal websites.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2737:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2737,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Since today, there are some test breakages:. - `adata[:, [True, True]]` now behaves like `adata[:, np.array([1, 1])]` instead of `adata[:, np.array([True, True])]` (exposed through `read_10x_mtx`); - `sc.pl.violin` now creates slightly wider plots through some dependency change. This fixes everything that broke.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2801:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2801,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: Modifies unreleased features. I made the code and comments/docs more explicit about when the nearest-neighbor-is-cell-itself comes into play and when it doesn’t, and tested a bunch of our utilities. @mumichae this might make it fast for your code; You can install this branch using. ```bash; pip install -U 'scanpy@git+https://github.com/scverse/scanpy.git@improve-neighbors-shortcut'; ```. | | Before | After |; | --------- |--------|--------|; | small data | ![master](https://github.com/scverse/scanpy/assets/291575/d79a7bbd-548d-4aea-9cff-6ee44ca544ac) | ![shortcut](https://github.com/scverse/scanpy/assets/291575/d802b0d5-09e3-4002-862f-4ae4c368b061) |; | bigger data | ![master](https://github.com/scverse/scanpy/assets/291575/627ccd9f-7a42-44be-81d9-7796e54e54b0) | ![shortcut](https://github.com/scverse/scanpy/assets/291575/000b06a4-1c6c-496b-9fda-b479414e182c)|",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2756:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2756,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev change. This allows us to test log entries with caplog instead of awkwardly through setting the logfile and dealing with strings,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2855:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2855,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: original PR not released,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2857:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2857,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: small dev change. We already use it everywhere in the lib code, this brings the tests in line",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2817:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2817,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: tests only change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2785:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2785,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes NA; - [x] Tests included or not required because: dev change; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev change. This allows clickable links when developing in a path with spaces. TODO: check if nunit attachment URLs are predictable. If yes, emit them instead of paths in CI runs, i.e. instead of the Expected/Actual/Diff part, display just this URL:. display: https://dev.azure.com/scverse/scanpy/_build/results?buildId=5698&view=ms.vss-test-web.build-test-results-tab&runId=18968&resultId=100831&paneView=attachments. /edit: doesn’t seem like it’s possible. The URL contains `resultId=100831`, and I don’t see how this could be known at runtime. I assume those get assigned after the NUnit XML gets uploaded. ----. Once https://github.com/microsoft/vscode/issues/176812 is fixed, we can change this to [OSC 8](https://github.com/Alhadis/OSC8-Adoption) links",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2860:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2860,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes https://github.com/scverse/anndata/issues/1210; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2719:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2719,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes https://github.com/scverse/scanpy/issues/2763; - [x] Tests included or not required because: manually checked using rtd PR build; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Adds [readthedocs-sphinx-search](https://readthedocs-sphinx-search.readthedocs.io/en/latest/) support via the scanpydoc theme, which contains JS and CSS customizations to make the search extension integrate with the theme. See. - https://github.com/theislab/scanpydoc/pull/121; - https://github.com/theislab/scanpydoc/pull/125. ### [rendered](https://icb-scanpy--2805.com.readthedocs.build/en/2805/). An alternative that looks nicer would be https://github.com/readthedocs/addons, but it’s still in alpha. PS: I didn’t add the same hack as in scanpydoc that makes the search work in PR builds, so you’ll only see “No results found” in the above. Check out https://icb-scanpydoc.readthedocs-hosted.com/en/latest/?rtd_search=scanpydoc to see rendered search results. You can see that the API works for scanpy:. ```console; $ http get https://icb-scanpy.readthedocs-hosted.com/_/api/v3/search/?q=project%3Aicb-scanpy%2Flatest+filter_cells; ╭──────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ count │ 2 │; │ next │ │; │ previous │ │; │ │ ╭───┬────────────┬────────────────╮ │; │ projects │ │ # │ slug │ versions │ │; │ │ ├───┼────────────┼────────────────┤ │; │ │ │ 0 │ icb-scanpy │ ╭───┬────────╮ │ │;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes scverse/anndata#1133; - [x] Tests included or not required because: docs only; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: just updates some tutorials. Mainly so we advertise anndata 0.11’s `read_elem_as_dask`: https://icb-scanpy--3216.com.readthedocs.build/en/3216/tutorials/experimental/dask.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3216:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3216,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Fixes #2830, fixes https://github.com/scverse/spatialdata/issues/440; - [x] Tests included or not required because: Tests are in #2816; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2832:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2832,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Fixes #2940 ; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: tbd. @ilan-gold, I put some comments where I'm most interested in your feedback for a first step forward",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2980:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2980,3,"['feedback', 'guid']","['feedback', 'guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: basically a dev change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3289:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Helps debugging #3068 ; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3069:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3069,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously; - [x] Tests included or not required because: Tested locally, no way to really test this; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: This is a bug fix of a warning suppression",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3298:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Tests included or not required because: it's formating; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: it's formatting. Why did we ever configure black?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2701:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2701,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Towards #2578 and maybe others? #2764; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:. Note I did not follow:; https://gist.github.com/Intron7/bbf5058794be7b81d3953ae39c17d8b8. This is because this PR is basically very simple. I just added an `axis_sum` function for dispatching on dask arrays (with sparse chunks) which now handles the needed functionality and then it propagates up to various functions as noted in the release note: `scanpy.pp.scale`, `scanpy.pp.filter_cells`, `scanpy.pp.filter_genes`, `scanpy.pp.scale` and `scanpy.pp.highly_variable_genes`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856,3,"['guid', 'simpl']","['guide', 'guidelines', 'simple']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Added the ```n_components``` parameter in the tsne function, similar to the one in umap and updated docstring. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #460; - [ ] Tests included or not required because: Not included but can add tests if necessary.; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: Only minor change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2803:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2803,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Fix for #2887. Updated and added basic tests to Leiden and Louvain clustering to check that the parameters are written to the user specified key. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2887 ; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2889:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2889,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2546:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Fixes:. * Error caused by new matplotlib release candidate (it had been deprecated for a while, we just hadn't caught it....); * Corrects deprecation warnings in igraph leiden clustering code and pearson residuals code. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Tests included or not required because: just fixin' warnings; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2999:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2999,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Follow up on https://github.com/scverse/scanpy/issues/2444#issuecomment-2022974986. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Tests included or not required because: it's docs; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2974:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2974,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Hello scanpy,. I noticed some inconsistencies in the [scanpy pca function](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.pca.html) even when fixing the random seed. I looked at the pca code and it looks like the random seed in the PCA initialization is not set. I fixed it. Best,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2469:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2469,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Hi there, ; I wrote a color annotation function for the Baseplot class called `add_colorblocks()`, which borrows some functionality of the `add_totals()` function and creates a colorblock for the annotation each assigned group. ```; sc.pl.dotplot(adata, [""CD79A"", ""MS4A1""], ""bulk_labels"", return_fig=True, show=False,; #swap_axes=True; ).add_colorblocks(color='Paired', size=0.1).show(); ```; ![image](https://github.com/scverse/scanpy/assets/24408322/eb50f18c-8fd7-4586-b63c-41c4e4f44a93). <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Addresses #2194; - [x] Tests included or not required because: plotting; <!-- Only check the following box if you did not include release notes -->; - [ x] Release notes not necessary because: tbd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3043:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3043,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Hi,; We are submitting PR for speed up of the _get_mean_var function. ; | | Time(sec)|; | -----------| ----- |; | Original | 18.49 |; | Updated | 3.97 |; | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```py; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To redu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,3,"['guid', 'simpl']","['guide', 'guidelines', 'simplefilter']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. I tried to follow the feedback described in a previous PR that contributed DensMAP https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449 but re-implemented on top of the state of the current scanpy main branch. I did not add release notes because the contributor guide says to wait for PR feedback https://scanpy.readthedocs.io/en/latest/dev/documentation.html#adding-to-the-docs. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1619 ; - [x] Closes #2684; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2946:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2946,5,"['feedback', 'guid']","['feedback', 'guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Pandas was throwing a warning:. `FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.`. This fixes that warning. The fix is a little weird, but it's what pandas says to do. Pandas explanation of the new behaviour is [here](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v2.1.0.html#new-implementation-of-dataframe-stack). Changes here:. `rank_genes_group_df`. * The sort order doesn't matter here since we sort again anyways; * `dropna=True` here actually doesn't drop null values from `filter_rank_genes_groups`. AFAICT, this doesn't change anything. `StackedViolin`. Here, we were already opting in to the future behaviour with `dropna=False`. ------. This also fixes a type signature for `sc.get.rank_genes_groups_df` and makes a better error reporting for a test I saw fail locally.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2864:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2864,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Sibling PR to: https://github.com/scverse/anndata/pull/1339. Basically, makes sure that we are running the tests on the commits we release from.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2834:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2834,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: just fixing some tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3333:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. The idea here is to raise errors where I have checked that things currently don't work, regardless of the reason why, and do not make any attempt to fix this problem. Once https://github.com/scverse/anndata/pull/1469 is merged, we can make concrete recommendations for how to handle out-of-core data. I think a decorator could work but we would have to check the type in the decorator like (instead of relying on current checks like in `filter_genes`):. ```python; if isinstance(arg1, AnnData) and arg1.isbacked:; raise NotImplementedErrror(...); ```. But then there is something like `log1p` where we quasi-support `backed` via this `chunked` kwarg, which would no really fit the above paradigm. Nonetheless, I think I need to go one-by-one through the functions to check what we support and don't. Separately, we may want to drop support where it exists already (which from my searching, is only `obs_df` and `var_df` and then `subsample_counts`). <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3004 and closes #2894; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this; 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already!; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: edited",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3306:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306,4,"['clear', 'guid']","['clear', 'clearly', 'guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2067; - [x] Tests included or not required because:; No additional rests are required as the already existing tests cover these changes; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3220:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This PR fixes a typo in the docstring of `scanpy.external.pp.scrublet()`: The `obs` column is actually called `predicted_doublet` instead of `predicted_doublets`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2615:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2615,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This PR introduces benchmarking for Scanpy with airspeed-velocity (asv) ([Link](https://asv.readthedocs.io/en/stable/index.html)),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2482:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2482,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This adds the missing bibtex formatter from my review in #2901. Since this PR is necessary to productively work with the bibliography without making a mess, and y’all are on a hackathon, I’ll merge it without review. ## Content. - Replace frail line based inclusions like `:end-line: 32` with markers. If someone destroys a marker, the doc build will fail instead of containing a garbled mess.; - Moves the flit-centric dev docs to non-opinionated tooling; - Since there are no functional bib formatters that run within pre-commit (See https://github.com/ge-ne/bibtool/issues/58), we’re going to have to live with using our own. For that purpose, I took the last open source version of `betterbib` and trimmed it down a bit. Once there’s something better that we don’t have to maintain, we should use that. Companion PR: https://github.com/scverse/scanpy-tutorials/pull/103",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2983:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2983,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This is a very small pull request to add `str` to the possible arguments for saving a figure from [`scanpy.pl.rank_genes_groups`][rank-genes-groups]. This addition matches other `save=` arguments, such as from [`scanpy.plotting.highly_variable_genes`][highly-variable-genes], [`sc.plotting.pca_variance_ratio`][pca-variance-ratio], and [`scanpy.plotting.umap`][umap]. [rank-genes-groups]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/__init__.py; [highly-variable-genes]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_preprocessing.py; [pca-variance-ratio]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/__init__.py; [umap]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/scatterplots.py. I have not included tests or release notes due to the single-line change nature of this pull request",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3076:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3076,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. When `RGB` color represented by 3D arrays, we cannot compare them as `list` vs `list`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1884:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1884,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Hi,; We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec)|; | -----------| ----- |; | Original | 297|; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Ge",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Added the boolean argument ""add_intercept"" to regress_out, and implemented code to optionally add the intercept back to the residuals in the result of regress_out().; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes https://github.com/theislab/single-cell-tutorial/issues/35; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2731:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2731,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Code changes pertaining to this issue: https://github.com/scverse/scanpy/issues/1673/; Added Discourse and twitter links to scanpy's pypi packaging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2452:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2452,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Fixes #2630,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2637:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2637,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Fixes #2668. `sc.pp.highly_variable_genes(adata, flavor='seurat')` modified the used `layer` in some cases, as described in #2668. This PR fixes this behaviour, and adds additional tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2698:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2698,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Here I changed the `test_read_visium_counts()` function to actually test `read_visium()` function instead of `read_10x_h5()`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; I think this is a better fix since we were already writing out `X_pca`. The tests seem to pass and `pca` should be idempotent so this really shouldn't break anything (hopefully).; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3074 ; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3079:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3079,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; I'd like to improve hvg selection documentation a bit.; I haven't verified the docs yet.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2379:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2379,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; In this contribution , we have introduced a new flavour 'katana' in the already existing leiden and louvain implementation. The implementation of this new flavour is based on the KatanaGraph library , which comes out to be a faster alternative to the already existing flavours, providing more than 2x speedup to the leiden and louvain execution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2409:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; It was unclear if the `groups` argument in `rank_genes_groups()` is equivalent to subsetting when `reference='rest'`(i.e. only the selected groups are tested against each other) or if only those groups are tested but all groups are still used as the reference. This PR adds a note explaining the current behaviour (option 2).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2657:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2657,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Modified (for `scanpy`) version of https://github.com/scverse/anndata/pull/564. Fixes https://github.com/scverse/anndata/issues/556. Big points of change:; 1. No more tuple-indices and related functionality (i.e., scoring pairwise); 2. Allow for `obs` and `var` group-by +`varm`, `obsm`, `layers` as options for data to aggregate; 3. Output is `AnnData` object instead of `DataFrame`; 4. `scanpy`-style public API. ## TODO (by @ivirshup):. Necessary:. - [x] Docs; - [x] Aggregate along other axis; - [x] Keep grouping cols in result; - [x] Reconsider API for non-anndata version (maybe return a dict of arrays?); - [ ] Decide on naming convention for `""nonzero""` variations, should this be `""nonzero_count""` so it's a little like `""nanmean""`. Optional, can do later:. - [ ] Weighted (although.... Idk, maybe can skip. Does ""weights"" affect ""count_nonzero""?); - [ ] Option for keeping around unseen groups, probably needs `fill_value` argument for those values; - [x] Support for `obsm`, `varm`; - [ ] Directly pass Series to groupby; - [ ] More aggregation functions (mean_nonzero, min, max, std, `nan*` variations); - [ ] Mask argument; - [ ] Dask support",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Resolves #1429.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2683:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2683,2,['guid'],"['guide', 'guidelines']"
Usability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Simple addition of the `layer` argument which is already included in `sc.tl.score_genes` so that you can use your own layer instead of being restricted to what is stored in `adata.X` . <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3138:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3138,3,"['Simpl', 'guid']","['Simple', 'guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; This PR fixes the case when `use_raw=None` in `scanpy.tl.score_genes`. It causes to first fetch `var_names` from `adata.var_names`, but later a subset on `adata.raw` can happen, which can have different gene names.; Also fixes the type of `use_raw` and adds a `ValueError` if `gene_pool` is empty (otherwise, crashes with non-informative error message). related issue: https://github.com/theislab/cellrank/issues/746",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1999:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1999,2,['guid'],"['guide', 'guidelines']"
Usability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```py; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:71,guid,guidelines,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,4,"['guid', 'learn', 'simpl']","['guide', 'guidelines', 'learn-intelex', 'simplefilter']"
Usability,"<!--; ⚠ If you need help using Scanpy, please ask in https://discourse.scverse.org/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...; Hello. I would like to plot the ""log1p_total_counts"" of a **visium transcriptomic dataset** (AnnData object with n_obs × n_vars) using sc.pl.spatial. **sc.pl.spatial(adatal, img_key=""hires"", color=[""gene1"", ""gene2"", ""gene3""]) -**-> gives 3 images (the 3 genes are plotted stacked on 3 images). But instead of I would like to plot **the mean of the 3 genes on 1 image**. is it possible ??; I hope the question is clear :); Manys thanks for your help; Sophie",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2252:593,clear,clear,593,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2252,1,['clear'],['clear']
Usability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->. In our analyses we wanted to try SCTransform normalization instead of default log-norm. I have done it quite crudely, but it works: I run SCT in Seurat and dump the counts on disk to load in scanpy.; While verifying that this approach worked, we encountered slight inconsistencies between clustering using (1) vanilla log-norm scanpy (2) SCT imported scanpy and (3) SCT in Seurat.; After investigation, it appears that vanilla scanpy sometimes better picks up some clusters than SCT+scanpy, despite the latter having more relevant genes in its HVG list. Here is the investigation: https://github.com/mxposed/notebooks/blob/master/sct-scanpy.ipynb. And here are the main questions that remain:; 1. Why Vanilla scanpy could resolve those populations, despite operating on less marker HVGs?; 2. What is the difference between kNN graph construction and clustering between Seurat and scanpy?; 3. How to be sure we did not undercluster and miss some smaller cell populations?. I would be glad for any feedback or input, and of course if someone knows the answers, that's great!. Best wishes,; Nick. PS. Thank you for scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531:1173,feedback,feedback,1173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531,1,['feedback'],['feedback']
Usability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...; Hi ; if samples contribute a different number of cells to my object, how to control for variability among samples? ; How to make sure that any difference between conditions I found is caused by biology and not because of samples variation? . downsampling, upsampling, bootstrapping, robustness test . Appreciate any feedback and any references for this issue. ![image](https://user-images.githubusercontent.com/23288387/155648374-f0d6178f-7024-4ecd-88c0-37547c5e7e19.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2155:498,feedback,feedback,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2155,1,['feedback'],['feedback']
Usability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; https://github.com/theislab/scanpy/tree/master/scanpy/tests/_data/10x_data/3.0.0 - this h5 object is 1107 cells by 507 genes but what is the data? Is it down-sampled pbmc3k or some other dataset? How was it generated?. I'm looking for a tiny h5 object like this for our own unit testing, but want to be clear on the data source, thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1908:480,clear,clear,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1908,1,['clear'],['clear']
Usability,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:1292,learn,learn,1292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,4,['learn'],['learn']
Usability,"==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, nan, nan]).; You can drop duplicate edges by setting the 'duplicates' kwarg; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; >scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1175:3253,learn,learn,3253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1175,1,['learn'],['learn']
Usability,"> ## `groups`; > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison.; > ; > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-scor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:742,clear,clear,742,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,2,['clear'],['clear']
Usability,"> > Docsearch; > ; > Gotta keep this (it's so much nicer). The UX is for sure, but if we could replace it with something that has similar UX but uses an index that’s built by the RTD build, I’d much prefer that. It’s silly that searching in PR builds or `stable` will result in getting redirected to the `latest` docs …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2220#issuecomment-1090258080:63,UX,UX,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2220#issuecomment-1090258080,4,['UX'],['UX']
Usability,"> @Zethson, do you think we should move those to scverse?. Do you mean to this page? https://scverse.org/learn/. Or generally all of them? We might require stronger filtering options then because with all tools combined we would have a lot of vignettes. Edit oh wait: I had no idea that we had this scanpy_usage repository. My answer is yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2260#issuecomment-1158039454:105,learn,learn,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2260#issuecomment-1158039454,2,['learn'],['learn']
Usability,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`?; > ; > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-480385619:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-480385619,2,['simpl'],['simple']
Usability,"> @hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream. Figure it out. In my case it's because I have both `umap` and `umap-learn` installed, see here: https://github.com/theislab/scanpy/issues/2045#issuecomment-963533994",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478:246,learn,learn,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478,2,['learn'],['learn']
Usability,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?; > ; > ; > ; > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`.; > ; > ; > ; > Thanks!; > ; > ; > ; > Thanks!. As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040743473:409,clear,clear,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040743473,2,['clear'],['clear']
Usability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:809,intuit,intuitive,809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,2,['intuit'],['intuitive']
Usability,"> Ah, the problem was that the string actually contained that return type!. This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did 🙂)? Bold font and spacings around colons?. ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484026464:327,simpl,simple,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484026464,2,['simpl'],['simple']
Usability,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:168,simpl,simply,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,2,['simpl'],['simply']
Usability,> Am I right that you simply. You are right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3307#issuecomment-2437524080:22,simpl,simply,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307#issuecomment-2437524080,2,['simpl'],['simply']
Usability,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:547,simpl,simple,547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777,4,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:889,clear,cleared,889,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,2,['clear'],['cleared']
Usability,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>; <summary><code>__array_function__</code> implementation</summary>. ```python; def __array_function__(self, func, types, args, kwargs):; result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs); if issparse(result):; result = SparseArray(result); elif isinstance(result, np.matrix):; result = np.asarray(result); return result; ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953:811,simpl,simple,811,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953,2,['simpl'],['simple']
Usability,"> Are any improvements here reasonably easy to do? I recognize that it's making two libraries talk to each other, and at least one of them can't be totally turned off, so this might be difficult. yes, both things are quite easy, I think. Event listeners can be registered in a way that they don’t capture the event, and toggling the popup is a simple `if (isModalVisible()) removeSearchModal() else showSearchModal()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889665103:344,simpl,simple,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889665103,2,['simpl'],['simple']
Usability,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553:523,simpl,simpler,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553,2,['simpl'],['simpler']
Usability,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:134,simpl,simple,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515,2,['simpl'],['simple']
Usability,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:147,simpl,simple,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075,2,['simpl'],['simple']
Usability,"> Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Fully agree. > ; > Also: If trying to call a t-test with non-logarithmized data, a warning should be written.; > . Also agree. > The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?. Oh true, it wasn't log transformed, true.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325:62,simpl,simply,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325,2,['simpl'],['simply']
Usability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:303,guid,guides,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,2,['guid'],['guides']
Usability,"> Do we give any indication of how many cells are in a group right now? I feel like this would be important for the user to even know the stats could be unreliable. `sc.pl.dotplot(..., return_fig=True).add_totals().show()`. is one way to check the cell numbers, but there is no intuitive way to do the filtering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1829#issuecomment-835850158:278,intuit,intuitive,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1829#issuecomment-835850158,2,['intuit'],['intuitive']
Usability,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450877926:138,simpl,simply,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450877926,2,['simpl'],['simply']
Usability,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:77,feedback,feedback,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677,2,['feedback'],['feedback']
Usability,"> For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above ([#794 (comment)](https://github.com/theislab/scanpy/pull/794#issuecomment-523515331)) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?. @ivirshup I see your point regarding the accurate representation of what is shown. My thoughts were more along the lines of making it easy to distinguish clusters. That would be particularly useful when you have >20 clusters that are hard to distinguish by colours alone. And the overlaps may not be as important for a large-scale overview.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-524373551:289,clear,clear,289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-524373551,2,['clear'],['clear']
Usability,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030:202,learn,learn,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030,4,['learn'],['learn']
Usability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). That’s surprising! I think numba is our most complex dependency, and umap’s dependency PyNNDescent is also compiled. I think if this isn’t a mistake and it’s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLite’s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isn’t a problem, it’s just an old package that doesn’t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:575,learn,learn,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,2,['learn'],['learn']
Usability,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). ; What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy; >; > Could does this relate to @davidsebfischer and diffxpy?. Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226:128,simpl,simple,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226,2,['simpl'],['simple']
Usability,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:110,simpl,simple,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,2,['simpl'],['simple']
Usability,"> Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with; > ; > ```; > pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; > ```; > ; > Seems to work now for me. Thank you. It just worked for me, in July 2024.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559:225,simpl,simple,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559,2,['simpl'],['simple']
Usability,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:798,usab,usable,798,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,2,['usab'],['usable']
Usability,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-380095294:947,simpl,simply,947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380095294,4,"['learn', 'simpl']","['learn', 'simply']"
Usability,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754:305,clear,clear,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754,2,['clear'],['clear']
Usability,"> Hi @JackieMium, I remember you said something similar in another issue.; > ; > If there’s things bugging you, how about making a PR that fixes it?. Not sure what you're referring to but I don't think I ever reported color pallette issue before. ; I hope I could help fix things but I am familiar with R/Seurat and Python/scanpy is a whole new universe to me. I am starting to learning the scanpy pipeline. How things work under the hood with scanpy or basically Python plotting are really beyond my capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040:378,learn,learning,378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040,2,['learn'],['learning']
Usability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:261,simpl,simpler,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920,2,['simpl'],['simpler']
Usability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:261,simpl,simpler,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457,2,['simpl'],['simpler']
Usability,"> Hi, please provide the data you use, otherwise this is not reproducible:; > ; > ```; > FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); > ```. Hey, the data is publicly available under this link: https://www.10xgenomics.com/resources/datasets/human-lung-cancer-ffpe-2-standard. I simple copied the `curl` bash script to download all the files and then unzipped the file corresponding to the images to get the ""spatial"" folder",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906:485,simpl,simple,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906,2,['simpl'],['simple']
Usability,"> How about creating a page about reproducibility in the docs, similar to the [one by pytorch](https://pytorch.org/docs/stable/notes/randomness.html)? It could gather all information around reproducibility with scanpy, such as; > ; > * use of containers; > * numba flags. @grst, this would be great. Would you be up for writing this at some point?. I'm thinking it could either go:. * As a how-to page if it's example driven; * A ""User Guide"" page, where we'd start a ""User Guide"" that would contain this and ""Usage principles""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047400078:436,Guid,Guide,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047400078,2,['Guid'],['Guide']
Usability,"> However, don't you think that this could be part of a the scanpy tutorials section?. Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441474788:555,simpl,simply,555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441474788,2,['simpl'],['simply']
Usability,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?. Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040013593:434,simpl,simply,434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040013593,2,['simpl'],['simply']
Usability,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:180,simpl,simply,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,2,['simpl'],['simply']
Usability,"> I am still worried that this doesn't make the relative numbers clear. It sounds like you're trying to see whether absolute numbers of cells expressing a gene is similar between clusters. I don't think this is a use case I've commonly heard of. Maybe you could explain why you're interested in this?. To me, it seems more relevant to know how common a gene is expressed within a population than how many of that cell type express that gene. Proportions of cell types vary due to tissue and collection, so I'm not sure when differences in total amount is what I want to know. If you wanted, you could try to map the size of the dot to the number of cells in the cluster expressing the gene? You can find some code for doing this kind of thing here (https://github.com/theislab/scanpy/issues/1876#issuecomment-987049315), though we would like that API to be nicer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016491736:65,clear,clear,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016491736,2,['clear'],['clear']
Usability,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:329,learn,learn,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606,4,['learn'],['learn']
Usability,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:925,simpl,simply,925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140,2,['simpl'],['simply']
Usability,"> I ended up with […] Does it looks ok to you?. Yeah! You could simplify though:. ```py; VMinMax = Union[str, float, Callable[[Sequence[float]], float]]. def embedding(; ...; vmin: Union[VMinMax, Sequence[VMinMax], None] = None,; vmax: Union[VMinMax, Sequence[VMinMax], None] = None,; ...; ): ... def _get_vmin_vmax(; vmin: Sequence[VMinMax],; vmax: Sequence[VMinMax],; ...; ) -> ...: ...; ```. > It's not a big deal but if we use numbers in 0-100 range for vmax-vmin, it's a percentile, so p80 makes more sense. you can also be more or less precise: `q001`→`0.001`, `q9`→`0.9`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523883673:64,simpl,simplify,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523883673,2,['simpl'],['simplify']
Usability,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:329,learn,learn,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241,2,['learn'],['learn']
Usability,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:144,learn,learn,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106,8,"['clear', 'learn', 'simpl']","['clear', 'learn', 'simple']"
Usability,"> I quite like the saving of figures as it means people can use scanpy who otherwise aren't as familiar with data science in python. Calling a function on an axis object or saving the last axis object that was is displayed is not always intuitive to new users. How about adding a ""plotting cookbook"" section to the docs instead? `plt.rc_context` is such a neat trick (also beyond scanpy), but it wasn't obvious to me either (#1648). . Obligatory quote from the ""Zen of Python"":; ```; There should be one-- and preferably only one --obvious way to do it.; Although that way may not be obvious at first unless you're Dutch.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508#issuecomment-841127632:237,intuit,intuitive,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-841127632,2,['intuit'],['intuitive']
Usability,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesn’t seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that I’m now member of the team on rtd.com (which I wasn’t before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450818246:210,simpl,simply,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450818246,2,['simpl'],['simply']
Usability,"> I solve this question that can’t import skmisc.loess as loess by chance。the solution is: 'python3.7'+'numpy-1.20.1+mkl-cp37-cp37m-win_amd64.whl'+'scipy==1.7.3'+'scikit-learn==1.0.2'+'scikit-misc==0.1.4'+'scanpy==1.9.1'. > It is 13 Jun ,2022 ,today.I need to creat a new evn ,in which I can run the code as **sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3')****. I meet the question that I can't **Import skmisc.loess as loess** ,while the code works fine in my last computer. So，there is a solution can solve this question ,now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1153550204:170,learn,learn,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1153550204,2,['learn'],['learn']
Usability,"> I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. I would definitely like this to be the case. I'm not sure I see . > Why? `sc.pp.neighbors` already has `method='gauss'`. To me, it’s largely of a maintenance and documentation issue. Most bugs I fix (here, and in upstream libraries) come from argument handling. The more features you lump into a function, the more complicated argument handling gets. There are questions of default values and fallbacks for different backends, and being sure users understand which arguments are valid for each backend. The use of the `Neighbors` class ends up making the `neighbors` function much more complicated than it needs to be. I think skipping out on that here can make this implementation much more simple. From an API stand point, I would like the ""blessed"" `tsne` workflow to be dead obvious. I'm thinking:. ```python; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```. How many arguments is it going to take to make this work if this functionality is in `sc.pp.neighbors`? At a minimum, `k=30, method=tsne_affinity, nn_method=""annoy""`, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450:814,simpl,simple,814,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450,2,['simpl'],['simple']
Usability,"> I think it would be fine to only cover the case of what `space ranger` actually outputs. I was thinking there could be an argument where the user manually passes an alternate path. This could be useful for cases where they've processed the image themselves some modifications to the image. space ranger doesn't output this image, as it's taken as input to assign spots and get scalefactors and metadata. This type of image is in the same folder just for chance in the 10x genomics dataset. ; In the `read_visium` function I would simply add an argument to pass the path of the image, and basically just assign it to the `adata.uns` metadata. Otherwise just assign None. THis way it's consistent for the spatial tool whichlater uses it in the image container.; It's also convenient to add it as argument so that `read_visium` could just be passed in that same way as it is now in `datasets.visium_sge`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-734735910:532,simpl,simply,532,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734735910,2,['simpl'],['simply']
Usability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:848,simpl,simplest,848,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,2,['simpl'],['simplest']
Usability,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:75,simpl,simply,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,4,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> I wasn't really expecting this feature PR to also include such a large refactor. It would have been necessary for the Dask Dataframe version. Now I 1. did the work and 2. improved readability, so it would be counter productive to undo it. > I'm still not 100% convinced the behaviour here is exactly the same as before. I have done a few tests, which have been okay, but I haven't tried much parameterization. I'm ~80% convinced the results should be the same. If you have any specific things in mind, you should probably make a PR that adds tests for the properties you think we should preserve. We can then merge that one, update this one, and see if it actually breaks something. I can’t check for speculative differences if I have no idea where those could be. > I would note that the dataframe returned when inplace=False has a different index than it did previously. Yup, now it actually matches instead of discarding the original Index and replacing it with a RangeIndex for no reason. > Apart from the comments, can we get a regression test for ""cell_ranger"" (e.g. generate results with an older version)? I don't think we have one in the test suite. Sure! That’s a concrete thing I can do. I’ll do that on thursday, I did the rest of what you asked today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931:232,undo,undo,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931,2,['undo'],['undo']
Usability,"> I would also like to see this merge. I've put a lot of time and effort into reviewing it, so we can get this over and done with.; > Your contributions are invaluable to the project, and I'd really like to see you contributing to other things. thank you, I really appreciate this :heart: . > The reason I'm so hard on this is that it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; > I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. Totally understood. My motivation to use flit is that it’s simpler and therefore better both for first-time contributors (to get started) and experienced people (to debug), whereas CLI, metadata, and code of setuptools/pip is very complex and a nightmare to debug. I know that due to flit being used less, there needs to be someone who understands the packaging ecosystem to fix things when they’re broken instead of cargo-culting one of the million answers around setuptools on StackOverflow. > Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have pip install -e listed, and there has to be a note saying flit -s installations will be overridden due to a bug in pip. This stuff can be removed once this is fixed upstream. OK, will do! Can you link me tp the upstream discussion of this problem please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064:900,simpl,simpler,900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064,2,['simpl'],['simpler']
Usability,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:133,learn,learn,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652,4,['learn'],['learn']
Usability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:1251,learn,learning,1251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,2,['learn'],['learning']
Usability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too – *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:1310,clear,clear,1310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,2,['clear'],['clear']
Usability,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:79,learn,learn,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308,10,"['clear', 'learn', 'simpl']","['clear', 'learn', 'simple', 'simply']"
Usability,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:366,clear,clear,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,2,['clear'],['clear']
Usability,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:265,simpl,simple,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,2,['simpl'],['simple']
Usability,"> It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of sc.pl.umap(adata, color='clusters') -> sc.pl.umap(adata, 'clusters'). 👍 . > The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage. The idea is to parse `components`, but allow you to directly pass the indices with dimensions. > The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. To me, I think it makes more sense to be consistent with python. I feel like it's very clear what is happening if the default argument is `sc.pl.pca(adata, dimensions=(0, 1))`. It makes it easier to work with programmatically if the values are equivalent to what you could use to index the array directly. For example, say you find the dimension which is maximally correlated with some gene. You can just pass the result of that into dimensions without having to remember to add 1. > For the plots being the product of color and components: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. Cool. I feel like this can be useful, but it would be useful if I could choose which arguments it worked with. I think this is a different function call though. For example, I might was to look at a gene under multiple embeddings, so pairwise combinations of `basis` and `colors`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877:117,simpl,simple,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877,4,"['clear', 'simpl']","['clear', 'simple']"
Usability,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:334,simpl,simple,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924,4,"['simpl', 'usab']","['simple', 'usable']"
Usability,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap.; Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304:540,learn,learn,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304,2,['learn'],['learn']
Usability,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1208,guid,guidelines,1208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,2,['guid'],['guidelines']
Usability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:472,simpl,simple,472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,2,['simpl'],['simple']
Usability,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:1016,guid,guidance,1016,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,2,['guid'],['guidance']
Usability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:23,simpl,simple,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,2,['simpl'],['simple']
Usability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:35,clear,clear,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,2,['clear'],['clear']
Usability,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?. It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599:141,clear,clear,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599,2,['clear'],['clear']
Usability,"> Or just the standard matplotlib palettes, yes:; > ; > ```python; > from matplotlib import cm; > ; > sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=cm.get_cmap('Set3')); > ```; > ; > The bug is that you can’t pass the colormap name. Passing a colormap directly works. Is it then, a good idea to state this the documents for the time being to clear the confusion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1646498818:365,clear,clear,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1646498818,2,['clear'],['clear']
Usability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:614,simpl,simple,614,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,2,['simpl'],['simple']
Usability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:1029,clear,clear,1029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,2,['clear'],['clear']
Usability,"> Should the reference object where you learn the transformation always be a subset of the data you're going to apply the transformation to? If so, instead of passing a separate object, could there be a mask of which samples to train on?; > ; > If not, what do you think about making this a separate function? Maybe `combat_by_reference`?. Thank you for your great suggestions. I think it's easier to add a mask for train/evaluate instead of splitting into 2 objects. ; I don't think it should be a separate `combat_by_reference` function, though, because the chance in the function is small and I preserved the original functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703:40,learn,learn,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703,2,['learn'],['learn']
Usability,> Sklearn has its implementation of [CCA](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper. Is it suitable for single cell data ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424271229:56,learn,learn,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424271229,2,['learn'],['learn']
Usability,"> So if I understand correctly you want to use quantile scaling to translate values to colour? If that is what you're suggesting, I'm not sure I'm such a fan of that idea. With quantile scaling you would lose all sense of gradient in your e.g. expression values. I would instead opt for trimming and scaling. The trimming could be done via a quantile threshold though. OK, I thought code was clear enough, here is more information :). vmin and vmax are used for determining the lowest and highest values of the colormap (see https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.colors.Normalize.html). By default, these values are mapped to the minimum and maximum of the color vector of the scatter plot (e.g. gene expression). Right now, we can define only one vmin/vmax value in a sc.pl.* call (e.g. `sc.pl.umap(ad, color=..., vmax=2.0)`). But when we plot multiple genes (`sc.pl.umap(ad, color=['a', 'b'], vmax=2.0)`), setting vmax to a specific value sometimes does not make sense because each gene might have a different outlier range. . What I propose is the flexibility to use quantiles to set vmin/vmax e.g. `sc.pl.umap(ad, color=['a', 'b'], vmax_quantile=0.99)` where vmax values will be calculated per panel (i.e. per gene) by Scanpy. I mean it's simply winsorization, nothing fancy :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775#issuecomment-519626141:392,clear,clear,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-519626141,4,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:839,clear,clear,839,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,6,"['clear', 'intuit']","['clear', 'intuitive']"
Usability,"> Thank you! With “tests” I mean “functions named `test_*` with `assert ...` statements inside”; ; Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, ; Khalid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243:115,guid,guidance,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243,2,['guid'],['guidance']
Usability,"> Thanks for the report.; > ; > Do you have NaN values in your expression matrix? If so, try filtering them out and seeing if that works. If not, could you report the versions of the library you're working with, and try to make a self contained example that I could run on my machine?. Hi, . Thank you for your reply. I think there is no NaN data in the matrix of the mito genes. Because I have already plotted the mitochondrial genes as follows. The version is scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0. I am sorry I don't know how to make a self-contained example. The plot is: ; ![highest_expre_genes_BHCF](https://user-images.githubusercontent.com/49381637/83712829-3ea3ab80-a5ec-11ea-8497-cc70a95a216e.png). Thank you. Best wishes,. Shangyu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1259#issuecomment-638585609:551,learn,learn,551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-638585609,2,['learn'],['learn']
Usability,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:32,clear,clear,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433,4,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:287,simpl,simpler,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128,4,"['clear', 'simpl']","['clear', 'simpler']"
Usability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:353,simpl,simpler,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,2,['simpl'],['simpler']
Usability,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302#issuecomment-441965409:468,responsiv,responsive,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441965409,2,['responsiv'],['responsive']
Usability,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:187,simpl,simply,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837,2,['simpl'],['simply']
Usability,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:844,simpl,simple,844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664,4,"['clear', 'simpl']","['clearly', 'simple']"
Usability,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5?. This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729:662,learn,learn,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729,2,['learn'],['learn']
Usability,"> This is the standard way of writing a numpydoc returns section. […] This solution is dropping support for them. It certainly shouldn’t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I don’t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I don’t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons?. I’ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the “optional” is redundant. What would it even mean to have “a parameter that isn’t optional but has a default value”?. I’m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417:487,simpl,simple,487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417,2,['simpl'],['simple']
Usability,> This might be a case of a `pip install umap` rather than `pip install umap-learn`. Suspecting exactly that :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220:77,learn,learn,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220,2,['learn'],['learn']
Usability,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:25,simpl,simpler,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026,6,"['learn', 'simpl']","['learn', 'simpler']"
Usability,"> To be able to reproduce and help, it is a big aid for us if you can supply a code sample that we can run: that is, with some dummy data (the datasets scanpy readily supplies are great for that), and the error/unexpected behaviour you get. Can you show such an example, with data? It is not immediately clear to me what specific you are trying to add or construct; I'm not sure whether basically the dataframe gets destroyed by the operation you intend to perform, or whether it is the violin plot failing (if the dataframe is crooked, it would be this to be fixed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546:304,clear,clear,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546,2,['clear'],['clear']
Usability,"> UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. Both of those clustering algorithms just use whatever graph is passed, so this shouldn't be an issue. > t-SNE, on the other hand, calculates 90 nearest neighbors by default.; > openTSNE does something similar to UMAP for adding new samples to existing embeddings. Could there just be a separate function for computing neighbors for tsne? `sc.pp.neighbors` can be considered to be ""compute nearest neighbors and connectivity as expected by UMAP"", while a separate function could use methods and defaults appropriate to openTSNE. @Koncopd would have more to say on how this should work w.r.t. `ingest`. > Is relying on a single k=15 from UMAP for everything really ok?. Ultimately, up to the user. There is an element of consistency and simplicity to using the same representation of the data for multiple parts of the analysis. I think there would have to be a good reason for using a different connectivity matrix for the 2d embedding and for the clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-657406330:890,simpl,simplicity,890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-657406330,2,['simpl'],['simplicity']
Usability,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:103,simpl,simply,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,2,['simpl'],['simply']
Usability,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:746,simpl,simply,746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973,2,['simpl'],['simply']
Usability,"> What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. Regarding this -- yes, that's what we thought to do. Do you think it makes sense? However, for computational efficiency, I would threshold the genes by expression and take e.g. 1000 or even 100 genes with the highest average expression (for the purpose of estimating theta). Lowly-expressed genes don't really constrain theta much, it's highly-expressed ones that do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-792059556:34,learn,learning,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-792059556,2,['learn'],['learning']
Usability,"> Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. My argument was mostly about API. But one big benefit of using kNN with k=15 is that it's *much faster* then using k=90 (with perplexity=30) which is what t-SNE is using by default (for historical reasons). It's not about uniform vs non-uniform, it's about k=15 vs k=90. Uniform on k=15 just happens to give almost the same results as perplexity=30 on k=90. So it's a lucky coincidence that I thought we could benefit from. > Second API: I'm not sure I completely agree with this. I think it would be the most clear for sc.pp.neighbors to essentially mean ""build umap's connectivity graph"", and functions like sc.tl.tsne or sc.tl.umap to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via connectivities_key) are the weights that get used. I understand what you saying, but the situation won't be symmetric because `neighbors` already exists, is *not* called `neighbors_umap`, and all users of Scanpy are very familiar with this function. I'd like to make it very easy to use t-SNE in scanpy and that it naturally fits into the scanpy's established workflow. That's why I think simply running `tsne()` after running `neighbors()` should be possible. Please note that t-SNE requires normalized weights (they should sum to 1). The weights constructed by UMAP in `neighbors` are not normalized. So if you run `neighbors()` and then `tsne()` then t-SNE should do *something* in order to be able to use this graph. My suggestion is that it discards the weights and uses normalized affinity matrix. I am not sure what exactly is your suggestion? Take UMAP weights and normalize them? This has never been explored in the literature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575:643,clear,clear,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575,4,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors.; > […](#); > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> .; > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-523779587:102,clear,clear,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-523779587,2,['clear'],['clear']
Usability,"> absolute numbers of cells expressing a gene is similar between clusters as a use case. I am aware that this is a bit of a niche problem, and I am not particularly happy with domino plots as a solution either. I have no better vehicle to discuss this than opening an issue :( Hopefully this inspires the next person who deals with this problem. As to the question, maybe sticking to this example will help me explain:. I am looking at the expression of Hb9/Mnx in my whole-body dataset. I notice from the feature scatter that it seems to be somewhat expressed in clusters 0, 2, and 18. Wanting to be sure, I look at the dotplot. The dotplot tells me that there is a greater proportion of cells in cluster 18 that express it, compared to 0 and 2. The dotplot might make me believe that Hb9 is a marker for cluster 18, and if I do an in-situ hybridisation, these are the cells I would be staining. However, the truth is that the vast majority of cells that express Hb9 are actually in clusters 0 and 2, different cell types than 18. The number of cells in each cluster correlates with the number of cells in the organism, so if I performed the in-situ I would get lots of cells that I could mistakenly all identify as cluster 18. Does this make more sense?. EDIT: I am not advocating for domino plots to be part of ScanPy. I am simply trying to start a discussion, and trying to see if there was an easy fix that I missed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889:1327,simpl,simply,1327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889,2,['simpl'],['simply']
Usability,"> hi @yotamcons ,; > ; > thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,; > ; > Thank you!. Would love to starting November, ping me if thats still relevant",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531:46,feedback,feedback,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531,2,['feedback'],['feedback']
Usability,"> it would make sense if I mirror the change in normalize_pearson_residuals(), right? I believe doing that will even simplify the function further. If ivirshup agrees, I could quickly do that :). Sounds good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-791140191:117,simpl,simplify,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-791140191,2,['simpl'],['simplify']
Usability,"> obs-like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:158,simpl,simply,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,4,"['intuit', 'simpl']","['intuitive', 'simply']"
Usability,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:177,clear,clear,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013,4,"['clear', 'simpl']","['clear', 'simple']"
Usability,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:561,guid,guidelines,561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681,2,['guid'],['guidelines']
Usability,"> thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. No worries!. I think communicating about the ideas we have for these tools can be fraught. > in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot). I don't think this is the case. . First, I believe there are non-visium grid based spatial methods (I remember seeing a product page for one, but can't find it atm). Second, I think you don't need segmentation info to use this function. You just need coordinates (probably derived from segmentation) and possibly an image. Like this:. > unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units. But I think a user already having done the segmentation, then coming to scanpy is a reasonable workflow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703:66,clear,clear,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703,2,['clear'],['clear']
Usability,"> thanks @jlause ! Really excited for this!; > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this?; > ; > thank you!. cool, looking forward to your feedback!; I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506:219,feedback,feedback,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506,4,"['feedback', 'simpl']","['feedback', 'simplify']"
Usability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:623,simpl,simply,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,2,['simpl'],['simply']
Usability,"> we don't have pre-commit in place, we are discussing it here #1563 . I do check flake8 but clearly didn't do it this time. I thought at one point you guys were checking flake8 with CI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1679#issuecomment-784310828:93,clear,clearly,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-784310828,2,['clear'],['clearly']
Usability,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:2930,learn,learn,2930,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,2,['learn'],['learn']
Usability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:464,simpl,simple,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,2,['simpl'],['simple']
Usability,"@Koncopd sorry for the late feedback, but I don't see the ""neighbors_key"" in the scanpy.tl.paga function. It'd be great to make sure that everything that uses the neighbor graph is covered :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1118#issuecomment-616757843:28,feedback,feedback,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118#issuecomment-616757843,2,['feedback'],['feedback']
Usability,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances; * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191:202,learn,learn,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191,2,['learn'],['learn']
Usability,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487410333:465,simpl,simple,465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487410333,2,['simpl'],['simple']
Usability,"@Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python; if umap.__version__ >= 0.5:; raise ImportError(""Ingest currently require umap-learn < 0.5""); ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1589:299,learn,learn,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589,1,['learn'],['learn']
Usability,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:242,clear,clear,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259,2,['clear'],['clear']
Usability,"@LuckyMD I am wondering have you tried it before? or did someone try it with the pbmc data with regards to the t-cells that you mentioned? I agree with you that it can not be an option for answering biological questions but in the case that there is no clear biological knowledge like looking for new sub-types or so it can be an option to get some idea (mathematically). . p.s. it just crossed my mind so I am pushing a technical question also here 😄 Since we would need to calculate the distance matrix, would the input for the function be adata.X ? should it be the raw file?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498068973:253,clear,clear,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498068973,2,['clear'],['clear']
Usability,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494539920:56,simpl,simply,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494539920,2,['simpl'],['simply']
Usability,"@LuckyMD Yeah, we can definitely do the differences in mean expression on the log-scale as a quick fix. However, I think it's a little bit more intuitive to express fold-changes with a base of 2, as opposed to a base of e, which is the transformation used for the data matrix. This also mimics what other packages usually report, but I'm happy either way!. For the differential testing itself, I completely agree with you about the assumption of normality with t-tests, so we definitely shouldn't change that. For the wilcoxon test, however, normality isn't assumed, so do you think it would be ok to run on raw counts?. Either way, we should definitely fix the double-log for the fold-change reporting. Is there a preference on whether to keep it was loge(base e) difference or log2 difference? For simplicity, this change in scale would only affect the fold-change reporting, all other tests I think we should keep in loge to keep it consistent. One final thought I wanted to ask your opinion on: log-mean vs mean-log. (e.g. log(mean(count)) vs mean(log(count)). For t-tests I think it makes sense to use mean-log, however, when calculating fold-change differences in mean expression, I'm not entirely sure what's preferred (particularly for methods like wilcoxon that don't use the mean expression to calculate scores). Thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-470250609:144,intuit,intuitive,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-470250609,4,"['intuit', 'simpl']","['intuitive', 'simplicity']"
Usability,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-439105490:377,simpl,simply,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-439105490,2,['simpl'],['simply']
Usability,"@LuckyMD genes at the bottom simply have the lowest rank but they could be expressed. By default the ranking is taking directly from `sc.get.rank_genes_groups_df` which ranks the genes by log fold change. Bottom genes tend to have significant p-value. . To make this more transparent we can add a parameter to select how to rank for example by p-value or log fold change. . But, first I need to figure out what is this mess with the new tests....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854:29,simpl,simply,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854,2,['simpl'],['simply']
Usability,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python); * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933:154,Guid,Guide,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933,1,['Guid'],['Guide']
Usability,"@LuckyMD: Yes, scran's size factor calculation would be very nice-to-have and should be a simple task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-469643226:90,simpl,simple,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-469643226,2,['simpl'],['simple']
Usability,"@PGmajev, I would recommend setting up pre-commit for the repo (as described in the contributing guide [here](https://scanpy.readthedocs.io/en/latest/dev/getting-set-up.html#pre-commit)). It should save you from dealing with these formatting errors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1074431745:97,guid,guide,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1074431745,2,['guid'],['guide']
Usability,"@VolkerBergen ; No, similar lines also should be changed in [`randomized_range_finder`](https://github.com/scikit-learn/scikit-learn/blob/3a884c5ee507f735e2df384727340c72c5219a8e/sklearn/utils/extmath.py#L148), which is used by `randomized_svd` function. Or the whole `safe_sparse_dot` function. But copying the file extmath.py (or the part of it related to svd) and changing this lines would be sufficient, yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446527430:114,learn,learn,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446527430,4,['learn'],['learn']
Usability,@VolkerBergen can you type a simple example on how to use this new functionality. I think that I want to use this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428458237:29,simpl,simple,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428458237,2,['simpl'],['simple']
Usability,"@VolkerBergen, is this really important? I had the intend of allowing passing a precomputed `counts_per_cell` vector, but I think it wasn't really ever used... So, for a simpler function and cleaner code, it would be nice to get rid of it; as @Koncopd did for the new version. Any objections?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-457869063:170,simpl,simpler,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-457869063,2,['simpl'],['simpler']
Usability,"@Zethson Ready to merge. Thanks for your feedback; I added to the release notes, and rebased on master.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184:41,feedback,feedback,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184,2,['feedback'],['feedback']
Usability,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:789,clear,clear,789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161,2,['clear'],['clear']
Usability,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:136,simpl,simple,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457,2,['simpl'],['simple']
Usability,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:258,simpl,simply,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215,4,['simpl'],['simply']
Usability,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:418,simpl,simply,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240,2,['simpl'],['simply']
Usability,"@dawe a cell cycle scoring function would be great! everything that's a bit more extensive and non-standard should go into [sc.tl](https://github.com/theislab/scanpy/tree/master/scanpy/tools), everything that's really just simple preprocessing and stats with a few lines can go to [sc.pp](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/simple.py). usually, there should be a plotting function in sc.pl that presents a canonical visualization of the annotation added in with the tool... writing a test for your function would also be great ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398:223,simpl,simple,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398,4,['simpl'],['simple']
Usability,"@dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings; - [ ] Recipes; - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable?. I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way?. The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561:897,simpl,simple,897,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561,3,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095:13,feedback,feedback,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095,2,['feedback'],['feedback']
Usability,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:739,guid,guidelines,739,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874,8,"['clear', 'guid']","['clear', 'clearer', 'guidelines', 'guidelines-for-repository-contributors']"
Usability,"@falexwolf ; Another question - now [normalize_per_cell_weinreb16_deprecated](https://github.com/theislab/scanpy/blob/b4c2479eed302707a4d098f8f3c85037c82f07ca/scanpy/preprocessing/simple.py#L579) doesn't filter anything, just divides by sums of chosen genes, this normalization looks strange; ```; X = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]]); normalize_per_cell_weinreb16_deprecated(X, max_fraction=0.7); array([[1. , 0. , 1. ],; [3. , 0. , 1. ],; [0.71428571, 0.85714286, 0.14285714]]); ```; Should it be this way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/301#issuecomment-438749595:180,simpl,simple,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301#issuecomment-438749595,2,['simpl'],['simple']
Usability,"@falexwolf @ivirshup it would be good to clear this up soon. If this is correct, then we choose HVGs incorrectly every time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/705#issuecomment-520516186:41,clear,clear,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-520516186,2,['clear'],['clear']
Usability,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:587,simpl,simple,587,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,2,['simpl'],['simple']
Usability,"@falexwolf Thanks for pointing out this package, I'll give it a try. I'll also give the pull request a shot - I'm still learning my way around this package but if I can do it I'll submit it for sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420358808:120,learn,learning,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420358808,2,['learn'],['learning']
Usability,@falexwolf regarding #204 the image that didn't work for you is this? I will address the conflict once this is clear because they are related. ![image](https://user-images.githubusercontent.com/4964309/42776678-05350dc6-8938-11e8-8109-901e94abbfee.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-405339417:111,clear,clear,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405339417,2,['clear'],['clear']
Usability,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730:26,feedback,feedback,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730,2,['feedback'],['feedback']
Usability,"@falexwolf thanks for the feedback. As @maximilianh suggested, I was able to export the expression matrix from the cellbrowser export function. Thank you for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-461540169:26,feedback,feedback,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-461540169,2,['feedback'],['feedback']
Usability,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-441809122:66,clear,clear,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-441809122,2,['clear'],['clear']
Usability,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:253,simpl,simple,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154,2,['simpl'],['simple']
Usability,"@falexwolf wrote:. > > Wow, this looks great! One remark for future PRs: We’re migrating to a new documentation style using type annotations; > ; > I'm still not convinced that we should use type annotations for Scanpy toplevel functions. People use Scanpy in Jupyter Lab and notebooks and not in Pycharm. Hence, there is no gain in the annotations, by contrast, the function annotations simply look super complicated and it's no longer feasible to grasp at first sight what's going on. This also regards the output of the help in Jupyter Lab and Notebooks.; > ; > So, while I think that for AnnData and everything in the background, type annotations may make sense for a few developers (not for me, as I'm doing everything on remote servers using emacs), it doesn't make sense for the Scanpy user.; > ; > Also, all the other big packages I work with all the time simply don't have it (numpy, seaborn, pandas, tensorflow) and it makes it harder and lengthier for contributors to contribute if they need to go through it.; > ; > Finally, I'm still not happy about how the automatically generated docs from the type annotation look:; > ![image](https://user-images.githubusercontent.com/16916678/48796750-6ebb8000-ecce-11e8-9cdc-33b6056d8957.png); > which is from; > ![image](https://user-images.githubusercontent.com/16916678/48796824-a0344b80-ecce-11e8-8570-e4754f4ccd96.png); > Clearly, the automatically generated line with `Union[...]` is just way too complicated for a human to make sense of. The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous.; > ; > So, please let's stay away from having more type annotations and corresponding docstrings at this stage and let's simply continue imitating what all the major packages are doing.; > ; > Also: regarding your comment about the use of '``' vs. '`' in the docs: again, I think it leads to an inhomogeneous appearance to have *two* types of markup for code-related things. I agree that the read-the-docs i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373:388,simpl,simply,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373,2,['simpl'],['simply']
Usability,"@falexwolf, yes I think overall fold changes with 2 as a basis are more intuitive than with e as a basis for most people, similar to basis=10, but basis 2 gives a more sensible dynamic range than 10 does on gene expression data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/446#issuecomment-457617601:72,intuit,intuitive,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446#issuecomment-457617601,2,['intuit'],['intuitive']
Usability,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450268629:249,simpl,simply,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450268629,2,['simpl'],['simply']
Usability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:227,simpl,simply,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,2,['simpl'],['simply']
Usability,@fidelram lemme know if the new comments make things more clear,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-526651900:58,clear,clear,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-526651900,2,['clear'],['clear']
Usability,"@fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`?. Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562:159,simpl,simple,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562,1,['simpl'],['simple']
Usability,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:235,simpl,simply,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,2,['simpl'],['simply']
Usability,@flying-sheep I quite like your style guidelines. It might be a good idea to designate a particular function that does it well and is complex enough to include all of these options. That way it's easy to follow style when writing a new function. Something like a contributors template.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523589188:38,guid,guidelines,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523589188,2,['guid'],['guidelines']
Usability,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642:125,simpl,simply,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642,2,['simpl'],['simply']
Usability,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:36,clear,clear,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120,2,['clear'],['clear']
Usability,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436:147,simpl,simply,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436,2,['simpl'],['simply']
Usability,"@flying-sheep This is generally the kind of simplification I was hoping we could do with plotting. ; It's not much, and is more about the dotplot, heatmap, etc. plotting methods. Also, the test errors I was running into are still happening. Another example would using a function to choose representations of X the same way for each function. Something like:. ```python; def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1109:44,simpl,simplification,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109,1,['simpl'],['simplification']
Usability,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:103,learn,learn,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135,2,['learn'],['learn']
Usability,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`?. Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437:19,user experience,user experience,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437,2,['user experience'],['user experience']
Usability,"@flying-sheep the first is easy to fix. For 2. , it's not clear to me what you're asking for here. It's been a while since I worked on this. Am I supposed to import this function in the `__init__.py` for `tl` and `pl`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947264348:58,clear,clear,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947264348,2,['clear'],['clear']
Usability,"@flying-sheep, I made two small changes:. * The 10x readers should no longer return views, fixing `test_read_10x`; * Slightly cleaner providing of categories for leiden/ louvain code. For the clustermap test, it's not clear to me that the problems are even related to pandas, though the cause might be: https://github.com/pandas-dev/pandas/issues/18720. There are two images which are compared in this test. I'll post the comparisons here:. # `master_clustermap.png`. I believe the difference is just the margin, so we should be good to just change the test image. ## Expected. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589759-d73af980-452e-11ea-9a77-89ecf9e752dc.png). ## Actual. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589766-e5891580-452e-11ea-9762-aa483399c8b3.png). # `master_clustermap_withcolor.png`. This one looks worse, but I'm not sure how to fix it. @fidelram might know better?. ## Expected. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589782-123d2d00-452f-11ea-828c-5e6fdc0b6091.png). ## Actual. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589788-21bc7600-452f-11ea-9661-ec55aeee07de.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973:218,clear,clear,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973,2,['clear'],['clear']
Usability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:481,simpl,simple,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,6,"['clear', 'intuit', 'simpl']","['clear', 'intuitively', 'simple']"
Usability,"@flying-sheep, any idea why `__version__` wouldn't work? Are we sure `importlib.metadata.version('umap-learn')` works when `__version__` doesn't? I'd be worried if they were basing the reported version on different sources.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-963430528:103,learn,learn,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963430528,2,['learn'],['learn']
Usability,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1753#issuecomment-804517933:55,clear,clear,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753#issuecomment-804517933,2,['clear'],['clear']
Usability,"@flying-sheep, does this still need to get merged?. It looks like a simple rebase, and the CI plot comparison feature is now active on azure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1047#issuecomment-777239623:68,simpl,simple,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047#issuecomment-777239623,2,['simpl'],['simple']
Usability,"@flying-sheep: Do you agree that we should add this to the travis setup? I thought about creating a `requirements_tests.txt` as for `anndata` and simply adding the line to `.travis.yml`. Good solution? Maybe it's even your solution, I don't remember. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-457869443:146,simpl,simply,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-457869443,2,['simpl'],['simply']
Usability,"@frederikziebell thanks for the feedback, we will discuss this in the next core meeting. An automated cross-package CI could be a solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3029#issuecomment-2366951904:32,feedback,feedback,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3029#issuecomment-2366951904,2,['feedback'],['feedback']
Usability,"@giovp I haven't been able to work around the issue. When I rebuild a container with different versions of scanorama, scanpy, numpy, scikit-learn I end up with errors. Most of the time it's this ValueError about the wrong shape. The only different error I noticed is when I tried scanorama 1.6 and there was an error about `concatenate()` not being an available function. . Whenever you find time to update the tutorial, that will be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633:140,learn,learn,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633,2,['learn'],['learn']
Usability,"@giovp thanks for your reply, I agree on all points :) Have a good vacation!; @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG).; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459:116,feedback,feedback,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459,2,['feedback'],['feedback']
Usability,"@giovp this issue can be closed since the documentation already states that ""To preserve the original structure of adata.uns[‘rank_genes_groups’], filtered genes are set to NaN."" . Users can simply drop the NANs for each cluster column in the adata.uns[‘rank_genes_groups_filtered’] dataframe.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1446#issuecomment-2211244159:191,simpl,simply,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-2211244159,2,['simpl'],['simply']
Usability,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`.; Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512:138,learn,learn,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512,4,['learn'],['learn']
Usability,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:766,clear,clearly,766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523,2,['clear'],['clearly']
Usability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:535,clear,clear,535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,7,"['Guid', 'clear', 'intuit', 'learn']","['Guide', 'clear', 'intuitive', 'learn']"
Usability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:91,simpl,simply,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,2,['simpl'],['simply']
Usability,"@havardtl, sorry for the late feedback, but this would be a great first PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434#issuecomment-783193441:30,feedback,feedback,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434#issuecomment-783193441,2,['feedback'],['feedback']
Usability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:484,simpl,simply,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,2,['simpl'],['simply']
Usability,"@ilan-gold, I believe `map_blocks` offers several advantages, the most significant being its simplicity. If you don't require `futures`, using `map_blocks` is likely the better choice. I'll delve more into this once I begin working on the multi-GPU version for RSC.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980697488:93,simpl,simplicity,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980697488,2,['simpl'],['simplicity']
Usability,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240:153,learn,learn,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240,4,['learn'],['learn']
Usability,"@ivirshup @falexwolf the new black 20 lets users manually control which parameter lists, lists, tuples, dicts, … to explode onto multiple lines. Another advantage is that the removal of e.g. a parameter doesn’t lead to long diffs because they won’t automatically get collapsed again. Finally, it’s useful for e.g. tests or so, where we can format consecutive similar lines of data consistently. I tried to figure out the cleanest version of every spot where black 20 made changes. If you want to change some, or add new spots that can now be formatted more clearly, just create a new PR. This one is supposed to quickly fix Travis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1394:557,clear,clearly,557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1394,1,['clear'],['clearly']
Usability,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831:75,clear,clearly,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831,2,['clear'],['clearly']
Usability,"@ivirshup I simplified the conditionals a bit and there are only two sets now. One to check for various `{Value/Import}Error`s and another to do the `clustering_kwargs` building. I think this is cleaner and faster since no code will run that doesn't have to. I didn't really see a way to do it with only one set of conditionals without code duplication. There's some code that's just common to both, but that shouldn't be run in the case of one of the `{Value/Import}Error`s .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952548908:12,simpl,simplified,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952548908,2,['simpl'],['simplified']
Usability,"@ivirshup I'd actually be interest in hearing those. My packages also have the test folder outside the package, but I am happy to learn why many major packages have theirs in the actual package and why that might be a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545:130,learn,learn,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545,2,['learn'],['learn']
Usability,"@ivirshup Thank you for the feedback. I will add a release note soon. I also thought about the naming of the parameter. However, if we assume that also in the future it is mostly used to subset the number of PCs in PCA arrays stored under different names, it should be fine?. I can not comment further on what these changes may break, at least ideally they should make the use of n_pcs more consistent and as expected. Would you suggest, I implement some further, more comprehensive tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928:28,feedback,feedback,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928,2,['feedback'],['feedback']
Usability,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106:182,simpl,simple,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106,2,['simpl'],['simple']
Usability,@ivirshup You are right about some of the pre-processing plots. I should have created a separate branch for those. I simply think hte outputs have changed and we never caught it but would be curious to see what you get. It could be my M1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952455566:117,simpl,simply,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952455566,2,['simpl'],['simply']
Usability,@ivirshup `integrated_anterior` came from `scanorama.correct_scanpy()` like you mentioned. I'm running my jupyter notebook out of a container. I rebuilt my container with scanorama=1.7 but faced the same issue. Is it possible the problem is the preprocessed data at `https://hmgubox.helmholtz-muenchen.de/f/4ef254675e2a41f89835/?dl=1` is simply of the wrong shape once it's read by scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051031200:338,simpl,simply,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051031200,2,['simpl'],['simply']
Usability,"@ivirshup and I went over this, it just simplifies the doc setup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2561#issuecomment-1693364079:40,simpl,simplifies,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561#issuecomment-1693364079,2,['simpl'],['simplifies']
Usability,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417:36,clear,clear,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417,2,['clear'],['clear']
Usability,"@ivirshup, @JBreunig using the absolute counts isn't a problem per se. It's simply that the scvelo paper used the spliced counts in `adata.X` based on which the highly variable genes are selected and PCA, neighbor graph and UMAP embedding are calculated.; @JBreunig, shouldn't be a problem to put spliced into `adata.X` as the dimensions of spliced and total counts are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735:76,simpl,simply,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735,2,['simpl'],['simply']
Usability,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place?. Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077:207,learn,learn,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077,8,"['clear', 'learn']","['clear', 'learn']"
Usability,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:69,learn,learn,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293,4,['learn'],"['learn', 'learning']"
Usability,@kaushalprasadhial We internal discussed adding `scikit-learn-intelex` as a dependency. We came to the conclusion that we dont want it as such. Since this a patch that the user can do regardless we think tath the best thing would be to have a notebook that would show the speedup of the patch. We could host this in a new notebook acceleration category.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571:56,learn,learn-intelex,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571,2,['learn'],['learn-intelex']
Usability,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:467,clear,clear,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714,2,['clear'],['clear']
Usability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:173,learn,learn,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,2,['learn'],['learn']
Usability,"@moqri bit difficult to answer this question. What are you referring to?. if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: ; ```; The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301:213,clear,clear,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301,2,['clear'],['clear']
Usability,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?. More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks!. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040009925:393,clear,clear,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040009925,2,['clear'],['clear']
Usability,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:123,simpl,simple,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604,2,['simpl'],['simple']
Usability,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/453:718,simpl,simple,718,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453,1,['simpl'],['simple']
Usability,"A simple way to do it is by creating a new `.obs` categorical column as follows:. ```python; adata = sc.datasets.pbmc68k_reduced(); # create new categorical column called `selection`; adata.obs['selection'] = pd.Categorical((adata.obs_vector('CD3G') > 2) & (adata.obs_vector('CD4') < 3)); # adjust colors; adata.uns['selection_colors'] = ['blue', 'yellow']; sc.pl.umap(adata, color='selection', add_outline=True, s=20); ```; ![image](https://user-images.githubusercontent.com/4964309/77539317-75996a80-6ea1-11ea-8762-bb29b00d8e43.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1120#issuecomment-603826676:2,simpl,simple,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1120#issuecomment-603826676,2,['simpl'],['simple']
Usability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:531,clear,clear,531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,2,['clear'],['clear']
Usability,Access to Diffusion Map methods (and other embedding methods) as Scikit-Learn style API,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3054:72,Learn,Learn,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3054,1,['Learn'],['Learn']
Usability,Actually the gray is hardcoded:. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L303. But you can simply use a color-like string to specify your favourite color (Unless you happend to have an `.obs` column named `#fe57a1` or so :grin:):. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L384-L386,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/692#issuecomment-504362945:160,simpl,simply,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692#issuecomment-504362945,2,['simpl'],['simply']
Usability,"Actually, very good point about `scater`. I think the name can be kept. It's more the description that I found slightly convoluted and confusing. For reference, here is the description of `percent.top` in `scater`:. > An integer vector specifying the size(s) of the top set of high-abundance genes. Used to compute the percentage of library size occupied by the most highly expressed genes in each cell. (https://rdrr.io/github/LTLA/scuttle/man/perCellQCMetrics.html). alongside the description of `percent_top` in `scanpy`. > Which proportions of top genes to cover. If empty or None don’t calculate. Values are considered 1-indexed, percent_top=[50] finds cumulative proportion to the 50th most expressed gene. (https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.calculate_qc_metrics.html). I find it clearer when the description starts with ""integer"" rather than ""proportions"". Here's my Pythonized suggestion for scanpy:. > A list of integers specifying the sizes of the top sets of genes used to compute the percentage of library size occupied by the most highly expressed genes in each cell. Happy to hear feedback and further edits!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2842#issuecomment-1935690589:812,clear,clearer,812,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2842#issuecomment-1935690589,4,"['clear', 'feedback']","['clearer', 'feedback']"
Usability,Add issues topic to contributing guidelines,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/568:33,guid,guidelines,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568,2,['guid'],['guidelines']
Usability,Add link to Scanpy in R guide,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1186:24,guid,guide,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1186,2,['guid'],['guide']
Usability,Add transfer learning (TL) technique such as ProjectR to scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1205:13,learn,learning,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205,2,['learn'],['learning']
Usability,Added Cube Gene Learning Algorithm to Ecosystem,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1883:16,Learn,Learning,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1883,1,['Learn'],['Learning']
Usability,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/186:339,usab,usable,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186,1,['usab'],['usable']
Usability,Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project!. For extra information:; https://micv.works; https://github.com/Cai-Lab-at-University-of-Michigan/MiCV; https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1577:549,guid,guidelines,549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577,2,['guid'],"['guide', 'guidelines']"
Usability,"Added VAE and LDVAE models from [scVI](https://github.com/YosefLab/scVI) to externel API. . The user passes an anndata object along with model and training arguments to `scvi()` which then runs and returns the learned representation as well as (optionally) the trainer object. . By default the method uses the model described in the scVI [paper](https://www.nature.com/articles/s41592-018-0229-2.epdf?author_access_token=5sMbnZl1iBFitATlpKkddtRgN0jAjWel9jnR3ZoTv0P1-tTjoP-mBfrGiMqpQx63aBtxToJssRfpqQ482otMbBw2GIGGeinWV4cULBLPg4L4DpCg92dEtoMaB1crCRDG7DgtNrM_1j17VfvHfoy1cQ%3D%3D). . If the user wants to use a linear-decoded VAE model, they can set the `linear_decoded` param to True and the method will use the model described in the LDVAE [paper](https://www.biorxiv.org/content/10.1101/737601v1.full.pdf). . This is my first PR so definitely let me know if you need changes!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1085:210,learn,learned,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085,1,['learn'],['learned']
Usability,"Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2509:148,guid,guidelines,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509,2,['guid'],"['guide', 'guidelines']"
Usability,"Added restrict_to parameter to leiden by using louvain code as template.; Tests are not yet provided. A simple example of execution and checks:. ```python; # First split on cluster 4; sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,; key_added='leiden_res0.4_4_sub'). # Additional split; sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),; resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together; sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',; 'leiden_res0.4_4_add_sub']). # Partition size check; ## Original size of clusters; adata.obs['leiden_res0.4'].value_counts(); 0 932; 1 853; 3 676; 2 676; 4 338; 5 57; Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits); ## Assignment of samples in original clusters to subsplit clusters; adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),; 'leiden_res0.4_4_sub'].value_counts(); 4,0 103; 4,1 68; 4,2 66; 4,3 57; 4,4 44; 5 0; 3 0; 2 0; 1 0; 0 0; Name: leiden_res0.4_4_sub, dtype: int64; ## Assignment of samples not in original clusters to subsplit clusters; adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),; 'leiden_res0.4_4_sub'].value_counts(); 0 932; 1 853; 3 676; 2 676; 5 57; 4,4 0; 4,3 0; 4,2 0; 4,1 0; 4,0 0; Name: leiden_res0.4_4_sub, dtype: int64. ...; ```; ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586:104,simpl,simple,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586,1,['simpl'],['simple']
Usability,Addressing #435 . I've opened this with a not-pretty version. I think this function could be simplified a lot by not requiring it to work on arrays. I'm willing to leave it ugly for now. Left to resolve: return type of `downsample_counts`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/551:93,simpl,simplified,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551,1,['simpl'],['simplified']
Usability,"Adds support for using dictionary inputs to sc.queries.enrich to make bulk queries:. So instead of. ```python; df1 = sc.queries.enrich(['KLF4', 'PAX5']); df2 = sc.queries.enrich(['SOX2', 'NANOG']); ...concat...; ```. we can now do. ```python; df = sc.queries.enrich({'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}); ```. (I mean this was already supported by gprofiler, but it was simply broken due to an explicit list conversion)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1488:385,simpl,simply,385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1488,1,['simpl'],['simply']
Usability,"After pp.neighbors and tl.louvain, I've been calculating the silhouette index of the clustering arrangements to get an idea of how well the data is clustered: sil_avg = silhouette_score(adata.X, adata.obs['louvain'], metric = 'euclidean'). I'm using a for loop to go through many combinations of paramaters, (e.g. n_neighbors for pp.neighbors and resolution for tl.louvain), but it seems that the silhouette indices being generated aren't what I would expect. . However, I've noticed that there is an option, metric ='precomputed', which can be used if a distance matrix has already been calculated, which is done so after using pp.neighbors : adata.uns['neighbors']['distances']. . Any idea how to use silhouette_score with adata.uns['neighbors']['distances']? ; I've tried a couple different ways, e.g. : ; silhouette_score(adata.X, adata.uns['neighbors']['distances'], metric='precomputed'). http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/222:909,learn,learn,909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222,1,['learn'],['learn']
Usability,"After remembering the originally specified number of genes:. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L73-L74. We set `n_genes` to `X.shape[1]`, …twice:. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L99-L100. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L105. Afterwards, we still use, `n_genes_user`, even though `np.argpartition` can only handle at most `X.shape[1]` genes. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L145. It’s not clear to me what `n_genes` is supposed to hold, and what the parameter to `np.argpartition` should be, but the current code crashes if `X.shape[1] < 100` (or the user-specified `n_genes`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/71:720,clear,clear,720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/71,1,['clear'],['clear']
Usability,"Agreed. I don’t think we should rush and include everything into scanpy, especially when it would be a simple wrapper of something existing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247:103,simpl,simple,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247,2,['simpl'],['simple']
Usability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:746,intuit,intuitive,746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,2,['intuit'],['intuitive']
Usability,"Ah sorry, I should have told you that. All of the preprocessing functions can be migrated to only work with AnnData; there is no important setting in which you want to pass an array or a sparse matrix. That's also remniscient from the early days when I thought people might not like to use AnnData. But that's of course stupid, they wouldn't use Scanpy in that case, anyway. I'm merging this for now so that we have something working for 1.4.1, but if you want to simplify and reduce this to AnnData-only, happy to merge a PR on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016:464,simpl,simplify,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016,2,['simpl'],['simplify']
Usability,"Ah, I cannot push to your fork it seems. I think you would have had to set ""allow maintainers to edit"" or something on the right-hand side. I'm simply merging this and editing after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479741041:144,simpl,simply,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479741041,2,['simpl'],['simply']
Usability,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didn’t do it yet. I didn’t have time to review the whole thing, but if y’all want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-462858876:102,learn,learned,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-462858876,2,['learn'],['learned']
Usability,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point.; ```; adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing; ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629:29,clear,clear,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629,2,['clear'],['clear']
Usability,"Ah, what I missed is the statement about metrics: I know that many people play around with different metrics. But then you mix ""representation engineering"" (preprocessing or machine learning) with manifold analysis. I'd say the cleanest is to always just use Euclidean distance and all the other work should be done already before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416730034:182,learn,learning,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416730034,2,['learn'],['learning']
Usability,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>; <summary> Basic PCA projection </summary>. ```python; def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings (just whether to center?) from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt; ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated.; * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up?; * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:; * Does the syntax still work for cases other than 1-to-1 transfer? ; * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`.; * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842:1087,learn,learning,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842,2,['learn'],['learning']
Usability,Also one problem is that i don't understand where to put `materialize_as_ndarray` as it used [here](https://github.com/theislab/scanpy/blob/44c038ad7b6488407958ab020858923b25368d97/scanpy/preprocessing/simple.py#L598) and in filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/371#issuecomment-441479438:202,simpl,simple,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-441479438,2,['simpl'],['simple']
Usability,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138:170,guid,guidelines,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138,2,['guid'],['guidelines']
Usability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:389,intuit,intuitive,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,2,['intuit'],['intuitive']
Usability,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/917#issuecomment-563302687:88,user experience,user experience,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917#issuecomment-563302687,2,['user experience'],['user experience']
Usability,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446371088:132,simpl,simple,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446371088,2,['simpl'],['simple']
Usability,Any news when diffxpy will be up on biorxiv? The package looks super interesting and I'd like to learn more about the background to the methods you implemented.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-462890590:97,learn,learn,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-462890590,2,['learn'],['learn']
Usability,"Are Louvain clustering results supposed to be reproducible? I ran the clustering on the 1.3 mln dataset, exactly following the code provided here https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py as follows:. ```; import scanpy.api as sc; sc.settings.verbosity = 2; adata = sc.read_10x_h5('1M_neurons_filtered_gene_bc_matrices_h5.h5') ; sc.pp.recipe_zheng17(adata) ; sc.pp.neighbors(adata) ; sc.tl.louvain(adata) ; adata.obs['louvain'].to_csv('clustering-scanpy.csv'); ```. This works fine and I get the clustering result that makes sense when visualized, however it differs quite strongly from the clustering result that Alex sent me some time ago (as I understood him, that were exactly the clustering results used for the visualisation in the Scanpy paper). My questions:; 1. Is re-running the above snippet supposed to give me the exact same results every time, or are there some random seeds used along the way?; 2. If yes, how can one modify the code to ensure reproducibility?; 3. Is there any way to modify the snippet to get the exact same results as shown here https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells and in the published Scanpy paper?; 4. If the answer to the previous question is no, could you make those results publicly available somewhere?. I noticed the following warning from `sc.pp.neighbors` that might be relevant:. > WARNING: You're trying to run this on 1000 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params.; Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325:1631,learn,learn,1631,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325,1,['learn'],['learn']
Usability,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:118,usab,usable,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952,2,['usab'],['usable']
Usability,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](; https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460:414,clear,clear,414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460,2,['clear'],['clear']
Usability,"Arrrgh, this prepending of the 0s is a bug in scikit-learn. Thank you very much for pointing it out. Let me briefly think about solving this elegantly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/264#issuecomment-423784002:53,learn,learn,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264#issuecomment-423784002,2,['learn'],['learn']
Usability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:1239,usab,usable,1239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,2,['usab'],['usable']
Usability,"As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations; > ; > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly; > ; > adata_small.obsm['X_model'] = model(adata_small.X); > ; > ingest.neighbors(adata_small) # adata_small with just 1000 observations; > ; > now, we have the updated neighbors graph with 1,001,000 observations; > we want to do the same things as always; > ; > by leveraging the neighbors of the new data within the old data, ; > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors; > ; > ingest.umap(adata_small); > ; > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster ; > ; > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651:231,learn,learned,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651,1,['learn'],['learned']
Usability,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298:62,learn,learn,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298,2,['learn'],['learn']
Usability,"As example for the change I [simplified scanpy.preprocessing.simple.filter_cells](https://github.com/theislab/scanpy/pull/119/commits/ae85520fcd16abbb1fb9748cf0b03fb35ce858b6#diff-1aa47c128676c77be1123acc601efd9eL19). Also I fixed a few docs problems. The new format is now custom, so if you dislike the grey headers, that’s easy. Before | After; ----------|----------; ![before](https://user-images.githubusercontent.com/291575/38499002-c6f9dda0-3bf5-11e8-8a76-07a423b366d3.png) | ![after](https://user-images.githubusercontent.com/291575/38512702-79d3ca24-3c1b-11e8-84af-87f033d5008e.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119:29,simpl,simplified,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119,2,['simpl'],"['simple', 'simplified']"
Usability,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673:69,learn,learn,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673,4,['learn'],['learn']
Usability,"As mentioned before, can you move everything you currently have in `/preprocessing/simple.py` to `qc.py`? We shouldn't grow that file even larger...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433422519:83,simpl,simple,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433422519,2,['simpl'],['simple']
Usability,"As mentioned in the commit, fantastic! :smile:. Which style mentioned in https://github.com/theislab/scanpy/pull/610#issuecomment-484124854 did you decide for?. Most manual corrections I saw in your PR pointed to solution (1) [`**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)`]. That's fine, but we should mention it in `CONTRIBUTING.md`. 🙂What do you think, @flying-sheep?. ----. Why did you decide against the sklearn-style solution, which would have been simple to get without manual fixes (adding `**name**`)? https://github.com/theislab/scanpy/pull/610#issuecomment-484027492",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484418235:483,simpl,simple,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484418235,2,['simpl'],['simple']
Usability,As of umap-learn 0.5.2 an ``n_epochs`` value of 0 is now a valid value and None is used instead to indicate a value should be generated. To maintain forward compatability and backward compatibility with older umap-learn versions we instead make use of the epch setting criterion internal to umap-learn here directly. This should resolve issue #2026 and lmcinnes/umap#798. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028:11,learn,learn,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028,5,"['guid', 'learn']","['guide', 'guidelines', 'learn']"
Usability,"As the title suggests, it can be suboptimal w.r.t computation time to use. ```python; list(map(lambda x: ..., SOME_LIST)); ```. In `readwrite.py` [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/readwrite.py#L190) for example, instead of chaining of the three functions, we can simply compare to a `pandas.Series`:. ```python; adata = adata[:, adata.var['genome'] == f""{genome}""]; ```. This improves readability and is slightly faster. In a quick check I ran in Jupyter Lab, this approach is three times faster. The following code should run in Jupyter Lab as is:. ```python; import random; import string. import numpy as np; import pandas as pd. from anndata import AnnData. letters = string.ascii_lowercase. adata = AnnData(pd.DataFrame(np.random.randn(1000, 30000))); adata.var[""genome""] = np.array([random.choice(letters) for _ in range(30000)]).reshape(-1, 1). genome = 'g'; %timeit adata[:, list(map(lambda x: x == f""{genome}"", adata.var['genome']))]. %timeit adata[:, adata.var['genome'] == f""{genome}""]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1401:324,simpl,simply,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1401,1,['simpl'],['simply']
Usability,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:225,simpl,simply,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844,4,"['intuit', 'simpl']","['intuitive', 'simply']"
Usability,At some point we moved scvelo's loom reading to anndata/scanpy and then simply called that one from within scvelo. . `scvelo.read` and `scanpy.read` are ecaxtly the same. And `read_loom` is called internally within `read`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1074#issuecomment-592336678:72,simpl,simply,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074#issuecomment-592336678,2,['simpl'],['simply']
Usability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:202,usab,usable,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,6,['usab'],"['usability', 'usable']"
Usability,"Awesome idea! :). Sorry that I haven't merged all your other PRs, yet. I got back to work on Scanpy yesterday or so after being sick for 2 weeks and another 2 weeks of complete chaos (sick babies)... ;). You'll get feedback very soon, but at first sight, all of them looked fine, anyway. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/489#issuecomment-470934197:215,feedback,feedback,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/489#issuecomment-470934197,2,['feedback'],['feedback']
Usability,"Awesome, Gokcen, thank you! :grin:. Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357785692:131,simpl,simple,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357785692,2,['simpl'],['simple']
Usability,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:364,simpl,simply,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797,2,['simpl'],['simply']
Usability,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:336,simpl,simpler,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,2,['simpl'],['simpler']
Usability,Backport PR #2028 on branch 1.8.x (Update for cope with issue introduced in umap-learn 0.5.2),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2033:81,learn,learn,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2033,2,['learn'],['learn']
Usability,Backport PR #2028: Update for cope with issue introduced in umap-learn 0.5.2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2033:65,learn,learn,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2033,1,['learn'],['learn']
Usability,Backport PR #2575 on branch 1.9.x (Simplify tests),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2599:35,Simpl,Simplify,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2599,1,['Simpl'],['Simplify']
Usability,Backport PR #2575: Simplify tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2599:19,Simpl,Simplify,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2599,1,['Simpl'],['Simplify']
Usability,Backport PR #3097: Simplify score_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3098:19,Simpl,Simplify,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3098,2,['Simpl'],['Simplify']
Usability,Backport guide,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1570:9,guid,guide,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1570,2,['guid'],['guide']
Usability,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:396,simpl,simple,396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732,2,['simpl'],['simple']
Usability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:294,guid,guides,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,2,['guid'],['guides']
Usability,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575:96,clear,clear,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575,2,['clear'],['clear']
Usability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:410,simpl,simply,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,4,"['feedback', 'simpl']","['feedback', 'simply']"
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2901""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104,2,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2962""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203,2,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2974""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380,2,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2984""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743,2,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3120""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017,2,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3216""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331,2,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3222""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303,2,['feedback'],['feedback']
Usability,"Checks if current axis is colorbar before trying to set the name, see #2681.; This might not be the best solution and does not yet integrate a unit test. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2682:225,guid,guidelines,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2682,2,['guid'],"['guide', 'guidelines']"
Usability,Clears up warnings from using the provided datasets. * Filter out the OldFormatWarnings from using anndata > 0.8; * Fixes numpy warning from ebi dataset parsing; * Fix old format warning from reading `pbmc68k_reduced` file directly,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2096:0,Clear,Clears,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2096,1,['Clear'],['Clears']
Usability,"Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2585:199,guid,guidelines,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585,2,['guid'],"['guide', 'guidelines']"
Usability,"Continuous color schemes are given with the `color_map` argument, categorical schemes are given with `palette`. All the scatter plots (`scatter`, `pca`, `tsne`, `umap`, etc...) share these arguments. Here's an example:. ```python; import scanpy as sc; import matplotlib as mpl; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata, color=[""louvain"", ""HES4""]); sc.pl.umap(adata, color=[""louvain"", ""HES4""], palette=""Set2"", color_map=mpl.cm.Reds); ```. It's not that clearly documented for `umap`, and is pretty easy to miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/476#issuecomment-462582018:467,clear,clearly,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476#issuecomment-462582018,2,['clear'],['clearly']
Usability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:418,simpl,simplicity,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,2,['simpl'],['simplicity']
Usability,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424799480:132,simpl,simple,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424799480,2,['simpl'],['simple']
Usability,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:192,simpl,simple,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485,2,['simpl'],['simple']
Usability,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:221,simpl,simpler,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109,2,['simpl'],['simpler']
Usability,"Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python; import seaborn as sns. iris = sns.load_dataset(""iris""); g = sns.FacetGrid(; iris, ; row=""species"", ; hue=""species"", ; palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}; ); g.map(sns.kdeplot, ""sepal_width"").show(); ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596:168,intuit,intuitive,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596,1,['intuit'],['intuitive']
Usability,Could you provide some more details? It'd be useful to see a script that can reproduce this behavior from scratch. There are some guides on how to write this up in the [contributing section](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md#before-filing-an-issue). Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-554926531:130,guid,guides,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-554926531,2,['guid'],['guides']
Usability,"Could you tell us about how you've found it useful, or point us towards some literature on it being used? I think this would be easy enough to implement, but when I tried a naive implementation the results weren't that compelling. It's very possible I should've played around with the parameters more. @flying-sheep, do you have thoughts on this?. Here's a simple implementation:. ```python; def ica(adata, n_components, inplace=True, **kwargs): ; from sklearn.decomposition import FastICA ; ica_transformer = FastICA(n_components=n_components, **kwargs) ; x_ica = ica_transformer.fit_transform(adata.X) ; if inplace:; adata.obsm[""X_ica""] = x_ica ; adata.varm[""ICs""] = ica_transformer.components_.T ; else:; return ica_transformer ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-519071187:357,simpl,simple,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519071187,2,['simpl'],['simple']
Usability,Currently output figures are often cut off at the edges. Adding bbox_inches = 'tight' to savefig no longer cuts off figure text at edges. As far as I've tested it this shouldn't cause any other problems and simply saves figures with proper padding.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/118:207,simpl,simply,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/118,1,['simpl'],['simply']
Usability,"Currently there's only a wrapper for `scanorama.integrate_scanpy`. From https://github.com/brianhie/scanorama: . > The function integrate_scanpy() will simply add an entry into adata.obsm called 'X_scanorama' for each adata in adatas. obsm['X_scanorama'] contains the low dimensional embeddings as a result of integration, which can be used for KNN graph construction, visualization, and other downstream analysis. ; > The function correct_scanpy() is a little more involved -- it will create new AnnData objects and replace adata.X with the Scanorama-transformed cell-by-gene matrix, while keeping the other metadata in adata as well.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2323:152,simpl,simply,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2323,1,['simpl'],['simply']
Usability,"Currently tl.diffmap generates slightly different results on consequent runs. This is because the v0 argument is not provided in the scipy.sparse.linalg.eigsh call (inside the compute_eigen function). . To fix this I set the random_state inside the compute_eigen function and generate a random vector to pass into scipy.sparse.linalg.eigsh as v0. ; I tried to follow how random_state is implemented in other scanpy functions, I hope this is consistent. To allow user control of the random_state I added random_state arguments to:; - tools/_diffmap.py _diffmap function (tl.diffmap function); - tools/_dpt.py _diffmap function; - neighbors/__init__.py compute_eigen function. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1858:746,guid,guidelines,746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1858,2,['guid'],"['guide', 'guidelines']"
Usability,"Data\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 366 (self.pipeline_name, pass_desc); 367 patched_exception = self._patch_error(msg, e); --> 368 raise patched_exception; 369 ; 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 354 pass_inst = _pass_registry.get(pss).pass_inst; 355 if isinstance(pass_inst, CompilerPass):; --> 356 self._runPass(idx, pass_inst, state); 357 else:; 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 ; C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 309 mutated |= check(pss.run_initialization, internal_state); 310 with SimpleTimer() as pass_time:; --> 311 mutated |= check(pss.run_pass, internal_state); 312 with SimpleTimer() as finalize_time:; 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 271 ; 272 def check(func, compiler_state):; --> 273 mangled = func(compiler_state); 274 if mangled not in (True, False):; 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 392 lower = lowering.Lower(targetctx, library, fndesc, interp,; 393 metadata=metadata); --> 394 lower.lower(); 395 if not flags.no_cpython_wrapper:; 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 166 if self.generator_info is None:; 167 self.genlower = None; --> 168 self.lower_normal_function(self.fndesc); 169 else:; 170 self.genlower =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:9511,Simpl,SimpleTimer,9511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['Simpl'],['SimpleTimer']
Usability,"Dear @fbrundu,. very sorry for the late response. I took a couple of days off with the family. Everything is dictated by the order in `.obs['mycategorical'].cat.categories`. If you're starting off from a string `.obs['mystring']` annotation, then this will default to `natsorted` categories. If you pandas `.reorder_categories` then this will be reflected, too. Now, in `AnnData`, we have the additional possibility to store colors for each category. The corresponding array matches `.obs['mycategorical'].cat.categories` just that instead of the category names, it stores the colors. Let me know if this clears things up for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/218#issuecomment-408749326:605,clear,clears,605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218#issuecomment-408749326,2,['clear'],['clears']
Usability,"Dear @wflynny, sorry for the late response. And sorry that I don't feel able to comment on imputation techniques, it's simply something that I don't have a lot of experience with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-403629451:119,simpl,simply,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-403629451,2,['simpl'],['simply']
Usability,"Dear all; I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error!. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems?. Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663:442,learn,learn,442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663,2,['learn'],['learn']
Usability,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks!. Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405:283,simpl,simple,283,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405,2,['simpl'],['simple']
Usability,"Did you recompute using `tl.paga` in between?. You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from ; > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/381#issuecomment-443398454:320,clear,clear,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381#issuecomment-443398454,2,['clear'],['clear']
Usability,"Did you simply set the verbosity to a higher level to get timings or did you have to use profiling or modify the code? If the latter, it might be helpful for more people to do more fine-grained timings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/810#issuecomment-528250968:8,simpl,simply,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528250968,2,['simpl'],['simply']
Usability,"Do you guys think this PR makes sense or is it too much to add R packages to the travis setup? @ivirshup @flying-sheep . There are bunch of useful R packages out there that will most likely not be reimplemented in Python (limma-voom pseudobulk DE, Liger, MAST etc.). I think it'd be cool to revive rtools and add access to such packages. I'm not sure if this is the right way but, any guidance is appreciated :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854:385,guid,guidance,385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854,2,['guid'],['guidance']
Usability,"Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/477:241,clear,clear,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477,1,['clear'],['clear']
Usability,"Does `scanpy==1.5.1` support multiple sections in one adata object? If I concatenate several anndata object I can't plot even with `sc.pl.spatial(img_key=None)`. Try concatenating 3 mouse brain adata object and plotting:; ```python; adata = adata1.concatenate([obj2, obj3], index_unique=None); sc.pl.spatial(adata[adata.obs[""sample""]==adata.obs[""sample""].unique()[0], :], ; color=[""Rorb"", ""Vip""], img_key=None,; vmin=0, cmap='magma',; gene_symbols='SYMBOL'); ```. This is the error I get:; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-9-8c84185773ec> in <module>; 6 color=[""Rorb"", ""Vip""], img_key=None,; 7 vmin=0, cmap='magma', #vmax=3.8,; ----> 8 gene_symbols='SYMBOL'; 9 ). /nfs/team283/vk7/software/miniconda3farm5/envs/cellpymc/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, size, **kwargs); 765 """"""; 766 if library_id is _empty:; --> 767 library_id = next((i for i in adata.uns['spatial'].keys())); 768 else:; 769 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'; ```. #### Versions:; scanpy==1.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1254:1289,learn,learn,1289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254,1,['learn'],['learn']
Usability,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:330,learn,learn,330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843,2,['learn'],['learn']
Usability,Does this problem also happens with the previous code? this is just to guide me on a solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/309#issuecomment-431269990:71,guid,guide,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309#issuecomment-431269990,2,['guid'],['guide']
Usability,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496837522:20,learn,learn,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496837522,2,['learn'],['learn']
Usability,"Each dataset’s documentation should contain. 1. what it contains (listing obs, …); 2. what steps have been run on it; 3. better links (e.g. is pbmc68k_reduced [this one](https://www.10xgenomics.com/datasets/fresh-68-k-pbm-cs-donor-a-1-standard-1-1-0)? the docstring isn’t clear. It was added by @fidelram in https://github.com/scverse/scanpy/pull/228 …). Especially important is if its `.X` is logarithmized, normalized, and/or filtered. See also: https://github.com/orgs/scverse/projects/18/views/1?pane=issue&itemId=62702062. cc @ilan-gold",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3051:272,clear,clear,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3051,1,['clear'],['clear']
Usability,"Else `argmax([])` is called later, which doesn’t make sense. This happens because `Dseg[tips[0]] + Dseg[tips[1]]` are all `inf` for some datasets. Is that simply a consequence of my data or is that a bug?. This PR assumes the former and results in such segments being skipped. In case of my data, this means that I end up with a single DPT group instead of multiple ones.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/266:155,simpl,simply,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/266,1,['simpl'],['simply']
Usability,"Encountered this same error, not clear what is causing it. . ``` ; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata.raw = adata; sc.pp.scale(adata, max_value=10); sc.pp.filter_genes(adata, min_cells=2); sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=2000); ; ```; With the same error:. ```; File ""/opt/venv/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 400, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n""; ValueError: Bin edges must be unique: array([ -inf, -4.47034836e-08, -2.98023224e-08, -2.98023224e-08,; -1.49011612e-08, -8.38190317e-09, 1.00000000e-12, 1.00000000e-12,; 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 4.09781933e-09,; 1.49011612e-08, 1.49011612e-08, 1.49011612e-08, 1.49011612e-08,; 2.98023224e-08, 4.47034836e-08, 4.47034836e-08, 5.96046448e-08,; inf]).; You can drop duplicate edges by setting the 'duplicates' kwarg; ```. Very possibly an input error, but the output doesn't point to anything useful as a starting point to debug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/509#issuecomment-1147252922:33,clear,clear,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509#issuecomment-1147252922,2,['clear'],['clear']
Usability,"Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal.; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.settings.set_figure_params(dpi=150); sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'; ...; ```; Output:; <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:; <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/944:764,learn,learn,764,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944,1,['learn'],['learn']
Usability,"Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/512:172,simpl,simply,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512,1,['simpl'],['simply']
Usability,"Essentially my problem is this. I have another dataframe (df) that I won't to merge with anndata.obs, via matching indices. Except df is a smaller length than anndata.obs. But I want the indices in which there isn't new information for (those missing values in df), to just appear as NaN. . I've tried many functions: merge, join, pd.concat, concatenate, etc; and have tried many different combinations of parameters but none are able to do this, even though merge and join have documentation stating they should be able to. However, if I left join, all the added info becomes NaN. If I right join, the all of the left dataframe (anndata.obs) becomes NaN. Inner makes an empty matrix, outer makes everything NaN. . Does anybody have a simple trick for this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/211:735,simpl,simple,735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/211,1,['simpl'],['simple']
Usability,"Even something simple doesn't work anymore, without going through h5ad:. ```; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; Traceback (most recent call last):; File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>; cellbrowser.cbScanpyCli(); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli; adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__; return self._getitem_view(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__; self._init_as_view(X, oidx, vidx); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view; self._raw = adata_ref.raw[oidx]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__; oidx, vidx = self._normalize_indices(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices; obs = _normalize_index(obs, self._adata.obs_names); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index; positions = positions[index]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__; return self._get_with(key); File ""/cluster/home/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138:15,simpl,simple,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138,2,['simpl'],['simple']
Usability,"Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867898442:86,guid,guidance,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867898442,2,['guid'],['guidance']
Usability,Exactly same error here.; info:; scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371:130,learn,learn,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371,2,['learn'],['learn']
Usability,Extended gex_only function to visium. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3113; - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: not a big change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3278:109,guid,guidelines,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278,2,['guid'],"['guide', 'guidelines']"
Usability,Extract simplifications from https://github.com/scverse/scanpy/pull/2921 because they can be backported,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3097:8,simpl,simplifications,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3097,1,['simpl'],['simplifications']
Usability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:596,learn,learn,596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,2,['learn'],['learn']
Usability,"FWIW, confirming `umap-learn 0.5.1` works with `numba==0.53.1` on my machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846837983:23,learn,learn,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846837983,2,['learn'],['learn']
Usability,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:27,guid,guidance,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751,2,['guid'],['guidance']
Usability,Feedback please!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/886:0,Feedback,Feedback,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/886,1,['Feedback'],['Feedback']
Usability,Figured I would try and emphasize tests a bit more in the contributing guide.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/772:71,guid,guide,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/772,1,['guid'],['guide']
Usability,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512171197:82,learn,learn,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512171197,2,['learn'],['learn']
Usability,"First of all congratulations for the awesome package, intuitive and works great. Just a suggestion if you have time to implement ridgeplots or joyplots like the ones available now in Seurat. They are useful to present several distributions in a compact (and attractive) way. Seems possible through the seaborn kdeplot functions (https://seaborn.pydata.org/examples/kde_joyplot.html). Just a suggestion, so feel free to close to issue at any point!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/84:54,intuit,intuitive,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/84,1,['intuit'],['intuitive']
Usability,"Fix progress bar, use requests, tqdm version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507:4,progress bar,progress bar,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507,2,['progress bar'],['progress bar']
Usability,"Fixes #241. I've allowed the choice of metric for finding nearest neighbors when `knn=False`. I've added a small test to make sure it works, but would open to adding more. The fix I've made killed a few code paths, so I've removed the dead code. I also reorganized the test cases a bit (split one monolithic test into separate tests), as having more granular feedback helped with a little debugging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/245:359,feedback,feedback,359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245,1,['feedback'],['feedback']
Usability,"Fixes #356 for me. It's a pretty simple change. It's a little hard for me to add a test at the moment, but I'm pretty sure this works.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/357:33,simpl,simple,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/357,1,['simpl'],['simple']
Usability,"Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version – entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset – 10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>; <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>; <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability; - [ ] Figure out if I should be be scaling the whitening matrix differently; - [ ] More in depth comparison of results with sklearn based ICA; - [ ] Documentation; - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941:560,learn,learn,560,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941,4,['learn'],['learn']
Usability,Fixes the current test failures. I could also do a simpler version where I just rechunk in one way instead of adding parameters here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162:51,simpl,simpler,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162,1,['simpl'],['simpler']
Usability,"Fixes; ```; Traceback (most recent call last):; File ""/tmp/tmptq4o33we/job_working_directory/000/50/configs/tmpe21tizb1"", line 29, in <module>; scale='width'); File ""/usr/local/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 564, in rank_genes_groups_violin; df[g] = X_col; File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item; self._ensure_valid_index(value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index; value = Series(value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__; data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True); File ""/usr/local/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array; raise Exception(""Data must be 1-dimensional""); Exception: Data must be 1-dimensional; ```. <!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669:1114,guid,guidelines,1114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669,2,['guid'],"['guide', 'guidelines']"
Usability,"Flit has a very tiny surface area. You can learn its full CLI in literally 2 minutes, as it doesn’t include any kind of new concept (like Poetry’s venv management).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904146:43,learn,learn,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904146,2,['learn'],['learn']
Usability,"Following @Koncopd 's idea, wouldn't it be sufficient to simply have line 340 in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py changed to ; ```; mu = M.mean(1).A1 if issparse(M) else M.mean(1); B = safe_sparse_dot(Q.T, M) - safe_sparse_dot(Q.T, mu[:, None]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446396916:57,simpl,simply,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446396916,6,"['learn', 'simpl']","['learn', 'simply']"
Usability,"For general help like this, please go to https://scanpy.discourse.group/. This is also what the issue template says. How could we have made the text more clear so that you’d have found your way there?. ![grafik](https://user-images.githubusercontent.com/291575/69068901-e0cfbd80-0a25-11ea-8095-52e567b86574.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-555083714:154,clear,clear,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-555083714,2,['clear'],['clear']
Usability,"For me the documentation seems clear, but the example may be confusing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905#issuecomment-557090359:31,clear,clear,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905#issuecomment-557090359,2,['clear'],['clear']
Usability,"For pca of sparse matrices i think it should work this way; 1) Create class for lazy evaluation of X - mean, i.e store X, mean separately and implement multiplication by some dense B as `sparse.csr_matrix.dot(X, B) - mean.dot(B)`.; 2) Pass instance of this class to [randomized_svd](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L233)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-445972448:309,learn,learn,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-445972448,4,['learn'],['learn']
Usability,Forgot to mention my versions: . > scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333#issuecomment-434298485:114,learn,learn,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-434298485,2,['learn'],['learn']
Usability,Found some typos in the docstring. Also searched them in other places.; <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2405:143,guid,guidelines,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2405,2,['guid'],"['guide', 'guidelines']"
Usability,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:137,learn,learn,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745,2,['learn'],['learn']
Usability,"From a quick look at the Seurat code this was borrowed from, I think normalised (but not scaled) data is used, so maybe the scaling isn't needed https://github.com/satijalab/seurat/blob/656fc8b562d53e5d0cedda9e09d9dda81e8c00e9/R/utilities.R#L192. Either way it would be good for this to be clear in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2909#issuecomment-1996819867:290,clear,clear,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2909#issuecomment-1996819867,2,['clear'],['clear']
Usability,"From what I can gather, one goal here is to refactor the dotplot function and give it a complex heatmap layout, with a central heatmap using circle patches with a color and size aesthetic (= the dotplot) and one or more annotation heatmaps for rows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:852,simpl,simple,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,2,['simpl'],['simple']
Usability,"Given that UMAP can be used for manifold learning, shouldn’t be possibile to align experiments using UMAP? Who wants to join me in this evaluation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424605733:41,learn,learning,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424605733,2,['learn'],['learning']
Usability,"Goals:. - flexibility in creation; - Neighbor querying API; - Harmonize storage of backends’ querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html); - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors); - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors); - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>; <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build; --- | --- | --- | --- | --- | ---; [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519:403,learn,learn,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519,2,['learn'],['learn']
Usability,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:33,learn,learning,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845,4,"['clear', 'learn']","['clear', 'learning']"
Usability,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:215,simpl,simply,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,4,['simpl'],['simply']
Usability,Gotcha! I'll prioritize getting the benchmarks up and then I'll need some guidance on how to organize it to fit in scanpy's codebase. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655:74,guid,guidance,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655,2,['guid'],['guidance']
Usability,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598:763,learn,learn,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598,4,['learn'],['learn']
Usability,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441598257:426,simpl,simply,426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441598257,2,['simpl'],['simply']
Usability,"Great! :smile:. Sure, next Wednesday is fine. Also, we can always simply make 1.4.2. I just need to check one tiny thing before we actually make the release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355:66,simpl,simply,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355,2,['simpl'],['simply']
Usability,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:340,learn,learning,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,6,['learn'],"['learned', 'learning']"
Usability,"Great, thank you, @andrea-tango and @Koncopd!. @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away?. Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests?. We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809:1347,simpl,simple,1347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809,2,['simpl'],['simple']
Usability,"Great, thanks for the feedback. Hopefully this merge and commit fix everything. I wasn't able to see what errors were causing the readthedocs build fail as the ""Details"" link just took me to a page that said ""SORRY / This page does not exist yet."", so let me know if there are any other issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338:22,feedback,feedback,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338,2,['feedback'],['feedback']
Usability,"Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with ; ```; pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; ```. Seems to work now for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996:216,simpl,simple,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996,2,['simpl'],['simple']
Usability,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:126,clear,clear,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683,2,['clear'],['clear']
Usability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:1230,simpl,simple,1230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,2,['simpl'],['simple']
Usability,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/710#issuecomment-507120679:78,learn,learn,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710#issuecomment-507120679,4,['learn'],['learn']
Usability,"Hello @Koncopd ,; Thanks for the response!. Step 1: I created a fresh new environment (py3.8.12); ```python; !pip install scanpy[leiden]. Successfully installed anndata-0.7.8 cycler-0.11.0 fonttools-4.28.5 h5py-3.6.0 igraph-0.9.9 joblib-1.1.0 kiwisolver-1.3.2 leidenalg-0.8.8 llvmlite-0.38.0 matplotlib-3.5.1 natsort-8.0.2 networkx-2.6.3 numba-0.55.0 numexpr-2.8.1 numpy-1.21.5 pandas-1.3.5 patsy-0.5.2 pillow-9.0.0 pynndescent-0.5.5 python-igraph-0.9.9 scanpy-1.8.2 scikit-learn-1.0.2 scipy-1.7.3 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.13.1 stdlib-list-0.8.0 tables-3.7.0 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:474,learn,learn-,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,4,['learn'],['learn-']
Usability,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1559:26,learn,learn,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559,2,['learn'],['learn']
Usability,"Hello All,; I am a totally new one in learning single cell RNA_seq. When I was doing quality control, I met this problem. Can anyone help me? Thank you so much",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/647#issuecomment-492876929:38,learn,learning,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647#issuecomment-492876929,2,['learn'],['learning']
Usability,"Hello everyone,. In order to get eigenvalues of the PCs after running PCA, I modified the simple.py code by adding a line to return pca_.explained_variance_ and store it in adata.uns in the pca function definition. This might be useful for users wanting to access eigenvalues of the PCs, which is not possible as far as I know with only the explained_variance_ratio. Would it be a good idea to open a pull request for that?. else:; logg.m('compute PCA with n_comps =', n_comps, r=True, v=4); result = pca(adata.X, n_comps=n_comps, zero_center=zero_center,; svd_solver=svd_solver, random_state=random_state,; recompute=recompute, mute=mute, return_info=True); X_pca, components, pca_variance_ratio, pca_eigenval = result; adata.obsm['X_pca'] = X_pca; adata.varm['PCs'] = components.T; adata.uns['pca_eigenvalues']=pca_eigenval; adata.uns['pca_variance_ratio'] = pca_variance_ratio; logg.m(' finished', t=True, end=' ', v=4). ....... if False if return_info is None else return_info:. return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_; else:; return X_pca. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/86:90,simpl,simple,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/86,1,['simpl'],['simple']
Usability,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/674:613,learn,learn,613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674,2,['learn'],['learn']
Usability,"Hello scanpy team,. this is my first pull request, I hope I at least vaguely adhered to your contribution guidelines. I did not find an issue tackling this exact situation, but if I overlooked it, feel free to point that out. This PR changes few lines of code involved in getting dimensionality reductions like X_pca. The current code skips subsetting the respective array to the n_pcs requested, if the representation name is not X_pca. I do not think this is optimal. This change allows to only take the dimensions needed also for non X_pca representations. This can be useful when using harmony for example or storing different PCA embeddings in the same object. Then it is not needed to first store them as X_pca to be able to properly use the n_pcs parameter in e.g. pp.neighbors. It might make sense then to adapt the documentation respectively as well. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. ----------. Fixes #1846",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179:106,guid,guidelines,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179,3,['guid'],"['guide', 'guidelines']"
Usability,"Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not.; It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me?; Maybe there are discussions recorded somewhere?. Best,; Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3272:90,learn,learning,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272,1,['learn'],['learning']
Usability,"Hello, . I am trying to calculate the cell cycle score for my 2 datasets by merging them together (using concatenate function) from the beginning and I am facing this error. Could anyone help me with this and explain what is the reason(s) for this error?. Thank you . **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; ## Mouse; folder = ""/home/pawandeep/Desktop/D2 Mdx/Macosko_cell_cycle_genes.txt""; cc_genes = pd.read_table(folder, delimiter='\t'); #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'); s_genes = cc_genes['S'].dropna(); g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]; g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]; g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens, use_raw=False); ```. ```pytb; Traceback (most recent call last); /tmp/ipykernel_2938/2560507023.py in <module>; 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]; 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]; ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens, use_raw=False). ~/anaconda3/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs); 255 ; 256 # default phase is S; --> 257 phase = pd.Series('S', index=scores.index); 258 ; 259 # if G2M is higher than S, it's G2M. ~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath); 303 data = data.copy(); 304 else:; --> 305 data = sanitize_array(dat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2156:296,guid,guide,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2156,1,['guid'],['guide']
Usability,"Hello, I am running into exactly the same bug when using both **scanpy-1.5.1** or **scanpy-1.5.0**. . **Versions**: ; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.3.2 pandas==1.0.4 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.0. **Input**: ; `import time; t0 = time.time(); sc.external.exporting.spring_project(adata, './SPRING',; 'umap', subplot_name='all', overwrite=True, cell_groupings=['leiden'],; custom_color_tracks=['total_counts']); print(time.time() - t0)`. **Output**: ; `WARNING: root:Overwriting the files in SPRING.; Writing subplot to SPRING\all; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-59-9c683583ff59> in <module>; 1 import time; 2 t0 = time.time(); ----> 3 sc.external.exporting.spring_project(adata, './SPRING',; 4 'umap', subplot_name='all', overwrite=True, cell_groupings=['leiden'],; 5 custom_color_tracks=['total_counts']). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, neighbors_key, overwrite); 179 ; 180 # Write graph in two formats for backwards compatibility; --> 181 edges = _get_edges(adata, neighbors_key); 182 _write_graph(subplot_dir / 'graph_data.json', E.shape[0], edges); 183 _write_edges(subplot_dir / 'edges.csv', edges). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in _get_edges(adata, neighbors_key); 217 ; 218 def _get_edges(adata, neighbors_key=None):; --> 219 neighbors = NeighborsView(adata, neighbors_key); 220 if 'distances' in neighbors: # these are sparse matrices; 221 matrix = neighbors['distances']. NameError: name 'NeighborsView' is not defined`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1260#issuecomment-645395239:207,learn,learn,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1260#issuecomment-645395239,2,['learn'],['learn']
Usability,"Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last); <ipython-input-5-dfc1e4d9ed06> in <module>(); ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:; scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ?. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544:574,learn,learn,574,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544,1,['learn'],['learn']
Usability,"Hello, I have a minor suggestion that I think will be very helpful for people to navigate and read the documentation. Right now Read the Docs does not display the full table of contents on the left (i.e. one link for every page).; For example, to go to the page on `scanpy.api.pp.calculate_qc_metrics` (which is https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.calculate_qc_metrics.html) I have two options:. First, I could go to the API page under prepocessing (https://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp) , and click the link to `pp.calculate_qc_metrics` link highlighted in the picture. However, it's not clear from the font used that it's clickable, because it looks like just markdown. I'd suggest making links blue, or underlined, or both, because that's what will convey that they are clickable. ![image](https://user-images.githubusercontent.com/12504176/50525392-065f7b80-0a90-11e9-8731-35dde488a456.png). Second, at the bottom of the page I could click `next`, which will take me to the next page. This only allows me to navigate the pages sequentially, and is not very practical because I don't know what comes next. . It would be extremely helpful for people navigating the doc if the table of contents unfurled into links for the individual pages contained in the section they are at. I made an example below of what I think this could look like, in it each page under `Preprocessing: PP` gets it's own link in the sidebar. ![image](https://user-images.githubusercontent.com/12504176/50525689-f052ba80-0a91-11e9-8ddc-9cdb86a0a7f5.png). Thanks. Eduardo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/407:646,clear,clear,646,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407,1,['clear'],['clear']
Usability,"Hello,. I am having the same issue as issue #1246 but my version of scipy being used with scanpy is not updating. I don't know if this is related to my using an ubuntu server or what's causing this but I was wondering if there is a workaround to make scanpy use a more updated version? I have scipy 1.4.1 installed when I check the version but for some reason scanpy is using 1.01 and I don't know how to change this. I'm a bit new to python so I'm sorry if this is a novice question. I appreciate any help you can offer. I am using an ubuntu server running python 3.6 with the following versions:; sc.logging.print_versions() ; scanpy==1.5.1 anndata==0.7.3 umap==0.4-dev numpy==1.15.0 scipy==1.0.1 pandas==0.23.3 scikit-learn==0.23.1 statsmodels==0.11.1. This is the error message:. ```pytb; computing tSNE; WARNING: You’re trying to run this on 16872 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; computing PCA; with n_comps=50; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-65-c244be664e51> in <module>(); ----> 1 sc.tl.tsne(adata, n_pcs = 50); 2 # UMAP, first with neighbor calculation; 3 sc.pp.neighbors(adata, n_pcs = 50, n_neighbors = 20); 4 sc.tl.umap(adata). ~/.local/lib/python3.6/site-packages/scanpy/tools/_tsne.py in tsne(adata, n_pcs, use_rep, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, n_jobs, copy); 78 start = logg.info('computing tSNE'); 79 adata = adata.copy() if copy else adata; ---> 80 X = _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs); 81 # params for sklearn; 82 params_sklearn = dict(. ~/.local/lib/python3.6/site-packages/scanpy/tools/_utils.py in _choose_representation(adata, use_rep, n_pcs, silent); 41 'Falling back to preprocessing with `sc.pp.pca` and default params.'; 42 ); ---> 43 X = pca(adata.X); 44 adata.obsm['X_pca'] = X[:, :n_pcs]; 45 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252:721,learn,learn,721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252,1,['learn'],['learn']
Usability,"Hello,. I found an issue with init_pos and rapids. Since cuMLs UMAP doesn't allow initial positions, it would be nice if `sc.tl.umap` would check if an `init_pos` other than spectral and random is used if method is rapids. The error `paga` produces in cuMLs UMAP is not really user friendly. . - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.tl.umap(adata, init_pos='paga', method='rapids'); ```. ```pytb; WARNING: .obsp[""connectivities""] have not been computed using umap. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <timed eval> in <module>. ~/miniconda3/envs/rapids-0.20/lib/python3.8/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 221 ) # 0 is not a valid value for rapids, unlike original umap; 222 X_contiguous = np.ascontiguousarray(X, dtype=np.float32); --> 223 umap = UMAP(; 224 n_neighbors=n_neighbors,; 225 n_components=n_components,. ~/miniconda3/envs/rapids-0.20/lib/python3.8/site-packages/cuml/internals/api_decorators.py in inner_f(*args, **kwargs); 792 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}); 793 ; --> 794 return func(**kwargs); 795 ; 796 # Set this flag to prevent auto adding this decorator twice. cuml/manifold/umap.pyx in cuml.manifold.umap.UMAP.__init__(). TypeError: %d format: a number is required, not str; ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.7.2; sin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1837:551,guid,guide,551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1837,1,['guid'],['guide']
Usability,"Hello,. I tried to swap BBKNN over to the new logging, and while the timing aspect of it is functional, the `deep` refuses to cooperate. I checked with `sc.pp.neighbors()` just to make sure I'm not screwing things up massively, and it turns out that its deep also doesn't work. <img width=""940"" alt=""Screen Shot 2019-07-23 at 09 37 10"" src=""https://user-images.githubusercontent.com/14993986/61696691-b434bf00-ad2d-11e9-83db-1092dcc61fea.png"">. Any idea what's going on here? I'm on python 3.6.7 on Bionic, jupyter 1.0.0, and all this:. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/746:634,learn,learn,634,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746,1,['learn'],['learn']
Usability,"Hello,. Thank you for developing and maintaining such a useful tool!; I'm trying to integrate two data sets, they're replicates of the same condition. . ```; var_names = adata_002.var_names.intersection(adata_003.var_names); adata_002 = adata_002[:, var_names]; adata_003 = adata_003[:, var_names]. sc.pp.pca(adata_002); sc.pp.neighbors(adata_002); sc.tl.umap(adata_002). sc.tl.ingest(adata_003, adata_002, obs='louvain'); ```. And I got the following error:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-42-b3f5427509ba> in <module>; ----> 1 sc.tl.ingest(adata_003, adata_002, obs='louvain'). AttributeError: module 'scanpy.tools' has no attribute 'ingest'; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1. I've already tried installing the package both with conda and pip and I continue having the same issue. . I would really appreciate your comments and suggestions. . Sara",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1092:869,learn,learn,869,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092,1,['learn'],['learn']
Usability,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:; correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2611#issuecomment-1677594358:399,simpl,simple,399,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611#issuecomment-1677594358,2,['simpl'],['simple']
Usability,"Hello. I tried umap visualization by:. ```; sc.pp.normalize_total(adata, target_sum=1e6); sc.pp.log1p(adata, base=2); sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=400). sc.pp.scale(adata, zero_center=True, max_value=None, copy=False, layer=None, obsm=None); sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=50). sc.tl.umap(adata, min_dist=0.5, spread=1.0); sc.pl.umap(adata, color='fullname', use_raw=False, save='samples_umap.pdf'); ```; But the cells can't separate well; ![image](https://user-images.githubusercontent.com/33963919/209233854-db64fddd-4266-4f87-805b-dced45b1547f.png). version; ```; anndata 0.7.5; scanpy 1.6.1; ```. I tried another small dataset with `scanpy` using the same parameters as before:; ![1](https://user-images.githubusercontent.com/33963919/209387824-3a5b1037-f226-49c8-9222-f54c04a62155.jpg). `sc.tl.umap` still failed to down dimension the data properly. Then I tried the original [`umap` package](https://umap-learn.readthedocs.io/en/latest/plotting.html) using the same data set:. ```; import umap; import umap.plot; mapper = umap.UMAP().fit(adata.X); umap.plot.points(mapper); ```. Now the original `umap` package can do down dimension very well:; ![2](https://user-images.githubusercontent.com/33963919/209387903-0161dfa6-0ca5-48cc-8661-465930e23fef.jpg). I think there may be something wrong with the `umap` function in `scanpy`. Can anyone please let me know the reason?; Thanks a lot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386:1032,learn,learn,1032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386,1,['learn'],['learn']
Usability,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:463,simpl,simple,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,2,['simpl'],['simple']
Usability,"Here is the old package information, in which I run Scanpy very well before.; Windows 10 x64 bit, 20H2. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.3 pandas==1.3.5 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5; scvelo==0.2.4 scanpy==1.8.2 anndata==0.7.8 loompy==3.0.6 numpy==1.20.3 scipy==1.7.3 matplotlib==3.5.1 sklearn==1.0.1 pandas==1.3.5; cellrank==1.5.0 scanpy==1.8.2 anndata==0.7.8 numpy==1.20.3 numba==0.54.1 scipy==1.7.3 pandas==1.3.5 pygpcca==1.0.2 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 scvelo==0.2.4 pygam==0.8.0 matplotlib==3.5.1 seaborn==0.11.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012619831:193,learn,learn,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012619831,4,['learn'],['learn']
Usability,"Here is the summary:; * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`; * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`; * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150:512,simpl,simplified,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150,2,['simpl'],['simplified']
Usability,"Here's [test code](https://gist.github.com/jorvis/da877d89fd159b2fb7dfba26705f7ceb) and my output is:. ```pytb; Initial shape: 737280x28002; After min_genes: 5128x28002; After max_genes: 1431x28002; Traceback (most recent call last):; File ""/tmp/test_cell_and_gene_filter.py"", line 22, in <module>; sc.pp.filter_genes(adata, min_cells=3); File ""/home/jorvis/git/scanpy/scanpy/preprocessing/simple.py"", line 152, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. Note that this same error displays on both of the following lines:. ```python; sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_genes(adata, max_cells=1000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317:390,simpl,simple,390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317,2,['simpl'],['simple']
Usability,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:218,simpl,simply,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,4,['simpl'],"['simple', 'simply']"
Usability,Here's that `spatial_graph` function I had mentioned @giovp and @Mirkazemi. I figure it should probably be named something different to make sure it's clear it would only work for hex wells.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383:151,clear,clear,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383,1,['clear'],['clear']
Usability,"Hey @a-munoz-rojas,. I normally wouldn't redo the batch correction. That can go wrong (or better tbh)... for scanorama it could be better, but for DL-based methods you would have fewer data points for learning the difference between batch and bio effects. So unless you have a large dataset, it might generate problems for those methods. Therefore I try to stay consistent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1060433444:201,learn,learning,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1060433444,2,['learn'],['learning']
Usability,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:308,feedback,feedback,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133,2,['feedback'],['feedback']
Usability,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:644,feedback,feedback,644,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292,2,['feedback'],['feedback']
Usability,"Hey @ivirshup , could you please give me a feedback on this if there should be any improvements?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-603993724:43,feedback,feedback,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-603993724,2,['feedback'],['feedback']
Usability,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up!; Cheers,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007:462,feedback,feedback,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007,2,['feedback'],['feedback']
Usability,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it.; 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,; Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1775#issuecomment-813543994:31,feedback,feedback,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775#issuecomment-813543994,2,['feedback'],['feedback']
Usability,"Hey @ivirshup,. thanks for your comments, that looks like good feedback! Hope your moving went well :); I'm busier than expected this week, but will take some time next week to respond / make the changes you suggested.; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-959860881:63,feedback,feedback,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-959860881,2,['feedback'],['feedback']
Usability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:783,usab,usable,783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,2,['usab'],['usable']
Usability,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:1153,simpl,simple,1153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428,2,['simpl'],['simple']
Usability,"Hey Isaac - thanks for the tip, I did not know about `add_totals`! Violin plots are, in principle, exactly what I would like to have, but they are very hard to read for genes with lots of zeros or for clusters with a lot of cells. I am still worried that this doesn't make the relative numbers clear. For instance, something like 40% of CD14+ monocytes express LDHB, while maybe 80% of dendritic cells do. This looks like it might be more ""relevant"", but in reality, the number of monocytes that express the gene is three times the number of total dendritic cells. Does this make sense?. I guess that in the end the onus is on me to avoid or highlight such ambiguities in the analysis, and remain cognisant of them while looking at the data. I will keep thinking about this and post here if I have an epiphany.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016453592:294,clear,clear,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016453592,2,['clear'],['clear']
Usability,"Hey Lisa!; I think I can see that connection now. But I had initially interpreted the ""If provided"" to mean if I specified, but seems like it also applies to the case where `mpl.rcParams[""axes.prop_cycle""]` is provided for me.; Instead of ""If provided, values of adata.uns[""{var}_colors""] will be set."" I would find it more clear to say something like ""adata.uns[""{var}_colors""] will be set if not already stored""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311#issuecomment-1258452834:324,clear,clear,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311#issuecomment-1258452834,2,['clear'],['clear']
Usability,Hey all!. Just wanted to let you know that I've also made a PR for the tutorial as @giovp suggested:. https://github.com/theislab/scanpy-tutorials/pull/43. Let me know what's left to do - looking forward to your feedback!. Jan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-889285607:212,feedback,feedback,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-889285607,2,['feedback'],['feedback']
Usability,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:99,feedback,feedback,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467,4,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:30,feedback,feedback,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,2,['feedback'],['feedback']
Usability,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-380066080:763,simpl,simply,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380066080,4,"['learn', 'simpl']","['learn', 'simply']"
Usability,"Hey! Sorry for the late reply:; 1. Yes, a separate file, please.; 2. Put both in the same function. I wouldn't call it coexpression though. Something along the lines of `cell_selection_by_genes()` or just `cell_selection()`.; 3. There is a `sort_order` keyword for plotting which works for continuous covariates. I imagine that should work.; 4. That may be overkill... but it would definitely be interesting. I think MAGIC is in `sc.external` and DCA is also easily usable in this framework. They are not part of the core package though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328:466,usab,usable,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328,2,['usab'],['usable']
Usability,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:989,clear,clearer,989,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347,2,['clear'],['clearer']
Usability,"Hey!. Scanpy does not seem to work correctly together with scikit-learn 0.21.1.; When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference.; ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine.; The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654:66,learn,learn,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654,2,['learn'],['learn']
Usability,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739:640,learn,learn,640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739,4,['learn'],['learn']
Usability,"Hey!; I wrote this function a while ago... it was definitely not the cleanest or quickest implementation. And it did take a while to run on ~5k cells at the time, but I thought it would be useful to have this functionality in scanpy. Just wanted to note that the intention was definitely to implement this without resampling. I clearly missed that the default was to use resampling in `np.random.choice`. Thanks for spotting this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/340#issuecomment-435326551:328,clear,clearly,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435326551,2,['clear'],['clearly']
Usability,"Hey!; So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527:534,intuit,intuitive,534,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527,2,['intuit'],['intuitive']
Usability,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:1551,feedback,feedback,1551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207,2,['feedback'],['feedback']
Usability,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. ; * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)?. I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790:850,guid,guide,850,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790,2,['guid'],['guide']
Usability,"Hey,. indeed there seems to be an [issue](https://github.com/mwaskom/seaborn/issues/3522) with our current usage of `seaborn`, not working with `seaborn 0.13.0`.; This has been fixed on the main branch [here](https://github.com/scverse/scanpy/pull/2661), and we'll eventually take over the newest `seaborn` version once this is cleared. For users running into this issue now ; - first check if you indeed have `seaborn 0.13.0`. If yes, then do; - `pip install seaborn==0.12.2` if using pip or; - `conda install seaborn=0.12.2` if using conda. this makes sure you are using the working version of seaborn. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215:328,clear,cleared,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215,2,['clear'],['cleared']
Usability,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/734:53,simpl,simple,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734,1,['simpl'],['simple']
Usability,"Hi . When attempting so simply read a h5 file with: . ```; Python version - 3.8.8; # results_file = path to 10X h5 file ; # adata = sc.read_10x_h5(results_file); ```. I get the following error which is fixed when rolling back to scanpy=1.8.2; ```pytb; ValueError Traceback (most recent call last); <ipython-input-3-8ddd0a13aab2> in <module>; 8 print(results_file); ----> 9 adata = sc.read_10x_h5(results_file); 10 adata.var_names_make_unique(); 11 adata.obs.index = meta.iloc[idx,2] + '-' + adata.obs.index. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only, backup_url); 181 v3 = '/matrix' in f; 182 if v3:; --> 183 adata = _read_v3_10x_h5(filename, start=start); 184 if genome:; 185 if genome not in adata.var['genome'].values:. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_h5(filename, start); 266 try:; 267 dsets = {}; --> 268 _collect_datasets(dsets, f[""matrix""]); 269 ; 270 from scipy.sparse import csr_matrix. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in _collect_datasets(dsets, group); 254 for k, v in group.items():; 255 if isinstance(v, h5py.Dataset):; --> 256 dsets[k] = v[:]; 257 else:; 258 _collect_datasets(dsets, v). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). /opt/conda/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args, new_dtype); 767 if self.shape == ():; 768 fspace = self.id.get_space(); --> 769 selection = sel2.select_read(fspace, args); 770 if selection.mshape is None:; 771 arr = numpy.ndarray((), dtype=new_dtype). /opt/conda/lib/python3.8/site-packages/h5py/_hl/selections2.py in select_read(fspace, args); 99 """"""; 100 if fspace.shape == ():; --> 101 return ScalarReadSelection(fspace, args); 102 ; 103 raise NotImplementedError(). /opt/conda/lib/python3.8/site-packages/h5py/_hl/selections2.py in __init__(self, fspace, args); 84 self.mshape = (); 85 else:; ---> 86 raise ValueErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203:24,simpl,simply,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203,1,['simpl'],['simply']
Usability,"Hi @GMaciag,. This looks like a simple function that people may like to use. Do you want to write a small helper function for this maybe? This might be nice to add to `sc.tl`. One way you could make it display nicely in `sc.pl.umap()` is by turning the values into `pd.Categorical`. In the end you want to show which cells are co-expressing your genes. . Also, this may be a good use for imputation methods. Otherwise you may struggle with the sparsity of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-587532154:32,simpl,simple,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-587532154,2,['simpl'],['simple']
Usability,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:213,simpl,simple,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,3,"['Simpl', 'simpl']","['Simply', 'simple']"
Usability,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward.; In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:; ```; tmp_cluster=adata.obs['leiden'].astype(int); ```; ```; %%R -i tmp_cluster -i adata -o tmp_allMarkers; tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""); tmp_allMarkers<-as.list(tmp_allMarkers); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216:189,simpl,simple,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216,2,['simpl'],['simple']
Usability,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:417,simpl,simple,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172,4,['simpl'],['simple']
Usability,"Hi @a-munoz-rojas, please use [fenced code blocks](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown) for code. The “language” for tracebacks is “pytb”: ```` ```pytb````. @ivirshup do you know what’s happening here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/884#issuecomment-545328026:59,guid,guides,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884#issuecomment-545328026,2,['guid'],['guides']
Usability,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211:154,simpl,simple,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211,2,['simpl'],['simple']
Usability,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:521,simpl,simplicity,521,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,2,['simpl'],['simplicity']
Usability,"Hi @atarashansky and everyone following this interesting discussion!. I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`!. Looking forward to you thoughts on this :); Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382:403,simpl,simplified,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382,6,"['learn', 'simpl']","['learn', 'learns', 'simplified']"
Usability,"Hi @chris-rands I hope so. . I'm one of the authors of `solo`. I'm currently working on getting `solo` into `scvi-tools`. After that it should be relatively simple to add to scanpy. Best,; Nick",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1432#issuecomment-788402755:157,simpl,simple,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-788402755,2,['simpl'],['simple']
Usability,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:; `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]; `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,; `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""); `; there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip.; I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124:36,simpl,simple,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124,2,['simpl'],['simple']
Usability,"Hi @falexwolf ; I am sending you the simpler code ,my anndata which can be reproduced in some way, and csv marker file calculated by Seurat to your email, you might to repeat my marker analysis if you would like. Sorry for doing it in this way, I not familiar about how to make public notebook...and our data is too preliminary to be public.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471373445:37,simpl,simpler,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471373445,2,['simpl'],['simpler']
Usability,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:235,clear,clear,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773,2,['clear'],['clear']
Usability,"Hi @fidelram !. I just reintroduced the simple multi-panel violin feature that is used right in the beginning of the [introductory tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb): https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. For now, I simply hope this didn't break anything in your https://github.com/theislab/scanpy/pull/199. For future work on the plotting part of Scanpy: I need to think about adding tests for this... In case that you have an idea for doing this in the best way, I'm happy to ready about it... As a simple solution, if you don't mind, I'd add the calls from your [gist](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c) to the introductory clustering notebook, so that I make sure that I don't break anything. Best,; Alex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/204:40,simpl,simple,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204,3,['simpl'],"['simple', 'simply']"
Usability,"Hi @fidelram,; One way in which I'd like to do it is like the following:. ```python; sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],; save=False, use_raw=False); ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second).; One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320:509,clear,clear,509,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320,2,['clear'],['clear']
Usability,"Hi @fidelram,; Thanks for your prompt reply! Sorry I didn't make my question clear. ; I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/732#issuecomment-510146221:77,clear,clear,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732#issuecomment-510146221,2,['clear'],['clear']
Usability,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:653,learn,learn,653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,2,['learn'],['learn']
Usability,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:699,simpl,simple,699,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404,2,['simpl'],['simple']
Usability,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:253,simpl,simpler,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268,2,['simpl'],['simpler']
Usability,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:846,learn,learned,846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,2,['learn'],['learned']
Usability,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy?. Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1387503006:706,simpl,simple,706,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1387503006,2,['simpl'],['simple']
Usability,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```; # compare expression levels of mel vs all other cell types in pairwise manner; sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-32-73add1f79f3a> in <module>; 1 # save as a data frame; 2 ; ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'); 4 ; 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols); 53 d = pd.DataFrame(); 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:; ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]; 56 if pval_cutoff is not None:; 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx); 517 ; 518 def __getitem__(self, indx):; --> 519 obj = super(recarray, self).__getitem__(indx); 520 ; 521 # copy behavior of getattr, except that here. ValueError: no field of name mel; ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616:67,clear,clear,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616,2,['clear'],['clear']
Usability,"Hi @ivirshup,. Yes, both of them are me. I incorporated feedback from the help forum which suggested that this function has a bug, hence, the bug report.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1485#issuecomment-722483698:56,feedback,feedback,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1485#issuecomment-722483698,2,['feedback'],['feedback']
Usability,"Hi @jorvis ,. ProjectR as far as I'm aware relies on some form of matrix decompositon (PCA, NMF) and do transfer learning with learnt weights. In some sense, it's similar to ingest. However, it would be a bit complicated to port it from R. ; It doesn't seem super complicated to have it in scanpy as an additional tool, but it's not really a priority now. If you have anything in mind and would want to try with submitting a PR, it would be very much appreciated and we would definitely have a look! ; I'll close this for now, but pls feel free to re-open if you want to continue discussion or would like to discuss implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075:113,learn,learning,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075,4,['learn'],"['learning', 'learnt']"
Usability,"Hi @transcriptomics, ; If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616,4,"['guid', 'learn']","['guide', 'learn']"
Usability,"Hi @venomandvenus, it's not clear if this is really a bug or you are just getting a bunch of warnings from seaborn, which can happen quite often. Did `sc.pl.violin` eventually output the plot you wanted? Also can you post a link to the tutorial you are following?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1742#issuecomment-802755481:28,clear,clear,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742#issuecomment-802755481,2,['clear'],['clear']
Usability,"Hi @vitkl . thanks a lot for the feedback, all noted, we'll work toward enabling large tissue image available for storing+plotting. Will keep this open for reference!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782:33,feedback,feedback,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782,2,['feedback'],['feedback']
Usability,"Hi Alex!. Sorry for this long delay, I just forgot completely.; Maybe I wasn't clear enough in my original post, here is where the issue lies:; ; When I run . ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'); ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'); ```. then I get the following error. ```pytb; 100 groups_order = [str(n) for n in groups_order]; 101 if reference != 'rest' and reference not in set(groups_order):; --> 102 groups_order += [reference]; 103 if (reference != 'rest'; 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']); ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624:79,clear,clear,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624,2,['clear'],['clear']
Usability,"Hi David, . thanks for your reply and your interest!. > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or cust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:150,learn,learned,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,4,['learn'],['learned']
Usability,"Hi David,; it's currently not a focus, at least for me... Our general perspective is to replace all of the manual preprocessing with some ""deep learning-based preprocessing""... We will soon have something on this... If it works, more advanced preprocessing becomes obsolete, I guess. If it doesn't, we'll definitely add more advanced stuff... Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/107#issuecomment-374180584:144,learn,learning-based,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/107#issuecomment-374180584,2,['learn'],['learning-based']
Usability,"Hi Davide!. Thank you very much for this! Sorry that I tend to be late these days, have two 6 week old baby twins to take care of... I'm happy to merge this and I'll add you to the author list! I hope it is ok if I rename the module and the top-level function to `score_gene_lists` and the second top-level function to `score_cell_cylce_genes`? Simply `score` is a bit generic... there might be many other scores in the future and then people will get confused. It's also good if both start with `score` so that auto-lookup gives you directly these suggestions? . Also, do you have a notebook with an example? It would be cool to see this at work. You could push this to a new subdirectory in `scanpy_usage`: https://github.com/theislab/scanpy_usage. I just sent you a collaborator invitation. From the example, we can then mayb design a test that goes a bit more into detail. Would be cool to benchmark with Seurat, for example. Also, one could think about providing a default list of genes, right? In particular for the cell cycle, it would be nice to directly call the function with default parameters - one can then still add user-specified lists. Do you want to provide such a list? I also sent you collaborator invitation for scanpy - maybe only temporarily if we get too many people at some point - so that you can quickly add this, if you like. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/76#issuecomment-363587569:345,Simpl,Simply,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76#issuecomment-363587569,1,['Simpl'],['Simply']
Usability,"Hi Davide,. thank you! Currently, we use the default behavior of pandas concatenate: see [here](https://github.com/theislab/anndata/blob/562954b43a9b8faa969e0ec01707bc56cbc021b0/anndata/base.py#L1371-L1400). I'll not be able to fix this during the next days. @flying-sheep, could you have a look and maybe figure out a meaningful option or meaningful default to circumvent this? It should be easy to simply pass an option to DataFrame.concat(). @dawe Thanks again for your pull request. I also put a new anndata version that incorporates it on PyPI. Cheers, ; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-354371241:400,simpl,simply,400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354371241,2,['simpl'],['simply']
Usability,"Hi Huidong,. thank you for the kind words!. `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do; ```; adata_subset = adata[ list_of_barcodes, :].copy(); ```; to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/165#issuecomment-393739002:95,Simpl,Simply,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165#issuecomment-393739002,1,['Simpl'],['Simply']
Usability,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:639,learn,learn,639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,2,['learn'],['learn']
Usability,"Hi Joshua, can you upload an example dataset somewhere? So that I can reproduce the figure above? I'm confident that I can speed this up...; PS: Still consolidating all the gene correlation stuff... Everything works, but we do not want to expose things to the user that have not been checked 3 times... in particular the conventions need to be intuitive etc.; PPS: The new cell cycle example could interest you:; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes_cell_cycle.html; Both link to the notebook in the ""Examples"" section; https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899:344,intuit,intuitive,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899,2,['intuit'],['intuitive']
Usability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:254,simpl,simply,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,2,['simpl'],['simply']
Usability,"Hi Michael:. I compare the `umap` in `scanpy` with the original [`umap`](https://umap-learn.readthedocs.io/en/latest/plotting.html) (https://umap-learn.readthedocs.io/en/latest/plotting.html) using the same dataset, the original `umap` works well, I think the problem is in `scanpy`. I edited my question to include this.; Do you have some suggestions?; Thanks. Dan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364160150:86,learn,learn,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364160150,4,['learn'],['learn']
Usability,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:202,clear,clearly,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498,2,['clear'],['clearly']
Usability,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:240,simpl,simply,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579,10,['simpl'],"['simple', 'simply']"
Usability,"Hi all! I've been using `scanpy` and ran into a similar problem. I was wondering if there's an easy / appropriate work-around, other than simply deleting `obsm[X_diffmap]`?. Thank you all for developing `scanpy`, it's a really wonderful piece of software.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-739622198:138,simpl,simply,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-739622198,2,['simpl'],['simply']
Usability,"Hi all, ; Following [this preprint](https://www.biorxiv.org/content/early/2018/02/14/258566) and the polemic [comment](https://liorpachter.wordpress.com/2018/02/15/watermans-egg/amp/?__twitter_impression=true) by its author, I wonder if Logistic Regression should be in scanpy environment.; I tried the `sklearn.linear_model.LogisticRegressionCV` classifier from scikit-learn. It is pretty fast and seems to do the job. Of course there is no urgent need to include in scanpy, as it can be used with two lines like. ```; clf = sklearn.linear_model.LogisticRegressionCV(); clf.fit(adata.X, adata.obs[group]); ```. among the returned elements, `clf.coef_` can be used to rank genes by their importance on each group, `clf.predict_proba` may be used to score the strength of cell/group association given the scored genes. ; Any thought?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/95:370,learn,learn,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95,1,['learn'],['learn']
Usability,"Hi all, we (@ste-depo) are trying to run scanpy on M1 architecture. We have created a conda environment using Miniforge for osx64-arm. However, when running `sc.pp.neighbors` we have this error. I understand that this is related to `umap-learn` and not directly to `scanpy`, but I think it's worth to mention here. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 743 try:; --> 744 yield; 745 except NumbaError as e:; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_block(self, block); 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_inst(self, inst); 327 val = self.lower_assign(ty, inst); --> 328 self.storevar(val, inst.target.name); 329 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 ; ; AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32; ; During handling of the above exception, another exception occurred:; ; LoweringError Traceback (most recent call last); <ipython-input-19-ef300851c737> in <module>; 1 n_neighbors = int(np.sqrt(adata.shape[0])/2); ----> 2 sc.pp.neighbors(adata, n_neighbors=n_neighbors, n_pcs=15); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:238,learn,learn,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['learn'],['learn']
Usability,"Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2); -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)); ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/501:674,feedback,feedback,674,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501,1,['feedback'],['feedback']
Usability,"Hi all,. I recently installed the newest version of scanpy:; ```; scanpy==1.4.5.post1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1; ```. And after this, I could no longer run a tSNE, while this worked fine before (scanpy==1.4.4.post1). I have not changed anything in my data or my code. ```pytb; TypeError Traceback (most recent call last); <ipython-input-54-e62d5f8d460c> in <module>; ----> 1 sc.tl.tsne(adata). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\tools\_tsne.py in tsne(adata, n_pcs, use_rep, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, n_jobs, copy); 108 if X_tsne is None:; 109 from sklearn.manifold import TSNE; --> 110 from . import _tsne_fix # fix by D. DeTomaso for sklearn < 0.19; 111 ; 112 # unfortunately, sklearn does not allow to set a minimum number. ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\tools\_tsne_fix.py in <module>; 32 verbose: int = 0,; 33 args: Iterable[Any] = (),; ---> 34 kwargs: Mapping[str, Any] = MappingProxyType({}),; 35 ) -> Tuple[np.ndarray, float, int]:; 36 """"""\. ~\Anaconda3\envs\UMCU\lib\typing.py in __getitem__(self, parameters); 1338 "" Got %.100r."" % (args,)); 1339 parameters = (tuple(args), result); -> 1340 return self.__getitem_inner__(parameters); 1341 ; 1342 @_tp_cache. ~\Anaconda3\envs\UMCU\lib\typing.py in inner(*args, **kwds); 680 except TypeError:; 681 pass # All real errors (not unhashable args) are raised below.; --> 682 return func(*args, **kwds); 683 return inner; 684 . ~\Anaconda3\envs\UMCU\lib\typing.py in __getitem_inner__(self, parameters); 1348 return super().__getitem__((_TypingEllipsis, result)); 1349 msg = ""Callable[[arg, ...], result]: each arg must be a type.""; -> 1350 args = tuple(_type_check(arg, msg) for arg in args); 1351 parameters = args + (result,); 1352 return super().__getitem__(parameters). ~\Anaconda3\envs\UMCU\lib\typing.py in <genexpr>(.0); 1348 return",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1067:162,learn,learn,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067,1,['learn'],['learn']
Usability,"Hi all,. thanks for the nice discussion!. Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:645,clear,clearly,645,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,2,['clear'],['clearly']
Usability,"Hi all,; I'm learning scanpy following tutorial, but the picture after tl.umap looks very strange. I'm not sure what's the problem is..; ![tl.umap](https://github.com/FionaMoon/Picture-Saving/blob/master/img/umap.pdf)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337:13,learn,learning,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337,1,['learn'],['learning']
Usability,"Hi all,; Thanks to develop the great tools for singlecell analysis.; <!-- Please give a clear and concise description of what the bug is: -->; The h5ad file was generated by scanpy workflow using function write.h5ad(), nevertheless, ; as of taking the outputed file h5ad of scanpy as file imported Seurat package using ReadH5AD triggerred error like below:; Error in file [[""obs""]] []: objects of category 'environment' cannot take a subset; What happen in this situation? ; and how to fix it?; any advices would be grateful; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; Python 3.8.2 ; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy 1.4.6; >",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1198:88,clear,clear,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1198,1,['clear'],['clear']
Usability,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517:472,intuit,intuitive,472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517,1,['intuit'],['intuitive']
Usability,"Hi all.; I've been running into issues installing scanpy on M1 Mac (Apple Silicone) hardware. (It looks like something is wrong with numba?) I'm sure this is not new to you. I've seen a few issues on that here and there but it's not clear to me at this time:; 1. whether there is any workaround to getting this to work? (I'm using M1 homebrew); 2. where can I track the status of the fix for this? . The lack of scanpy is quite debilitating in my daily workflows, so it would be awesome if there is a way to get it to work even temporarily while waiting for the fix. Thanks ahead!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2237:233,clear,clear,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2237,1,['clear'],['clear']
Usability,"Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```; scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1; pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Cmds:. ```py; import scanpy.external as sce; sce.pp.bbknn(adata, batch_key='sample', copy=False); ```. Error info:. ```pytb; sce.pp.bbknn(adata, batch_key='sample', copy=False); computing batch balanced neighbors; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-34-5b7ebd13c9e6> in <module>; 1 # Correct; 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'); ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn); 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], ; 273 knn_indices.shape[1], bandwidth=bandwidth,; --> 274 											 local_connectivity=local_connectivity); 275 #optional trimming; 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/632:262,learn,learn,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632,1,['learn'],['learn']
Usability,"Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/870:169,simpl,simple,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870,1,['simpl'],['simple']
Usability,"Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately?. for example:; here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,; Shen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/970:312,simpl,simple,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970,1,['simpl'],['simple']
Usability,"Hi scanpy folks,. This PR adds our cell identity classification model [`scnym`](https://github.com/calico/scnym) as a tool in `sc.external.tl.scnym`. The `scnym` API was inspired by `scanpy` and intended to be compatible, so the implementation in `external/_scnym.py` is simply a wrapper.; I also added a test in `tests/external/test_scnym.py` that passes.; Everything was linted with `black,flake8,autopep8` through `pre-commit`. Please let me know if there are any issues or changes you'd like to see.; Thanks for building a great ecosystem!. Best,; Jacob",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1775:271,simpl,simply,271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775,1,['simpl'],['simply']
Usability,"Hi there,. While running ```sc.pp.highly_variable_genes(adata.X)``` I got the following error:. ```AttributeError: X not found```. I then ran ```sc.pp.highly_variable_genes(adata)``` and got the following:. ```ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). You can drop duplicate edges by setting the duplicates kwarg ```. The older ```sc.pp.filter_genes_dispersion(adata.X)``` works fine. Do you know how to fix this?. Thank you!. **Info**: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391:625,learn,learn,625,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391,1,['learn'],['learn']
Usability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1520,simpl,simpler,1520,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,2,['simpl'],['simpler']
Usability,"Hi! Good to read! :smile:. The PAGA edges simply mean that clusters are topologically connected - in the single-cell graph, there is a significant number of inter-cluster-edges, above noise-level. They absolutely don't have an orientation. Regarding velocyto: yes, it's possible to use it to orient the edges in PAGA. You can get that functionality following [this](https://github.com/falexwolf/paga/blob/master/planaria/planaria_paga_velocyto.ipynb); however, until this becomes really well-documented etc. this will still take a while... the model behind this will also be subject to change, I guess... Get Scanpy 1.1 for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393738263:42,simpl,simply,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393738263,2,['simpl'],['simply']
Usability,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/120#issuecomment-380016607:523,simpl,simple-pseudotime,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380016607,2,['simpl'],['simple-pseudotime']
Usability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:117,clear,clear,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,2,['clear'],['clear']
Usability,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136:354,simpl,simple,354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136,1,['simpl'],['simple']
Usability,"Hi! Sorry for frustrating you :( if you want I can fix and merge it manually. You're doing great work!. You simply need to import the things you're using in the annotations, then it'll work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-439104094:108,simpl,simply,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439104094,2,['simpl'],['simply']
Usability,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/673#issuecomment-528510773:221,simpl,simplify,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673#issuecomment-528510773,2,['simpl'],['simplify']
Usability,"Hi! Sorry, my bad. That parameter was undocumented, and I added it to the wrong docstring building block. It’s available in all scatterplots for dimension reductions, like `pca`, `tsne` and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/545#issuecomment-475158480:38,undo,undocumented,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545#issuecomment-475158480,2,['undo'],['undocumented']
Usability,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place!. Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859#issuecomment-538903445:434,simpl,simply,434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-538903445,2,['simpl'],['simply']
Usability,"Hi! thank you for the praise! I’m afraid this is a documentation bug!. We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/875#issuecomment-545958927:336,simpl,simplify,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875#issuecomment-545958927,2,['simpl'],['simplify']
Usability,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/606:768,learn,learn,768,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606,1,['learn'],['learn']
Usability,Hi!. If you used a neural network approach you could use scArches to leverage transfer learning to map things across without re-integrating (only minimal additional training done there). You could also map into the embedded space using `sc.tl.ingest` for example. But there is always the danger that there is a residual batch effect that cannot be removed without de-novo integration.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384:87,learn,learning,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384,2,['learn'],['learning']
Usability,"Hi!. Thanks for reaching out!. We have an option to compute connectivity based on minimum distance, right. The default choice, however, is based on edge-statistics (actual inter-edges between clusters vs. expected number of edges in random connections). Currently, I'm working on the revision of the algorithm. The option for minimum distance will disappear and everything will become much cleaner. I'm also trying to improve the statistical model for connectivity and provide a clearer option for its significance threshold. Right now, the only relevant option in the whole AGA [given the single-cell graph is computed and clustered] is `tree_based_confidence=True`; if you set this to `False`, the significance value for edges to appear will be much lower and you'll get a much sparser abstracted graph. However, this graph is sometimes too sparse. If `tree_based_confidence=True`, as per default, this works fine on very connected datasets, but sometimes gives results that are too dense on disconnected datasets. For now, you could simply try setting `tree_based_confidence=False` and see whether this is satisfying. If not; probably too sparse, it would be great if you could try the new AGA version in a couple of days. Also, I'd be very happy to run the method on your data and look at specific issues. You can also approach me via email... Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-370139940:479,clear,clearer,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-370139940,4,"['clear', 'simpl']","['clearer', 'simply']"
Usability,"Hi!; Sorry for that, the command-line interface got a bit behind. I fixed everything, simply pull again.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113:86,simpl,simply,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113,2,['simpl'],['simply']
Usability,"Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. ; I tried it using `subplots`, but this is what happens: . ```python; fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)); p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1); p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2); ```; ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions; ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```. ### Full repex; ```python; import scanpy as sc; import numpy as np; from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1); adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) ; p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False); p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) ; fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)); p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1); p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/815:840,learn,learn,840,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815,1,['learn'],['learn']
Usability,"Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:; ```pytb; /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key); 280 n_top_genes=n_top_genes,; 281 n_bins=n_bins,; --> 282 flavor=flavor,; 283 ); 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor); 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]; 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower; --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]; 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off; 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898; ```. I run scanpy in Python 3.7 (linux machine) with the following versions:; ```; scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/834:1247,learn,learn,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834,1,['learn'],['learn']
Usability,"Hi, . Thanks for developing a good tool for analyzing scRNAseq data. In the process of learning scRNAseq analysis with scanpy I have come across a few places in the documentation that left me a bit confused. . The most confusing is that I could not find a description for the variable `n_genes_by_counts` calculated by the function scanpy.pp.calculate_qc_metrics and mentioned in the tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html. . I asked a question about this in [the discourse](https://scanpy.discourse.group/t/clarification-of-qc-metrics/295) and was asked to open a github issue. . I guess an explanation for the docs could be something like `n_genes_by_counts: The number of genes with at least 1 count in a cell. Calculated for all cells.` . Since I am writing this I might add that I did not understand exactly how the normalization function [scanpy.pp.normalize_total](https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.normalize_total.html) works before I stumbled upon the description of the deprecated function with the same purpose: [scanpy.pp.normalize_per_cell](https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.normalize_per_cell.html) . The deprecated function has a helpful line saying: `Normalize each cell by total counts over all genes, so that every cell has the same total count after normalization.` which at least helped me clear things up. Maybe this line should be added to the new version of the function?. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434:87,learn,learning,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434,2,"['clear', 'learn']","['clear', 'learning']"
Usability,"Hi, . Thanks for the great tools included in scanpy. . I was searching to see what the logfoldchange numbers in the `rank_genes_groups` are. i.e. is it natural log/log base 10 or log2? The code in https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py shows that it is log2foldchange. . Perhaps this should be made clear in the documentation? Or `logfoldchange` should be chnaged to `log2foldchange`. . Cheers, ; S",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/446:342,clear,clear,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446,1,['clear'],['clear']
Usability,"Hi, . we developed [scirpy](https://github.com/icbi-lab/scirpy), a scanpy extension to analyse single-cell TCR data. You can learn more about the project in our [preprint](https://www.biorxiv.org/content/10.1101/2020.04.10.035865v1), the [apidocs](https://icbi-lab.github.io/scirpy/api.html) and the [tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html). . Since I heard that some people around here (@davidsebfischer, @b-schubert) are really interested in this topic, I created this issue to coordinate efforts. I would love to work together with you guys to take this to the next level. . Currently, some ideas of mine are; * extension to BCR data; * integration with @davidsebfischer's [tcellmatch](https://github.com/theislab/tcellmatch) as a distance metric for clonotype networks; * integration with epitope databases. Let me know what you think! . Best, ; Gregor . CC @ffinotello, @szabogtamas, @mlist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163:125,learn,learn,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163,1,['learn'],['learn']
Usability,"Hi, ; I tested your fix and it works! ; ```; scanpy==1.4.5.2.dev6+gfa408dc7 anndata==0.7.1 ; umap==0.3.10 numpy==1.17.4 scipy==1.3.1 ; pandas==0.25.2 scikit-learn==0.21.3 ; statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```; BTW:; `matplotlib==3.1.3`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599:157,learn,learn,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599,2,['learn'],['learn']
Usability,"Hi, ; we want to add SCALEX, an online integration method that integrate different single-cell experiments including scRNA-seq and scATAC-seq, which also enable accurate projecting new incoming datasets onto the existing cell space. This method is published on Nature Communications. SCALEX is developed based-on scanpy. We appreciate and will be honored to contribute to the scanpy community. Thank you!; Lei . <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2353:483,guid,guidelines,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2353,2,['guid'],"['guide', 'guidelines']"
Usability,"Hi, I am reproducing this tutorial https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. the line sc.pp.neighbors(adata) produces the following error:. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!. Ubuntu 18.04; Python 3.6.6. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Can you help me? Thank You",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280:417,learn,learn,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280,1,['learn'],['learn']
Usability,"Hi, I have the same problem, sorry that I just started learning python, so I don't really understand some of the improvements and I also tried some of them, it didn't work. ; Could you please let me know what kind of solution will be good for this issue?. Thanks! ; <img width=""765"" alt=""Screen Shot 2020-05-24 at 17 43 08"" src=""https://user-images.githubusercontent.com/50899584/82768801-15ae3a00-9de6-11ea-9552-88eb59b19405.png"">; <img width=""324"" alt=""Screen Shot 2020-05-24 at 17 43 42"" src=""https://user-images.githubusercontent.com/50899584/82768808-2199fc00-9de6-11ea-8624-61bd67c5aae7.png"">. Yi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1103#issuecomment-633326096:55,learn,learning,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103#issuecomment-633326096,2,['learn'],['learning']
Usability,"Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```; scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1; ```. ```; adata = sc.datasets.pbmc3k(); sc.pp.calculate_qc_metrics(adata, inplace=True); ```; output:; ```; ---------------------------------------------------------------------------; TypingError Traceback (most recent call last); <ipython-input-5-0d8cf2779f18> in <module>; 1 adata = sc.datasets.pbmc3k(); ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel); 281 percent_top=percent_top,; 282 inplace=inplace,; --> 283 X=X,; 284 ); 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel); 107 if percent_top:; 108 percent_top = sorted(percent_top); --> 109 proportions = top_segment_proportions(X, percent_top); 110 for i, n in enumerate(percent_top):; 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns); 364 mtx = csr_matrix(mtx); 365 return top_segment_proportions_sparse_csr(; --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int); 367 ); 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 399 e.patch_message(msg); 400 ; --> 401 error_rewrite(e, 'typing'); 402 except errors.UnsupportedError as e:; 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/sit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/978:184,learn,learn,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978,1,['learn'],['learn']
Usability,"Hi, I would like to request Cubé be added to the Scanpy Ecosystem page (https://scanpy.readthedocs.io/en/stable/ecosystem.html). Cubé (https://github.com/connerlambden/Cube/) is an intuitive network algorithm that searches for multiplicative combinations of genes that have high/low correlation and are members of a known biological pathway. Given an input gene1, Cubé searches for gene2 and gene3 such that the expression of gene1 ~= gene2 * gene3 only in cells that express all three genes. Let me know if you have any questions.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1878:181,intuit,intuitive,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1878,1,['intuit'],['intuitive']
Usability,"Hi, I'm trying scanpy for the first time, but following the first tutorial I got the error below.; Any idea of what is happening?. ```pytb; >>> filter_result = sc.pp.filter_genes_dispersion(; ... adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5); Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 328, in filter_genes_dispersion; disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin]; File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 938, in __setitem__; setitem(key, value); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 929, in setitem; self._where(~key, value, inplace=True); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7539, in _where; level=level, fill_value=np.nan); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 3248, in align; broadcast_axis=broadcast_axis); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7366, in align; fill_axis=fill_axis); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7435, in _align_series; return_indexers=True); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3772, in join; return_indexers=return_indexers); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 4012, in _join_monotonic; ridx = self._left_indexer_unique(sv, ov); File ""pandas/_libs/join_helper.pxi"", line 774, in pandas._libs.join.left_join_indexer_unique_object; ValueError: Buffer dtype mismatch, expected 'Python object' but got 'signed char'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158:403,simpl,simple,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158,1,['simpl'],['simple']
Usability,"Hi, here my settings:. scanpy==1.0.4 anndata==0.6.1 numpy==1.14.2 scipy==1.1.0 pandas==0.23.0 ; scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1. Ivan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390656723:103,learn,learn,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390656723,2,['learn'],['learn']
Usability,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help?. This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058:101,learn,learn,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058,2,['learn'],['learn']
Usability,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False); sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab10"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab20b"", s=0.1); ```; ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False); sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",; s=0.1, vmin=-0.6, vmax=3.5); ```; ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:; scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889:1592,learn,learn,1592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889,2,['learn'],['learn']
Usability,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1397,simpl,simple,1397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190,2,['simpl'],['simple']
Usability,"Hi, thanks for the contribution!. The converting to dense is quite iffy, we should probably add real support for sparse here. [We could use this as a base](https://github.com/scikit-learn/scikit-learn/blob/45cf8ec555a026c4263e8bef12850755a83df10e/sklearn/utils/sparsefuncs.py#L685). Do you think you can do that or should we help?. This is also missing release notes in 1.11.0.md",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449:182,learn,learn,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449,4,['learn'],['learn']
Usability,"Hi, thanks for the feedback! Another interesting option is [AUCell](https://github.com/aertslab/pySCENIC/blob/master/src/pyscenic/aucell.py) from the [SCENIC workflow](https://www.nature.com/articles/nmeth.4463) that does the comparison based on ranked gene expression - haven't tried it myself though. Best wishes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/638#issuecomment-491638466:19,feedback,feedback,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638#issuecomment-491638466,2,['feedback'],['feedback']
Usability,"Hi, thanks for the report!. Note that the plots in the documentation are generated on the fly when building the documentation. The plot you currently see on https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.dotplot.html has therefore been created with scanpy 1.10.1. Must be a dependency issue, I’ll try to reproduce with the environment you provided. /edit: I can reproduce it with that environment:. <details><summary>environment.yml</summary>. ```yaml; name: scanpy-3062; channels:; - conda-forge; dependencies:; - ipykernel. - python==3.10.10; - anndata==0.10.7; - scanpy==1.10.1; - IPython==8.13.2; - pillow==10.0.0; - astunparse==1.6.3; - backcall==0.2.0; - cffi==1.15.1; - cloudpickle==2.2.1; - colorama==0.4.4; - cycler==0.10.0; - cytoolz==0.12.0; - dask==2023.10.1; #- dateutil==2.8.2; - decorator==5.1.1; - defusedxml==0.7.1; - dill==0.3.6; - entrypoints==0.4; - exceptiongroup==1.1.1; - executing==1.2.0; - fasteners==0.17.3; - gmpy2==2.1.2; - h5py==3.8.0; #- icu==2.11; - python-igraph==0.11.2; - jedi==0.19.1; - jinja2==3.1.2; - joblib==1.2.0; - kiwisolver==1.4.4; - leidenalg==0.10.2; - llvmlite==0.42.0; - lz4==4.3.2; - markupsafe==2.1.2; - matplotlib==3.8.3; - mpmath==1.3.0; #- msgpack==1.0.5; - natsort==8.3.1; - numba==0.59.1; - numcodecs==0.11.0; - numexpr==2.7.3; - numpy==1.26.4; - packaging==23.1; - pandas==1.5.3; - parso==0.8.3; - pexpect==4.8.0; - pickleshare==0.7.5; - plotly==5.14.1; - prompt_toolkit==3.0.38; - psutil==5.9.5; - ptyprocess==0.7.0; - pure_eval==0.2.2; - pyarrow==10.0.1; - pydot==1.4.2; - pygments==2.15.1; - pyparsing==3.0.9; - pytz==2023.3.post1; - scipy==1.13.0; #- session_info==1.0.0; #- setuptools==67.7.2; - six==1.16.0; - scikit-learn==1.2.2; - stack_data==0.6.2; - sympy==1.11.1; - tblib==1.7.0; - texttable==1.6.7; - threadpoolctl==3.1.0; #- tlz==0.12.0; - toolz==0.11.2; #- pytorch==2.1.1; - tqdm==4.65.0; - traitlets==5.9.0; - wcwidth==0.2.6; #- yaml==5.4.1; - zarr==2.14.2; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516:1691,learn,learn,1691,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516,2,['learn'],['learn']
Usability,"Hi, thanks for your interest in scanpy!. For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub!. Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2626#issuecomment-1748396363:627,guid,guidance,627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626#issuecomment-1748396363,2,['guid'],['guidance']
Usability,"Hi, thanks for your interest in scanpy!. I’ll try to comment on your observations here with your code example:. ```; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet.; → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test).; ```; def my_scale_function(X, clip=False):; # need to make a copy of X; Y = X.copy(); mean, var = mean_var(Y, axis=0); Y -= mean; std = np.sqrt(var); #std[std == 0] = 1; Y /= std; if clip:; Y = np.clip(X, -10, 10); return np.matrix(Y); ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```; ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""); print(np.allclose(adata.X, mtx_rescaled)); ```. ```; Do a numpy check for closeness of floats:; False; ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that.; ```; adata.X.var(0); ```. ```; array([0.9996213 , 0.97964925, 0.29805112, ..., ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273:838,simpl,simply,838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273,2,['simpl'],['simply']
Usability,"Hi,. I am working on a project with a labmate and we are using the same dataset. We have found that, when running the same pipeline on the same adata the neighbors / bbknn + UMAP + leiden results, even with the same seed, the clustering solution and UMAP are considerably different. This renders the analysis _unreproducible_ and makes the downstream analysis far more difficult to do, since I have to map my clustering solutions and UMAP plots with hers using markers, and it is quite impractical. We have the same versions of scanpy, leiden, umap, and bbknn on the two computers:. - `scanpy==1.4.5.post2`; - `umap-learn==0.3.10`; - `leidenalg==0.7.0`; - `bbknn==1.3.6`. To try to reproduce the issue, we have created a random matrix with the same seed (10), and create one annData with `sc.pp.neighbours`, and another one with `bbknn`. We have made the adatas to have two batches, so that we can use bbknn. ```; seed = 10; np.random.seed(seed); a = np.random.rand(100, 100); b = np.random.rand(100, 100); print(np.sum(a), np.sum(b)). adata = sc.AnnData.concatenate(sc.AnnData(X=a), sc.AnnData(X=b), batch_categories=['a', 'b']); sc.tl.pca(adata); sce.pp.bbknn(adata, metric='angular'); sc.tl.umap(adata, random_state=seed); sc.tl.leiden(adata, resolution=0.5, random_state=seed); sc.pl.umap(adata, color=['batch', 'leiden'], alpha=0.3); print(adata.uns['neighbors']['connectivities'].sum()). adata_neigh = adata.copy(); sc.pp.neighbors(adata_neigh, metric='cosine', random_state=seed); sc.tl.umap(adata_neigh, random_state=seed); sc.tl.leiden(adata_neigh, resolution=0.6, random_state=seed); sc.pl.umap(adata_neigh, color=['batch', 'leiden'], alpha=0.3); print(adata_neigh.uns['neighbors']['connectivities'].sum()); ```. Our matrices are the same (the sums are 4918.370372081173 and 5005.088472351332), so the random generation works, but then the UMAPs and clustering solutions are different. For the adata run with `sc.pp.neighbors` (left are batches and right are `leiden` cluster labels):; Mine;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1009:616,learn,learn,616,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009,1,['learn'],['learn']
Usability,"Hi,. I have modified version 1.4, but i think git only allow latest version to; fork. Is there any other simple way, so that i can share my code for Scanpy; version 1.4. Thanks,; Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc; > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>); > reverts a lot of changes we made since.; >; > i assume you just copied all your code over the current master branch, and; > not the version of the master branch as it was when you made the changes.; >; > you need to find the version of scanpy that you downloaded before you made; > your changes and modify that one to have just the changes you want to; > commit. otherwise we have no idea what your actual changes are.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492076397:105,simpl,simple,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492076397,2,['simpl'],['simple']
Usability,"Hi,. I noticed that Scanpy doesn't have a ready function for filtering cells with a high percentage of reads mapping to genes in the mitochondrial genome. Is there still some easy way to do this? Apparently this type of cells can be bad. _In the absence of spike-in transcripts, the proportion of reads mapped to genes in the mitochondrial genome can also be used. High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane._. https://www.bioconductor.org/help/workflows/simpleSingleCell/#examining-gene-level-metrics",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/122:718,simpl,simpleSingleCell,718,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122,1,['simpl'],['simpleSingleCell']
Usability,"Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````; # get subset of cells that express GENE1; gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work; adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works!; adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1; ````; Would you know what the recommended way of doing this is and why is my first solution not working?. I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/612:708,feedback,feedback,708,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612,2,"['feedback', 'learn']","['feedback', 'learning']"
Usability,"Hi,. I wonder how one should use the `covariates` argument of combat. I cannot find an example or more guidance about it. Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/780:103,guid,guidance,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780,1,['guid'],['guidance']
Usability,"Hi,. If you solely `pip install scanpy` it will use the latest versions of both, Scanpy and umap-learn. These are certainly compatible. Scanpy 1.7.2 not being perfectly compatible with the latest umap-learn package is an artifact of us not pinning the dependencies too hard and being more on the lenient side. I'd suggest to simply use the latest versions of both packages and you should not run into any problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568:97,learn,learn,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568,6,"['learn', 'simpl']","['learn', 'simply']"
Usability,"Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872:166,guid,guide,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872,1,['guid'],['guide']
Usability,"Hi,. May be I do not understand this clearly, but does the function `sc.pp.normalize_per_cell` not supposed normalize the counts? However when I run this function, I see that the values do not change at all. Am I doing something wrong here?. raw_data.X.toarray().sum(axis=1); ; Out[18]: array([ 23037., 18883., 20755., ..., 14785., 20996., 7604.], dtype=float32). normed_data = sc.pp.normalize_per_cell(raw_data, copy=True); normed_data.X.toarray().sum(axis=1). Out[19]: array([ 23037., 18883., 20755., ..., 14785., 20996., 7604.], dtype=float32). Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/47:37,clear,clearly,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47,1,['clear'],['clearly']
Usability,"Hi,. Thanks @carversh for opening the discussion!. Trying to get things a bit into order here, as at the moment some wrong impressions are around I think:. **1. Incorrect comparisons done here**; To my current knowledge,; - `sc.pp.highly_variable_genes(…, flavor=“seurat”)` mimics `FindVariableFeatures(…, method=“mean.var.plot”)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(…, flavor=“seurat_v3”)` mimics `FindVariableFeatures(…, method=“vst”)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:949,guid,guidance,949,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,2,['guid'],['guidance']
Usability,"Hi,. UMAP gives me nice embeddings when the cell types are mostly continuous. But I couldn't get it to work well on a dataset with more discrete cell types. When I use smaller min_dist to separate the clusters better, the clusters tend to become very tiny and distant. When I set larger min_dist to make the clusters occupy more space, the spatial separation between coarse clusters is gone. Adjusting other parameters such as n_neighbors, spread, gamma doesn't give me good results either. . This issue is somewhat similar to the first two UMAP plots in this notebook before doing special initialization: https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. `min_dist=0.2, n_neighbor=15`: Clear separation between coarse clusters, but clusters become tiny with too much white space. The clusters on the left seem to be repelling those on the right. ; <img src=https://user-images.githubusercontent.com/5046690/41402335-46cf92c4-6f7f-11e8-9b72-48d23d553356.png width=50%>. `min_dist=0.4`: The coarse-level clusters on the right are intermingled (the continuity between cluster 4 and 6 is now broken); <img src=https://user-images.githubusercontent.com/5046690/41402348-57054968-6f7f-11e8-8e24-696ad4afb243.png width=50%>. `min_dist=0.6`: Cells are distributed more smoothly, but the separation between coarse clusters are much less obvious. ; <img src=https://user-images.githubusercontent.com/5046690/41402394-7252b00c-6f7f-11e8-9ffb-2d09a03fc131.png width=50%>. t-SNE in this case gives me more evenly distributed clusters; <img src=https://user-images.githubusercontent.com/5046690/41403178-8257e722-6f81-11e8-9b64-be274e82eadc.png width=50%>. Is there a way to make the UMAP look more intuitive in this kind of situations? Thanks!. (I changed the original question as I feel that this one is more important.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174:707,Clear,Clear,707,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174,2,"['Clear', 'intuit']","['Clear', 'intuitive']"
Usability,"Hi,. Using Seurat, in their variable gene function I've had some success using the `equal_frequency` option, where each bin contains an equal number of genes. Would it possible to implement this option in scanpy? . If you'd like I could submit a PR to implement this feature. I think it could be as simple as using `pd.qcut` instead of `pd.cut` or you could use a similar style as in the `cell_ranger` flavor with `pd.cut(df['mean'], np.r_[-np.inf,; np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])`. I don't know how useful it would be, but I could also add the option to have more bins in the `cell_ranger` flavor by replacing `np.arange(10,105,5)` with `np.linspace(10, 100, n_bins - 1)`. Best,; David",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/415:299,simpl,simple,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415,1,['simpl'],['simple']
Usability,"Hi,; I seem to be running into a problem where UMAP projections for my subsetted adata appear distorted, but only for certain clusters. I'm not sure if this is actually a bug or if the function is behaving properly. Oddly, some subsets appear 'undistorted' such that their UMAP projections are identical to that of the original unsubsetted adata. However, other subsets appear distorted compared to the original unsubsetted adata, such that their aspect ratios seem to have changed slightly. Please see below. Is this a bug or am I not passing the correct options? Any guidance would be greatly appreciated, and my apologies if this belongs in the community thread. ## Distorted example ##; `sc.pl.umap(adata, color='louvain_r0.8_sub1', palette=sc.pl.palettes.vega_20_scanpy, groups=['2','3','4','5','6','7','8','9'])`; ![image](https://user-images.githubusercontent.com/56206488/78223316-49c34980-748c-11ea-8f96-171c39444d87.png); `ec = adata[adata.obs['louvain_r0.8_sub1'].isin(['2','3','4','5','6','7','8','9']),:].copy()`; `sc.pl.umap(ec, color='louvain_r0.8_sub1', palette=sc.pl.palettes.vega_20_scanpy)`; ![image](https://user-images.githubusercontent.com/56206488/78223415-711a1680-748c-11ea-8a02-b02c5701d911.png). ## Undistorted example ##; `sc.pl.umap(adata, color='louvain_r0.8_sub1', palette=sc.pl.palettes.vega_20_scanpy, groups=['11'])`; ![image](https://user-images.githubusercontent.com/56206488/78223506-9a3aa700-748c-11ea-9608-f4fa8ba4e34c.png); `tc = adata[adata.obs['louvain_r0.8_sub1'].isin(['11']),:].copy()`; `sc.pl.umap(tc, color='louvain_r0.8_sub1', palette=sc.pl.palettes.vega_20_scanpy)`; ![image](https://user-images.githubusercontent.com/56206488/78223568-b9d1cf80-748c-11ea-93f8-0d0e9138f0bf.png). #### Versions:; scanpy==1.4.4.post1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.3.0 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. Best,; Seth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1141:569,guid,guidance,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141,2,"['guid', 'learn']","['guidance', 'learn']"
Usability,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/208:780,learn,learn,780,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208,1,['learn'],['learn']
Usability,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/645:1151,learn,learn,1151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645,1,['learn'],['learn']
Usability,"Hi,; I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:; ```py; import numpy as np; import pandas as pd; import matplotlib.pyplot as pl; from matplotlib import rcParams; import scanpy as sc; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(); ```. ```; /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>; warnings.warn(f""Found an util with public name: {obj}""); /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>; warnings.warn(f""Found an util with public name: {obj}""); /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>; warnings.warn(f""Found an util with public name: {obj}""); /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>; warnings.warn(f""Found an util with public name: {obj}""); /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>; warnings.warn(f""Found an util with public name: {obj}""); ```. ```; scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/840:1722,learn,learn,1722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840,1,['learn'],['learn']
Usability,"Hi,; We don't have a simple function for this atm. But you could check out the dotplot, maybe that already does what you'd like. I'm not sure about Seurat, as I predominantly use scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-572642526:21,simpl,simple,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-572642526,2,['simpl'],['simple']
Usability,"Hi,; Your question is actually quite difficult to answer. It depends on how you define cell identity. For example, depending on how you set the resolution parameter for your clustering you can get a very different number of clusters. As clusters often have super- and sub-structure, you cannot easily say when a sub- or superstructure is not meaningful (e.g., T-cells vs CD4+ and CD8+ T-cells). Unfortunately many clustering techniques do not incorporate an assessment of uncertainty to tell you when you are fitting noise. And even when they do, this is based on a model of what a cell cluster should look like, which does not have to conform to the biological reality. The heuristic that tends to be used is that if you can biologically interpret your clusters based on marker genes and other signatures, then they are meaningful. That does however not mean that sub- or superstructures involving these clusters are not also meaningful. Sorry, I can't give a clearer answer than that. In terms of assessing differences between clusters, this is a somewhat related problem. An approach that you could use is to look at the distribution of Euclidean distances in gene- or PCA-space. However, that comes with the caveat that Euclidean distance does not take into account the biological manifold on which the cells lie (and on which distances will be more informative). You can try to use pseudotime as a measure for distance over this manifold, however that would require you to have sampled enough of the manifold to fit it in your data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874:961,clear,clearer,961,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874,2,['clear'],['clearer']
Usability,Hi. I have written a guide to interacting with Scanpy from R using the **{reticulate}** package which you can view at https://theislab.github.io/scanpy-in-R/. This PR just adds a link to the guide to the tutorials page in the documentation.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1186:21,guid,guide,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1186,2,['guid'],['guide']
Usability,"Hi. This is a small function I have written in response to the issue #490. It takes a list of genes as an input and outputs a list cells that express (>0) those genes at the same time. The list contains values `True` and `False` and so I found it easiest to visualise by adding `groups=[True]` parameter to `plotting` functions. Then the selected cells are coloured and on top and the rest are grey in the background. However, that adds an `NA` category to the plots legend, which I'm not sure how to remove. . As it was suggested in the issue, I include an option to impute the expression values with MAGIC. It does make the results more accurate. . As evident by the the original issue and my experience I think a lot of people may find the function useful. It presents what many people would call co-expression, although I refrained from using this name, as it could suggest something more biologically significant than what this simple function really does.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1657:933,simpl,simple,933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1657,1,['simpl'],['simple']
Usability,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/760:339,learn,learn,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760,1,['learn'],['learn']
Usability,"Hi; Thanks for the brilliant tool! And my poblem is when I use sc.pl.rank_genes_groups_violin() function, the y axis limits of the output seems impalpable.Here's my output:; ![image](https://user-images.githubusercontent.com/65101587/112634253-3a50b500-8def-11eb-84dd-28591804266b.png); Can I modify the y axis limits? I'm sorry I haven't find the parameters yet.; And when I try to use `use_raw=False`, I got error:; `ValueError: Data must be 1-dimensional`; `ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series`; And my code:; `sc.tl.rank_genes_groups(merge_data, 'sampleID', groups=['WT_BM'], reference='KO_BM', method='wilcoxon',corr_method='bonferroni')`; and my version:; `scanpy==1.7.1 anndata==0.7.5 umap==0.4.6 numpy==1.19.1 scipy==1.5.4 pandas==1.2.0 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3`. Thank you. Hope for you answer!; Best,; Ariel.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1766:818,learn,learn,818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1766,1,['learn'],['learn']
Usability,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:96,simpl,simply,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,2,['simpl'],['simply']
Usability,"Hi，here is a bug in the funtion named ""scanpy.tl.rank_genes_groups"". I tried to filter the genes starting with ""HLA"", then I used ""scanpy.tl.rank_genes_groups"" to check the marker genes in each group. However, the genes starting with ""HLA"" still existed. ### Minimal code sample (that we can copy&paste without having any data). ```python; no_HLA_genes =~adata.var_names.str.startswith(('HLA')); adata = adata[:, no_HLA_genes].copy(); print(adata.var_names[adata.var_names.str.startswith(('HLA'))]); ```; output: Index([], dtype='object'). ```python; sc.tl.rank_genes_groups(adata , 'leiden_r2', method='wilcoxon',n_genes=-1); pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(20); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges']}).head(20); ```; ![image](https://user-images.githubusercontent.com/53402047/96684056-8feee480-13ad-11eb-9aef-00858ce3394e.png). #### Versions. <details>; scanpy==1.5.1 anndata==0.7.1 umap==0.4.6 numpy==1.17.3 scipy==1.5.2 pandas==0.23.4 scikit-learn==0.23.2 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1461:1132,learn,learn,1132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1461,1,['learn'],['learn']
Usability,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment?. ```; anndata==0.9.1; asttokens==2.2.1; backcall==0.2.0; contourpy==1.1.0; cycler==0.11.0; decorator==5.1.1; executing==1.2.0; fonttools==4.40.0; h5py==3.9.0; igraph==0.10.4; ipython==8.14.0; jedi==0.18.2; joblib==1.2.0; kiwisolver==1.4.4; llvmlite==0.40.1; louvain==0.8.0; matplotlib==3.7.1; matplotlib-inline==0.1.6; natsort==8.4.0; networkx==3.1; numba==0.57.1; numpy==1.24.3; packaging==23.1; pandas==2.0.2; parso==0.8.3; patsy==0.5.3; pexpect==4.8.0; pickleshare==0.7.5; Pillow==9.5.0; prompt-toolkit==3.0.38; ptyprocess==0.7.0; pure-eval==0.2.2; Pygments==2.15.1; pynndescent==0.5.10; pyparsing==3.1.0; python-dateutil==2.8.2; python-igraph==0.10.4; pytz==2023.3; scanpy==1.9.3; scikit-learn==1.2.2; scipy==1.11.0; seaborn==0.12.2; session-info==1.0.0; six==1.16.0; stack-data==0.6.2; statsmodels==0.14.0; stdlib-list==0.9.0; texttable==1.6.7; threadpoolctl==3.1.0; tqdm==4.65.0; traitlets==5.9.0; tzdata==2023.3; umap-learn==0.5.3; wcwidth==0.2.6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1607052906:902,learn,learn,902,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1607052906,4,['learn'],['learn']
Usability,"Hm, I researched a bit more. psutil doesn't seem to cause problems and also, this has not been a problem within Scanpy for any user up to now. If you start a terminal with `python` and type; ```; import psutil; psutil.process_iter(); ```; does this throw an error? I'd really like to know what's going on. If you want a quick fix; you can simply comment out line 773 in your file `/ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py`; this should cause no problem for your applications.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821:339,simpl,simply,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821,2,['simpl'],['simply']
Usability,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,2,['learn'],['learn']
Usability,"Hm, how about simply ranking things yourself, like ; ```; sort_idcs = np.argsort(adata.var['PCs'][:, 0]); genes_ranked_by_loading_in_PC1 = adata.var_names[sort_idcs]; ```; This is what the plotting functions do internally.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/338#issuecomment-435641437:14,simpl,simply,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/338#issuecomment-435641437,2,['simpl'],['simply']
Usability,"Hm, it was decided to suspend this pr earlier.; There is an analogous pr in scikit-learn, but i'm not sure it will got forward.; I'm not sure what to do with this pr...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-573320770:83,learn,learn,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-573320770,2,['learn'],['learn']
Usability,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-457868949:441,simpl,simple,441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457868949,2,['simpl'],['simple']
Usability,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/408#issuecomment-450882561:4,simpl,simply,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408#issuecomment-450882561,2,['simpl'],['simply']
Usability,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are; ```; scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216:278,learn,learn,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216,2,['learn'],['learn']
Usability,"Hm, that’s probably the doing of [`sklearn.utils.check_random_state`][]:. https://github.com/scverse/scanpy/blob/c3cfa74b1316d780568411175316cd9f139efb22/scanpy/_utils/__init__.py#L71-L72. @ilan-gold seems like using the legacy RandomState class wasn’t ideal. The new [`Generator`](https://numpy.org/doc/stable/reference/random/generator.html) actually emits 64 bit ints by default and I bet has other advantages. [`sklearn.utils.check_random_state`]: https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085057657:467,learn,learn,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085057657,2,['learn'],['learn']
Usability,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think?. But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run?. The latest version of the tutorial says; ```; scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495:365,learn,learn,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495,2,['learn'],['learn']
Usability,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/231#issuecomment-412140748:128,simpl,simply,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-412140748,2,['simpl'],['simply']
Usability,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:34,simpl,simpler,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463,2,['simpl'],['simpler']
Usability,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:527,clear,clearer,527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116,2,['clear'],['clearer']
Usability,"Hmm, seems like `PCA` with `svd_solver='arpack'` works differently in sklearn 1.5 and flips the coordinates in some tests:. https://github.com/scikit-learn/scikit-learn/issues/28826. E.g. this used to work, now it doesn’t. ```py; from __future__ import annotations. import numpy as np; from sklearn.decomposition import PCA. data = np.asarray([[-1, 2, 0], [3, 4, 0], [1, 2, 0]]).T; expected = np.array(; [[-1.579575e-15, 1.490712], [-2.44949, -0.745356], [2.44949, -0.745356]],; dtype=np.float32,; ). pca = PCA(n_components=2, svd_solver=""arpack"", random_state=np.random.RandomState(0)); transformed = pca.fit_transform(data).astype(np.float32); np.testing.assert_almost_equal(transformed, expected, decimal=5); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3047#issuecomment-2098389353:150,learn,learn,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3047#issuecomment-2098389353,4,['learn'],['learn']
Usability,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830832355:550,clear,clearly,550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830832355,2,['clear'],['clearly']
Usability,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678:208,simpl,simplest,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678,2,['simpl'],['simplest']
Usability,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:671,learn,learn,671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350,2,['learn'],['learn']
Usability,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:498,clear,clear,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542,2,['clear'],['clear']
Usability,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:140,clear,clear,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['clear'],['clear']
Usability,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-483216799:228,simpl,simpler,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-483216799,2,['simpl'],['simpler']
Usability,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-479587681:112,clear,clear,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-479587681,2,['clear'],['clear']
Usability,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358:852,Intuit,Intuitively,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358,1,['Intuit'],['Intuitively']
Usability,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:234,learn,learn,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309,4,['learn'],['learn']
Usability,I am facing the same issue. I have recently updated my scanpy to the latest version.; I think that it was working before that. Here is rest of my software versions; scanpy==1.4.6 anndata==0.7.1 umap==0.3.9 numpy==1.17.4 scipy==1.3.1 pandas==0.25.3 scikit-learn==0.22 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992:255,learn,learn,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992,2,['learn'],['learn']
Usability,"I am finding that my analysis is not perfectly reproducible across different computational platforms. I thought I was going crazy but I have since reproduced this finding using the minimal 3000 PBMC dataset clustering example. Essentially I run the same code either on a virtual machine with 8 CPUs or one with 16 CPUs and I get non-identical PCA results. It doesn't seem to matter if I use the arpack or the randomized solver even though using the randomized solver gives the warning:. `Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future.`. I'd like to just attach the jupyter notebook but it won't seem to let me do that so I'm copying the code below.; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; # First run on a machine with 8 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata). # Then run on a machine with 16 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187:505,learn,learn,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187,1,['learn'],['learn']
Usability,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:502,learn,learn,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096,2,['learn'],['learn']
Usability,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:304,learn,learn,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994,4,['learn'],['learn']
Usability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:558,learn,learn,558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,2,['learn'],['learn']
Usability,"I assume I'm missing something here, but when I try a simple example of plotting a gene dispersion I get two plots 'normalized' and 'not normalized' version in Jupyter, but when I use the save argument to sc.pl.filter_genes_dispersion() I get an image with only one of these. Screenshot attached. Just in case, I tried also passing the multi_panel argument but that caused an error. . Also, is it no possible to specify the path where the files should be stored when using the save arguments to the plotting methods? I want to point to a directory where it should place them, but it seems ""./figures/"" is hard-coded and you can only modify the end of that. Thanks. The attached screenshot shows the dual image within Jupyter, but only the single plot which appears in the PNG file exported. ![screenshot from 2018-01-30 12-34-56](https://user-images.githubusercontent.com/330899/35584410-945d7bb6-05ba-11e8-89fc-14f615a9c6a6.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/73:54,simpl,simple,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/73,1,['simpl'],['simple']
Usability,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555:317,learn,learn,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555,2,['learn'],['learn']
Usability,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:866,clear,clear,866,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922,2,['clear'],['clear']
Usability,"I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy ; Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package networkx conflicts for:; scanpy -> networkx; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts for:; scanpy -> natsort; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package pytables conflicts for:; scanpy -> pytables; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package tqdm conflicts for:; scanpy -> tqdm; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package patsy conflicts for:; scanpy -> patsy; Package louvain conflicts for:; scanpy -> louvain; Package python conflicts for:; scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990:904,learn,learn,904,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990,4,['learn'],['learn']
Usability,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:89,user experience,user experience,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,2,['user experience'],['user experience']
Usability,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-460071018:30,simpl,simply,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-460071018,2,['simpl'],['simply']
Usability,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:894,learn,learn,894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671,2,['learn'],['learn']
Usability,"I could not reproduce this bug, I am using . `scanpy==1.5.1 anndata==0.7.4 umap==0.3.10 numpy==1.19.2 scipy==1.5.2 pandas==1.1.2 scikit-learn==0.23.2 statsmodels==0.12.0 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.7.0`. @giovp maybe be a good idea to close this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1300#issuecomment-718771033:136,learn,learn,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1300#issuecomment-718771033,2,['learn'],['learn']
Usability,"I did a time comparison on the scanpy function to compute mean and variance and the sklearn function that does the same. Here some results with a very large matrix (84.7 million values,):. ```python; adata.X; ```; > <8847360x58051 sparse matrix of type '<class 'numpy.float32'>'; > 	with 84702299 stored elements in Compressed Sparse Row format>; > . **Using current implementation**:; ```python; import scanpy.preprocessing.simple as simple; %timeit simple._get_mean_var(adata.X); ```; 2.3 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). **Using sklearn sparse functions**:; ```python; import sklearn.utils.sparsefuncs as sparsefuncs; def unbiased_estimator(X):; mean, var =sparsefuncs.mean_variance_axis(X, 0); # enforce R convention (unbiased estimator) for variance; var *= (X.shape[0]/(X.shape[0]-1)); return mean, var; %timeit unbiased_estimator(adata.X); ```; > 141 ms ± 6.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each); >. The results returned by both methods are different only in the precision:. ```python; # the variance of both methods; simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1]; ```; > (array([0.0073651 , 0.00061313, 0.005346 , ..., 0.05514137, 0.01749513,; > 0.05664281], dtype=float32),; > array([0.0073651 , 0.00061313, 0.00534599, ..., 0.05514137, 0.01749513,; > 0.05664282], dtype=float32)); > . ```python; np.allclose(simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1], rtol=1e-4,); ```; > True. This comparison is for the variance, but for the mean, the same results are obtained. I suggest to replace the `_get_mean_var` function for the sklearn function. **Note** the sklearn function only works with a sparse matrix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/163:425,simpl,simple,425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163,5,['simpl'],['simple']
Usability,"I did figure out what's going on. I worked on a view of an AnnData object, where the original AnnData object did not have the X_pca field and it could not be added only in the view. I updated to the latest scanpy and anndata version; > scanpy==1.4+18.gaabe446 anndata==0.6.18+3.g3e93ed7 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . this is my AnnData object:; ```; adata; print(adata); ```; > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I now filter my AnnData object for highly variable genes I only got a ""View"" of my AnnData object; ```; adata2 = adata[:, adata.var['highly_variable']]; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:336,learn,learn,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,2,['learn'],['learn']
Usability,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:203,simpl,simply,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,2,['simpl'],['simply']
Usability,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:309,learn,learn,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500,4,['learn'],['learn']
Usability,"I don't know how they do it Seurat, but I'd simply do; ```; filenames = ['name0.h5', 'name1.h5', 'name2.h5']; adatas = [sc.read_10x_h5(filename) for filename in filenames]; adata = adatas[0].concatenate(adatas[1:]); ```; Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-424547172:44,simpl,simply,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-424547172,2,['simpl'],['simply']
Usability,I don't think that `legend_loc` would be the correct parameter. You are not even interacting with it during the example that @LisaSikkema posted (thanks for that btw!). I see two options:; 1. Adding a general statement to the `color` argument stating that reordering the categorical column to color by can customize the legend order; 2. Adding your use-case to the example section in the pl.umap documentation. Leaning more towards the second option. What do you think? Would be great if you could attempt to submit a PR then. Happy to provide some guidance if required.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257189686:549,guid,guidance,549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257189686,2,['guid'],['guidance']
Usability,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track; but ist not because it does not understand coordinates. It simple groups; the cells by the given groupby condition and then plots the value of each; gene in a separate track. The y value is the gene expression (or in your; case the ATAC-seq value). The x coordinate, simple puts all cells one after; the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,; >; > Thanks for this amazing package.; >; > I have been playing with scanpy on scATACSeq data generated from 10x. And; > in comparison to the cellranger analysis, I think analysis scanpy does; > pretty descent job and adds more possibilities. I would like to displays; > some peaks that are highly present if some clusters using the genome; > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is; > it possible to the same thing but with the peak averaged for all cells; > within the same cluster?; >; > import matplotlib.pyplot as plt; > genes =['chr15:101708546_101718131','chr11:117961932_117970696',; > 'chr19:5821847_5852441','chr15:101422873_101429606',; > 'chr17:39842811_39849028','chr13:6108971_6109684']; > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]); >; > [image: atacseq]; > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/447>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/447#issuecomment-457810196:180,simpl,simple,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447#issuecomment-457810196,4,['simpl'],['simple']
Usability,"I encounter the same error as well when i tried sc.pp.normalize_total(adata, target_sum=5e4). . Environment:; scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. [conda list]; https://github.com/phamidko/codesnippets/blob/master/scanpy-conda-list.txt; [ipynb]; https://github.com/phamidko/codesnippets/blob/master/Tissue-Tcell-activation.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477:199,learn,learn,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477,2,['learn'],['learn']
Usability,"I face the same problem. However I used latest version ; </scanpy==1.4.6 anndata==0.7.4 umap==0.5.1 numpy==1.20.3 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 , bbknn : 1.5.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-860196125:148,learn,learn,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-860196125,2,['learn'],['learn']
Usability,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:429,simpl,simply,429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033,2,['simpl'],['simply']
Usability,"I finished the edits to conform with #1667! Now looking forward to your thoughts :). One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. This causes many changes that are unrelated to the code I wrote. That's why I disabled pre-commit again (so you don't have to go over all these changes), and tried to follow the style guide manually as good as possible. Hope that is ok for now.. Do you have any advice how to handle that? Should I just do one ""style"" commit (that fixes all these issues throughout the files I work on here) once you've checked the new parts of the code I wrote? Or should the style be ok in all ""old"" parts of the code, implying that I set up pre-commit wrong? I'm new to it so that could very well be the case as well..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-791524373:496,guid,guide,496,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-791524373,2,['guid'],['guide']
Usability,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-488610378:274,clear,clearly,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-488610378,2,['clear'],['clearly']
Usability,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/275:283,learn,learn,283,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275,1,['learn'],['learn']
Usability,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:315,learn,learn,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116,2,['learn'],['learn']
Usability,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073:252,intuit,intuitive,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073,2,['intuit'],['intuitive']
Usability,I got the same error with scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.2 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1; But I am so glad to find answer here and thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485:130,learn,learn,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485,2,['learn'],['learn']
Usability,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494730898:179,simpl,simply,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494730898,2,['simpl'],['simply']
Usability,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:285,learn,learning,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342,8,['learn'],"['learn', 'learned', 'learning']"
Usability,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409996535:453,learn,learn,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409996535,2,['learn'],['learn']
Usability,"I have a short script which reads a tab file and writes h5 using scanpy. I've found that unless I provide a full path to the write() function or at least a relative one via ""./foo.h5"" it fails. Simplified version:. ```py; adata = sc.read(args.input_file, ext='txt', first_column_names=True).transpose(); adata.write('./test.h5') # this works; adata.write('test2.h5') # this fails; ```. Here's the stack:. ```pytb; WARNING: This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; Traceback (most recent call last):; File ""./convert_gear_group_single_cell_to_hdf5.py"", line 47, in <module>; main(); File ""./convert_gear_group_single_cell_to_hdf5.py"", line 43, in main; adata.write('test2.h5'); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1471, in write; compression=compression, compression_opts=compression_opts); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1513, in _write_h5ad; os.makedirs(os.path.dirname(filename)); File ""/usr/lib/python3.5/os.py"", line 241, in makedirs; mkdir(name, mode); FileNotFoundError: [Errno 2] No such file or directory: ''; _____________________________. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/66:194,Simpl,Simplified,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/66,1,['Simpl'],['Simplified']
Usability,"I have pytorch and tensorflow alongside scanpy in several conda envs. I would close this for now, also because it's not clear what ""probably it does not finish"" means. ; Feel free to reopen it if problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1286#issuecomment-702367885:120,clear,clear,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-702367885,2,['clear'],['clear']
Usability,"I have to admit that I'm still not an expert in these forums. I'm happy if we go with https://gitter.im/scanpyhelp as a solution. I also know discourse is super popular among many people and I'm happy if we go with it if all three of you, @outlace, @ivirshup and @flying-sheep, think this could be a better place. I simply can't judge myself as I haven't used either of them. Most importantly, let's put what you guys choose on the top of the webpage and properly announce it; it would be terrible to have several of these chat rooms; one on gitter, one discourse, etc. with potentially even different names `scanpyhelp`, `scanpy` etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-509026804:316,simpl,simply,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-509026804,2,['simpl'],['simply']
Usability,"I have two suggestions/questions about dot/matrix plots:. 1- If `standard_scale='var'` is given, we can write `Mean expression\nin group\n(min-max scaled)` on the color legend to be more accurate about what is being displayed. 2- People/journals usually expect gene names to be written with italicized characters (don't ask why, see https://en.wikipedia.org/wiki/Gene_nomenclature). So I was wondering if we can simply do that in the plots. One important thing to consider is whether this is organism-dependent. I guess it's not but would be cool to discuss.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913:412,simpl,simply,412,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913,1,['simpl'],['simply']
Usability,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965:77,simpl,simplest,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965,2,['simpl'],['simplest']
Usability,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory.; Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499:242,simpl,simple,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499,2,['simpl'],['simple']
Usability,I hope that solves all workflow woes! @ivirshup?. - Users still only need `pip` and can do `pip install scanpy[extras]`; - Installation from source happens via `pip install .[extras]` or `flit install --deps`/`--extras`; - Dev mode install is nonstandard and therefore happens by simply `ln -s scanpy path/to/env/site-packages/` or flit:. ![grafik](https://user-images.githubusercontent.com/291575/90508913-c8c5cf80-e158-11ea-802a-2e0e47578bd6.png). PS: we could also mention `--pth-file` for the 3 windows users who refuse to update to win10 and therefore can’t create symlinks.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377:280,simpl,simply,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377,1,['simpl'],['simply']
Usability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1223,learn,learn,1223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,4,['learn'],['learn']
Usability,I just add the versions I used:; ```python; sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/536#issuecomment-474301705:150,learn,learn,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536#issuecomment-474301705,2,['learn'],['learn']
Usability,"I just encountered this as well. It seems like it's not running UMAP at all unless I give it a `maxiter` parameter. Not clear why that is, but passing an argument there worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1261103537:120,clear,clear,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1261103537,2,['clear'],['clear']
Usability,"I just found a small mistake in the documentation of `scanorama_integrate`:; **kwargs are passed to assemble, not integrate. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2647:196,guid,guidelines,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2647,2,['guid'],"['guide', 'guidelines']"
Usability,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:48,Learn,Learn,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214,2,['Learn'],['Learn']
Usability,"I just tried again with the reprex above and it works for me; ```python; import scanpy as sc; adata = sc.datasets.paul15(); sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.dpt(adata); sc.tl.paga(adata, groups='paul15_clusters'); sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']); ```; ![image](https://user-images.githubusercontent.com/25887487/107772027-f1133d00-6d3b-11eb-8866-f514acea297a.png). I'm on a separate branch but it's on par with current master; ```; scanpy==1.7.0rc2.dev25+g56303580.d20210212 anndata==0.7.4 umap==0.4.6 numpy==1.19.4 scipy==1.5.2 pandas==1.1.4 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-778186427:604,learn,learn,604,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778186427,2,['learn'],['learn']
Usability,"I like @VolkerBergen's suggestion. On the other Hand, @LuckyMD uses the scran estimate of size factors for normalization. Processing something like that would need a `counts_per_cell` argument (which I'd call `normalization_factor` today, I guess). If one needs to manually compute the `counts_per_cell` before calling the function, then the whole convenience and purpose of the function is gone, though. So, I'd say the convenience of an argument `by_initial` absolutely outweighs the flexibility of an argument `normalization_factor` (`size_factor`). In case we have another size factor estimator in Scanpy, it will definitely not occur in `normalize_total` or `normalize_quantile` (the names already suggest that this is simple normalization) but in a new function `normalize_...`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-460612249:724,simpl,simple,724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-460612249,2,['simpl'],['simple']
Usability,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:34,simpl,simple,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884,4,"['pause', 'simpl']","['pause', 'simple']"
Usability,"I like the idea and I see your point, however I wonder how intuitive the concept of a style is for beginners. Also you'd have to know exactly what you are looking to plot during your analysis to set up a style for all plots for the future (e.g. do you need arrows? Will you use a marker gene dotplot?). Context managers definitely look clean (I love that I have my own ^^), but it would make things a bit more difficult for beginners to get an intuitive feel for scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/956#issuecomment-567412942:59,intuit,intuitive,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567412942,4,['intuit'],['intuitive']
Usability,I like this idea a lot. I recently learned about automated linting and blacking of code per commit and have started using it for the single cell open problems project upon the suggestion of @scottgigante.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-753945848:35,learn,learned,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-753945848,2,['learn'],['learned']
Usability,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where.; * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python; from anndata import AnnData; from datetime import datetime; from functools import wraps; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(func):; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; call_start_record = dict(call_id=call_id, called_func=func.__name__); if type(args[0]) is AnnData:; call_start_record[""adata_id""] = id(args[0]); logger.msg(""call"", **call_start_record). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt); if type(output) is AnnData:; call_finish_record[""returned_adata_id""] = id(output); logger.msg(""call_finish"", **call_finish_record, call_id=call_id); return output; return func_wrapper. # Usage. @logged; def foo(adata, x, copy=False):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1); # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo; # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:556,Simpl,Simple,556,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,1,['Simpl'],['Simple']
Usability,"I literally never had problems with test discovery, so idk what to look for. As said: Numpy and pandas have separated their testing utils from their tests. For the time being I want just that, no change to where the tests are. Would you accept a PR that simply moves the test utils into private submodules of `scanpy.testing` and switches the import mode to (future default, drawback-less) `importlib`?. Any change to the test layout can come later or never. I’d like to follow pytest’s recommendation (`/src/scanpy/` and `/tests/`) but this issue is orthogonal to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096566148:254,simpl,simply,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096566148,2,['simpl'],['simply']
Usability,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:313,clear,clearly,313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559,2,['clear'],['clearly']
Usability,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:487,learn,learn,487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,2,['learn'],['learn']
Usability,"I meant apart from the long label at ""right margin"", basically adding a number to each label and display that number ""on data"". Right margin would look like this:; 1. Cell type A; 2. Cell type B; 3. Cell type C. And the UMAP will simply show 1, 2, 3 ... on the plot. If the numbers could be automatically generated when one runs `sc.pl.umap`, and added to the plot, that would be awesome. The issue is adding the text label on the plot make them overlap with each other, but the corresponding numbers would do just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2112#issuecomment-1023417880:230,simpl,simply,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2112#issuecomment-1023417880,2,['simpl'],['simply']
Usability,"I must've mixed up `normalize_total` and `normalize_per_cell`, which I know is deprecated. I know it's clearly stated in the anndata docs regarding the float32 copy issue, but it's really quite confusing!. ```python; In [1]: import scanpy as sc. In [2]: import numpy as np. In [3]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [4]: adata = sc.AnnData(a). In [5]: sc.pp.normalize_total(adata). In [6]: adata.X; Out[6]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [7]: a; Out[7]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [9]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [10]: adata = sc.AnnData(a). In [11]: sc.pp.normalize_per_cell(adata). In [12]: adata.X; Out[12]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [13]: a; Out[13]: ; array([[ 0., 1., 2., 3.],; [ 4., 5., 6., 7.],; [ 8., 9., 10., 11.],; [12., 13., 14., 15.]], dtype=float32); ```. Edit: So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1415#issuecomment-694916916:103,clear,clearly,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415#issuecomment-694916916,2,['clear'],['clearly']
Usability,"I noticed that 10x has released some new spatial gene expression datasets. Could you include them? It should be a simple change in this line [https://github.com/theislab/scanpy/blob/ab9247bdf8b7a3decc34a15b26fec813ea8fba0d/scanpy/datasets/_datasets.py#L323](url). Also, I've encounter errors when using `scanpy.datasets.visium_sge`. It seems that the url is outdated. The link to the datasets is changed to be; `; https://support.10xgenomics.com/spatial-gene-expression/datasets/{version_id}/{sample_id}; `. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1475:114,simpl,simple,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1475,1,['simpl'],['simple']
Usability,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:403,simpl,simply,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,2,['simpl'],['simply']
Usability,I quite like the saving of figures as it means people can use scanpy who otherwise aren't as familiar with data science in python. Calling a function on an axis object or saving the last axis object that was is displayed is not always intuitive to new users.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508#issuecomment-735835548:235,intuit,intuitive,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-735835548,2,['intuit'],['intuitive']
Usability,"I really like this! I was trying to figure out how to easily do this within the current `plot_scatter` framework, but I couldn't figure out how. One thing though... You could add an option `group='all'`, which should be the default. That would be much more user-friendly than the current setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/628#issuecomment-488715711:257,user-friendly,user-friendly,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628#issuecomment-488715711,2,['user-friendly'],['user-friendly']
Usability,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:116,progress bar,progress bars,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054,4,"['progress bar', 'simpl']","['progress bars', 'simply']"
Usability,I renamed pts and pts_rest to `fraction_group` and `fraction_rest`. I'd like to merge this PR if there is no other feedback. We can think about how to provide aggregate statistics somewhere else. Then we can revisit this function and merge this info too.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-678829691:115,feedback,feedback,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-678829691,2,['feedback'],['feedback']
Usability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:74,simpl,simpler,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,2,['simpl'],['simpler']
Usability,I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/537:85,simpl,simplest,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537,1,['simpl'],['simplest']
Usability,"I see! OK, so `.. automodule` [doesn’t by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we don’t have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I don’t understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/408#issuecomment-450887824:545,simpl,simply,545,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408#issuecomment-450887824,2,['simpl'],['simply']
Usability,"I see, [densmap](https://umap-learn.readthedocs.io/en/latest/densmap_demo.html). Hmm, I think that `method='densmap'` and `method_kwds={...}` would be a better API for us (which would then be translated into `densmap=True, densmap_kwds=method_kwds`). This also needs tests and a release note. Also we probably should just remove the umap 0.4 compatibility code, what do you think @ivirshup?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449:30,learn,learn,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449,2,['learn'],['learn']
Usability,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py; def _plot_legend(self, legend_ax, return_ax_dict, normalize): ; self._plot_colorbar(legend_ax, normalize) ; return_ax_dict['color_legend_ax'] = color_legend_ax; ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829:230,simpl,simply,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829,2,['simpl'],['simply']
Usability,"I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1583:2,simpl,simplified,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583,1,['simpl'],['simplified']
Usability,"I simplified this quite a bit. I decided against blocking this on an upstream `anndata` helper for multiple reasons:. 1. we test on older anndata versions that won’t have that parameter immediately; 2. needs some designing, e.g. the API should be able to do “use `ceil(shape[0] / 2)` as chunk size for dim 0, and `-1` (=full size) for dim 1”; 3. it would make no sense for the non-dask array types, should we still add it for them?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2250061054:2,simpl,simplified,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2250061054,2,['simpl'],['simplified']
Usability,I simply meant to do what I described in that issue: https://github.com/theislab/scanpy/issues/271#issuecomment-431634492,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-435734288:2,simpl,simply,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-435734288,2,['simpl'],['simply']
Usability,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584222835:236,simpl,simpler,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584222835,2,['simpl'],['simpler']
Usability,"I subseted the scanpy object to view some subpopulations. But I was amazed that whether conducting deep copy yields different results. Here are the snippets: . #VERSION 1:; import copy; Tcells= copy.copy(adata[adata.obs[""Leiden0.7""].isin([""2"",""3"",""8"",""15""])] ); sc.tl.pca(Tcells, svd_solver='arpack',n_comps=50); sc.pp.neighbors(Tcells, n_neighbors=10, n_pcs=40); sc.tl.umap(Tcells, min_dist=0.2, random_state=42); sc.pl.umap(Tcells,color=[""Leiden0.7""]). #VERSION 2:; import copy; Tcells= adata[adata.obs[""Leiden0.7""].isin([""2"",""3"",""8"",""15""])] ; sc.tl.pca(Tcells, svd_solver='arpack',n_comps=50); sc.pp.neighbors(Tcells, n_neighbors=10, n_pcs=40); sc.tl.umap(Tcells, min_dist=0.2, random_state=42); sc.pl.umap(Tcells,color=[""Leiden0.7""]). The two VERSIONS of snippet just give different umaps (even though I fixed the random seed), and different number of clusters (that's unacceptable, as far as I know, clustering is based on PCs). #### Versions:; scanpy==1.4.5.post1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0. So should I make a deep copy of the scanpy object? It looks like deep copy ruined something in the object.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1239:1046,learn,learn,1046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239,1,['learn'],['learn']
Usability,"I support the idea of tidying up plotting arguments. I think there are mainly two problems. 1) the high number of plotting arguments 2) lack of reusability of plotting ""styles"". . Chaining looks really cool and improves 1). Also, it logically partitions the plotting arguments. However, it doesn't solve 2). In other words, if we plot two figures, we'll need to copy the entire thing, and it'll be very verbose:. ```python; sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). sc.pl.umap(adata2, color='fluffy').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1); ```. One thing that comes to mind for reusability is to store the result of the chain somewhere and, well, reuse it:. ```python; style = sc.pl.styles.scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). # using simple arguments, similar to https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/lmeControl.html; sc.pl.umap(adata, color='clusters', style=style); sc.pl.umap(adata2, color='fluffy', style=style). # using context managers, similar to https://seaborn.pydata.org/tutorial/aesthetics.html#temporarily-setting-figure-style; with style:; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # overriding an existing style object; with style.legend(fontsize=12):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # or use predefined styles (?); with sc.pl.style('malte'):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). ```. WDYT?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810:956,simpl,simple,956,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810,2,['simpl'],['simple']
Usability,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:162,feedback,feedback,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466,2,['feedback'],['feedback']
Usability,"I thing that is not so difficult to achieve this. I submit a PR soon. Fidel Ramírez . > On 26 Mar 2019, at 22:02, Alex Wolf <notifications@github.com> wrote:; > ; > @fidelram, as discussed today, could we adopt pl.rank_genes_groups_dotplot so that it reads this information from .uns['rank_genes_groups']?; > ; > Maybe just a simple switch? Or having arguments color and size be a choice from a selection {pvals, pvals_adj, log2FC, expression, frac-genes-expressed}.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-477015393:326,simpl,simple,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-477015393,2,['simpl'],['simple']
Usability,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674:790,simpl,simply,790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674,2,['simpl'],['simply']
Usability,"I think anndata’s `rename_categories` should accept non-unique values as argument. Then one could simply do things like. ```py; cluster_markers = {; 'CD4 T': {'IL7R'},; 'CD14+\nMonocytes': {'CD14', 'LYZ'},; 'B': {'MS4A1'},; 'CD8 T': {'CD8A'},; 'NK': {'GNLY', 'NKG7'},; 'FCGR3A+\nMonocytes': {'FCGR3A', 'MS4A7'},; 'Dendritic': {'FCER1A', 'CST3'},; 'Mega-\nkaryocytes': {'PPBP'},; }; marker_matches = sc.tl.marker_gene_overlap(adata, cluster_markers); adata.rename_categories('leiden', marker_matches.idxmax()); ```. As it stands, things like the `pbmc3k` tutorial are super flaky because they hardcode things like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-1153213330:98,simpl,simply,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-1153213330,2,['simpl'],['simply']
Usability,I think it is more or less complete.; Here are the tutorials; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-realistic.ipynb; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-simple.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519155566:223,simpl,simple,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519155566,2,['simpl'],['simple']
Usability,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298:741,Simpl,Simple,741,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298,1,['Simpl'],['Simple']
Usability,"I think maybe i found a solution to solve this problem.; Maybe this problem is caused by the version of scikit—misc，when you use pip install --user scikit-misc or pip install scikit-misc，the system will install scikit-misc==0.1.4.; so,i try to install another verion of scikit-misc,you can use install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1.; In addition, this line of command needs to be used when python is greater than or equal to 3.8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497:327,simpl,simple,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497,2,['simpl'],['simple']
Usability,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:166,clear,clear,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,4,['clear'],['clear']
Usability,"I think pbmc68k_reduced was processed something like. ```py; sc.pp.normalize_total(adata, target_sum=1e6); sc.pp.log1p(adata); sc.pp.scale(adata); ```. still no idea what’s in “raw” as it’s clearly not counts …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3051#issuecomment-2107394169:190,clear,clearly,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3051#issuecomment-2107394169,2,['clear'],['clearly']
Usability,"I think that the best is to have separate scatter functions. For the; remaining use cases of scatter a much simpler version can be devised. But; meanwhile, @bebatut <https://github.com/bebatut> why did you try that; combination of parameters? Is this in some tutorial?. On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good; > situation. @bebatut <https://github.com/bebatut> components does only; > make sense if you provide basis as an argument. a list-like color was; > also only meant for that case. both is now deprecated as @fidelram; > <https://github.com/fidelram> wrote a whole new scatter backend; however,; > which for now, misses the x and y parameters...; >; > In any case, pl.scatter should continue to work with the canonical calls; > and also with non-list-like color.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-432533052:108,simpl,simpler,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-432533052,2,['simpl'],['simpler']
Usability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:38,simpl,simple,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,4,"['clear', 'simpl']","['clear', 'simple']"
Usability,"I think the docs for this can be improved, particularly in the ""visualising marker genes"" vignette and in the sc.pl.xxx api documentation. I noticed the following:. (1) The ""use_raw"" parameter documentation for e.g. sc.pl.dotplot does not make it clear that .raw is used by default if present. It seems this issue is addressed upstream but not picked up for the html docs yet. Please compare: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.dotplot.html with https://github.com/theislab/scanpy/blob/9360422823fade22a625f3c7840bef132364027e/scanpy/plotting/_anndata.py#L109. (2) It would be very helpful to provide and link in the docs to the script used to pre-process/create the example datasets such as sc.datasets.pbmc68k_reduced(). . (3) The visualisation vignette (https://scanpy-tutorials.readthedocs.io/en/multiomics/visualizing-marker-genes.html) does not state that normalised log1p data is being used for the example visualisations (and is presumably recommended). By itself the warning against use of scaled data leaves the situation ambiguous. . Because of these issues, and apologies if I missed the explanation, it is currently not straightforward to be sure what data is being plotted in the dot plots shown in this vignette. From inspecting the anndata object, reading the _anndata.py code and reading the 3k vignette I assume that it is the log1p normalised data! . Many thanks, S.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1998:247,clear,clear,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1998,1,['clear'],['clear']
Usability,"I think the problem is the option `sort_order` which is True by default for; numerical data. This changes the ordering of the dots and thus it messes; up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot; > with the following code; >; > import scanpy.api as sc; > import numpy as np; > sc.settings.figdir = ""testdir""; > sc.settings.file_format_figs = ""png""; > sc.logging.print_versions(); >; > With these libraries; > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4; > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Running the following code bit. I use some dummy variable for size.; >; > somedata = sc.datasets.paul15(); > sc.pp.pca(somedata); > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); > sc.tl.leiden(somedata, resolution=0.5, random_state=42); > z = np.abs(somedata.obsm['X_pca'][:,0])**1; > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); >; > I get the following two figure as output; > [image: umapcontinuous_expr]; > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>; > [image: umapgroup_value]; > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>; >; > I would expect to see a similar size allocation/distribution but they are; > very different. I Could not really find a cause for this looking at the; > scatter plot function so it might be somewhere deeper.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/478>, or ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152:671,learn,learn,671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152,2,['learn'],['learn']
Usability,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196:37,guid,guide,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196,6,['guid'],"['guide', 'guides']"
Usability,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:363,simpl,simplexes,363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203,2,['simpl'],['simplexes']
Usability,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:275,learn,learning,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251,2,['learn'],['learning']
Usability,"I think this is getting to a good place for an initial addition. Parts that definitely need expanding include:. * Code guidelines; * These are pretty minimal at the moment.; * Documentation; * Information on restructured text ; * sphinx extensions we use; * More on the structure of a doc-string; * Updated examples (I took these from the existing `CONTRIBUTING.md`). I think these can be expanded at a later date. The main goal here was to make sure there was some base organization for a contributing guide and dev-docs. . I'm probably also not the best person to expand on the documentation, since I still barely understand sphinx :wink:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865:119,guid,guidelines,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865,4,['guid'],"['guide', 'guidelines']"
Usability,"I think this looks good and simple enough. Could you please fix the CI errors?. Also there’s 3 added optional deps: cuml, cudf, and cugraph. I assume they’re all different CUDA packages. Could you add them into an `extra` in setup.py?. The RAPIDS umap branch doesn’t use a metric argument. Does it support metrics other than euclidean?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279:28,simpl,simple,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279,2,['simpl'],['simple']
Usability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:140,clear,clear,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,2,['clear'],['clear']
Usability,I think we should remove the that uses the divergent colormap and keep things simpler.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/512#issuecomment-470914298:78,simpl,simpler,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512#issuecomment-470914298,2,['simpl'],['simpler']
Usability,"I think you misunderstood a bit. 1. There’s three *types* of return sections – prose, tuple, added anndata.obs/var fields.; 2. There’s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. I’ll automate the other cases in scanpydoc, then we can remove the manually formatted ones.; 3. There’s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484427693:233,simpl,simple,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484427693,4,"['clear', 'simpl']","['clear', 'simple']"
Usability,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403311910:159,learn,learn,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403311910,2,['learn'],['learn']
Usability,I tried downgrading umap-learn to 0.4.6 but then sc.pp.neighbors won't work. I've been using scanpy 1.5.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114:25,learn,learn,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114,2,['learn'],['learn']
Usability,"I try to use `sc.pl.pca` selecting components. According to the documentation the following should work:. ```python; import scanpy.api as sc; sc.logging.print_versions(); adata = sc.datasets.blobs(); sc.tl.pca(adata); sc.pl.pca(adata, components=['1,2', '2,3']); ```. However, I get an error. The output of the code above is:. ```python; scanpy==0+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection els",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/254:420,learn,learn,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254,1,['learn'],['learn']
Usability,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699:85,clear,clear,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699,2,['clear'],['clear']
Usability,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219,2,['learn'],['learn']
Usability,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270:472,simpl,simplify,472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270,1,['simpl'],['simplify']
Usability,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:236,simpl,simple,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,4,"['intuit', 'learn', 'simpl']","['intuitive', 'learned', 'simple']"
Usability,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361?. I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638:691,clear,clear,691,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638,2,['clear'],['clear']
Usability,"I was eventually able to contact the maintainer and he's looking into making a new release. Will see what happens. Nevertheless, it might not be a bad idea to simplify the code and to only support `igraph`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980:159,simpl,simplify,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980,2,['simpl'],['simplify']
Usability,"I was following Scanpy's tutorial for preprocessing and clustering the 3k PBMC data set, as seen [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). Unfortunately, many of the most informative marker genes are simply missing/discarded from the data set. Some of the genes a contributor has pointed out are missing from this set are: CD14, CD68, FTH1, SERPINA1, LYZ. Similar R tools, such as [rook/pagoda1](http://pklab.med.harvard.edu/cgi-bin/R/rook/10x.pbmc/index.html), have these genes displayed, some of them with quite high variance values (e.g. LYZ). Is this an issue with the tutorial itself, or is there a bug in scanpy that we are unaware of?. ```; >>> import anndata; >>> adata = anndata.read('./src/tests/test.h5ad'); >>> adata; AnnData object with n_obs × n_vars = 2638 × 1838; obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'cell_ids'; var: 'gene_ids', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std', 'gene_names'; >>> adata.var; gene_ids n_cells mt n_cells_by_counts mean_counts ... dispersions dispersions_norm mean std gene_names; TNFRSF4 ENSG00000186827 155 False 155 0.077407 ... 2.086050 0.665406 -3.672069e-10 0.424481 TNFRSF4; CPSF3L ENSG00000127054 202 False 202 0.094815 ... 4.506987 2.955005 -2.372437e-10 0.460416 CPSF3L; ATAD3C ENSG00000215915 9 False 9 0.009259 ... 3.953486 4.352607 8.472988e-12 0.119465 ATAD3C; C1orf86 ENSG00000162585 501 False 501 0.227778 ... 2.713522 0.543183 3.389195e-10 0.685145 C1orf86; RER1 ENSG00000157916 608 False 608 0.298148 ... 3.447533 1.582528 7.696297e-11 0.736050 RER1; ... ... ... ... ... ... ... ... ... ... ... ...; ICOSLG ENSG00000160223 34 False 34 0.016667 ... 2.585818 1.652185 9.322493e-12 0.217672 ICOSLG; SUMO3 ENSG00000184900 570 False 570 0.292963 ... 4.046776 2.431045 -3.685750e-10 0.723121 SUMO3; SLC19A1 ENSG00000173638 31 False 31 0.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338:229,simpl,simply,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338,1,['simpl'],['simply']
Usability,I was having the same issue these days. Happy to find this thread.; Fixed the issue by simply using the latest version of scanpy. Thanks a lot!!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1020766690:87,simpl,simply,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1020766690,2,['simpl'],['simply']
Usability,"I was just trying to run the [1.3 million cell clustering example](https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells), but have come across some strange behavior. When loading in the `hdf5` file from 10x to an AnnData object, the whole process uses about 30gb. If I write that AnnData object to disk with `adata.write`, then try to load that file (with `sc.read`) I end up using all the memory on the machine (~60g) before segfault-ing. I'd think that any dataset I wrote from memory, I should be able to read back into memory. I've put the full scripts to reproduce on my system in [this gist](https://gist.github.com/ivirshup/42e70a745704b8c71d78e57dd43e3b0b), but essentially I've run:. ```python; # gen_h5ad.py; import scanpy.api as sc; adata = sc.read_10x_h5(""{DATAPTH}/1M_neurons_filtered_gene_bc_matrices_h5.h5"") # Uses about 30gb; adata.write(""./write/1M_neurons.h5ad""); ```; ```python; # load_anndata.py; import scanpy.api as sc; adata = sc.read(""./write/1M_neurons.h5ad"") # Ends with segfault; ```. I'm running `scanpy` installed with conda with the following versions:. ```; scanpy==1.0.4 anndata==0.6 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0; ```. Thanks for the great package! Let me know if you'd like any more details on my setup.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/146:1202,learn,learn,1202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146,1,['learn'],['learn']
Usability,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>; <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>; <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>; <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>; <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864:687,clear,clear,687,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864,2,['clear'],['clear']
Usability,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39:716,simpl,simple,716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39,1,['simpl'],['simple']
Usability,"I would agree the results of `sc.pp.filter_genes(..., inplace=False)` are not the most intuitive. Instead of returning a filtered anndata, it returns which cells would have been filtered and the stats which were used to make this decision. This is documented under the `Returns` section for these functions. What you might want is. ```python; mask, _ = sc.pp.filter_cells(adata, min_genes=200, inplace=False); a = adata[mask].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137:87,intuit,intuitive,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137,2,['intuit'],['intuitive']
Usability,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:584,clear,clearer,584,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,2,['clear'],['clearer']
Usability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:54,clear,clear,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,2,['clear'],['clear']
Usability,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973:251,simpl,simple,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973,2,['simpl'],['simple']
Usability,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083:147,clear,clear,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083,2,['clear'],['clear']
Usability,"I'd like to bump the version requirement down a bit, since it seems like it's not that uncommon to pin `map-learn` lower than 0.5.5: https://github.com/search?q=%2F%5B%22%27%5D%3Fumap-learn%5B%22%27%5D%3F%5B%3D%3E%3C%5D%2B%2F&type=code. The cellxgene one is worrying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2870#issuecomment-1957629969:108,learn,learn,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2870#issuecomment-1957629969,4,['learn'],['learn']
Usability,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue?. This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914:443,simpl,simple,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914,2,['simpl'],['simple']
Usability,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:; ```; X = data; if isinstance(data, AnnData):; X = data.X; ```; or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433397997:301,simpl,simply,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433397997,4,['simpl'],['simply']
Usability,I'll fix this PR up in ~10 days. Appreciate your comments @ivirshup and think that some of your feedback could have been mitigated by me not going for short cuts :). Will request (final hopefully) review then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-789127611:96,feedback,feedback,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-789127611,2,['feedback'],['feedback']
Usability,"I'll reopen this cause I think it's quite relevant still and could be very straightforward to implement with [sklearn resample](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html). also, there is an entire package for subsampling strategies which is probably quite relevant: https://github.com/scikit-learn-contrib/imbalanced-learn. line here for reference: https://github.com/theislab/scanpy/blob/48cc7b38f1f31a78902a892041902cc810ddfcd3/scanpy/preprocessing/_simple.py#L857",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-972943098:143,learn,learn,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-972943098,6,['learn'],"['learn', 'learn-contrib']"
Usability,"I'm also a bit confused about how the CLR is applied. In the CITE-seq paper, I think it was done within a cell (over proteins), then I think they had switched to within a protein (over cells), and now in Seurat v4 it appears to be back to within a cell. Any per cell normalization is a bit tricky because the panels will differ between datasets as well as the titration of antibodies used. The simplest thing to me seems to be a simple log transformation combined with per protein scaling, as values between proteins are not comparable to begin with. We have some additional thoughts in the appendix of our totalVI paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290:394,simpl,simplest,394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290,4,['simpl'],"['simple', 'simplest']"
Usability,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027:619,simpl,simple,619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027,2,['simpl'],['simple']
Usability,"I'm doing the same pathway I've done on hundreds of datasets but this particular one fails when I try to calculate PCA with:. ```; /opt/Python-3.7.0/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py in svds(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors); 1768 ; 1769 if k <= 0 or k >= min(n, m):; -> 1770 raise ValueError(""k must be between 1 and min(A.shape), k={0}, A.shape={1}"".format(k, A.shape)); 1771 ; 1772 if isinstance(A, LinearOperator):. ValueError: k must be between 1 and min(A.shape), k=50, A.shape=(48, 2066); ```; Looking into this, I looped through adata.var['n_cells'] and no values were greater than 48, so I'm not sure why this is happening. Dropbox link with both the input test [H5AD file and notebook here](https://www.dropbox.com/sh/t2qb7ffz5msyc5e/AAD256Vs6HqLwNBjcBeCsGVGa?dl=0). Am I missing as simple error?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/331:857,simpl,simple,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331,1,['simpl'],['simple']
Usability,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-478296037:977,simpl,simple,977,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-478296037,2,['simpl'],['simple']
Usability,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782,2,['learn'],['learn']
Usability,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:1035,simpl,simple,1035,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,2,['simpl'],['simple']
Usability,"I'm hesitant to add another embedding method without clear benefits over existing implementations. Could you give some detail on benefits here, ideally with direct comparisons?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1050880241:53,clear,clear,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1050880241,2,['clear'],['clear']
Usability,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:272,simpl,simply,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,2,['simpl'],['simply']
Usability,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python; self.n_samples_, self.n_features_ = n_samples, n_features; self.components_ = V; self.n_components_ = n_components. # Get variance explained by singular values; self.explained_variance_ = (S ** 2) / (n_samples - 1); total_var = np.var(X, ddof=1, axis=0); self.explained_variance_ratio_ = \; self.explained_variance_ / total_var.sum(); self.singular_values_ = S.copy() # Store the singular values.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978:95,learn,learn,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978,4,['learn'],['learn']
Usability,"I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis (like UMAP or Louvain) on this kNN graph. Is that right? I suppose this is currently not implemented for t-SNE? With openTSNE it'd be easy to use the pre-built kNN graph and run t-SNE directly on that. Also, the default parameters of t-SNE in scanpy could IMHO be improved, see https://www.nature.com/articles/s41467-019-13056-x. Some of these recommendations (learning rate, initialization) are now default in openTSNE. There are some open issues at scanpy related to t-SNE such as https://github.com/theislab/scanpy/issues/1150 and https://github.com/theislab/scanpy/issues/996 but I think this suggestion would supersede them. We had a brief discussion of this at openTSNE here https://github.com/pavlin-policar/openTSNE/issues/102. I can see four somewhat separate suggestions:. 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph; 2. add tSNE support for `ingest` using openTSNE functionality.; 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults.; 4. add some tSNE ""recipes"" based on https://www.nature.com/articles/s41467-019-13056-x. What of this, if any, makes sense from the scanpy point of view?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233:689,learn,learning,689,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233,2,['learn'],['learning']
Usability,"I'm probably not seeing something very simple:; https://scanpy.readthedocs.io/en/latest/api/scanpy.external.html#module-scanpy.external; renders fine whereas; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.html#module-scanpy.api; https://scanpy.readthedocs.io/en/latest/api/scanpy.plotting.html#module-scanpy.plotting; both append an auto-generated list of all functions and classes defined in the module. As there is an annotated manual list already, we obviously don't want this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/408:39,simpl,simple,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408,1,['simpl'],['simple']
Usability,"I'm trying to run BBKNN on a AnnData object built with `scanpy==1.4.6` but it's failing when getting the graph. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; sc.external.pp.bbknn(; adata,; batch_key=""batch"",; n_pcs=21,; neighbors_within_batch=5,; trim=0); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Command error:; Traceback (most recent call last):; File ""~/sc_batch_effect_correction.py"", line 160, in <module>; trim=args.trim); File ""/opt/venv/lib/python3.7/site-packages/scanpy/external/pp/_bbknn.py"", line 120, in bbknn; **kwargs,; File ""/opt/venv/lib/python3.7/site-packages/bbknn/__init__.py"", line 281, in bbknn; approx=approx, metric=metric, **kwargs); File ""/opt/venv/lib/python3.7/site-packages/bbknn/__init__.py"", line 325, in bbknn_pca_matrix; neighbors_within_batch=neighbors_within_batch); File ""/opt/venv/lib/python3.7/site-packages/bbknn/__init__.py"", line 168, in get_graph; knn_indices[ind_from[:,None],col_range[None,:]] = ckdout[1]; ValueError: shape mismatch: value array of shape (240,4) could not be broadcast to indexing result of shape (240,5); ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.2 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1222:1356,learn,learn,1356,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1222,1,['learn'],['learn']
Usability,"I'm trying to use an array for the size argument to my umap/scatterplot with the following code; ```; import scanpy.api as sc; import numpy as np; sc.settings.figdir = ""testdir""; sc.settings.file_format_figs = ""png""; sc.logging.print_versions(); ```; With these libraries; `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size.; ```; somedata = sc.datasets.paul15(); sc.pp.pca(somedata); sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); sc.tl.leiden(somedata, resolution=0.5, random_state=42); z = np.abs(somedata.obsm['X_pca'][:,0])**1; sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); ```; I get the following two figure as output; ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png); ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478:353,learn,learn,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478,1,['learn'],['learn']
Usability,"I'm unable to complete the [Nestrorowa et al. 2016](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/nestorowa16/nestorowa16.ipynb) paga tutorial. On ln[3] . ```; adata.raw = adata; sc.pp.recipe_weinreb17(adata, log=False); sc.tl.pca(adata); ```. I keep getting this output:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-4-bfa4168a87e6> in <module>; 1 adata.raw = adata; ----> 2 sc.pp.recipe_weinreb17(adata, log=False); 3 sc.tl.pca(adata). /opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_recipes.py in recipe_weinreb17(adata, log, mean_threshold, cv_threshold, n_pcs, svd_solver, random_state, copy); 48 ); 49 adata._inplace_subset_var(gene_subset) # this modifies the object itself; ---> 50 X_pca = pp.pca(; 51 pp.zscore_deprecated(adata.X),; 52 n_comps=n_pcs,. AttributeError: module 'scanpy.preprocessing._simple' has no attribute 'pca'; ```. Versions:; `scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.7.0`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1326:1086,learn,learn,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326,1,['learn'],['learn']
Usability,"I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1; I'm trying to do a pca on a annData object; `sc.tl.pca(adata, svd_solver='arpack')`; and get the following error even after restarting the jupyter notebook:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-26-eb775d53dbfd> in <module>; ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 504 ; 505 if data_is_AnnData:; --> 506 adata.obsm['X_pca'] = X_pca; 507 if use_highly_variable:; 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504:87,learn,learn,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504,1,['learn'],['learn']
Usability,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:540,simpl,simple,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,2,['simpl'],['simple']
Usability,"I've been able to successfully use scanpy for MEX-formatted datasets, and the documentation here was great in the Jupyter notebooks. Many of our other datasets are in a simple tab format where the first column is gene symbol and the rest are in GROUP--CELLID format, like this:. ```; Gene_symbol	lymph_1--cell_avg	lymph_1--Cell_1	lymph_1--Cell_10; A1BG.AS1	13.9085855833156	0	54.3778851449283; A2M.AS1	10.2185780428145	0	0; A2MP1	0	0	0; AADACL2	0	0	0; AAGAB	136.889472532613	0	0; AAR2	76.3090843598131	0	0; AATF	360.127068564485	0	0; AATK	2.93980819712579	0	0; AATK.AS1	0	0	0; ```. These contain up to 30,000 rows and (so far) 400,000 columns. . I'd love to see more use cases for how to absorb different formats of data into scanpy. I'd be happy to help write it, but so far have only got MEX data to work.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/65:169,simpl,simple,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/65,1,['simpl'],['simple']
Usability,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246:197,progress bar,progress bars,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246,2,['progress bar'],['progress bars']
Usability,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:236,simpl,simplification,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421,2,['simpl'],['simplification']
Usability,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do; ```; In [3]: import umap ; In [4]: umap.__version__ ; Out[4]: '0.3.9'; ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181:152,learn,learn,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181,4,['learn'],['learn']
Usability,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:635,clear,clear,635,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296,2,['clear'],['clear']
Usability,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129:87,feedback,feedback,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129,2,['feedback'],['feedback']
Usability,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:698,simpl,simplified,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096,4,"['guid', 'simpl']","['guide', 'simplified']"
Usability,"If adata.n_obs < 4096 and umap version >= 0.4 and if `metric` is a distance that is not supported by scikit-learn (like ll_dirichlet or hellinger), we get a ValueError:. Code for reproducing with UMAP >= 0.4:. ```python; import scanpy as sc. adata = sc.datasets.paul15(); sc.pp.neighbors(adata, metric='hellinger'); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-5-e2c66b650fd3> in <module>; 2 ; 3 adata = sc.datasets.paul15(); ----> 4 sc.pp.neighbors(adata, metric='hellinger'). ~/.anaconda3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 108 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 109 method=method, metric=metric, metric_kwds=metric_kwds,; --> 110 random_state=random_state,; 111 ); 112 adata.uns['neighbors'] = {}. ~/.anaconda3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 686 # non-euclidean case and approx nearest neighbors; 687 if X.shape[0] < 4096:; --> 688 X = pairwise_distances(X, metric=metric, **metric_kwds); 689 metric = 'precomputed'; 690 knn_indices, knn_distances, forest = compute_neighbors_umap(. ~/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds); 1550 raise ValueError(""Unknown metric %s. ""; 1551 ""Valid metrics are %s, or 'precomputed', or a ""; -> 1552 ""callable"" % (metric, _VALID_METRICS)); 1553 ; 1554 if metric == ""precomputed"":. ValueError: Unknown metric hellinger. Valid metrics are ['euclidean', 'l2', 'l1', 'manhattan', 'cityblock', 'braycurtis', 'canberra', 'chebyshev', 'correlation', 'cosine', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule', 'wminkowski',",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1011:108,learn,learn,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1011,1,['learn'],['learn']
Usability,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825:25,simpl,simple,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825,2,['simpl'],['simple']
Usability,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:19,learn,learn,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206,2,['learn'],['learn']
Usability,"If you're planning to look into the code: There will be a new version of PAGA in Scanpy 1.2, which will feature two connectivity models... The code will be much clearer. We'll also see whether we can upload an extensive revision of the preprint - unfortunately, the review process at the journal took ages and coming up with the revision, too. All of this should happen in the next days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393819773:161,clear,clearer,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393819773,2,['clear'],['clearer']
Usability,"In a recent paper, we found the brute force KNN computation to become very expensive as the data sizes increase. I’ve noticed the kNN graph computed during the “neighbors” computation can be cached and reused when Umap-learn is called downstream but when Cuml UMAP is used, the kNN graph is recomputed each time. . In cuml 0.13 we added an optional `knn_graph` argument to umap’s training and inference methods to allow it to accept pre-computed kNN graph. This will allow the kNN graph to be computed once and reused when `n_neighbors` has not been changed. I think this would further accelerate the exploratory data analysis and visualization process with scanpy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1279:219,learn,learn,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1279,1,['learn'],['learn']
Usability,"In my experience, this happens if batch key is not None and one or more batches have low number of cells. Does it make sense to catch this error and simply skip the problematic batch or inform the user that batch doesn't have enough cells?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060:149,simpl,simply,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060,2,['simpl'],['simply']
Usability,In order to make the learning rate of accessible from scanpy it will be needed to update the dca; version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/793:21,learn,learning,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793,2,['learn'],['learning']
Usability,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553490520:221,simpl,simple,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553490520,2,['simpl'],['simple']
Usability,"In some edge cases, the control gene selection retrieves the same gene(s) that are also in the gene_list used for scoring.; As a result, when the following line is called, we end up with an empty control gene set, causing the downstream error in #2153; https://github.com/scverse/scanpy/blob/383a61b2db0c45ba622f231f01d0e7546d99566b/scanpy/tools/_score_genes.py#L173. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2153 ; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2875:439,guid,guidelines,439,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2875,2,['guid'],"['guide', 'guidelines']"
Usability,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:265,simpl,simple,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,2,['simpl'],['simple']
Usability,"In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```; ax = fig.add_subplot(gs[1,0]); ```. but I can't seem to overwrite the default axis labels or add new lines as commands like; ```; ax.set_ylabel('new label'); ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks!. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/979:1069,learn,learn,1069,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979,1,['learn'],['learn']
Usability,"In the tutorial there are lines grouping the genes at the top of the dotplot. <img width=""667"" alt=""Screen Shot 2020-01-15 at 11 47 42 AM"" src=""https://user-images.githubusercontent.com/10859440/72465969-eb3fd680-378c-11ea-925a-4f6b9f8039ed.png"">. A minimal example does not generate those lines... ![Screen Shot 2020-01-15 at 11 44 30 AM](https://user-images.githubusercontent.com/10859440/72465706-81bfc800-378c-11ea-9700-f2e4b4b69eac.png). ```python; scanpy==1.4.5.post2 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.0 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; matplotlib==3.0.3; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/998:558,learn,learn,558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/998,1,['learn'],['learn']
Usability,"Ingest simplification, more convenience embedding_density",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/965:7,simpl,simplification,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/965,2,['simpl'],['simplification']
Usability,Ingest simplification: remove return_joint,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/964:7,simpl,simplification,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/964,2,['simpl'],['simplification']
Usability,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:0,Intuit,Intuitively,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,3,"['Intuit', 'usab']","['Intuitively', 'usability']"
Usability,"Is there a way to interactively visualize and filter spots in Visium data based on coordinate values from `adata.obsm['spatial']`? There are some spots lying outside of the tissue which I'd like to filter from the data or even removing rows/ columns of spots from the array. Since the spaceranger coordinates are not perfectly aligned, writing a simple condition does not extract all spots of interest.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2481:346,simpl,simple,346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2481,1,['simpl'],['simple']
Usability,Is there any feedback？,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951#issuecomment-885545697:13,feedback,feedback,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-885545697,2,['feedback'],['feedback']
Usability,"Is there any way to estimate the number of branching points? It seems that this number has to be explicitly given before running the algorithm, which might be difficult if it's not clear from simply looking at the dimensionally reduced data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/11:181,clear,clear,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"Is there anybody meeting the same error with me?; I used de.test.wald() to do differentially expressed genes analysis with totally 47K cells. ### Minimal code sample (that we can copy&paste without having any data). ```python; test_sf = de.test.wald(; data=adata.layers['counts'],; formula_loc=""~ 1 + disease + size_factors"",; factor_loc_totest=""disease"",; as_numeric=['size_factors'],; gene_names=adata.var_names,; sample_description=adata.obs; ); ```. ```pytb; error: 'i' format requires -2147483648 <= number <= 2147483647; ```. #### Versions. <scanpy==1.7.1 anndata==0.7.5 umap==0.4.6 numpy==1.19.2 scipy==1.5.2 pandas==1.1.3 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3>. #### It seems this error happens when cell amount is over 10K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1874:637,learn,learn,637,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1874,1,['learn'],['learn']
Usability,"It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of `sc.pl.umap(adata, color='clusters')` -> `sc.pl.umap(adata, 'clusters')`. About the changes that you suggest: I have concerns with breaking previous functionality and I wonder what is your position with respect to this. The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage from the very firsts versions of scanpy but maybe you have some good ideas for transitioning this from a string to a tuple. . The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. . For the plots being the product of `color` and `components`: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. . For your question about replacing `components` by `dimensions`. We need to be careful here because in many places the use of components is in the context of PCA as in `sc.pp.neighbors` with the parameter `n_pcs`. I think that the replacement of `components` by `dimensions` should only be done for the embedding functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392:115,simpl,simple,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392,2,['simpl'],['simple']
Usability,"It looks like your adata object is corrupted. You should be able to type; `adata.X` to get the matrix. How are you generating the adata object?. On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,; >; > When running sc.pp.highly_variable_genes(adata.X) I get the following; > error:; >; > AttributeError: X not found; >; > I then ran sc.pp.highly_variable_genes(adata) and got the following:; >; > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,; > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]).; > You can drop duplicate edges by setting the duplicates kwarg; >; > The older sc.pp.filter_genes_dispersion(adata.X) works fine.; >; > Do you know how to fix this?; >; > Thank you!; >; > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0; > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > louvain==0.6.1; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/391>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-444950693:864,learn,learn,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-444950693,2,['learn'],['learn']
Usability,"It seems like the dendrogram feature has been merged into the master branch according to this thread #308 and it is documented in the API. However, after updating my scanpy to 1.3.2, I still cannot access the dendrogram feature and get the following error when I set `dendrogram=True` when using `sc.pl.heatmap`:; `AttributeError: Unknown property dendrogram`. Here are the versions that I am using:; scanpy==1.3.2 anndata==0.6.10 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. Do you plan on releasing the dendrogram feature? Which package version currently has it?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/344:480,learn,learn,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344,1,['learn'],['learn']
Usability,"It seems that upgrading from 1.8.1 to 1.8.2 introduce an error on umap version checking, mentioned by #1978 (and a potential solution); > Why are we using umap.__version__ instead of importlib.metadata.version('umap-learn')?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681:216,learn,learn,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681,2,['learn'],['learn']
Usability,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:499,learn,learn,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497,4,['learn'],['learn']
Usability,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,6,"['clear', 'learn']","['clearer', 'learn']"
Usability,It turned out that this is definitely something wrong with my system setup. After I circumvented the bug above by clearing `README.rst` I found another package that spits out the same error (`louvain`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102:114,clear,clearing,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102,2,['clear'],['clearing']
Usability,"It's `'./write/'`, so it's not a hidden directory - i guess it wouldn't be a good idea to save large files in a hidden fashion; whereas the config was hidden in `'.scanpy/'` - but the latter is not really needed anymore and I could simply remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453:232,simpl,simply,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453,2,['simpl'],['simply']
Usability,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:321,intuit,intuitive,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,4,"['intuit', 'learn']","['intuitive', 'learn']"
Usability,It's not clear to me why these `test_10x` tests are failing here and not on master -- there shouldn't be anything in this diff that affects those tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898:9,clear,clear,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898,2,['clear'],['clear']
Usability,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041:218,simpl,simply,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041,2,['simpl'],['simply']
Usability,"I’m not sure why it came up now, I saw changes to the chunking in https://github.com/scverse/anndata/pull/1550 and assumed that introduced it, but maybe not. regarding complexity and parameters, we could simplify things by doing. ```py; def maybe_rechunk_1d(a: NDArray | DaskArray | ...) -> NDArray | DaskArray | ...:; if isinstance(a, DaskArray):; return a.rechunk((a.chunksize[0], -1)); return a; ```. and using that in all the functions that fail (after running things through `array_type`). Would you prefer that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245646540:204,simpl,simplify,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245646540,2,['simpl'],['simplify']
Usability,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:293,simpl,simple,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026,2,['simpl'],['simple']
Usability,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-398506015:519,simpl,simply,519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398506015,2,['simpl'],['simply']
Usability,"Joining on the ""smart subsample"" part which we talked about a few months ago. The under/oversampling methods of [imbalanced-learn](https://imbalanced-learn.org/stable/index.html) was something we chatted about back then I remember. I opened a small dummy scverse-package draft [here](https://github.com/eroell/scimb) just to check how well this can be transferred for AnnData, but never really got to push it much. Not sure if we somehow could find ground to join on making something for stuff like that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862#issuecomment-1971428782:124,learn,learn,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862#issuecomment-1971428782,4,['learn'],['learn']
Usability,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>; <summary>Here's a doc-string for what I'm thinking:</summary>. ```python; def biomart_annotations(org, attrs, host=""www.ensembl.org""):; """"""; Retrieve gene annotations from ensembl biomart. Parameters; ----------; org : `str`; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.; attrs : `List[str]`; Attributes to query biomart for.; host : `str`, optional (default: ""www.ensembl.org""); A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns; -------; A `pd.DataFrame` containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(; ""hsapiens"",; [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434:123,simpl,simple,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434,4,['simpl'],['simple']
Usability,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:314,simpl,simple,314,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807,4,"['simpl', 'usab']","['simple', 'usable']"
Usability,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:64,learn,learn,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994,6,['learn'],['learn']
Usability,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:194,simpl,simple,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140,2,['simpl'],['simple']
Usability,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:848,simpl,simply,848,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698,2,['simpl'],['simply']
Usability,"Just saw this by chance. As we're planning to merge with scvelo's plotting modules soonish, that would simply become; `scv.pl.scatter(adata, groups=[[c] for c in adata.obs['clusters'].cat.categories], color='clusters', ncols=4)`, simply passing a list of lists to `groups` (without copying a whole anndata object). Will that be sufficient?. ![image](https://user-images.githubusercontent.com/31883718/71019675-10dcb000-20fb-11ea-80da-78301d37b958.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/955#issuecomment-566661312:103,simpl,simply,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566661312,4,['simpl'],['simply']
Usability,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051:64,learn,learn,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051,2,['learn'],['learn']
Usability,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):; ```; adata.obs['new_clusters'] = (; adata.obs[""old_clusters""]; .map(lambda x: {""a"": ""b""}.get(x, x)); .astype(""category""); ); ```; Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:; ```; adata.obs['new_clusters'] = (; adata.obs[""old_clusters""]; .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)); .astype(""category""); ); ```; @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-941184403:503,simpl,simply,503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-941184403,2,['simpl'],['simply']
Usability,"Just to be clear, is **n_genes** = `nFeature_RNA` and **n_genes_by_counts** = `nCounts_RNA `?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434#issuecomment-2029488023:11,clear,clear,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434#issuecomment-2029488023,2,['clear'],['clear']
Usability,"Latest BBKNN versions (>=1.5.x) [uses annoy_n_trees in place of n_trees](https://github.com/Teichlab/bbknn/commit/288e05990c74467601c263a23538779991fb7635), just applying that fix here. . <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1873:259,guid,guidelines,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873,2,['guid'],"['guide', 'guidelines']"
Usability,"Long term, I'd prefer to just use pynndescent since it would be a more simple implementation. That could change some results. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-846959398:71,simpl,simple,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846959398,2,['simpl'],['simple']
Usability,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`?. If we do that or plan to in the future, how should the added `weights` parameter to all these functions work?. I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesn’t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494718710:272,simpl,simply,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494718710,2,['simpl'],['simply']
Usability,Looks good to me! Docstring is also clearer now. Do you think it's worth adding a warning at a hard minimum threshold of 5-10 cells per category or sth like this? Although it would probably also just look strange to anyone plotting it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1936#issuecomment-875418541:36,clear,clearer,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1936#issuecomment-875418541,2,['clear'],['clearer']
Usability,"Looks great! You can get rid of `expect_warning`, as you can just simply check `if expected_warning_message is not None` instead, otherwise pretty ideal!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2563#issuecomment-1682177199:66,simpl,simply,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563#issuecomment-1682177199,2,['simpl'],['simply']
Usability,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:35,undo,undoubtedly,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085,2,['undo'],['undoubtedly']
Usability,"Looks like the default values for plotting Visium spots are way outsized. Try this in your lymph node notebook:; ```python; sc.pl.spatial(adata, img_key = ""hires"", cmap='magma',; color=['total_counts', 'n_genes_by_counts'],; gene_symbols='SYMBOL'); ```; This is what I get:; ...; ![image](https://user-images.githubusercontent.com/22567383/83098134-acf2e600-a0a1-11ea-9248-c611f7330547.png). #### Versions:; scanpy==1.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1253:499,learn,learn,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1253,1,['learn'],['learn']
Usability,"Looks simple enough! Please deduplicate the tests though, they have too many identical lines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042#issuecomment-2092792623:6,simpl,simple,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042#issuecomment-2092792623,2,['simpl'],['simple']
Usability,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800:133,simpl,simple,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800,2,['simpl'],['simple']
Usability,"MAP(); pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]); dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""); sc.tl.embedding_density(dblt, ""umap""); ```; </details>. <details> ; <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python; pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data; dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values; dbltdf[""density""] = dblt.obs[""umap_density""].values; ```. Get plotting imports and canvas:. ```python; import datashader as ds; from datashader import transfer_functions as tf; from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,; x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), ; y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),; x_axis_type='linear', y_axis_type='linear'); ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python; real = canvas.points(pbmcdf, 'x', 'y', ds.count()); sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(; tf.shade(real, name=""pbmcs""),; tf.shade(sim, name=""doublet""),; tf.shade(sim / (real + sim)),; ); ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making the plots look more similar (though there's something weird going on with non-overlapping areas here):. ```python; real_density = canvas.points(pbmcdf, 'x', 'y', ds.mean(""density"")); sim_density = canvas.points(dbltdf, 'x', 'y', ds.mean(""density"")). tf.Images(; tf.spread(tf.shade(real_density, name=""pbmc""), px=2),; tf.spread(tf.shade(sim_density, name=""doublet""), px=2),; tf.spread(; tf.shade(; sim_density / (sim_density + real_density), ; cmap=palettes.Viridis256; ), ; px=2,; name=""sim / (real + sim)""; ),; tf.spread",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:3766,simpl,simple,3766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,2,['simpl'],['simple']
Usability,Make the dca learning rate accessible from scanpy.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/813:13,learn,learning,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813,2,['learn'],['learning']
Usability,Makes sense! Thanks it's clear now why the corr matrix,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1614#issuecomment-771145250:25,clear,clear,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614#issuecomment-771145250,2,['clear'],['clear']
Usability,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950:317,Simpl,Simply,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950,3,"['Simpl', 'simpl']","['Simply', 'simply']"
Usability,"Maybe I am doing something wrong. I try to recover the observation from annData as a dataframe, and I am playing with some of the commands. This is an example. ```; import scanpy.api as sc; data = sc.read_text('/media/SETH_DATA/SETH_Alex/Fibroblasts.txt').T. print(data.data); ```. Then this error happens:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-14-1f44700b9ea5> in <module>(); ----> 1 print(data.data). ~/anaconda3_6/lib/python3.6/site-packages/anndata/utils.py in new_func(*args, **kwargs); 104 def new_func(*args, **kwargs):; 105 warnings.simplefilter('always', DeprecationWarning) # turn off filter; --> 106 warnings.warning(; 107 'Use {0} instead of {1}, {1} will be removed in the future.'; 108 .format(new_name, func.__name__),. AttributeError: module 'warnings' has no attribute 'warning'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/394:651,simpl,simplefilter,651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/394,1,['simpl'],['simplefilter']
Usability,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-673944797:16,clear,clear,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-673944797,2,['clear'],['clear']
Usability,"Maybe the documentation is not clear; ```; counts_per_cell_after : float or None, optional (default: None); If None, after normalization, each cell has a total count equal to the median of the counts_per_cell before normalization.; ```; I thought that only ""counts_per_cell_after"" would multiply by the median, but the argument is:; ```; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); ```; not ""counts_per_cell_after"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905#issuecomment-552922125:31,clear,clear,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905#issuecomment-552922125,2,['clear'],['clear']
Usability,"Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):; ```python; import scanpy as sc; sc.logging.print_versions(); from pypairs import __version__; print('pypairs==', __version__). adata = sc.datasets.blobs(); marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names); ```. ```python; scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; pypairs== v3.1.0. ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-20-33f7b7cad989> in <module>; 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}; 8 ; ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs); 132 ; 133 from pypairs.pairs import cyclone; --> 134 from . import settings; 135 from pypairs import settings as pp_settings; 136 . ImportError: cannot import name 'settings'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/655:499,learn,learn,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655,1,['learn'],['learn']
Usability,"Minimal example:; ```python; import scanpy.api as sc; sc.logging.print_versions(). adata = sc.datasets.blobs(). sc.pp.highly_variable_genes(adata); ```; Output:; ```python; **scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-11-5d93fbf298b7> in <module>; 4 adata = sc.datasets.blobs(); 5 ; ----> 6 sc.pp.highly_variable_genes(adata). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 115 # a normalized disperion of 1; 116 one_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450:254,learn,learn,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450,1,['learn'],['learn']
Usability,"Minor bug I assume. Using fewer than 50 cells raises the following error when trying to run `sc.tl.pca`. The code handles this when the `n_vars < 50` but not when `n_obs` is. ```pytb; ValueErrorTraceback (most recent call last); <ipython-input-823-4c11b9b62e6d> in <module>; ----> 1 sc.tl.pca(bla). ~/.virtualenvs/default/lib/python3.6/site-packages/scanpy/preprocessing/simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 504 pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state); 505 X = adata_comp.X; --> 506 X_pca = pca_.fit_transform(X); 507 ; 508 if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype). ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in fit_transform(self, X, y); 357 ; 358 """"""; --> 359 U, S, V = self._fit(X); 360 U = U[:, :self.n_components_]; 361 . ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit(self, X); 404 # Call different fits for either full or truncated SVD; 405 if self._fit_svd_solver == 'full':; --> 406 return self._fit_full(X, n_components); 407 elif self._fit_svd_solver in ['arpack', 'randomized']:; 408 return self._fit_truncated(X, n_components, self._fit_svd_solver). ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit_full(self, X, n_components); 423 ""min(n_samples, n_features)=%r with ""; 424 ""svd_solver='full'""; --> 425 % (n_components, min(n_samples, n_features))); 426 elif n_components >= 1:; 427 if not isinstance(n_components, (numbers.Integral, np.integer)):. ValueError: n_components=50 must be between 0 and min(n_samples, n_features)=38 with svd_solver='full'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/432:371,simpl,simple,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432,1,['simpl'],['simple']
Usability,"Mmh no, it does work as you expect, just need to turn the dendrogram off.; ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""CD79A"", ""MS4A1""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=False,; swap_axes=True,; ); ```. or just pass the list of markers (list, not mapping); ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_list = [""GNLY"", ""NKG7""]. sc.pl.heatmap(; pbmc,; marker_genes_list,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. If you pass a dict with incomplete annotation and request dendrogram, then it fails, and the warning says this clearly:; ```; WARNING: Groups are not reordered because the `groupby` categories and the `var_group_labels` are different.; categories: 0, 1, 2, etc.; var_group_labels: 1, 0, 2, etc.; ```; and it still produces a plot (yet unordered). The `var_group_labels` is also described in `help(sc.pl.heatmap)` as you pointed out. I think the misunderstanding is that passing a mapping or a list the behaviour is different, although potentially expected since a mapping and a list are different things. This could probably be explained clearer in the `var_names` argument yes. Just to go back to your original problem, in your case you were using as mapping categories that were not present in your `groupby` key altogether. This is a different issue, and probably the function should have thrown an error saying `var_group_labels` are not present in `categories`. . If you feel like opening a PR for this, we would really appreciate!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024:909,clear,clearly,909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024,4,['clear'],"['clearer', 'clearly']"
Usability,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:105,simpl,simply,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,2,['simpl'],['simply']
Usability,Most interesting things would be (from Severin's conversations) would be autocorrelation (via NN graph). Current workflow is to just use squidpy by it would be simple to add/port from squidpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3137#issuecomment-2275898105:160,simpl,simple,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3137#issuecomment-2275898105,2,['simpl'],['simple']
Usability,"My intuition would be neighbor finding would take more time as dataset size increases. What exact fraction of the time will depend a lot on number of samples, number of features, and possibly distance metric. If you're investigating yourself, I think trying `line_profiler`'s `%lprun` on `umap.UMAP.fit` would be a good bet. I'd also bet that they'd have a better idea over at `UMAP` or Pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146:3,intuit,intuition,3,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146,2,['intuit'],['intuition']
Usability,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python; dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])); # and instead could just do:; adata.uns[key + ""_colors""]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-480739679:151,simpl,simpler,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-480739679,4,['simpl'],['simpler']
Usability,"My main reasoning was that:. * Use dask array/ delayed is more simple; * One can go from dask array/ dask delayed -> `Future` with `client.compute`, but the other way around isn't really possible since `Future`s kick off computation immediately. So I'd like to use the higher level interface as much as possible, and only use lower level when necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332:63,simpl,simple,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332,2,['simpl'],['simple']
Usability,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:29,simpl,simple,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738,4,['simpl'],"['simple', 'simply']"
Usability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:16,intuit,intuitive,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,2,['intuit'],['intuitive']
Usability,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:1464,simpl,simpler,1464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,2,['simpl'],['simpler']
Usability,"My version of scanpy:; scanpy 1.8.1 pyhd8ed1ab_0 conda-forge; I'm working on a linux system based server, and uses miniconda3 for environment management.; After some changes in my environment, I tried to run the routine process of my analysis.; But when running sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40), I encountered the following error: . > Traceback (most recent call last):; File ""/data1/exhaustT/process.py"", line 118, in <module>; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""/data1/exhaustT/umap.py"", line 48, in <module>; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; ModuleNotFoundError: No module named 'umap.umap_'; 'umap' is not a package. I've tried to re-install umap-learn from conda-forge, and/or simply pip install umap, neither worked for me. #update, the problem found to be that I named my own script umap.py, even though it's in a different direction, it still caused trouble.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1987:1753,learn,learn,1753,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1987,2,"['learn', 'simpl']","['learn', 'simply']"
Usability,"Need a file `highly_variable_genes.py` in `scanpy.preprocessing` with a function `highly_variable_genes`. This function is very similar to `filter_genes_dispersion`. However, by default, it assumes data has been logarithmized using `sc.pp.log1p`. Hence, in the “Seurat” method, an exponentiation with `expm1` is necessary (the current way in which the parameter `log` treats things is inconsistent as it doesn’t properly transform back using `expm`). Also, the new function doesn’t actually perform the filtering but simply annotates the data (`subset=False`). No need in an option `subset` in the new function. Of course, the old function remains for backwards compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/300:517,simpl,simply,517,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/300,1,['simpl'],['simply']
Usability,"Nice to hear that it worked. As a side product, you can now create a cell; browser html directory from the generated directory. On Thu, Feb 7, 2019 at 10:21 AM aditisk <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> thanks for the feedback. As; > @maximilianh <https://github.com/maximilianh> suggested, I was able to; > export the expression matrix from the cellbrowser export function. Thank; > you for your help.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262#issuecomment-461540169>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TXGCkdaQWO8ks_x7uOm-P2_ISArRks5vLG6RgaJpZM4Wne7Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-461557503:262,feedback,feedback,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-461557503,2,['feedback'],['feedback']
Usability,"Nice! Thank you! That's certainly very meaningful. It should just go somewhere else, not in `preprocessing.simple`... Let me think about where to put these queries... We'll have more of this sort of thing in the future! I'll certainly merge this and move it around... However, we'll not make bioservices a hard requirement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-386769886:107,simpl,simple,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-386769886,2,['simpl'],['simple']
Usability,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:467,learn,learn,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179,8,"['guid', 'learn', 'simpl']","['guide', 'learn', 'simpler']"
Usability,"No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds?. Package metadata and content will be correctly derived from the git repo’s status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`.; 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy you’re currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`; 2. if `setuptools_scm` can’t be imported or we’re not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/798:64,simpl,simpler,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798,1,['simpl'],['simpler']
Usability,"No worries. It can be something like:. > Subset of groups, e.g. ['g1', 'g2', 'g3'], for which the list of DE genes should be computed. Each group of cells is always compared to the remaining cells, even if they don't belong to the subset of groups. However, it would be probably more useful to change the API and to implement subsetting of the groups through the 'groups' parameter. I think this would be more intuitive, also together with the use of the 'reference' parameter.; In particular, because retrieving of DE genes for specific groups can be more easily done with the [get](https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html#scanpy.get.rank_genes_groups_df) interface.; But there may be other use cases I haven't considered in which the actual implementation may be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315:410,intuit,intuitive,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315,2,['intuit'],['intuitive']
Usability,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:423,clear,clear,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483,8,"['clear', 'user-friendly']","['clear', 'user-friendly']"
Usability,"No, not right now. You have to do this with numpy or pandas. adata is just a collection of numpy arrays and a dict (`print(adata)`) So you can simply do this manually. If you explain in a more detailed way what you want, I can probably also quickly implement it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-310073367:143,simpl,simply,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310073367,2,['simpl'],['simply']
Usability,"No, there is no way to produce a single file with data and metadata. Having genes as rows can simply be achieved by transposing the matrix (`adata.T.write_csvs(...)`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-460475684:94,simpl,simply,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-460475684,2,['simpl'],['simply']
Usability,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:541,simpl,simple,541,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,4,['simpl'],"['simple', 'simply']"
Usability,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:555,learn,learn,555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,2,['learn'],['learn']
Usability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:182,learn,learn,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,2,['learn'],['learn']
Usability,"Not sure, according to [this page](https://joblib.readthedocs.io/en/latest/auto_examples/parallel/distributed_backend_simple.html#sphx-glr-auto-examples-parallel-distributed-backend-simple-py) function code should be changed. It would be handy to add the backend as option to `regress_out`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2781#issuecomment-1860968991:182,simpl,simple-py,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2781#issuecomment-1860968991,2,['simpl'],['simple-py']
Usability,"Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python 3.8; # Your code here; ```sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon', use_raw=False). ```pytb; [Paste the error output produced by the above code here]; ```; ![image](https://user-images.githubusercontent.com/75048821/142975910-ee42c23e-976d-4980-a351-dcb53672b978.png). #### Versions. <details>. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.2 pandas==1.3.4 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5. </details>. ***************; Hello Scanpy,. Because the scRNA-seq data usually have mitochondrial gene contamination, it's reasonable to regress out mito genes by sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) and do scaling, and use this 'clear' data for determining the marker genes of each cluster by setting use_raw=False in sc.tl.rank_genes_groups(). However, I found that. 1. if using unregressed data by sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon', use_raw=True), the top marker genes have positive logFC, which is reasonable because these are top upregulated genes helping us to determine the annotations of clusters. ; ![image](https://user-images.githubusercontent.com/75048821/142977363-a7ce9cd6-5c2b-48f7-9e21-eccc66650f78.png). 2. the weird thing is, if using regressed data by sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon', use_raw=False), the logFC of top marker genes will become negative and even disappear, which means the downregulated genes and genes with unknown logFC (why no logFC?) becomes the marker genes, which doesn't make sense.; ![image](https://user-images.githubusercontent.com/75048821/142977508-a9d3421d-ff66-4f4c-a4f4-71bc1bbd7dda.png). This bug comes from the official jupyter not",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2057:1229,clear,clear,1229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2057,1,['clear'],['clear']
Usability,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475999342:140,learn,learn,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475999342,2,['learn'],['learn']
Usability,OK I computed the neighbors using umap-learn 0.5.1 and then downgraded to 0.4.6 for UMAP. Not elegant but so far so good.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091:39,learn,learn,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091,2,['learn'],['learn']
Usability,"OK guys, I'm happy with all that!. I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479385067:597,simpl,simple,597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479385067,2,['simpl'],['simple']
Usability,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:32,simpl,simply,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565,2,['simpl'],['simply']
Usability,"OK, issues like this are almost always either memory or dependency problems: Something’s miscompiled or compiled for the wrong architecture (e.g. a newer CPU than you have) or simply buggy. We have no native code in Scanpy, so we don’t cause segfaults. If there’s anything we can mitigate, we will, if someone demonstrates a reproducible problem with up-to-date dependencies",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108:176,simpl,simply,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108,2,['simpl'],['simply']
Usability,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672:83,learn,learn,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672,4,['learn'],['learn']
Usability,"OK, so now the question is: should this become part of legacy-api-wrap?. I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py; @renamed_args(new=""old""); ```. feels more natural than. ```py; @deprecated_arg_names({""old"": ""new""}); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422:153,clear,clearer,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422,2,['clear'],['clearer']
Usability,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:98,learn,learn,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962,6,['learn'],['learn']
Usability,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:1043,simpl,simply,1043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672,2,['simpl'],['simply']
Usability,"OK; now I have more time. The error thrown at ; ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mean_bin']].values) \; 209 / disp_std_bin[df['mean_bin']].values; ```; astonishes me. The line has been working for me on pandas 0.19.2 and 0.20.3 and for others for other versions for many months already. Do you have an old pandas version? The line should work as `disp_mean_bin` has been computed from `disp_grouped = df.groupby('mean_bin')['dispersion']` [here](https://github.com/theislab/scanpy/blob/65503d34d6b9d0a1d23e831d6daeba86856b3eee/scanpy/preprocessing/simple.py#L215); i.e., the Series 'mean_bin' was used to initialize the index of `disp_mean_bin`. Hence, you should be able to index with 'mean_bin'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115:637,simpl,simple,637,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115,2,['simpl'],['simple']
Usability,OS: Windows 10; Python version: 3.7.7; sc.logging.print_versions() gives; scanpy==1.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038:165,learn,learn,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038,2,['learn'],['learn']
Usability,Of course that's fine. You can simply push such things to master. ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/591#issuecomment-480219397:31,simpl,simply,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591#issuecomment-480219397,2,['simpl'],['simply']
Usability,"Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? . I think we also discussed why not to use intersection by default in the PR: https://github.com/theislab/scanpy/pull/614#issuecomment-485875031 . If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Edit: adata.var[""highly_variable_intersection""] wasn't even implemented in the beginning of the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616845594:33,clear,clear,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616845594,2,['clear'],['clear']
Usability,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)); colors = pd.Series(data[:,0], dtype='category'); sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes); ```; I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:934,learn,learning,934,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,2,['learn'],['learning']
Usability,Oh that's reaaaally bad. I did a quick git bisect on sklearn:. ![image](https://user-images.githubusercontent.com/1140359/58111323-40582800-7bbf-11e9-8905-e7f3a73cd057.png). Here is the commit that broke our umaps: https://github.com/scikit-learn/scikit-learn/pull/13554,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494454599:241,learn,learn,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494454599,4,['learn'],['learn']
Usability,"Oh yes, it's no longer created on purpose since 1.0. But the plotting function obviously still assumes it. I'll fix it. Simply use the 'dpt_pseudotime' annotation instead. `pl.dpt_timeseries` should work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129#issuecomment-383915649:120,Simpl,Simply,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129#issuecomment-383915649,1,['Simpl'],['Simply']
Usability,"Oh, damn... It wasn't clear to me that you were actually saying that this is broke... Sorry, I went over a few pull requests too quickly. Anyways, so the master branch was broke for a day. It's resolved in https://github.com/theislab/scanpy/commit/96890730972162aa531c3289b38ad728a7585c85. The next pull request goes smoother... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-368081444:22,clear,clear,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-368081444,2,['clear'],['clear']
Usability,"Oh. Well, that’s just not right. They need to undo that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933#issuecomment-874661288:46,undo,undo,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874661288,2,['undo'],['undo']
Usability,Ok @ivirshup I think we're ready to go here. Thanks for the guidance!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1665700077:60,guid,guidance,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1665700077,2,['guid'],['guidance']
Usability,"Ok, good to read that it wasn't log-transformed!. @Koncopd, could you quickly implement these simple changes? Before continuing to work on the UMAP? These simple changes are for 1.4.1, the UMAP will be for 1.5. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478391082:94,simpl,simple,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478391082,4,['simpl'],['simple']
Usability,"Ok, now I see why you are confused! The x-axis in the figure above is somehow confusingly labeled 'dpt_order' even though it simply corresponds to `range(adata.n_smps)`, as evident from the code snippet I posted above; posting it again with the weird axis labeling that confuses you; ```; import matplotlib.pyplot as pl; pl.plot(range(adata.n_smps), adata.smp['dpt_pseudotime'][adata.smp['dpt_order']]); pl.xlabel('dpt_order'); pl.show(); ```. I definitely have to change this naming. Still it is meaningful that `adata.smp['dpt_order']` is an index vector that ""generates"" the order. OK, very soon, I could do the following. I'll rename the index array that generates the order `adata.smp['dpt_generate_order']` and the order you see in the plot `adata.smp['dpt_ordering_id']`, if you think this is better. So, one probably fast computation of this is the following; ```; ordering_id = np.zeros(adata.n_smps, dtype=int); for count, idx in adata.smp['dpt_generate_order']:; ordering_id[idx] = count; ```. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/27#issuecomment-314744802:125,simpl,simply,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314744802,2,['simpl'],['simply']
Usability,"Okay, I solved the issue. In my environment, scanpy==1.7.2 does not work with umap==0.5.2. I pip uninstall scanpy and umap-learn. Next I installed umap-learn through conda which was umap==0.5.1. When installing scanpy again it works as expected. . Not sure if its worth looking further into it but pip install scanpy also installs umap==0.5.2 (which does not work at least for me). . Many thanks! ; ![umap_0 5 1](https://user-images.githubusercontent.com/20926246/148250255-5ac00a46-cbc9-4608-a893-0472e32f5fb5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077:123,learn,learn,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077,4,['learn'],['learn']
Usability,"Old, simple method for flat return sections",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610:5,simpl,simple,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610,2,['simpl'],['simple']
Usability,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-473500100:393,guid,guide,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-473500100,4,"['guid', 'intuit']","['guide', 'intuitive']"
Usability,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-674659520:139,clear,clear,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-674659520,2,['clear'],['clear']
Usability,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:1483,simpl,simplify,1483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688,2,['simpl'],['simplify']
Usability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:300,user experience,user experience,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,2,['user experience'],['user experience']
Usability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:436,learn,learn,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,2,['learn'],['learn']
Usability,"Otherwise this fails with:; ```; ~/miniconda3/envs/anndata/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 1828 import IPython; 1829 IPython.embed(); -> 1830 raise ValueError(; 1831 'groupby has to be a valid observation. '; 1832 f'Given {group}, is not in observations: {adata.obs_keys()}' + msg. ValueError: groupby has to be a valid observation. Given ['cell_type'], is not in observations: ['cell_type'] or index name ""index""; ```. <!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1656:617,guid,guidelines,617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656,2,['guid'],"['guide', 'guidelines']"
Usability,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:102,clear,clearly,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463,2,['clear'],['clearly']
Usability,"PS: If you have a conda environment for Python 3, you do not need to use `pip3`; simply use `pip`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/29#issuecomment-321767948:81,simpl,simply,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321767948,2,['simpl'],['simply']
Usability,"PS: You can of course also simply upload here on GitHub in a comment, as you want. ; PPS: The canonical way of saving AnnData's is via `.write('myfile.h5ad')`. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/365#issuecomment-440424406:27,simpl,simply,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440424406,2,['simpl'],['simply']
Usability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:153,simpl,simply,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,2,['simpl'],['simply']
Usability,"Part of the UMAP computation is creating an approximate KNN graph. Scanpy uses that part of the UMAP package to create it's neighborhood graph. UMAP uses whatever distance metric and feature space the user specifies for finding neighbors. Scanpy uses euclidean distance in PCA space as the default for finding neighbors, but that can be changed. I hope that's more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/277#issuecomment-426845759:365,clear,clear,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-426845759,2,['clear'],['clear']
Usability,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct; * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877:107,simpl,simplifies,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877,6,"['learn', 'simpl']","['learn', 'simplifies']"
Usability,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:361,simpl,simple,361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924,2,['simpl'],['simple']
Usability,"Please go ahead!. On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes.; > Alex is currently a bit ill I learned, which is why he probably didn’t do; > it yet. I didn’t have time to review the whole thing, but if y’all want I; > can do that too; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-463080680:203,learn,learned,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-463080680,2,['learn'],['learned']
Usability,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:739,pause,pausefilter,739,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937,2,['pause'],['pausefilter']
Usability,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/248:88,simpl,simple,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248,2,['simpl'],['simple']
Usability,"Quick question to you @ivirshup, can't we simply replace all the `adata_neighbors` stuff with `scanpy.datasets.pbmc68k_reduced`? It already has the neighbor graph etc. in it and is smaller, that is, would speed up tests considerably.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/581#issuecomment-479418054:42,simpl,simply,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581#issuecomment-479418054,2,['simpl'],['simply']
Usability,Quite a simple addition meant to fix a bug when one works with specific layers.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2183:8,simpl,simple,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2183,1,['simpl'],['simple']
Usability,"Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into?. <details>; <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots; - [ ] `sc.pl.embedding`; - [ ] `sc.pl.draw_graph`; - [ ] `sc.pl.diffmap`; - [ ] `sc.pl.pca`; - [ ] `sc.pl.tsne`; - [ ] `sc.pl.umap`; - [ ] `sc.pl.spatial`; - [x] `sc.pl.embedding_density`; - [ ] PCA specific; - [ ] `sc.pl.pca_loadings`; - [ ] `sc.pl.pca_overview`; - [ ] `sc.pl.pca_scatter`; - [ ] `sc.pl.pca_variance_ratio`; - [ ] PAGA; - [ ] `sc.pl.paga`; - [ ] `sc.pl.paga_adjacency`; - [ ] `sc.pl.paga_compare`; - [ ] `sc.pl.paga_path`; - [ ] DPT pseudotime; - [ ] `sc.pl.dpt_groups_pseudotime`; - [ ] `sc.pl.dpt_timeseries`; - [ ] Groupby; - [x] `sc.pl.dotplot`; - [ ] `sc.pl.matrixplot`; - [ ] `sc.pl.clustermap`; - [ ] `sc.pl.heatmap`; - [ ] `sc.pl.dendrogram`; - [ ] `sc.pl.stacked_violin`; - [ ] `sc.pl.tracksplot`; - [ ] `sc.pl.violin`; - [ ] Preprocessing; - [ ] `sc.pl.filter_genes_dispersion`; - [ ] `sc.pl.highest_expr_genes`; - [ ] `sc.pl.highly_variable_genes`; - [ ] DE; - [ ] `sc.pl.rank_genes_groups`; - [ ] `sc.pl.rank_genes_groups_dotplot`; - [ ] `sc.pl.rank_genes_groups_heatmap`; - [ ] `sc.pl.rank_genes_groups_matrixplot`; - [ ] `sc.pl.rank_genes_groups_stacked_violin`; - [ ] `sc.pl.rank_genes_groups_tracksplot`; - [ ] `sc.pl.rank_genes_groups_violin`; - [ ] Misc/ to be classified; - [ ] `sc.pl.ranking`; - [ ] `sc.pl.scatter`; - [ ] `sc.pl.sim`; - [ ] `sc.pl.correlation_matrix`; - [ ] `sc.pl.matrix`; - [ ] Time series (???); - [ ] `sc.pl.timeseries`; - [ ] `sc.pl.timeseries_as_heatmap`; - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`; - [x] `sc.tl.embedding_density`; - [ ] `sc.get.obs_df`; - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1632:117,simpl,simple,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632,1,['simpl'],['simple']
Usability,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... ; ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932:324,simpl,simply,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932,2,['simpl'],['simply']
Usability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:171,simpl,simply,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,4,"['clear', 'simpl']","['clear', 'simply']"
Usability,"Regarding your other bug: `scanpy.plotting` used to have the attribute and `scanpy.api.plotting` would simply import the module. To make the [overview of the API](https://scanpy.readthedocs.io/en/latest/api.html) work, I had to introduce a [dummy module](https://github.com/theislab/scanpy/blob/master/scanpy/api/pl.py). In order to avoid duplication, I removed all exports from `scanpy.plotting.__init__`. I readded it to fix the bug on the development branch, but I need to think of a better solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094:103,simpl,simply,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094,2,['simpl'],['simply']
Usability,"Relevant issues about `scverse.org/learn`: . * https://github.com/scverse/scverse-tutorials/issues/58; * https://github.com/scverse/scverse-tutorials/issues/60. Tutorial registry is [here](https://github.com/scverse/scverse-tutorials/tree/main/tutorial-registry) and works in principle, but needs to be filled with content as described in https://github.com/scverse/scverse-tutorials/issues/58",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2502#issuecomment-1580575932:35,learn,learn,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502#issuecomment-1580575932,2,['learn'],['learn']
Usability,"Removes the need to specify a genome string in 10x h5 files by default. This removes a personal annoyance of mine, where I had to figure out what the reference was called when there's often only one reference used per file. For cellranger `v3.0.0+` files, specifying a genome acts as a filter on input, as it did already. However, it doesn't only act if `gex_only` is `True`. Additionally, the behavior of `gex_only` has been changed to fit the documentation, i.e. it just filters for gene expression variables. For legacy files:. * If the file only has one genome group, that one is used by default; * If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. As the behavior of the function has meaningfully changed, this is a breaking change (though I'd be surprised if it affected many people). Personally, I haven't seen many 10x files which contain multiple genomes, so I'd appreciate feedback or examples from people who have.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/442:1017,feedback,feedback,1017,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442,1,['feedback'],['feedback']
Usability,"Reproducible example:. ```python; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.mnn_correct(pbmc, batch_key=""phase""); ```. It looks like `mnn_correct` is only returning one variable, through its documentation looks like it should return three. @chriscainx, could you offer some guidance here?. As a workaround for now, you could just call `mnnpy.mnn_correct` with the same signature you've been using. It'll return a one-tuple with a modified anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637:358,guid,guidance,358,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637,2,['guid'],['guidance']
Usability,"Retrieve cell names in the Seurat object used to create the embedding, then simply reorder AnnData accordingly (adata = adata[cell_names]).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1718#issuecomment-791195880:76,simpl,simply,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-791195880,2,['simpl'],['simply']
Usability,"Right now we get different PCs when using sparse data (applying TruncatedSVD) compared to using dense data (applying PCA). . `TruncatedSVD(X-X.mean(0))` would be equivalent to `PCA(X)`. `X-X.mean(0)` would obviously not be sparse anymore, which is why it is currently implemented as `TruncatedSVD(X)`. The first PC will be mainly representing the vector of means, thus be very different from zero-centered PCA. The following components would approximately resemble PCA. However, since all subsequent PCs are orthogonal to the first PC, we will never get to the exact solution. Hence, the PCs are questionable, in particular when the very first ones are quite misleading. That's not desirable. I think we should obtain the same PCA representation regardless of the data type. Don't we have to densify `X` at some point anyways, as we would have to compute `X.dot(X.T)`. Thus it might be worth thinking of some EM approach?. Whatsoever, I think as long the data is manageable and fits into the RAM, we should just use the densified `X`. . Line 486 in preprocessing/simple I don't quite understand:; ```; if zero_center is not None:; zero_center = not issparse(adata_comp.X); ```; It doesn't depend on the actual value of the attribute `zero_center` anymore. Is that a bug, or what is the rationale behind this?. For now, we can change that into something like; ```; zero_center = zero_center if zero_center is not None else False if issparse(adata.X) and adata.X.shape[0] > 1e4 else True; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393:1063,simpl,simple,1063,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393,1,['simpl'],['simple']
Usability,"Right now, figures for tutorials and documentation have to be manually generated when we make a release. This often leads to out of date figures, and a fair amount of pain for otherwise small style changes. It would be good if this could be part of an automatic build process. For the sphinx docs, we could probably do this on the readthedocs servers since we have extra capacity. For notebooks, this could take some consideration. Some notebooks deal with large data that we don't want to reprocess frequently. A good step could just be some simple scripting around automatically running the notebooks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357:543,simpl,simple,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357,1,['simpl'],['simple']
Usability,Same issue here. . scanpy==1.6.0 anndata==0.7.4 umap==0.4.4 numpy==1.19.0 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1411#issuecomment-695065780:108,learn,learn,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411#issuecomment-695065780,2,['learn'],['learn']
Usability,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:537,learn,learn,537,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518,2,['learn'],['learn']
Usability,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014:39,simpl,simply,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014,2,['simpl'],['simply']
Usability,"Same issue: . ```; UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:280,learn,learn,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012,4,['learn'],['learn']
Usability,"Same thing: If the line is under-indented, the first line summary can’t be properly extracted. I’ll make the message more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728:122,clear,clear,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728,2,['clear'],['clear']
Usability,"Same! Seems like -maxiter gets set/clobbered to 1. I'm seeing it on one machine (which I have limited access too, its a galaxy installation using scanpy scripts) but not another (my local), both of which are apparently running scanpy 1.8.1. . Im wondering if there's a umap-learn version issue? In order to set the umap n_epochs(aka maxiter) default , it looks like older versions of umap-learn expected 0 (e.g. https://github.com/lmcinnes/umap/blob/0.5.0/umap/umap_.py), whereas the newer expect None. My working installation has umap-learn 0.5.2 (which seems to expect None), and I'm not sure about the one on the other server. Might be barking up the wrong tree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544,6,['learn'],['learn']
Usability,Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/568:68,guid,guidelines,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568,1,['guid'],['guidelines']
Usability,Scanpy not working correctly with scikit-learn 0.21.1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654:41,learn,learn,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654,2,['learn'],['learn']
Usability,"Scikit-learn 1.5.0 will no longer have `issparse` exported, so let’s import it from where it belongs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3047:7,learn,learn,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3047,1,['learn'],['learn']
Usability,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708:129,learn,learned,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708,2,['learn'],['learned']
Usability,"Seems like the logic was broken. ```; # we test for raw, but check in adata.var_names; elif use_raw and value_to_plot in adata.var_names:; color_vector = adata.raw[:, value_to_plot].X; # use_raw might be false but we still check adata.raw.var_names; elif value_to_plot in adata.raw.var_names:; color_vector = adata[:, value_to_plot].X; ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/579:371,simpl,simplify,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579,1,['simpl'],['simplify']
Usability,Seems like this is still an issue. I am getting this error below randomly. It disappears after trying sometime later with the exact code and files... scanpy==1.9.4 anndata==0.9.2 umap==0.5.3 numpy==1.23.4 scipy==1.11.2 pandas==2.1.4 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 louvain==0.8.1 pynndescent==0.5.10. `adata=sc.read_h5ad('foo.h5ad')`; Error:; `AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.group.Group'> from /.`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1890353810:240,learn,learn,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1890353810,2,['learn'],['learn']
Usability,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console; $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'; 0.18; $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*; ~/.local/lib/python3.6/site-packages/umap; ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info; $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.9; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294:564,learn,learn,564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294,2,['learn'],['learn']
Usability,"Sergei (@Koncopd) tested it out and will get back to you. He also found a peak memory usage of 121 GB. I have to admit that I never made checks with that degree of detail and I fear that for now, I'll simply update the documentation stating that peak memory usage can go up to ~120 GB. I'm still puzzled by that, and maybe some efficiency found it's way into the code which wasn't there (simple guess: is everything in `float32`?). But we'll need some time to work it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466:201,simpl,simply,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466,4,['simpl'],"['simple', 'simply']"
Usability,"Should the reference object where you learn the transformation (currently `adata`) always be a subset of the data you're going to apply the transformation to (`adata2`)? If so, instead of passing a separate object, could there be a mask of which samples to train on?. If not, what do you think about making this a separate function? Maybe `combat_by_reference`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047,2,['learn'],['learn']
Usability,"Should this be relayed to scikit-learn then? If so, that should probably be done by someone who knows where in the `sc.pp.neighbors()` function this is breaking...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494457995:33,learn,learn,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494457995,2,['learn'],['learn']
Usability,Similar pull request exists already in sklearn.; https://github.com/scikit-learn/scikit-learn/pull/12841; Will watch.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-460239298:75,learn,learn,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-460239298,4,['learn'],['learn']
Usability,"Simple fix for sc.pp.neighbors(..., metric_kwds={...}).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/149:0,Simpl,Simple,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/149,1,['Simpl'],['Simple']
Usability,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323:0,Simpl,Simple,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323,1,['Simpl'],['Simple']
Usability,Simpler gene annotations in plotting functions,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646:0,Simpl,Simpler,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646,1,['Simpl'],['Simpler']
Usability,Simpler scatter functions,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244:0,Simpl,Simpler,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244,1,['Simpl'],['Simpler']
Usability,Simplification of _ranks in rang_genes_groups. Passing pandas index to scipy dendrogram now causes an error. This fixes the problem.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1290:0,Simpl,Simplification,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1290,1,['Simpl'],['Simplification']
Usability,Simplify embeddings a bit,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538:0,Simpl,Simplify,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538,1,['Simpl'],['Simplify']
Usability,Simplify score_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3097:0,Simpl,Simplify,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3097,1,['Simpl'],['Simplify']
Usability,Simplify tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2575:0,Simpl,Simplify,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575,1,['Simpl'],['Simplify']
Usability,Simply adding the matplotlib `is_color_like` seems to do the trick; ```; and (color is None or color in adata.obs.keys() or color in adata.var.index or is_color_like(color))):; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/692#issuecomment-505559808:0,Simpl,Simply,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692#issuecomment-505559808,1,['Simpl'],['Simply']
Usability,"Simply leaving this out, solves it and guarantees exactness. Imho, there's no reason to do this type conversion. It's, the threshold value, thus just a single value that gets converted to float32, which won't yield any considerable speedup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1456#issuecomment-709197166:0,Simpl,Simply,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1456#issuecomment-709197166,1,['Simpl'],['Simply']
Usability,"Simply use standard slicing. The most elegant way in this case would be something like [this](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/datasets/__init__.py#L108-L109), I guess. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/353#issuecomment-437726312:0,Simpl,Simply,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353#issuecomment-437726312,1,['Simpl'],['Simply']
Usability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:12,learn,learn,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,2,['learn'],['learn']
Usability,"Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console; $ git checkout 1.4 -b stable; $ git cherry-pick 4b1504c c78de5b # the logo commits; ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/571:308,simpl,simply,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571,1,['simpl'],['simply']
Usability,Sklearn has its implementation of [CCA](; http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](; https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423799757:56,learn,learn,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423799757,2,['learn'],['learn']
Usability,Slightly unrelated feedback: I think that your API documentation is broken or incomplete: https://bento-tools.readthedocs.io/en/latest/api.html#tools. Would be great to have it complete so that people can use your tool easily.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2334#issuecomment-1291804266:19,feedback,feedback,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2334#issuecomment-1291804266,2,['feedback'],['feedback']
Usability,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404504765:52,simpl,simpler,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404504765,2,['simpl'],['simpler']
Usability,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:; 1. Return cluster labels as `ints`; 2. Support non-string indexes (and adopt `loc` vs `iloc`); 3. Support `ufuncs` with `AnnData`; 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conveni",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178:788,simpl,simple,788,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178,2,['simpl'],['simple']
Usability,"So for a fix, we’d simply need to change. https://github.com/scverse/scanpy/blob/414092f68b4b40aa99153556377c32839b392636/scanpy/preprocessing/_highly_variable_genes.py#L197-L199. into. ```py; X = X.copy(); if 'log1p' in adata.uns_keys() and adata.uns['log1p'].get('base') is not None:; X *= np.log(adata.uns['log1p']['base']); np.expm1(X, out=X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2668#issuecomment-1766402734:19,simpl,simply,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2668#issuecomment-1766402734,2,['simpl'],['simply']
Usability,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:249,learn,learn,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989,2,['learn'],['learn']
Usability,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8; umap-learn 0.4.0; pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. gives the warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310:87,learn,learn,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310,2,['learn'],['learn']
Usability,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:7,user experience,user experience,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,4,"['learn', 'user experience']","['learn', 'user experience']"
Usability,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:61,clear,clear,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,2,['clear'],['clear']
Usability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:171,learn,learn,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,2,['learn'],['learn']
Usability,"Solved same problem for me as well. . For the record, the output of `sc.logging.print_versions()` in my conda environment is: . `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654:233,learn,learn,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654,2,['learn'],['learn']
Usability,"Some effort to simplify how the embedding plots are handled. Right now this consists of simplifying the `components` argument to `dimensions`. Dimensions is a list of collections of ints. Each element in this list has the length of the number of dimensions to be plotted. `components` is no longer used once it can be transformed into dimensions. What does this do?. * Let's us delete `_get_data_points`, an awful function; * Get rid of `data_points` a list of coordinates that most code assumed would only ever have one element; * Move spatial specific code to spatial specific functions; * Gets rid of edge cases where `components` was either `None` or `[None]` (not sure how). Side note, also made a modification to a testing fixture that had been making tests fail when run out of order. ## Some questions:. ### Should `dimensions` be exposed? If so, should `components` be deprecated?. I think it's weird to pass the dimensions as strings `""1,2""` as opposed to dimensions `(0, 1)`. * Why is it one indexed?; * Isn't it the same amount of keystrokes?; * How useful is `""all""`?. I also think it's weird that the amount of plots generated is the product of `components` and `color`. This differs from every other ""vectorized"" argument to `embedding`. Changing arguments and deprecating `components` would be an opportunity to change this. ### If dimensions should be exposed, how many places does this need to be implemented?. We use `components` as an argument in a number of places in the codebase. Should we think about doing a large-scale replacement?. ### I broke a plotting test. I can't tell the difference. @giovp, does this look fine to you? It's the spatial plots with no image. <details>; <summary> </summary>. Expected. ![master_spatial_visium_empty_image](https://user-images.githubusercontent.com/8238804/101748159-4f475800-3b20-11eb-9007-5a987a881828.png). Actual. ![master_spatial_visium_empty_image](https://user-images.githubusercontent.com/8238804/101748219-64bc8200-3b20-11eb-931",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538:15,simpl,simplify,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538,2,['simpl'],"['simplify', 'simplifying']"
Usability,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:209,simpl,simplifying,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225,2,['simpl'],['simplifying']
Usability,"Sorry @ivirshup , still not quite getting what you're after. The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Non-integer values don't necessarily mean normalised or log'd data, so I can't add a check in the code for that. I could add a check on the values of the simulated doublets, that they are the sum of the parent counts we send to scrublet's simulate function, but that seems outside the scope of these fixes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519:212,simpl,simple,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519,2,['simpl'],['simple']
Usability,Sorry about that. Any idea how we can be clearer?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531833776:41,clear,clearer,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531833776,2,['clear'],['clearer']
Usability,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png); ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png); ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411:297,simpl,simplicity,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411,2,['simpl'],['simplicity']
Usability,"Sorry but the question is not clear. The plotting functions underwent a refactoring recently but that one should still work no? I'll close this for the moment, feel free to reopen it but please do so with a reproducible example, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813:30,clear,clear,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813,2,['clear'],['clear']
Usability,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:154,learn,learn,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575,2,['learn'],['learn']
Usability,Sorry for the lack of feedback! I got caught up in my own work. * Have you made any progress on the travis issue?; * Is it an issue of getting different results from computing variance?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182#issuecomment-624644914:22,feedback,feedback,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-624644914,2,['feedback'],['feedback']
Usability,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:374,simpl,simplest,374,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077,2,['simpl'],['simplest']
Usability,"Sorry for the late response! This seems to have come just after I went through the issues last weekend...; ; It looks great! :smile:. Some small notes:; * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...); * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916:852,clear,clear,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916,2,['clear'],['clear']
Usability,"Sorry for the late response!. In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622244074:235,simpl,simplifying,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622244074,2,['simpl'],['simplifying']
Usability,Sorry to be unresponsive for a while. I think exporting `plot_scatter` as `pl.scatter_embedding` and keeping `pl.scatter` sounds a like a simple and good solution!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-490000909:138,simpl,simple,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-490000909,2,['simpl'],['simple']
Usability,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620:191,simpl,simplify,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620,2,['simpl'],['simplify']
Usability,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572:218,clear,clear,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572,2,['clear'],['clear']
Usability,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster.; > ; > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes; > ; > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549:153,clear,clear,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549,2,['clear'],['clear']
Usability,"Sorry, there is a small bug in the wilcoxon method, that might hit sometimes. @a-munoz-rojas, it should be resolved after merging your fix, don't you think so? I'd be happy to move forward as soon as the code-overhead issue around double logarithmization is fixed. Should be very simple. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/530#issuecomment-474301716:280,simpl,simple,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530#issuecomment-474301716,2,['simpl'],['simple']
Usability,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:364,learn,learn,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,2,['learn'],['learn']
Usability,"Sounds great!. Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution?. > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being sa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:731,learn,learn,731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,2,['learn'],['learn']
Usability,"Sounds like a good idea. Since we have a hexagonal grid, we can just connect the centers of the hexagons in a regular fashion instead of running delauney triangulation. But it’s fast enough to do that too if we want to have it easy and there’s a delauney implementation in something we already import (e.g. scipy maybe?). @giovp the result would simply be a smoothly changing shading. Like this, but with a hex grid instead of a square grid:. ![](https://upload.wikimedia.org/wikipedia/commons/f/f5/Interpolation-bicubic.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1287#issuecomment-706180849:346,simpl,simply,346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1287#issuecomment-706180849,2,['simpl'],['simply']
Usability,"Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576:182,clear,clear,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576,1,['clear'],['clear']
Usability,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-406512161:377,simpl,simply,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-406512161,6,"['guid', 'simpl']","['guidelines', 'guidelines-for-repository-contributors', 'simply']"
Usability,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:195,clear,clear,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361,4,"['clear', 'guid']","['clear', 'guidance']"
Usability,"Tbh, I found out about `groups` after writing the function and looking for a way to put the dots in front. Maybe there is a simpler way to do this... But then the command you suggest gives an error on my own data if I don't also specify `color='bulk_labels'` (works for the pbmc68k, but doesn't colour anything in), and then it just puts all the labels on the same plot and doesn't create small multiples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331:124,simpl,simpler,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331,2,['simpl'],['simpler']
Usability,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/327:266,simpl,simpler,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327,1,['simpl'],['simpler']
Usability,"Thank you all for the feedback!. In the end the best solution has been to store activities in `.obsm` and then use the plotting functions via an `extract` function like in `squidpy`. Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-807060037:22,feedback,feedback,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-807060037,2,['feedback'],['feedback']
Usability,"Thank you all for this! In particular, @Koncopd!. I know that this will cause a little more headache, but could we consider renaming to `rank_genes`?. The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573:448,simpl,simple,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573,4,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/319#issuecomment-432482743:23,feedback,feedback,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319#issuecomment-432482743,2,['feedback'],['feedback']
Usability,"Thank you both for taking the time to answer. I was hoping there is a python port/re-implementation of clustree, but this is clearly not the case. Nonetheless, the workarounds you suggest are helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785816727:125,clear,clearly,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785816727,2,['clear'],['clearly']
Usability,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:480,simpl,simple,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901,2,['simpl'],['simple']
Usability,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:445,simpl,simple,445,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076,4,['simpl'],"['simple', 'simply']"
Usability,"Thank you for these thoughts!. I guess the high documentation quality in R stems from the Bioconductor project, which really set some standards. Nothing like this exists in Python - everyone just does what he or she wants. There are few people thinking about setting up something similar to Bioconductor for Python - but this will likely take some time... With Scanpy, we try to provide documentation at the Standards of the big packages: numpy, scipy, statsmodels, seaborn, scikit-learn, h5py, pytables, etc. There are many more and all of them have great docs. I think, with Scanpy, one can still do a lot better. Tuturials tend to be too short. Also, there should be a properly rendered html output of the notebooks - with a button where you can simply download it and then run it yourself to start playing around with it. Hope we will have this in a couple of weeks. And yes, other packages maybe just need to take time. But I'd guess that this will get much better soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498:482,learn,learn,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498,4,"['learn', 'simpl']","['learn', 'simply']"
Usability,Thank you for this; maybe there was one version where this was inconsistent and I simply don't remember... @AnatoleKing have you used version 0.4.4?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370147087:82,simpl,simply,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370147087,2,['simpl'],['simply']
Usability,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python; import matplotlib as mpl; mpl.rcParams['figure.facecolor'] = 'white'; ```. I verified the same thing on another environment, but it didn't happen.; So this might be critically relating to my personal environment.; I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python; %matplotlib inline. # 2 lines below solved the facecolor problem.; # import matplotlib as mpl; # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/473#issuecomment-462346509:566,simpl,simple,566,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473#issuecomment-462346509,2,['simpl'],['simple']
Usability,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:129,simpl,simple,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,2,['simpl'],['simple']
Usability,"Thank you so much for the feedback!; I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using ; conda create -n scanpy python=3.6 scanpy; conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341:26,feedback,feedback,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341,2,['feedback'],['feedback']
Usability,"Thank you very much for the super clean examples here! Is fixed via https://github.com/theislab/scanpy/commit/0ed304a64038f7d2c11b36fe5883ab9765ffba57 (@yugeji, you fed a pandas series into scikit learn, which is generally a bad idea :wink: ).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/273#issuecomment-427033148:197,learn,learn,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273#issuecomment-427033148,2,['learn'],['learn']
Usability,"Thank you! I'll rename the result according to the scikit-learn convention, though. If they call it `explained_variance_`, we should also call it that way - we can add in the docs, that it equals the eigenvalues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/87#issuecomment-366189008:58,learn,learn,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/87#issuecomment-366189008,2,['learn'],['learn']
Usability,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:99,guid,guides,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336,4,['guid'],['guides']
Usability,"Thank you! This is great and very helpful! :smile:. Yes, it would be nicer if the user were allowed to pass both. But isn't it possible simply by virtue of not passing `gene_symbols`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/430#issuecomment-456008629:136,simpl,simply,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/430#issuecomment-456008629,2,['simpl'],['simply']
Usability,Thank you!; It was just not clear from the tutorial in the beginning that the same thing is used to define colors in plots :(,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/383#issuecomment-445461792:28,clear,clear,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383#issuecomment-445461792,2,['clear'],['clear']
Usability,"Thank you, @giovp . I see. So when using heatmap, the mapping of marker genes must represent all clusters.; It won't work if markers are supplied for only a subset of the clusters. As far as I can tell, though, this is not apparent from the documentation of the function `help(sc.pl.heatmap)`.; Also, in the plotting tutorial, which I referred to in the beginning, only a subset of markers are supplied.; So, either the documentation and tutorial should be adjusted to make clear that markers must be supplied for all clusters. Or, which I would find great, heatmap could be adjusted to be able to display markers for a subset of the clusters, which I had assumed to be the case in the beginning. Btw. from looking at the tutorial, I believe the same issue may apply to some other plotting functions, e.g. for [Tracksplot](https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html#Tracksplot)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723037276:474,clear,clear,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723037276,2,['clear'],['clear']
Usability,"Thank you. Updating and releasing a new scanpydoc version is very simple:. First you make and check out your changes in scanpydoc’s own documentation, like in any sphinx project:. ```console; $ $EDITOR scanpydoc/theme/static/css/scanpy.css; [hack away]; $ cd docs; $ make html; $ $BROWSER _build/html/index.html; [check if it looks right]; ```. Then you can very quickly commit, tag, and release:. ```console; $ git add scanpydoc/theme/static/css/scanpy.css; $ git commit -m 'Made layout even wider (o________o)'; $ git tag v0.5.1 # Don’t forget the “v”!; $ flit publish; ```. That’s literally all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969:66,simpl,simple,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969,2,['simpl'],['simple']
Usability,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user; want to sample data with some other weighting technique. So we should ask; them to just put the weights for observations, then we need to modify PCA; as well and i think my code will support most of plots and marker genes,; but not PCA, because my input is PCA matrix with weights for each; observations. Thanks,; Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all; > the time is possible, but not very nice for users. So a few questions come; > to mind:; >; > Should we add scanpy.pp.coreset, which would create a sampling and add; > adata.obs['coreset_weights'] or simply adata.obs['weights']?; >; > If we do that or plan to in the future, how should the added weights; > parameter to all these functions work?; >; > I think it might default to 'coreset_weights', and the functions would; > automatically use that .obs column if it exists. Users should also still; > be able to specify weights manually as in this PR.; >; > So the type of the parameter would be Union[str, pd.DataFrame,; > Sequence[Union[float, int]]].; > ------------------------------; >; > All of that doesn’t really affect this PR, as we can merge it as it is and; > include anndata-stored weights later.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494724138:763,simpl,simply,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494724138,2,['simpl'],['simply']
Usability,"Thanks @adamgayoso, it's clear now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1630#issuecomment-775835519:25,clear,clear,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630#issuecomment-775835519,2,['clear'],['clear']
Usability,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094:38,feedback,feedback,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094,2,['feedback'],['feedback']
Usability,"Thanks a lot, @falexwolf. . I have one more question. Suppose, I have clustered cells with a restricted gene set & arrived at 8 Louvain clusters (considering one like the example above). . `adata.obsm['X_geneset1`] = adata[:, ['Tmem72', 'Adgrg2', 'Scn4b', 'Ust', 'Htr1d', 'Prnp', 'Rom1', 'Gpr158', 'Robo2', 'Cntn2', 'Rab11fip5', 'Lhfpl2', 'Emb', 'Gabrg2', 'Ptk7', 'Prkar1a', 'Ehd3', 'Kitl', 'Slc6a19', 'Spry4', 'Nceh1', 'Lin7a',....,'Lpin1']].X`. ![Input_subgroup_030519](https://user-images.githubusercontent.com/44576210/57128124-bae91100-6d92-11e9-96d8-6d3d322d5f12.png). Now, for marker identification, I have ranked genes in each community using Logistic regression & Wilcoxon. Naturally, there can be genes with a very high score (those were not used to cluster the cells). ; ![Input_CMarkers_LogReg](https://user-images.githubusercontent.com/44576210/57132746-d5c28200-6da0-11e9-816b-e3b71ee01a0f.png). Now, if I find a few genes in a cluster that acts as contradictory markers (like, the markers for vascular cells in a dendritic cluster) will I be able to eliminate contaminating cells from the group that expresses those vascular genes? To simply put it, I want to cluster genes with ['gene1', 'gene2', 'gene3', 'gene4',..'gene400'] but I don't want to include cells that express ['gene10' AND 'gene35' AND 'gene100'..etc]. Will I be able to omit all such cells & rerun the cluster? I can do one gene at a time. . `ldata_modified = ldata[ldata[: , 'Rpl13'].X > 0.5, :] `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-489055148:1150,simpl,simply,1150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-489055148,2,['simpl'],['simply']
Usability,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and; push when it passed tests for all 5 plots. Thanks,; Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,; >; > I have removed issue from the pull request by the testing tool, now the; > tools showed me duplications, which are mostly from other code and 1-2 from; > my code. Please have a look into it. It's my first pull request and its; > taking too much time :(; >; > Thanks; > Khalid; >; > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:; >; >> Ok , thanks for letting me know. Please check the pull request. I have; >> verified my code by keeping weights 1 and it has same values when; >> observations has no weights or all weights equal to 1.; >>; >> I also suggest to update PCA for weighted sampled data.; >>; >> Thanks,; >> Khalid Usman; >>; >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>; >> wrote:; >>; >>> You can just open a new one, I’ll close this one then 🙂; >>>; >>> —; >>> You are receiving this because you authored the thread.; >>> Reply to this email directly, view it on GitHub; >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; >>> or mute the thread; >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; >>> .; >>>; >>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520:41,learn,learned,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520,2,['learn'],['learned']
Usability,Thanks for all the good feedback @ivirshup - I'll work on it and re-request review when I'm done.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-726918992:24,feedback,feedback,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-726918992,2,['feedback'],['feedback']
Usability,"Thanks for bearing with me Isaac 😅 🙏 took some of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, coul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:327,simpl,simply,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,2,['simpl'],['simply']
Usability,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:; ```; WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)); Reason for being yanked: License Violation; ```; Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615:452,simpl,simple,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615,2,['simpl'],['simple']
Usability,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:562,learn,learn,562,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,['learn'],['learn']
Usability,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:130,Simpl,Simple,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241,1,['Simpl'],['Simple']
Usability,"Thanks for the PR! I've just renamed the variable to be a bit more clear. I do think this test could be a bit better (e.g. check that the structure of the object is correct), but also this is an improvement so LGTM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692:67,clear,clear,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692,2,['clear'],['clear']
Usability,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting.; * This should work with other solvers from scipy, like `lobpcg`, right?; * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that?. ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958,2,['learn'],['learn']
Usability,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:266,simpl,simply,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256,4,['simpl'],['simply']
Usability,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/727#issuecomment-508614967:84,guid,guides,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508614967,2,['guid'],['guides']
Usability,"Thanks for the clarification, @falexwolf. The documentation is clear, not sure how I missed it. I agree that euclidean distance is well-approximated by PCA (as long as populations are sufficiently large). For other metrics, that may not be the case (and for Jaccard it bails hard), and so maybe a warning would be appropriate in those cases rather than changing the behavior of `choose_representation`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-400070267:63,clear,clear,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-400070267,2,['clear'],['clear']
Usability,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:34,clear,clear,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,2,['clear'],['clear']
Usability,"Thanks for the detailed feedback @ivirshup , I've started to and will be working on that in the next days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-900656064:24,feedback,feedback,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-900656064,2,['feedback'],['feedback']
Usability,Thanks for the explanation and walkthrough on where everything is located and how to access it! This is actually very useful. This makes it easier to navigate the addata object. Having access to the Scikit-Learn style API would be useful for incorporating with other sklearn compatible methods. The biggest thing is the .transform method to project new samples into the diffusion space. I've been trying to figure out how to implement this on my own but I hit a snag: https://stackoverflow.com/questions/78486471/how-to-add-a-transform-method-to-project-new-observations-into-an-existing-spac. pyDiffMap has an implementation for [Nystroem out-of-sample extensions used to calculate the values of the diffusion coordinates at each given point.](https://github.com/DiffusionMapsAcademics/pyDiffMap/blob/22adc99faa83708e9ac05224015fa02c3a7f3c91/src/pydiffmap/diffusion_map.py#L294). The backend implementations of the algorithms are different so I'm not sure if I can just port this method over. It would also be great if said sklearn-api would have an option for custom transformers. It looks like this was already implemented but having direct access to a standalone model object w/ this capability would be incredibly useful! Nothing like this exists for DiffusionMaps right now. I'm trying to implement it myself but I also hit a snag when trying to generalize the transformer objects to build connectivity graphs: https://stackoverflow.com/questions/78486997/how-to-reproduce-kneighbors-graphinclude-self-true-using-kneighborstransfor. Any help on this front would be amazing especially if I could just use It directly w/ scanpy as this is my preferred analysis package (I actually started to deprecate my own software suite https://github.com/jolespin/soothsayer because scanpy worked so well). . I work quite a bit in both the microbial ecology realm and single cell transcriptomics using scanpy for both. I'm trying to make a push for the microbial ecology community to start using this software,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3054#issuecomment-2117801388:206,Learn,Learn,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3054#issuecomment-2117801388,1,['Learn'],['Learn']
Usability,"Thanks for the feedback and sorry for the delay. The code snippet using `sklearn.decomposition.FastICA` gave me quite similar results to PCA for my data in terms of the downstream UMAP visualisations (when I simply embedded 50 components in the neighbourhood graph). One difference is that the ICA was slower to compute. I am not confident to create a vignette, as I'm unclear what the 'correct' results should look like. I have not tried the `picard` implementation yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004,4,"['feedback', 'simpl']","['feedback', 'simply']"
Usability,"Thanks for the feedback! It astonishes me a bit, though, that Monocle 2 doesn't work well on that example. For the Paul et al, Cell (2015), it should give very nice results ([link to preprocessing](https://github.com/theislab/scanpy/blob/a2a330fa4640fdd4847fb48970a743242936e1df/scanpy/examples/builtin.py#L183-L199) / [link to plots](https://github.com/theislab/scanpy_usage/blob/master/EXAMPLES.md#paul15)), as they show in the recent preprint I reference above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/11#issuecomment-302212941:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11#issuecomment-302212941,2,['feedback'],['feedback']
Usability,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808,4,"['feedback', 'learn']","['feedback', 'learn']"
Usability,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387,2,['feedback'],['feedback']
Usability,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:934,learn,learning,934,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,2,['learn'],['learning']
Usability,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317:554,simpl,simply,554,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317,2,['simpl'],['simply']
Usability,"Thanks for the quick reaction. While the parameters seem to be respected in Scnapy 0.3.2, there is still some weird caching issue. Everything works OK when the ```write``` directory is empty, but when running multiple simulations in a row, e.g.:. ```; adam_krumsiek11_2 = sc.tl.sim('krumsiek11.txt', nrRealizations=1); sc.pl.sim(adam_krumsiek11_2). adam_krumsiek11_2 = sc.tl.sim('krumsiek11.txt', nrRealizations=2); sc.pl.sim(adam_krumsiek11_2); ```. I sometime get the same result (and both calls report reading from the very same simulation result file). However, when I clear the ```write``` directory between the calls to ```sc.tl.sim```, the results are as expected. This problem occurs only for certain parameters (for example, varying seed this way works as expected - I get two different figures).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/52#issuecomment-348160053:573,clear,clear,573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348160053,2,['clear'],['clear']
Usability,"Thanks for the quick response, @flying-sheep!. I can confirm that updating _pandas-2.2.2_ does fix this. I totally missed this possibility; it's not clear to me why the dots would change ordering, but the totals wouldn't (maybe _scanpy_ relies on default _pandas_ behaviour that changed between 1.x and 2.x?). That said, _pandas-2.x_ unfortunately breaks some dependencies in our environment, so I'll either pin _scanpy_ or use your workaround. Regarding the ordering and issue title change. Maybe a nit, but it's my understanding that the default ordering is alphabetical (which makese perfect sense as a default!). If this is correct, then I'd suggest that the wrong ordering is not the totals, but the categories themselves. Given this, the workaround that gives me the expected behaviour would be `dp.categories_order = dp.dot_color_df.index.sort_values()`:; <img width=""439"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/5192495/6f7622f5-14b5-4ea5-a44f-288c4507c4f0"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629:149,clear,clear,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629,2,['clear'],['clear']
Usability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:99,simpl,simple,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,2,['simpl'],['simple']
Usability,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:456,simpl,simple,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398,8,"['clear', 'intuit', 'simpl']","['clear', 'intuitive', 'simple', 'simply']"
Usability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:30,clear,clear,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,4,"['clear', 'simpl']","['clear', 'simply']"
Usability,Thanks for you detailed guidance. All I want is to display zero count cells as some color (gray) and this is what cellranger and seurat do. I can do it now following the above code. This is just what I want !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1550#issuecomment-748139666:24,guid,guidance,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748139666,2,['guid'],['guidance']
Usability,"Thanks for your comment Jason!; Don't you think the sentence before the one you quoted:; ""If None, mpl.rcParams[""axes.prop_cycle""] is used unless the categorical variable already has colors stored in adata.uns[""{var}_colors""].""; in combination with the default being ""None"" would make this clear?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845:290,clear,clear,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845,2,['clear'],['clear']
Usability,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/276:505,clear,clearly,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276,1,['clear'],['clearly']
Usability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:290,learn,learning,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,4,"['learn', 'simpl']","['learning', 'simply']"
Usability,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed.; ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png); ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354:852,simpl,simply,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354,2,['simpl'],['simply']
Usability,Thanks for your reply. I will try that and may given more feedback. Cheers!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312780478:58,feedback,feedback,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312780478,2,['feedback'],['feedback']
Usability,Thanks for youre great job. But it is a bit diffcult for me to draw picture with your guide in https://scanpy.readthedocs.io/en/latest/basic_usage.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/255:86,guid,guide,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/255,1,['guid'],['guide']
Usability,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862:876,simpl,simple,876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862,2,['simpl'],['simple']
Usability,"Thanks very much for your reply. . Yes, I am aware ‘dpi’ can help increase the resolution. However, I noticed that with the same ‘dpi’ , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing ‘dpi’ will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1791#issuecomment-815761921:335,simpl,simply,335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791#issuecomment-815761921,2,['simpl'],['simply']
Usability,"Thanks! A little bit of context. We needed this aggregation for one of the projects using pseudobulks of the data. We could use scanpy aggregation methods for simple averaging, but to test the outlier-robust median aggregation, we had to write our code. scanpy didn't have it for some reason, so @farhadmd7 kindly agreed to contribute here. Perhaps someone else will find it helpful, too. @eroell, what do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730:159,simpl,simple,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730,2,['simpl'],['simple']
Usability,"Thanks! Can I ask for two clarifications before replying:. > Right now, we tend to use a connectivity graph built by UMAP ... UMAP uses Pynndescent to construct the kNN graph. So does it mean that you depend on Pynndescent to construct the kNN graph, and then if the user calls UMAP, it's run on this previously constructred kNN graph?. By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy (like Euclidean or cosine). It seems to work quite a bit faster. For sparse input data and/or fancy metrics, it uses Pynndescent. > ... but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. What are the use cases here that you thinking of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550:422,simpl,simple,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550,2,['simpl'],['simple']
Usability,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:24,learn,learn,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435,2,['learn'],['learn']
Usability,"Thanks, however I think the [documentation](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups) is not perfectly clear about it:. > groups : str, Iterable[str]; Subset of groups, e.g. ['g1', 'g2', 'g3'], to which comparison shall be restricted, or 'all' (default), for all groups. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531820364:167,clear,clear,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531820364,2,['clear'],['clear']
Usability,"Thanks, now is becoming clearer. Can you specify the maximum percentage; value or this is taken from the data?. What would be a nice name for a parameter to add to the function?; `max_fraction` ?. I would not be able to work on this for the next days but at least we can; throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>; wrote:. > Sure - this is an output from a Seurat analysis. You can see that the; > scale of the dot size goes from 10% to 60%, such that the group with 60%; > expressing cells is scaled to the max dot size:; >; > [image: pastedgraphic-3]; > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>; >; > As a comparison, this is a (different) output from scanpy that; > automatically scales to 100% and causes the dots to be too small:; > [image: dotplotex]; > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>; >; > Please let me know if this makes sense!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/396#issuecomment-447898456:24,clear,clearer,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396#issuecomment-447898456,4,"['clear', 'simpl']","['clearer', 'simple']"
Usability,"Thanks, this is very helpful to pinpoint what is happening!. So what's going on here is that within one gene (column), all cells (rows) have the almost same float value.; This causes the variance per gene to be ~0, usually just a tiny value, in this dataset e.g. on the order of 1e-21.; Handling such tiny values, tiny offsets due to numerics can yield negative values for the variance. When doing np.sqrt() [here](https://github.com/scverse/scanpy/blob/fdfb9a1a48d480a30c23e5f14499a18a6388e418/src/scanpy/preprocessing/_scale.py#L186C5-L186C23) on such a nan, this yields this entire gene column to obtain nans. From our side, this could be addressed by considering to set such tiny negative values to 0. `sklearn` circumvents this by directly [computing the standard deviation](https://github.com/scikit-learn/scikit-learn/blob/2621573e60c295a435c62137c65ae787bf438e61/sklearn/preprocessing/_data.py#L248) from numpy, which likely has such a mechanism within it directly. However, trying to ""scale"" a feature of a constant value should be omitted in the first place very likely: as scaling involves dividing by standard deviation (which is ~0 then), the resulting numbers obtained are not actual biology, but just artifacts from a stability correction. I'd assume that in scRNAseq, this typically is a gene which is never observed (that is, 0 all the time), and as such could be filtered for in the preprocessing with e.g. `sc.pp.filter_genes(adata, min_counts=...)`, where `a` would be a number > 0. We might consider to raise a warning here though, as just introducing the nans without further comment can be quite confusing... What do you think about this @sophiamaedler?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2199782706:806,learn,learn,806,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2199782706,4,['learn'],['learn']
Usability,That could also work. But this would require a bit of a rewrite. I think the current solution is simpler and also really fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797:97,simpl,simpler,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797,2,['simpl'],['simpler']
Usability,That doc change looks good to me!. For any change to `reference` it would be good to keep it simple. Maybe it could accept a list of groups?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-747197402:93,simpl,simple,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-747197402,2,['simpl'],['simple']
Usability,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:942,learn,learn,942,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171,2,['learn'],['learn']
Usability,That should be simply `groupby='sample'` instead of multiple plot calls.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609649845:15,simpl,simply,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609649845,2,['simpl'],['simply']
Usability,"That sounds mostly right. We use the `nearest_neighbors` function from `umap`, which uses `pynndescent` if it's installed. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/neighbors/__init__.py#L270-L280. > By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy. I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. I'm definitely for being more generic about how the neighbors graph is generated and weighted. I haven't seen anything yet which looks at the character of the inaccuracies for each method, something that's probably important when they're used for classification. > What are the use cases here that you thinking of?. Mainly cases of merged graphs, like when you have multiple datasets or modalities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092:331,simpl,simple,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092,2,['simpl'],['simple']
Usability,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:462,simpl,simple,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702,2,['simpl'],['simple']
Usability,"That's good to know! That means, `filter_genes` would simply annotate the genes kept just like `highly_variable_genes`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-460689393:54,simpl,simply,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-460689393,2,['simpl'],['simply']
Usability,"That's still the case (at least for randomized PCA @Koncopd linked above), though it looks like there may be a another path forward using other solvers: https://github.com/scikit-learn/scikit-learn/issues/12794. Still needs an implementation though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-577471793:179,learn,learn,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-577471793,4,['learn'],['learn']
Usability,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/120#issuecomment-380350482:795,clear,clear,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380350482,2,['clear'],['clear']
Usability,"The CLI option is different from the config option …. no review necessary, simple fix",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2537:75,simpl,simple,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2537,1,['simpl'],['simple']
Usability,"The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053:351,simpl,simpler,351,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053,1,['simpl'],['simpler']
Usability,"The best thing to do would just be to put a link to the data here, and paste the code. It's actually easier to debug the more you can simplify the data that replicates the bug. Ideally, you could just send code that generates the data to replicate the bug. If that isn't working out, you could send me a DM on the discourse forum?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1170#issuecomment-618177408:134,simpl,simplify,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1170#issuecomment-618177408,2,['simpl'],['simplify']
Usability,"The biggest advantage would be the possibility to create such panels as shown in the example, where e.g. quality metrics have different cmaps as gene expression.; Another advantage would be that cmaps could be defined globally for each parameter, resulting in simpler plotting calls. ```; adata = sc.datasets.paul15(); adata.X = adata.X.astype('float64'); sc.pp.filter_cells(adata, min_genes=100); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); adata.uns['n_counts_all_cmap'] = 'copper'; adata.uns['n_genes_cmap'] = 'copper'; sc.pl.pca(adata, color=['paul15_clusters', 'n_counts_all', 'n_genes', 'Zyx', 'calp80', 'slc43a2'], ncols=3); ```; ![test](https://user-images.githubusercontent.com/23263654/99387978-28a55100-28d5-11eb-975d-f91211370c16.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186:260,simpl,simpler,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186,2,['simpl'],['simpler']
Usability,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480:263,simpl,simple,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480,2,['simpl'],['simple']
Usability,"The current paga plot output a n_row =1 plot, which is not very user-friendly. It would be nice to have an option to control n_col as other plot functions in scanpy do. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1203:64,user-friendly,user-friendly,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1203,1,['user-friendly'],['user-friendly']
Usability,The distance matrix you are passing might not be what sklearn wants: by densifying you'll get many zeros that sklearn probably assumes to be true zeros (it likely expects a dense distance matrix with all values actually be computed). Simply pass the data matrix `X_pca` to circumvent this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/378#issuecomment-443073890:234,Simpl,Simply,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/378#issuecomment-443073890,1,['Simpl'],['Simply']
Usability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:288,clear,clear,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,2,['clear'],['clear']
Usability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:778,learn,learn,778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,4,['learn'],['learn']
Usability,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744:251,clear,clearly,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744,2,['clear'],['clearly']
Usability,The new documentation index looks great! It's exactly what I was thinking about! Thanks so much for being so responsive!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/407#issuecomment-451605378:109,responsiv,responsive,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407#issuecomment-451605378,2,['responsiv'],['responsive']
Usability,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py; if not issparse(X):; mean = np.mean(X, axis=0, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64); var = mean_sq - mean ** 2; elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py; if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138,4,['learn'],['learn']
Usability,"The phate too makes a call to a function named `_settings_verbosity_greater_or_equal_than()`, which appears to no longer exist, preventing phate from running at all. I exchanged that call for a simple comparison that yields what I believe the old function returned. In any case, the change results in phate being able to run.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/716:194,simpl,simple,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/716,1,['simpl'],['simple']
Usability,"The previous docs PRs moved the generated docs path, which breaks old urls. This fixes that. This also adds responsive handling of the grid on the main index page.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2484:108,responsiv,responsive,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2484,1,['responsiv'],['responsive']
Usability,"The previous test failed but is not clear to me why, as it passes the local tests (anndata 0.7.5). It seems that on travis server, backed slicing requires integer indices and will not work with a boolean vector. I changed to sorted integers hoping that this will solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048:36,clear,clear,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048,2,['clear'],['clear']
Usability,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-487917974:72,simpl,simpler,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-487917974,4,['simpl'],"['simpler', 'simplified']"
Usability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:370,clear,clearly,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,2,['clear'],['clearly']
Usability,"The same error happens to me, @Blohrer . **Versions:**. > scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557:147,learn,learn,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557,2,['learn'],['learn']
Usability,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-474290381:39,learn,learn,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-474290381,2,['learn'],['learn']
Usability,"The tSNE looks ""good"" if parts of the data that you expect to be connected actually look connected in the tSNE and do not cluster apart. In your case, it looks a bit too clustered, but seeing the DiffMaps, everything turns out to be fine. The second example simply doesn't seem to have a branching.; PS: There will soon be a new default tSNE that will ensure that the correspondence between DiffMap and tSNE is better. Still, of course, both visualization methods focus on different aspects of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-310611678:258,simpl,simply,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310611678,2,['simpl'],['simply']
Usability,"The time to plot seems like an issue to me because it's such a simple plot that ends up being generated. It's not obvious to me what part of making that plot would take a long time to calculate, so maybe something unexpected is happening. I really think @falexwolf or @fidelram are in a better position to give advice on how to implement this plot, and troubleshoot matplotlib. It would be useful to see examples of the output you're getting, along with the code that generated them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-605811146:63,simpl,simple,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-605811146,2,['simpl'],['simple']
Usability,"The way pynndescent does it looks simply wrong: They knew that one specific backend caused problems, but instead of changing it only when that backend is set, they do it indiscriminately, which is pretty rude. I might be missiong something, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667061:34,simpl,simply,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667061,2,['simpl'],['simply']
Usability,"Their guide is also applicable to us, even if it uses a lot of pandas examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343#issuecomment-666343302:6,guid,guide,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666343302,2,['guid'],['guide']
Usability,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:; I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode.; <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:; <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639:192,clear,clear,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639,2,['clear'],['clear']
Usability,"There's my initial attempt, but it fails with:. ```pytb; File ""/usr/local/lib/python3.6/dist-packages/scanpy/preprocessing/simple.py"", line 169, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. If it isn't obvious to you what's wrong I'll return to it after my conference this weekend. Every hour is critical right now. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/77#issuecomment-363822066:123,simpl,simple,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/77#issuecomment-363822066,2,['simpl'],['simple']
Usability,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/415#issuecomment-452283384:31,simpl,simple,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415#issuecomment-452283384,2,['simpl'],['simple']
Usability,"Thinking about this more I think ideally we wouldn't really on the obs column name ""--"" convention and rather place and filter this based on data put in .uns, right? Just have to learn how to do that ....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-367770658:179,learn,learn,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-367770658,2,['learn'],['learn']
Usability,"This PR addresses https://scanpy.discourse.group/t/workflow-for-selecting-number-of-marker-genes-in-sc-queries-enrich/286. I wanted to have a simple interface to get the top n marker genes. Right now, `rank_genes_groups_df` only allows to threshold on logfc and pval, but especially for marker genes pval computation might not be statistically meaningful. It adds the following kind of functionality:; ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(adata, 'louvain'). print(sc.get.rank_genes_groups_df(adata, ""1"", n_top_genes=2)); ```; output is just the top 2 genes of the list.; ```; names scores logfoldchanges pvals pvals_adj; 0 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; ```. it also works for multiple groups:. ```python; print(sc.get.rank_genes_groups_df(adata, None, n_top_genes=2)); ```; ```; group names scores logfoldchanges pvals pvals_adj; 0 0 CD3D 26.250046 3.859759 4.379061e-75 2.233321e-73; 1 0 LDHB 21.207499 2.134979 1.488480e-67 5.993089e-66; 2 1 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 3 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; 4 2 LYZ 38.981312 5.096991 1.697105e-172 1.298285e-169; 5 2 CST3 34.241749 4.388617 1.448193e-149 5.539337e-147; 6 3 NKG7 34.214161 6.089183 2.356710e-55 2.575547e-53; 7 3 CTSW 24.584066 5.091688 2.026294e-39 9.118324e-38; 8 4 CD79A 52.583344 6.626956 4.032974e-84 7.713062e-82; 9 4 CD79B 32.102913 4.990217 1.958507e-51 1.872822e-49; 10 5 FTL 26.084383 1.844273 1.236398e-74 2.364611e-72; 11 5 LST1 25.554073 3.170759 5.653851e-81 4.325196e-78; 12 6 LYZ 31.497107 4.328516 9.041131e-106 6.916466e-103; 13 6 CST3 23.850258 3.281016 2.491629e-83 9.530482e-81; 14 7 CST3 33.024582 4.195395 5.768439e-136 4.412856e-133; 15 7 LYZ 31.264187 4.267053 9.712334e-101 1.485987e-98; 16 8 PPIB 39.260998 3.990153 7.159966e-47 3.651583e-45; 17 8 MZB1 33.305500 8.979518 7.611322e-26 1.878278e-24; 18 9 STMN1 27.133045 5.9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2145:142,simpl,simple,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2145,1,['simpl'],['simple']
Usability,"This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/824:244,feedback,feedback,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824,1,['feedback'],['feedback']
Usability,"This PR contains a work in progress version of development docs. I would like to have an initial version of this in the next release. Right now we just have [CONTRIBUTING.md](), but this not particularly discoverable, complete, or up to date. The idea here is to be a useful resource both to first time developers and maintainers about development processes and how contributing works. Inspirational sources (which we can probably crib from with attribution):. * [Pandas development docs](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html); * [mdanalysis contributing guide](https://userguide.mdanalysis.org/stable/contributing.html). ## Key features:. * Guidelines on external tools; * Guide for testing; * Guide for writing and building docs; * Release and merging guides",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1544:593,guid,guide,593,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544,5,"['Guid', 'guid']","['Guide', 'Guidelines', 'guide', 'guides']"
Usability,"This PR does a few things:. - Adds `fontoutline` argument to sc.pl.paga as a convenient shortcut. Similar to https://github.com/theislab/scanpy/pull/640. This can also be achieved via text_kwds but it's super verbose and less intuitive for beginners.; - Removes the usage of empty dictionaries as default arguments, because it might lead to [erroneous results in successive calls](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments). ; - `sc.pl.paga_compare` now adds `legend_font{size,weight,outline}` arguments to paga_graph_params by default which results in better agreement between the embedding view and paga view.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/806:226,intuit,intuitive,226,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806,2,"['guid', 'intuit']","['guide', 'intuitive']"
Usability,This PR updates scanpy to accommodate the latest changes made in spaceranger 2.0 which will break the released version of scanpy. Will provide backwards compatibility to pre 2.0 releases. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2296:259,guid,guidelines,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2296,2,['guid'],"['guide', 'guidelines']"
Usability,"This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python; sc.pp.neighbors(adata, method='rapids'); sc.tl.louvain(adata, flavor='rapids'); sc.tl.umap(adata, method='rapids'); ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |; | --------- | ------------ | ------------ | ------- |; | Neighbors | 47 | 15 | 3x |; | Louvain | 70 | 1 | 70x |; | UMAP | 186 | 15 | 12x |. Comments:; * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.); * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate.; * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar.; * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug.; * UMAP computes the nearest neighbors again from scratch - it can’t reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes.; * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any).; * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/830:1854,learn,learning-vm,1854,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830,1,['learn'],['learning-vm']
Usability,"This certainly looks strange. Would you mind making a minimal complete example? If you need, [here's a useful guide](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to making one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/530#issuecomment-474171747:110,guid,guide,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530#issuecomment-474171747,2,['guid'],['guide']
Usability,"This distributes the reexports in `scanpy/api/` in two modules: the core functionality goes into the root (`scanpy/__init__.py`) and the external functionality goes into `scanpy/ext/`. The structure of the latter mirrors the one of `scanpy.api` so that users simply need to adjust their imports; ```; import scanpy as sc; import scanpy.ext as sce; ```; instead of previously; ```; import scanpy.api as sc; ```. Backward compatibility for use of `scanpy.api` is fully maintained, but from now on, its use and development are deprecated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406:259,simpl,simply,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406,1,['simpl'],['simply']
Usability,"This error is certainly caused by ""scikit-learn"". I abandoned this conda environment and created a new one by `conda create -n Scanpy -c conda-forge scikit-learn`[https://scikit-learn.org/stable/install.html](url).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729:42,learn,learn,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729,6,['learn'],['learn']
Usability,"This fixes the small progress when downloading the data as `sc.datasets.paul_15()`, etc..; Also fixes removing the file when `KeyboardInterrupt` occurs (previously, it was just on exceptions and at least for me I sometimes stop a download by interupting the jupyter kernel - now it also correctly removes the partially downloaded file). I've also opted for `requests` library, since it's much cleaner than urllib3 and as a dependency.; However, this is completely unrelated to the 2 things I've mentioned above and can be quickly reverted. Relevant PRs I could find:; - tqdm version (I opted for bumping the version [search through open issues, lowest one had 4.32]): https://github.com/theislab/scanpy/issues/1244. Not relevant:; My [reaction](https://www.youtube.com/watch?v=nixR6wVa4HY&t=72) when using the progress bar (slightly NSFW [foul language]).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507:810,progress bar,progress bar,810,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507,1,['progress bar'],['progress bar']
Usability,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/592:90,learn,learn,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592,1,['learn'],['learn']
Usability,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/486:276,simpl,simple,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486,1,['simpl'],['simple']
Usability,"This is a version of Sphix-book-based scanpy docs. **What's lost?**; - Docsearch; - Edit on github button (was weird anyway as no other package does that). **What's gained?**; - Aesthetics, easier to read; - Git submodule for tutorials; - sphinx.ext.viewcode so each function has a source button (effectively replaces edit on github button). **What else changed?**; - I removed the release latest stuff, not sure what value it was adding; - API docs are split logically. View the rendered docs here:. https://icb-scanpy--2220.com.readthedocs.build/en/2220/index.html. What's left to do:. - [x] Update tutorials index page to use nbgallery feature of nbsphinx; - [x] Make home index page use a grid of cards, two columns; - [x] Add to contributing guide how to manage the git submodule (really just occasionally do `git submodule update --remote`); - [x] Annotate CSS overrides; - [x] Exchange sphinx ext viewcode for linkcode (probably in scanpydoc)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2220:747,guid,guide,747,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2220,1,['guid'],['guide']
Usability,This is a very simple change to propagate the `random_state` argument from `tool.umap` into the RAPIDS UMAP estimator.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1474:15,simpl,simple,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1474,1,['simpl'],['simple']
Usability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:404,simpl,simply,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,2,['simpl'],['simply']
Usability,"This is caused by an adjustment that tries to keep the legends closer to; the figure compared to the default placing. Clearly, there is some; dependency with the font size that I was not aware of. I will prepare a fix; soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:; >; > import scanpy.api as sc; > adata = sc.datasets.pbmc68k_reduced(); > sc.tl.pca(adata); > sc.pp.neighbors(adata); > sc.tl.umap(adata); >; > when you plot the umap of the bulk labels contained in adata.obs without; > specifying any further settings (i.e. sc.pl.umap(adata, color =; > ['bulk_labels']) ) everything looks fine.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>; >; > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,; > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller; > than the default font size it selects for your legend, the legend overlaps; > with the right edge of the plot.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>; >; > For me this sometimes leads to issues that I can no longer export figures; > with my desired fontsize for presentations, etc. without it overlapping the; > plot in an ugly way.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/374>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446:118,Clear,Clearly,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446,1,['Clear'],['Clearly']
Usability,This is great! 👍 . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924:73,simpl,simply,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924,2,['simpl'],['simply']
Usability,"This is identical with #2339. The simplest way is to downgrade `python-igraph` to `0.9.9`, https://github.com/scverse/scanpy/issues/2339#issuecomment-1261132252",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2341#issuecomment-1271333881:34,simpl,simplest,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2341#issuecomment-1271333881,2,['simpl'],['simplest']
Usability,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797848694:328,clear,clear,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797848694,2,['clear'],['clear']
Usability,This is the output of `sc.logging.print_versions()`:; ```; scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520:150,learn,learn,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520,4,['learn'],['learn']
Usability,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as ; ```; <dl class=""field-list simple"">; <dt class=""field-odd"">Parameters</dt>; ```; whereas; ```; <p class=""rubric"">Notes</p>; ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need; ```; .rst-content dl:not(.docutils) dl dt {; font-weight: bold;; font-size: 16px;; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484428516:188,simpl,simply,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484428516,6,"['learn', 'simpl']","['learn', 'simple', 'simply']"
Usability,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811:416,simpl,simple,416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811,2,['simpl'],['simple']
Usability,"This is very interesting! It would be awesome if you linked to a small example to learn what you do exactly! I guess, a PR would then be more than welcome! 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-437729769:82,learn,learn,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-437729769,2,['learn'],['learn']
Usability,"This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get ; `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:; ```; import scanpy as sc; adata = sc.datasets.pbmc3k(); sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.tsne(adata); ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:; `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/874:883,learn,learn,883,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874,1,['learn'],['learn']
Usability,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:304,clear,clear,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596,2,['clear'],['clear']
Usability,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:204,simpl,simple,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779,6,['simpl'],"['simple', 'simply']"
Usability,This matches the else branch and fixes problem when `figsize` or `color` for instance; are passed in. <!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1655:174,guid,guidelines,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1655,2,['guid'],"['guide', 'guidelines']"
Usability,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:75,learn,learn,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129,2,['learn'],['learn']
Usability,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:134,guid,guides,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141,2,['guid'],['guides']
Usability,This pr moves the (in)famous `_prepare_dataframe` function from scanpy.plotting._anndata to sc.get as `_indexed_expression_df` (bcs why would it be in plotting anyway) and implements a simple public interface called `sc.get.summarized_expresion_df` which simply provides nonzero mean/var and fraction using `_indexed_expression_df` function. . As discussed here (https://github.com/theislab/scanpy/pull/1388#issuecomment-678739734) we can use this in rank_genes_groups_df.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1390:185,simpl,simple,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1390,2,['simpl'],"['simple', 'simply']"
Usability,"This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,; We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |; | -- | -- |; | Original | 297 |; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:642,simpl,simplefilter,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['simpl'],['simplefilter']
Usability,This reverts commit 02a0f7ba1ae71d94f7ca8e1e04efec248881c746. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2694:133,guid,guidelines,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2694,2,['guid'],"['guide', 'guidelines']"
Usability,This reverts commit 284c987dedfcf2fc28dd79e682ef721ccd3ff40d. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3153:133,guid,guidelines,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3153,2,['guid'],"['guide', 'guidelines']"
Usability,This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1509#issuecomment-748156450:59,learn,learn,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-748156450,2,['learn'],['learn']
Usability,This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy. _Originally posted by @gokceneraslan in https://github.com/theislab/scanpy/issues/1509#issuecomment-748156450_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1619:59,learn,learn,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619,1,['learn'],['learn']
Usability,"This should allow people to reconstruct count data from scaled values. Moving away from accepting numpy arrrays and sparse matrices in the preprocessing functions has always been on my mind to simplify the code. Nobody is passing anything different from an AnnData. In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135:193,simpl,simplify,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135,1,['simpl'],['simplify']
Usability,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/407#issuecomment-451494959:179,feedback,feedback,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407#issuecomment-451494959,2,['feedback'],['feedback']
Usability,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst; parameter : some.type; Description; ```. Later I’ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then I’ll check if there’s a section like. ```rst; one_identifier; Desc; second_identifier; Desc; ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value; 2. The above for returning a tuple; 3. “this function adds some AnnData.obs/var fields”. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``; 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```; 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484124854:188,simpl,simple,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484124854,2,['simpl'],['simple']
Usability,"This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844:5,simpl,simplifies,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844,1,['simpl'],['simplifies']
Usability,"This was giving out-of-memory errors since scanorama's default is batch_size=None, and setting batch_size to another value wasn't doing anything. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2374:217,guid,guidelines,217,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2374,2,['guid'],"['guide', 'guidelines']"
Usability,"Those are good! Short and simple. On Sat, Mar 30, 2019 at 9:02 PM Philipp A. <notifications@github.com> wrote:. > The ones from vscode are pretty good:; > https://github.com/Microsoft/vscode/issues/new/choose; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/568#issuecomment-478229664>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AH221DYPmPqIrPkmsxrzuc2eWtN_QRLzks5vbzY9gaJpZM4cRcJj>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/568#issuecomment-478230333:26,simpl,simple,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568#issuecomment-478230333,2,['simpl'],['simple']
Usability,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```; adata = sc.read_10x_h5(""source.h5"", ""GRCh38""); adata.obs['n_counts'] = adata.X.sum(1); adata.obs['log_counts'] = np.log(adata.obs['n_counts']); adata.obs['n_genes'] = (adata.X > 0).sum(1); adata.write(""test.h5ad"", compression='gzip', compression_opts=1); ```. Then read it three different ways:. ```; In [7]: %memit a1 = sc.read(""test.h5ad""); peak memory: 2333.84 MiB, increment: 2170.77 MiB; ```. ```; In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""); peak memory: 4400.07 MiB, increment: 2137.85 MiB; ```. ```; In [9]: %memit a3 = custom_read(""test.h5ad""); peak memory: 4390.11 MiB, increment: 66.90 MiB; ```. where `custom_read` is defined as:. ```; def custom_read(filename):; adata = AnnData(); hf = h5py.File(filename); adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]; adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]; adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]; adata.obs_names = [x[0] for x in hf['obs'][()]]; adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]; adata.var_names = [x[0] for x in hf['var'][()]]; return adata; ```. and here's some verification the custom read is actually reading the data. ```; In [16]: a2.obs_keys(); Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(); Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs); Out[18]: 384000. In [19]: len(a3.obs); Out[19]: 384000. In [20]: a2.var_keys(); Out[20]: ['gene_ids']. In [21]: a3.var_keys(); Out[21]: ['gene_ids']. In [22]: len(a2.var); Out[22]: 33694. In [23]: len(a3.var); Out[23]: 33694; ```. Here's some version info I neglected in the original comment:. ```; In [28]: sc.logging.print_versions(); scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/434#issuecomment-456056835:1886,learn,learn,1886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434#issuecomment-456056835,2,['learn'],['learn']
Usability,"To answer the following question:. > 2. what you said: The original approach samples from the full list of genes in each bin, then restricts the sample to valid ones. Your approach samples from the valid genes in each bin.; >; > So if a bin e.g. contains mostly invalid genes, the original code adds only a few genes for that bin, while yours adds the maximum possible number.; >; > So the questions is: is the sampling bias introduced in the original code wanted? If not, you not only made the code more resilient, but also more objective. After going through the original [code from Seurat](https://github.com/satijalab/seurat/blob/c54e57d3423b3f711ccd463e14965cc8de86c31b/R/utilities.R#L280C3-L303), it seems to me that there's not equivalent to removing genes to be scored from the control gene set.; From what I can tell, if one of the genes to be scored happens to be chosen as the background, it will be included in the calculation.; But please correct me if that's not the case. So if the original implementation does not remove score genes from the control gene set, we would simply need to remove the following line: https://github.com/scverse/scanpy/blob/ec4457470618efd85da3c7b29f951cab01a49e3a/scanpy/tools/_score_genes.py#L169. (Note: if we want to keep the current behaviour, we should still remove the line above, since it would be redundant)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358:1085,simpl,simply,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358,2,['simpl'],['simply']
Usability,"To elaborate a bit on my comment on pull request #284 that `sc.pp.pca(adata, use_highly_variable=True)` does not reproduce the same umap embedding as subsetting the genes. I have done the following:. ```; disp_filter = sc.pp.filter_genes_dispersion(adata.X, flavor='cell_ranger', n_top_genes=4000, log=False); adata_hvg = adata.copy(); adata_hvg = adata_hvg[:, disp_filter['gene_subset']]; sc.pp.filter_genes_dispersion(adata, flavor='cell_ranger', n_top_genes=4000, log=False, subset=False). sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack'); sc.pp.neighbors(adata); sc.tl.umap(adata). sc.pp.pca(adata_hvg, n_comps=50, svd_solver='arpack'); sc.pp.neighbors(adata_hvg); sc.tl.umap(adata_hvg). sc.pl.umap(adata, color='n_counts', use_raw=False); sc.pl.umap(adata_hvg, color='n_counts', use_raw=False); ```. The umap output is:; ![screen shot 2018-10-25 at 10 37 36](https://user-images.githubusercontent.com/13019956/47487680-29c53a80-d843-11e8-8535-de3c84f3a3b3.png). There is clearly a difference between the two cases also in the tsne, draw_graph and diffmap visualisations. Only pca seems to be consistent judging by eye.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/324:1003,clear,clearly,1003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324,1,['clear'],['clearly']
Usability,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:507,simpl,simplicity,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,8,['simpl'],['simplicity']
Usability,Underlying wish: Why don't we simply ditch list-based coloring everywhere :D,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1392#issuecomment-680962922:30,simpl,simply,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1392#issuecomment-680962922,2,['simpl'],['simply']
Usability,"Unfortunately, all of this discussion here was not really further pursued, I have to admit. In principle, these are very simple things. However, I'm a bit afraid of offering a canonical function as I fear that there are also a lot of bad ways of visualizing gene correlation plots and I don't feel capable of judging this. If no one else wants to make a pull request for that (maybe using what @tcallies already did, but I fear it's not really serving the purpose of the discussion here: [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/top_genes.py), [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/plotting/top_genes_visual.py)) it would be cool if someone sent me an example case, which clearly shows what you want. Maybe @jorvis, you can send images for the examples you have in mind?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-399897165:121,simpl,simple,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-399897165,4,"['clear', 'simpl']","['clearly', 'simple']"
Usability,"Unfortunately, changing; ```; .rst-content dl:not(.docutils) dl dt {; font-weight: normal;; ```; to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though; ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484027492:250,learn,learn,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484027492,2,['learn'],['learn']
Usability,"Update `pyproject.toml` to use [PEP-621](https://www.python.org/dev/peps/pep-0621/) and [PEP-631](https://www.python.org/dev/peps/pep-0631/) metadata. Basically, simplify by removing most of the `tool.flit` stuff. @flying-sheep, would you like to do this/ do you foresee any blockers?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1776:162,simpl,simplify,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776,1,['simpl'],['simplify']
Usability,Update contributing guide to include tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/772:20,guid,guide,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/772,2,['guid'],['guide']
Usability,Update for cope with issue introduced in umap-learn 0.5.2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028:46,learn,learn,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028,2,['learn'],['learn']
Usability,Update release guide,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2184:15,guid,guide,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2184,2,['guid'],['guide']
Usability,Updating release guide to contain current practices.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2184:17,guid,guide,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2184,1,['guid'],['guide']
Usability,Upgrading scikit-learn from 0.20.1 to 0.21.1 messes up paul15 example,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/665:17,learn,learn,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/665,2,['learn'],['learn']
Usability,"Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2508:155,guid,guidelines,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508,2,['guid'],"['guide', 'guidelines']"
Usability,Use simplified dendrogram for speed,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/715:4,simpl,simplified,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/715,2,['simpl'],['simplified']
Usability,"Using the progress bar from tqdm.auto causes a `ImportError` when `ipywidgets` is not installed. ; The progressbar from the top level `tqdm` module does not have this dependency. . Repex: . ```python; import scanpy as sc; sc.datasets.moignard15(); ```. Output: ; ```; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols); 97 else: # No total? Show info style bar with no progress tqdm status; ---> 98 pbar = IProgress(min=0, max=1); 99 pbar.value = 1. NameError: name 'IProgress' is not defined. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-5-ec5b1e8cd660> in <module>; ----> 1 sc.datasets.moignard15(). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/datasets/__init__.py in moignard15(); 108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filena",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:10,progress bar,progress bar,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['progress bar'],['progress bar']
Usability,"Vector images would be rasterized when `settings._vector_friendly` is True. This should be fixed so that when `settings._vector_friendly` is True, image is NOT rasterized, and when False, image should be rasterized. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1405; - [x] Tests included or not required because: simple reversion of booleans; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: did not create a release on this",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3181:287,guid,guidelines,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3181,3,"['guid', 'simpl']","['guide', 'guidelines', 'simple']"
Usability,"Was playing around with an incremental PCA earlier and saw the arguments were undocumented. Figured I could document it while I was waiting for it to run. I think I got all the style right, let me know if not.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/250:78,undo,undocumented,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/250,1,['undo'],['undocumented']
Usability,We are submitting PR for speed up of the filtering; | | Time |; | -- | -- |; | Original | 290.59 |; | Updated | 187.03 |; | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge; <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3330:256,guid,guidelines,256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330,2,['guid'],"['guide', 'guidelines']"
Usability,"We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers.; Hi,; We are submitting PR for speed up of the _get_mean_var function.; | | Time(sec) |; | -- | -- |; | Original | 18.49 |; | Updated | 3.97 |; | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge; ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:564,simpl,simplefilter,564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['simpl'],['simplefilter']
Usability,"We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers.; This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge.; ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279:193,learn,learn-intelex,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279,2,"['learn', 'simpl']","['learn-intelex', 'simplefilter']"
Usability,"We have images which are saved which can be compared against, typically one would add a new image which you have visually confirmed to be correct, so if the behavior changes later we know. There is a little more about this in the [contribution guide](https://scanpy.readthedocs.io/en/stable/dev/testing.html#plotting-tests).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248:244,guid,guide,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248,2,['guid'],['guide']
Usability,"We have replaced the existing nearest neighbor implementation from umap with scikit-learn's implementation. By using Intel extension for scikit-learn, this implementation can be upto 2x faster for large datasets.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2426:84,learn,learn,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2426,2,['learn'],['learn']
Usability,"We have some failing tests due to a couple bugs introduced in pandas 1.3.0:. * https://github.com/pandas-dev/pandas/issues/42380. I think this one is small in scope. Has problems when `df.agg` is called, when all columns are categorical and index is non-unique. Definitely a bug in pandas, and I don't think we do this much. Switching to `df.apply` works around the problem. * https://github.com/pandas-dev/pandas/issues/42376. Assignment of single columns `np.matrix` to dataframe columns no longer works as if the matrix were a 1d array. I think this is a bug since it's an undocumented behaviour change. Fixes are pretty easy, since we can just wrap occurrences with `np.ravel`, however I wouldn't be surprised if there were many places in the codebase that this happened. I would be good if these didn't happen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1917:576,undo,undocumented,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1917,1,['undo'],['undocumented']
Usability,"We should make dynamic 3D plots ;-) . If I remember correctly, in the past we have the issue that the categorical colors were given by the adata.obs order and we change them such that they follow the order of the categories. Yet, I agree that a good mix of categorical colors is good sometimes. To address this issue I think that we can simply randomize the order if `sort_order=False` to avoid adding any new parameters. . Isaac's solution looks great for dealing with of lots of cells, something that I imagine will become more frequent. I think we should have a 'cookbook' where we can keep this and other information. I find this better than adding more and more functionality to the scatter plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279:337,simpl,simply,337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279,2,['simpl'],['simply']
Usability,"We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646:382,simpl,simple,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646,1,['simpl'],['simple']
Usability,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302:328,clear,cleared,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302,1,['clear'],['cleared']
Usability,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:861,learn,learning,861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609,4,"['learn', 'simpl']","['learning', 'simply']"
Usability,"We've been dealing with long queue times for CI builds. This is at least partially because for each PR four jobs start, each of which takes at least 12 minutes. Since travis gives us at most five concurrent jobs, only one PR can be built at a time. This becomes worse if a PR is based on a branch on the main repo, since CI runs on those too. Azure offers 10 free concurrent jobs. Seems like an easy win. * 10 free concurrent jobs; * Easier to do multiple checks per build (i.e. linting and testing can happen in the same build, but be independent checks); * Output looks easy to navigate, has good integration with github; * We could test on windows (depending on how hard this is to set up); * (possible) Some projects seem to use multiple cores for testing. Cons:. * New system, will take some time to learn; * Maybe microsoft will start being evil again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358:805,learn,learn,805,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358,1,['learn'],['learn']
Usability,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:569,guid,guides,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984,2,['guid'],['guides']
Usability,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676#issuecomment-499043170:28,undo,undocumented,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676#issuecomment-499043170,2,['undo'],['undocumented']
Usability,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:216,learn,learn,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,2,['learn'],['learn']
Usability,"What about comparing communities between for example CPM and RBERVertexPartition at the same resolution, using modularity score? This way we are not comparing scores obtained by optimization functions, just simple ""external"" measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127:207,simpl,simple,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127,2,['simpl'],['simple']
Usability,"What happened?. The `pca_params` fixture randomly picks one of the possible solvers, but since a little while, the `""lobpcg""` solver fails for our small test data, [as explained by its docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lobpcg.html):. > The LOBPCG code internally solves eigenproblems of the size 3k on every iteration by calling the dense eigensolver eigh, so if k is not small enough compared to n, it makes no sense to call the LOBPCG code. Moreover, if one calls the LOBPCG algorithm for 5k > n, it would likely break internally, so the code calls the standard function eigh instead. It is not that n should be large for the LOBPCG to work, but rather the ratio n / k should be large. It you call LOBPCG with k=1 and n=10, it works though n is small. The method is intended for extremely large n / k. The workaround was to rerun failed tests until the fixture randomly returned another solver, which isn’t great. I therefore simply `xfail` that test for now to not block PRs in https://github.com/scverse/scanpy/pull/2745. ### Minimal code sample. ```bash; # try one of the following to reproduce (if necessary multiple times); pytest --runxfail 'scanpy/tests/test_pca.py::test_pca_warnings[scipy_csr-zero_center-valid]'; pytest --runxfail 'scanpy/tests/test_pca.py::test_pca_warnings[scipy_csc-zero_center-valid]'; ```. ### Error output. (note that since the code is run with `warnings.simplefilter('error')`, the below is an error that fails the test). ```pytb; UserWarning: The problem size 5 minus the constraints size 0 is too small relative to the block size 4. Using a dense eigensolver instead of LOBPCG iterations.No output of the history of the iterations.; ```. ### Versions. <details>. ```; -----; anndata 0.10.2; scanpy 1.10.0.dev156+gd1a2c8f8.d20231110; -----; PIL 10.0.1; asciitree NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2023.10.0; dateutil 2.8.2; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jinja2 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2744:1248,simpl,simply,1248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2744,1,['simpl'],['simply']
Usability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:43,simpl,simple,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,2,['simpl'],['simple']
Usability,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:416,learn,learn,416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825,2,['learn'],['learn']
Usability,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:1431,learn,learn,1431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,4,['learn'],['learn']
Usability,"What stays the same:. - `pip install scanpy`; - `pip install . `; - `pip install git+https://...`; - you can install your deps with conda; - you can do a dev install. What changes:. - Please check the [install docs](https://scanpy.readthedocs.io/en/flit-for-isaac/installation.html#development-version), in short:; - `pip install -e .[every,single,extra]` → `flit install -s` for dev installs; - `beni pyproject.toml > environment.yml` for conda; - Extremely simple `flit build` and `flit publish`. Maybe install `keyring` to store your publish password, and you know everything you need to.; - `flit build` doesn’t clutter your dev directory with `build/` and `*.egg-info/` junk, it just creates `dist/scanpy-*{.whl,.tar.gz}`.; - No more obscure stuff nobody understands (MANIFEST.in, package_data, …); - Centralized setup configuration in pyproject.toml instead of spread over multiple files",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527:459,simpl,simple,459,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527,1,['simpl'],['simple']
Usability,"When I import Scanpy, I go this output:; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:; sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:; ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png); GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316:144,learn,learn,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316,2,['learn'],['learn']
Usability,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:288,learn,learn,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,4,['learn'],['learn']
Usability,"When I run this. ```; import scanpy as sc. sc.logging.print_versions(). pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[pbmc.obs['louvain'] == '0', :]; sc.pp.scale(pbmc); ```. I get this. ```; scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.2 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0; /stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:1120: RuntimeWarning: invalid value encountered in sqrt; scale = np.sqrt(var); Trying to set attribute `.X` of view, making a copy.; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 860, in scale; scale(adata.X, zero_center=zero_center, max_value=max_value, copy=False); File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 876, in scale; _scale(X, zero_center); File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 1126, in _scale; scale[scale == 0] = 1e-12; File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/anndata/base.py"", line 392, in __setitem__; getattr(adata_view, attr_name)[idx] = value; IndexError: boolean index did not match indexed array along dimension 0; dimension is 130 but corresponding boolean dimension is 765; ```. Any idea what's going on?. Thanks,. -Eric",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/699:286,learn,learn,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699,1,['learn'],['learn']
Usability,"When less than 30 features are present in adata.X, pca_loadings will plot some components twice.; The patch solves the problem by adding an n_point parameter (default value 30, same as anndata.ranking())that is then passed to anndata.ranking(). . <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2061:318,guid,guidelines,318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061,2,['guid'],"['guide', 'guidelines']"
Usability,"When less than 30 features are present in adata.X, pca_loadings will plot some components twice.; The patch solves the problem by adding an n_point parameter (default value 30, same as anndata.ranking())that is then passed to anndata.ranking(). the patch also modifies rankings plot to account for remove the dots when all elements in order_scores are plotted; the patch replaces the previously submitted patch. ps: had to struggle a lot with gi. the contribution guide should be updated... I may work on it",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075:464,guid,guide,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075,1,['guid'],['guide']
Usability,Why are we using `umap.__version__` instead of `importlib.metadata.version('umap-learn')`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898544845:81,learn,learn,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898544845,2,['learn'],['learn']
Usability,Why is nothing happening. <!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1685:98,guid,guidelines,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1685,2,['guid'],"['guide', 'guidelines']"
Usability,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:471,learn,learn,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250,4,['learn'],['learn']
Usability,"Why not simply as in the [tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)?; ```; sc.tl.rank_genes_groups(adata, 'louvain', groups=['0'], reference='1'); ```; Or am I missing your problem? A few lines of code documenting your call wouldn't hurt.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/346#issuecomment-436337697:8,simpl,simply,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-436337697,2,['simpl'],['simply']
Usability,"Why not using https://nbsphinx.readthedocs.io? It works completely fine for me. So, I would simply moved forward with it as soon as there is some bandwidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302#issuecomment-441476438:92,simpl,simply,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441476438,2,['simpl'],['simply']
Usability,"With respect to the heatmap, indeed it is possible to transpose the matrix.; Currently, this option is only available for `stacked_violin`. I thought; about adding this option to other plots like heatmap, matrixplot and; dotplot but I have not find the time and it is always possible to save the; figure and rotate it so it has low priority for me. The changes are not as; trivial as simply rotating the matrix as all other elements need to be; adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this...; > 😄; >; > —; > You are receiving this because you were mentioned.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839:384,simpl,simply,384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839,2,['simpl'],['simply']
Usability,"With the most recent version of scanpy, I'm getting the above mentioned warning, which causes the [`dynverse/ti_paga`](https://travis-ci.org/dynverse/ti_paga/builds/544731805) builds to fail. . The problem can be replicated by building the `dynverse/ti_paga` container manually and trying to run it. For the sake of simplicity, I have built the container for you and pushed it to `dynverse/ti_paga_issue`. ### Minimum reproducible example; Inside terminal:; ```bash; # fetch newest dynverse/ti_paga container in which this problem occurs; docker pull dynverse/ti_paga_issue. # enter the container ; docker run --entrypoint bash -it dynverse/ti_paga_issue. # create an example dataset and save it at /input.h5; /code/example.sh /input.h5. # enter python; python; ```; Inside python; ```python; import dynclipy; task = dynclipy.main([""--dataset"", ""/input.h5"", ""--output"", ""/output.h5""]). import scanpy.api as sc; import anndata. counts = task[""counts""]. adata = anndata.AnnData(counts); sc.pp.recipe_zheng17(adata, n_top_genes=101); sc.tl.pca(adata, n_comps=50); sc.pp.neighbors(adata, n_neighbors=15); ```; Which generates the following warning:; ```; /usr/local/lib/python3.7/site-packages/umap/umap_.py:349: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 467:; def fuzzy_simplicial_set(; <source elided>; if knn_indices is None or knn_dists is None:; knn_indices, knn_dists, _ = nearest_neighbors(; ^. @numba.jit(); /usr/local/lib/python3.7/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. self.func_ir.loc)); /usr/local/lib/python3.7/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/688:316,simpl,simplicity,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688,1,['simpl'],['simplicity']
Usability,Would be a nice use case for working on the usability of benchmarks (related to #2977),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2986#issuecomment-2044397793:44,usab,usability,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2986#issuecomment-2044397793,2,['usab'],['usability']
Usability,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1601#issuecomment-763294926:69,simpl,simple,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601#issuecomment-763294926,2,['simpl'],['simple']
Usability,"Wow! Again simply awesome! :smile:. PS: Sorry for the late response, I was on holidays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/175#issuecomment-398683241:11,simpl,simply,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175#issuecomment-398683241,2,['simpl'],['simply']
Usability,"Wow, I learned something new! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/896#issuecomment-547448945:7,learn,learned,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896#issuecomment-547448945,2,['learn'],['learned']
Usability,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821:41,clear,clear,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821,2,['clear'],['clear']
Usability,"Yeah, I agree. Go for it ;) Saturday, 23 January 2021, 11:48AM +01:00 from Philipp A. notifications@github.com :. >Flit has a very tiny surface area. You can learn its full CLI in literally 2 minutes, as it doesn’t include any kind of new concept (like Poetry’s venv management).; >—; >You are receiving this because you commented.; >Reply to this email directly, view it on GitHub , or unsubscribe .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904369:158,learn,learn,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904369,2,['learn'],['learn']
Usability,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-456715454:203,intuit,intuitive,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-456715454,2,['intuit'],['intuitive']
Usability,"Yeah, I was concerned about the complexity. But AnnData doesn’t do anything wrong, it just uses chunks in both directions, which that one specific algorithm doesn’t support. But I think in general we should test for 2D chunks, that’s why I think AnnData’s test helpers work as intended. Any idea how to do this more simply?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245560938:316,simpl,simply,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245560938,2,['simpl'],['simply']
Usability,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here?. Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003:40,simpl,simple,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003,2,['simpl'],['simple']
Usability,"Yeah, plotting in python is difficult, and our plotting code doesn’t make things simpler. This is the relevant part:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_anndata.py#L273-L284. The code that behaves like advertised in the docs is in here, but that function does more things after that:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_utils.py#L364. There’s also this:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_tools/scatterplots.py#L1181. I think things should be unified so they use the same palette selection logic. But I understand that that’s a pretty complex part of our code base. I meant this comment: https://github.com/scverse/scanpy/issues/1258#issuecomment-1626690231",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114:81,simpl,simpler,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114,2,['simpl'],['simpler']
Usability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:389,simpl,simply,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,2,['simpl'],['simply']
Usability,"Yes , the sampling is done with weights and I used the coreset technique; for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>; wrote:. > I understand the benefits of sampling regarding computational speed up.; > What I'm not clear on is how you choose your weights for the calculations; > you perform here. You mentioned that you get wrong marker gene results when; > you sample and don't use weights. That makes sense if you get a; > non-representative set of cells in your sample. I wonder how you select the; > weights to fix this. I guess you don't just try a lot of different values; > until one works, right?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494:254,clear,clear,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494,2,['clear'],['clear']
Usability,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:; ```; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version; return distribution(package).version; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution; return Distribution.from_name(package); File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name; raise PackageNotFoundError(name); importlib_metadata.api.PackageNotFoundError: umap-learn; ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512160342:323,learn,learn,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512160342,4,['learn'],['learn']
Usability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:627,simpl,simply,627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,2,['simpl'],['simply']
Usability,"Yes, but I’m not happy about the spaghetti code in pl.scatter. We should make pl.embedding and pl.scatter share most of their code, and make that code as simple as @VolkerBergen’s.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553945910:154,simpl,simple,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553945910,2,['simpl'],['simple']
Usability,"Yes, we already have a good mask for sparse scaling. Boolean arrays are very effective for indicating where computations should be performed, as they eliminate the need for copying and reintegration. One clear example is the `tl.score_genes` function. masks there as booleans for the nanmean is a lot more efficent but less pythonic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711:204,clear,clear,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711,2,['clear'],['clear']
Usability,"Yes, we can simply add @bebatut's packages as an `ext` flag in the setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/305#issuecomment-435734577:12,simpl,simply,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-435734577,2,['simpl'],['simply']
Usability,"Yes, you simply pass a single gene name to the violin plot... :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370355474:9,simpl,simply,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370355474,2,['simpl'],['simply']
Usability,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:188,simpl,simply,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169,4,['simpl'],['simply']
Usability,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:471,responsiv,responsive,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,4,"['learn', 'responsiv']","['learning', 'responsive']"
Usability,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will; give you a gradient from a clear to a darker color. Maybe that helps but I; agree with Philipp, 120 clusters is a lot to visualize with different; colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-496543294:101,clear,clear,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-496543294,2,['clear'],['clear']
Usability,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846:60,learn,learn,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846,2,['learn'],['learn']
Usability,"You can pass `legend_loc=None`. Thanks for noting this. Looking at the docs, I can definitely see how this isn't clear. ```; legend_loc : str, optional (default: 'right margin'); Location of legend, either `'on data'`, `'right margin'` or a valid keyword; for the `loc` parameter of :class:`~matplotlib.legend.Legend`.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1502#issuecomment-730259879:113,clear,clear,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-730259879,2,['clear'],['clear']
Usability,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```; def sparse_pca(X,npcs,mu = None):; # X -- scipy sparse data matrix; # npcs -- number of principal components; # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features; if mu is None: ; mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means; mmat = mdot = mu.dot ; # dot product operator for the transposed means; mhmat = mhdot = mu.T.dot ; # dot product operator for the data; Xmat = Xdot = X.dot ; # dot product operator for the transposed data; XHmat = XHdot = X.T.conj().dot ; # dot product operator for a vector of ones; ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means; def matvec(x): ; return Xdot(x) - mdot(x); def matmat(x): ; return Xmat(x) - mmat(x); def rmatvec(x): ; return XHdot(x) - mhdot(ones(x)); def rmatmat(x): ; return XHmat(x) - mhmat(ones(x)); ; # construct the LinearOperator; XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,; matmat = matmat,; shape = X.shape,; rmatvec = rmatvec, rmatmat = rmatmat); ; u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs); ; # i like my eigenvalues sorted in decreasing order; idx = np.argsort(-s); S = np.diag(s[idx]); # principal components; pcs = u[:,idx].dot(S) ; # equivalent to PCA.components_ in sklearn ; components_ = v[idx,:] ; return pcs,components_; ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727:100,learn,learn,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727,2,['learn'],['learn']
Usability,"You just need to specify `key=` to `rank_genes_groups_dotplot`, as described in the **Examples** section of the [documentation of `filter_rank_genes_groups`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.filter_rank_genes_groups.html):. > ```py; > sc.pl.rank_genes_groups(adata, key='rank_genes_groups_filtered'); > # visualize results using dotplot; > sc.pl.rank_genes_groups_dotplot(adata, key='rank_genes_groups_filtered'); > ```. I don’t know if we can make it clearer. If you have any suggestions, or if I misinterpreted what you want, please leave a comment!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3049#issuecomment-2106955398:479,clear,clearer,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3049#issuecomment-2106955398,2,['clear'],['clearer']
Usability,"You would like the legend on the right to have both the short and long names, while the ""on data"" annotation would only have the short names – right?. I'm not sure I can think of a great way to do this without making the process complicated. It may be easier for you to add the text labels to the plot yourself. Here is some code for adding the labels with `""adjustText""` (which needs some parameter fiddling to look nice) #1513. ```python; def gen_mpl_labels(; adata, groupby, exclude=(), ax=None, adjust_kwargs=None, text_kwargs=None; ):; if adjust_kwargs is None:; adjust_kwargs = {""text_from_points"": False}; if text_kwargs is None:; text_kwargs = {}. medians = {}. for g, g_idx in adata.obs.groupby(groupby).indices.items():; if g in exclude:; continue; medians[g] = np.median(adata.obsm[""X_umap""][g_idx], axis=0). if ax is None:; texts = [; plt.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items(); ]; else:; texts = [ax.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items()]. adjust_text(texts, **adjust_kwargs); ```. which can be simplified if you're alright with just plotting on the medians. I personally think interactivity and hover-over becomes quite useful at this point, though that can be difficult to scale and share.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2112#issuecomment-1015716949:1067,simpl,simplified,1067,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2112#issuecomment-1015716949,2,['simpl'],['simplified']
Usability,You're calling this function in the wrong way. Simply type; ```; adata.write_loom('myfilename.loom'); ```; if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/154#issuecomment-389316074:47,Simpl,Simply,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389316074,1,['Simpl'],['Simply']
Usability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:970,simpl,simple,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,2,['simpl'],['simple']
Usability,[WIP] Simplify plotting,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1109:6,Simpl,Simplify,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109,1,['Simpl'],['Simplify']
Usability,"[`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged; - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471; - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 ; *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the “euclidean” condition makes not much sense IMHO; - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'` … ; https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617; - I also changed `method` to only mean “connectivity method”. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR; - [x] figure out what the `_more_tags` methods are ; - [x] allow specifying algorithm and/or backend; - [x] revert 75c6670, move connectivities code out of backends; - [x] switch our stuff to KNeighborsTransformer; - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity); - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*; - [x] check out where we have coverage; - is there paga specific stuff? *not in the parts I changed*; - gauss: dense matrix when knn=True (“build a symmetric mask”, …) *not covered, but also the logic shouldn’t have changed*; - pre-computed in umap transformer; handlin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:59,learn,learn,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,1,['learn'],['learn']
Usability,"\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 207, in embedding; use_raw=use_raw, gene_symbols=gene_symbols,; File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 865, in _get_color_values; values = adata.raw.obs_vector(value_to_plot); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 413, in obs_vector; idx = self._normalize_indices((slice(None), k)); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 364, in _normalize_indices; var = _normalize_index(var, self.var_names); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 155, in _normalize_index; return name_idx(index); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 142, in name_idx; .format(i)); IndexError: Key ""XKR4"" is not valid observation/variable name/index. ```; However, the gene XKR4 did exist in the var_names:; ```; >>> post_adata.var_names; Index(['XKR4', 'RP1', 'SOX17', 'MRPL15', 'LYPLA1', 'TCEA1', 'RGS20', 'ATP6V1H',; 'OPRK1', 'NPBWR1',; ...; '2700089I24RIK', 'RAB11FIP2', 'E330013P04RIK', 'NANOS1', 'EIF3A',; 'FAM45A', 'SFXN4', 'PRDX3', 'GRK5', 'CSF2RA'],; dtype='object', length=16249); ```. The anndata object looked as below and it was fine when I tried to show the louvain clusters:. ```; >>> post_adata; AnnData object with n_obs × n_vars = 88291 × 16249; obs: 'CellID', 'batch_indices', 'labels', 'local_means', 'local_vars', 'louvain', 'clusters'; var: 'gene_id'; uns: 'neighbors', 'louvain', 'louvain_colors'; obsm: 'X_scVI', 'X_umap'. >>> sc.pl.umap(post_adata, color=['louvain']); ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. #### Versions:; scanpy==1.4.5.post3 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.3.2 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1+5.3b99dbf6 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1039:2402,learn,learn,2402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039,1,['learn'],['learn']
Usability,\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (7.1.1); Requirement already satisfied: networkx>=2.3 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.6.3); Requirement already satisfied: importlib-metadata>=0.7 in c:\users\charles\anaconda3\lib\si,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:1273,learn,learn,1273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,2,['learn'],['learn']
Usability,"_,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5762 kwargs.setdefault('snap', False); 5763 ; -> 5764 collection = mcoll.PolyCollection(verts, **kwargs); 5765 ; 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs); 164 ; 165 self._path_effects = None; --> 166 self.update(kwargs); 167 self._paths = None; 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v); 910 func = getattr(self, 'set_' + k, None); 911 if not callable(func):; --> 912 raise AttributeError('Unknown property %s' % k); 913 return func(v); 914 . AttributeError: Unknown property standard_scale; ```; Any idea of why I'm getting this? . Package info:. ```; scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Thank you!. PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/559:3196,learn,learn,3196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559,1,['learn'],['learn']
Usability,_0 conda-forge; pyro-ppl 1.8.6 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 py39hca03da5_0 ; python 3.9.18 hd7ebdb9_1_cpython conda-forge; python-dateutil 2.9.0 pyhd8ed1ab_0 conda-forge; python-fastjsonschema 2.16.2 py39hca03da5_0 ; python-igraph 0.11.5 py39hd012b80_1 conda-forge; python-json-logger 2.0.7 py39hca03da5_0 ; python-tzdata 2024.1 pyhd8ed1ab_0 conda-forge; python_abi 3.9 4_cp39 conda-forge; pytorch 1.12.1 cpu_py39h6ba7f14_0 ; pytorch-lightning 1.9.4 pyhd8ed1ab_1 conda-forge; pytz 2024.1 py39hca03da5_0 ; pyyaml 6.0.1 py39h0f82c59_1 conda-forge; pyzmq 25.1.2 py39h313beb8_0 ; qt-main 5.15.8 hf679f28_21 conda-forge; qtconsole 5.5.1 py39hca03da5_0 ; qtpy 2.4.1 py39hca03da5_0 ; re2 2023.02.01 hb7217d7_0 conda-forge; readline 8.2 h1a28f6b_0 ; referencing 0.30.2 py39hca03da5_0 ; requests 2.32.2 py39hca03da5_0 ; rfc3339-validator 0.1.4 py39hca03da5_0 ; rfc3986-validator 0.1.1 py39hca03da5_0 ; rich 13.7.1 pyhd8ed1ab_0 conda-forge; rpds-py 0.10.6 py39hf0e4da2_0 ; scanpy 1.8.1 pypi_0 pypi; scikit-learn 1.1.2 py39h598ef33_0 conda-forge; scikit-misc 0.3.1 pypi_0 pypi; scipy 1.13.1 py39h3d5391c_0 conda-forge; scvi-tools 0.20.3 pyhd8ed1ab_0 conda-forge; seaborn 0.12.2 hd8ed1ab_0 conda-forge; seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge; send2trash 1.8.2 py39hca03da5_0 ; session-info 1.0.0 pyhd8ed1ab_0 conda-forge; setuptools 69.5.1 py39hca03da5_0 ; sinfo 0.3.4 pypi_0 pypi; sip 6.7.12 py39h313beb8_0 ; six 1.16.0 pyh6c4a22f_0 conda-forge; sniffio 1.3.0 py39hca03da5_0 ; soupsieve 2.5 py39hca03da5_0 ; sqlite 3.45.3 h80987f9_0 ; stack_data 0.2.0 pyhd3eb1b0_0 ; statsmodels 0.14.2 py39h161d348_0 conda-forge; stdlib-list 0.10.0 pyhd8ed1ab_0 conda-forge; tbb 2021.8.0 h48ca7d4_0 ; terminado 0.17.1 py39hca03da5_0 ; texttable 1.7.0 pyhd8ed1ab_0 conda-forge; threadpoolctl 3.5.0 pyhc1e730c_0 conda-forge; tinycss2 1.2.1 py39hca03da5_0 ; tk 8.6.14 h6ba3021_0 ; tomli 2.0.1 py39hca03da5_0 ; toolz 0.12.1 pyhd8ed1ab_0 conda-forge; torchmetrics 1.0.3 pyhd8ed1ab_0 conda-forge; tornado 6.4.1 py39h,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:14480,learn,learn,14480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['learn'],['learn']
Usability,_3 anaconda; pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda; pillow 10.0.0 py39h1641143_0 conda-forge; pip 23.2 pyhd8ed1ab_0 conda-forge; platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge; pooch 1.7.0 pyha770c72_3 conda-forge; prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda; pthread-stubs 0.4 h27ca646_1001 conda-forge; ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda; pure_eval 0.2.2 pyhd3eb1b0_0 anaconda; py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge; pygments 2.11.2 pyhd3eb1b0_0 anaconda; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; pytables 3.8.0 py39h0da393b_2 conda-forge; python 3.9.16 hea58f1e_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge; python_abi 3.9 3_cp39 conda-forge; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyzmq 22.3.0 py39hc377ac9_2 anaconda; readline 8.2 h92ec313_1 conda-forge; requests 2.31.0 pyhd8ed1ab_0 conda-forge; scanpy 1.7.2 pyhdfd78af_0 bioconda; scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge; scipy 1.11.1 py39ha6b2cbd_0 conda-forge; seaborn 0.12.2 hd8ed1ab_0 conda-forge; seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge; setuptools_scm 7.1.0 hd8ed1ab_0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; snappy 1.1.10 h17c5cce_0 conda-forge; stack_data 0.2.0 pyhd3eb1b0_0 anaconda; statsmodels 0.14.0 py39h8a366b7_1 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; tbb 2021.9.0 hffc8910_0 conda-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.12 he1e0b03_0 conda-forge; tomli 2.0.1 pyhd8ed1ab_0 conda-forge; tornado 6.1 py39h1a28f6b_0 anaconda; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.1.1 pyhd3eb1b0_0 anaconda; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023c h71feb2d_0 conda-forge; umap-learn 0.5.3 py39h2804cbe_1 conda-forge; u,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564:7168,learn,learn,7168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564,1,['learn'],['learn']
Usability,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'; obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/933:2621,simpl,simply,2621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933,1,['simpl'],['simply']
Usability,"_No response_. ### Versions. <details>. ```; -----; anndata 0.8.0; scanpy 1.9.3; -----; OpenSSL 19.0.0; PIL 10.0.0; apport_python_hook NA; backcall 0.2.0; certifi 2019.11.28; cffi 1.15.0; chardet 3.0.4; cloudpickle 2.2.1; colorama 0.4.3; colorcet 3.0.1; cryptography 2.8; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.5.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; gseapy 1.0.5; h5py 3.7.0; idna 2.8; igraph 0.10.6; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.10.0; llvmlite 0.39.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.6.1; matplotlib_inline NA; more_itertools NA; mpl_toolkits NA; natsort 8.2.0; netifaces 0.10.4; numba 0.56.3; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 1.5.3; parso 0.8.2; patsy 0.5.3; pexpect 4.6.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.15.0; prompt_toolkit 3.0.20; psutil 5.9.4; ptyprocess 0.7.0; pyarrow 12.0.1; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pyparsing 2.4.7; pytz 2022.4; requests 2.22.0; scipy 1.10.1; seaborn 0.12.0; session_info 1.0.0; setuptools 68.0.0; simplejson 3.16.0; sitecustomize NA; six 1.14.0; sklearn 1.3.0; socks 1.7.1; statsmodels 0.14.0; storemagic NA; tblib 2.0.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.1; toolz 0.12.0; tornado 6.1; traitlets 5.1.0; typing_extensions NA; urllib3 1.25.8; wcwidth 0.2.5; yaml 5.3.1; zipp NA; zmq 22.3.0; zope NA; -----; IPython 7.28.0; jupyter_client 7.0.6; jupyter_core 4.8.1; notebook 6.4.5; -----; Python 3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]; Linux-5.15.0-1040-aws-x86_64-with-glibc2.29; -----; Session information updated at 2023-08-11 23:46; sc.pl.stacked_violin(adata,['GATA3','CD8A','CD4'],groupby='sample_id',cmap='PuRd', order=new_order); sc.pl.stacked_violin(adata,['GATA3','CD8A','CD; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2611:2393,simpl,simplejson,2393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611,1,['simpl'],['simplejson']
Usability,"_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. ""; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil); ; ~/m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:7187,Simpl,SimpleTimer,7187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['Simpl'],['SimpleTimer']
Usability,"_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel); 286 X.eliminate_zeros(); 287 ; --> 288 obs_metrics = describe_obs(; 289 adata,; 290 expr_type=expr_type,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel); 119 for qc_var in qc_vars:; 120 obs_metrics[f""total_{expr_type}_{qc_var}""] = (; --> 121 X[:, adata.var[qc_var].values].sum(axis=1); 122 ); 123 if log1p:. C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\_index.py in __getitem__(self, key); 49 return self._get_sliceXslice(row, col); 50 elif col.ndim == 1:; ---> 51 return self._get_sliceXarray(row, col); 52 raise IndexError('index results in >2 dimensions'); 53 elif row.ndim == 1:. C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\csr.py in _get_sliceXarray(self, row, col); 321 ; 322 def _get_sliceXarray(self, row, col):; --> 323 return self._major_slice(row)._minor_index_fancy(col); 324 ; 325 def _get_arrayXint(self, row, col):. C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\compressed.py in _minor_index_fancy(self, idx); 737 """"""; 738 idx_dtype = self.indices.dtype; --> 739 idx = np.asarray(idx, dtype=idx_dtype).ravel(); 740 ; 741 M, N = self._swap(self.shape). C:\ProgramData\Anaconda3\lib\site-packages\numpy\core\_asarray.py in asarray(a, dtype, order, like); 100 return _asarray_with_like(a, dtype=dtype, order=order, like=like); 101 ; --> 102 return array(a, dtype, copy=False, order=order); 103 ; 104 . ValueError: cannot convert float NaN to integer; ```. #### Versions. <details>. scanpy==1.7.1 anndata==0.7.5 umap==0.5.1 numpy==1.20.1 scipy==1.6.1 pandas==1.1.3 scikit-learn==0.23.2 statsmodels==0.12.0 python-igraph==0.9.0 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1708:2887,learn,learn,2887,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1708,1,['learn'],['learn']
Usability,"_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d[k] = read_dataframe(f[k]); 412 else: # Base case; --> 413 d[k] = read_attribute(f[k]); 414 ; 415 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/envs/rpy2_3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 160 else:; 161 parent = _get_parent(elem); --> 162 raise AnnDataReadError(; 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions:; ```; scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:3533,learn,learn,3533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['learn'],['learn']
Usability,"```; Python 3.9.15 (main, Nov 24 2022, 14:31:59) ; [GCC 11.2.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/user/anaconda3/lib/python3.9/site-packages/scanpy/__init__.py"", line 16, in <module>; from . import plotting as pl; File ""/home/user/anaconda3/lib/python3.9/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/user/anaconda3/lib/python3.9/site-packages/scanpy/plotting/_anndata.py"", line 28, in <module>; from . import _utils; File ""/home/user/anaconda3/lib/python3.9/site-packages/scanpy/plotting/_utils.py"", line 35, in <module>; class _AxesSubplot(Axes, axes.SubplotBase, ABC):; TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases; ```. - [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```. ```pytb; [Paste the error output produced by the above code here]; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2445:1194,guid,guide,1194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2445,1,['guid'],['guide']
Usability,"```; def paul15_raw():; filename = 'data/paul15/paul15.h5'; backup_url = 'http://falexwolf.de/data/paul15.h5'; adata = sc.read(filename, 'data.debatched', backup_url=backup_url); # each row has to correspond to a sample, therefore transpose ; adata = adata.transpose() # cluster assocations identified by Paul et al.; clusters = sc.read(filename, 'cluster.id', return_dict=True)['X'].flatten(); # names reflecting the cell type identifications from the paper; cell_types = {i: 'Ery' for i in range(1, 7)}; cell_types[7] = 'MEP'; cell_types[8] = 'Mk'; cell_types[9] = 'GMP'; cell_types[10] = 'GMP'; cell_types[11] = 'DC'; cell_types[12] = 'Baso'; cell_types[13] = 'Baso'; cell_types[14] = 'Mo'; cell_types[15] = 'Mo'; cell_types[16] = 'Neu'; cell_types[17] = 'Neu'; cell_types[18] = 'Eos'; cell_types[19] = 'Other'; adata.smp['paul15_clusters'] = [str(i) + cell_types[i] for i in clusters.astype(int)]; infogenes_names = sc.read(filename, 'info.genes_strings', return_dict=True)['X']; # just keep the first of the two equivalent names per gene ; adata.var_names = np.array([gn.split(';')[0] for gn in adata.var_names]); # remove 10 corrupted gene names ; infogenes_names = np.intersect1d(infogenes_names, adata.var_names); # restrict the data to the 3461 informative genes ; adata = adata[:, infogenes_names]; adata.add['iroot'] = np.flatnonzero(adata.smp['paul15_clusters'] == '7MEP')[0]; return adata; ; adata = paul15_raw(); afilter = sc.pp.recipe_zheng17(adata, n_top_genes=1000, zero_center=True, plot=True, copy=True); ```. or ; ```; afilter = sc.pp.filter_genes_dispersion(adata, n_top_genes=1000); ```. both fail with ; ```AttributeError: 'Series' object has no attribute 'is_dtype_equal'```; when computing the dispersion norm (line 207, simple.py); ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mean_bin']].values) \; 209 / disp_std_bin[df['mean_bin']].values; ```. Running Scanpy version 0.2.6 on 2017-08-23 14:35.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34:1864,simpl,simple,1864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34,1,['simpl'],['simple']
Usability,"```; sc.logging.print_versions(); scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```. ```; adata_2.raw.shape; > (5558, 2000); adata_2.X.shape; > (5558, 2000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665:124,learn,learn,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665,2,['learn'],['learn']
Usability,"```py; >>> sc.pp.regress_out(adata, ['n_counts', 'percent_mito', 'S_score', 'G2M_score'], n_jobs = 1); regressing out ['n_counts', 'percent_mito', 'S_score', 'G2M_score']; sparse input is densified and may lead to high memory use; ... storing 'phase' as categorical; ```. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 783, in regress_out; res = list(map(_regress_out_chunk, tasks)); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 809, in _regress_out_chunk; result = sm.GLM(data_chunk[:, col_index], regres, family=sm.families.Gaussian()).fit(); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1012, in fit; cov_kwds=cov_kwds, use_t=use_t, **kwargs); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1109, in _fit_irls; raise ValueError(""The first guess on the deviance function ""; ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.; ```. Hello, after going through the data once and evaluating which clusters are potential unwanted cell types, I grabbed the barcodes (cell sample names or observation indices) and removed them from the dataset after freshly reloading the dataset so that I can do the preprocessing steps without those cells. When I go to regress out, this occurs, but it didn't before I removed those cell types. . Here's an example of how I removed those cell types:. ```py; keep_cells = [i for i in adata.obs.index if i not in e13_blood2.obs.index]; adata = adata[keep_cells, :]; adata; ```. Any help appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230:459,simpl,simple,459,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230,2,['simpl'],['simple']
Usability,"`log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/929:307,simpl,simple,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929,1,['simpl'],['simple']
Usability,"`log_transformed` disappeared in the last commit here... Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Also: If trying to call a t-test with non-logarithmized data, a warning should be written. The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809:117,simpl,simply,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809,2,['simpl'],['simply']
Usability,"`louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/570:75,learn,learned,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570,1,['learn'],['learned']
Usability,"`pd.crosstab` is a function from `pandas`, and would only work on a pandas dataframe. You can find some documentation on that [here](http://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html#cross-tabulations). I'm not sure why you would be losing any info. I'd also note this seems less related to scanpy, and more related to pandas, since `adata.obs` is just a pandas dataframe. Have you tried looking through the [pandas user guide](https://pandas.pydata.org/pandas-docs/stable/) to figure out how to do what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/584#issuecomment-482929037:441,guid,guide,441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584#issuecomment-482929037,2,['guid'],['guide']
Usability,"`pl.scatter` is, unfortunately, not in the best shape anymore. Since Fidel rewrote a large part of the plotting API, it's not used by any of the frequently used embeddings scatters anymore and would need to be simplified a lot, before actually adding new functionality (like `ncols`). Would you mind correcting the docs? What are you using `pl.scatter` for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476000764:210,simpl,simplified,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476000764,2,['simpl'],['simplified']
Usability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:177,guid,guide,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,2,['guid'],['guide']
Usability,`sc.datasets.paul15_raw()` fails with `attempted relative import beyond top-level package` error due to the wrong module path in `sc.utils.check_presence_download `. . Simply run `sc.datasets.paul15_raw()` or `sc.datasets.paul15()` to reproduce.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/67:168,Simpl,Simply,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/67,1,['Simpl'],['Simply']
Usability,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:137,simpl,simple,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,4,['simpl'],['simple']
Usability,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/339#issuecomment-435639112:174,Simpl,Simply,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339#issuecomment-435639112,1,['Simpl'],['Simply']
Usability,"a clear and concise description of what the bug is: -->; ...When run bbknn on adata which has been calculated the pca, umap, and leiden, the AttributeError shows 'tuple' object has no attribute 'tocsr'. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.umap(adata); ...; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:27); computing neighbors; using 'X_pca' with n_pcs = 50; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:24); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:01:27). %%time; sc.external.pp.bbknn(adata, batch_key='batch'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; computing batch balanced neighbors; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-9-9b24f504f73c> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', ""sc.external.pp.bbknn(adata, batch_key='batch')""). 6 frames; <decorator-gen-60> in time(self, line, cell, local_ns). <timed eval> in <module>(). /usr/local/lib/python3.6/dist-packages/bbknn/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 63 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 64 ; ---> 65 return distances, connectivities.tocsr(); 66 ; 67 def create_tree(data,approx,metric,use_faiss,n_trees):. AttributeError: 'tuple' object has no attribute 'tocsr'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.2 leidenalg==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1249:1940,learn,learn,1940,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1249,1,['learn'],['learn']
Usability,"a.obs[groups_key].astype(str).values; 731 if reference == 'rest':. ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/frame.py in __setitem__(self, key, value); 2936 else:; 2937 # set column; -> 2938 self._set_item(key, value); 2939 ; 2940 def _setitem_slice(self, key, value):. ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/frame.py in _set_item(self, key, value); 2997 """"""; 2998 ; -> 2999 self._ensure_valid_index(value); 3000 value = self._sanitize_column(key, value); 3001 NDFrame._set_item(self, key, value). ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value); 3052 if not len(self.index) and is_list_like(value) and len(value):; 3053 try:; -> 3054 value = Series(value); 3055 except (ValueError, NotImplementedError, TypeError):; 3056 raise ValueError(. ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath); 303 data = data.copy(); 304 else:; --> 305 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True); 306 ; 307 data = SingleBlockManager(data, index, fastpath=True). ~/miniconda3/envs/scrna/lib/python3.7/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure); 480 elif subarr.ndim > 1:; 481 if isinstance(data, np.ndarray):; --> 482 raise Exception(""Data must be 1-dimensional""); 483 else:; 484 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional; ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.2 numpy==1.18.3 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.6.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1199:2695,learn,learn,2695,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1199,1,['learn'],['learn']
Usability,"aadata, target_sum=1e4); sc.pp.log1p(aadata); aadata.raw = aadata. sc.pp.highly_variable_genes(aadata, flavor = 'seurat_v3', n_top_genes=2000,; layer = ""counts"", batch_key=""batch"", subset = True)#, span =0.5; ```. ### Error output. ```pytb; ValueError Traceback (most recent call last); Cell In[37], line 7; 4 sc.pp.log1p(aadata); 5 aadata.raw = aadata; ----> 7 sc.pp.highly_variable_genes(aadata, flavor = 'seurat_v3', n_top_genes=2000,; 8 layer = ""counts"", batch_key=""batch"", subset = True)#, span =0.5. File ~/mambaforge/envs/soupxEnv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:441, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 439 sig = signature(_highly_variable_genes_seurat_v3); 440 n_top_genes = cast(int, sig.parameters[""n_top_genes""].default); --> 441 return _highly_variable_genes_seurat_v3(; 442 adata,; 443 layer=layer,; 444 n_top_genes=n_top_genes,; 445 batch_key=batch_key,; 446 check_values=check_values,; 447 span=span,; 448 subset=subset,; 449 inplace=inplace,; 450 ); 452 if batch_key is None:; 453 df = _highly_variable_genes_single_batch(; 454 adata,; 455 layer=layer,; (...); 462 flavor=flavor,; 463 ). File ~/mambaforge/envs/soupxEnv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:87, in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 85 x = np.log10(mean[not_const]); 86 model = loess(x, y, span=span, degree=2); ---> 87 model.fit(); 88 estimat_var[not_const] = model.outputs.fitted_values; 89 reg_std = np.sqrt(10**estimat_var). File _loess.pyx:899, in _loess.loess.fit(). ValueError: b'Extrapolation not allowed with blending'; ```. ### Versions. <details>. ```; scanpy==1.9.8 anndata==0.10.2 umap==0.5.4 numpy==1.24.4 scipy==1.11.3 pandas==2.1.1 scikit-learn==1.3.1 statsmodels==0.14.0 igraph==0.10.8 pynndescent==0.5.10. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2853:2719,learn,learn,2719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2853,1,['learn'],['learn']
Usability,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404199758:418,simpl,simple,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404199758,2,['simpl'],['simple']
Usability,"aceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 'Key ""{}"" is not valid observation/variable name/index.'; --> 250 .format(i)); 251 i = i_found[0]; 252 return i. IndexError: Key ""dropout_per_gene"" is not valid observation/variable name/index.; ```. The whole thing works for:; ```; sc.pl.violin(adata_counts.T, keys='dropout_per_gene'); sc.pl.violin(adata_counts, keys='dropout_per_cell); ```. So it's clearly just not taking `.var` columns for `sc.pl.violing()`. . I've also reproduced this with `adata = sc.datasets.blob()`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375:2354,clear,clearly,2354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375,1,['clear'],['clearly']
Usability,"ackages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; change = 1; count = 0; ^. self.func_ir.loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide; b_prior[i],; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164:4696,learn,learn,4696,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164,1,['learn'],['learn']
Usability,"aconda3/envs/scRNA/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py"", line 188, in pca; X_pca = pca_.fit_transform(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/utils/_set_output.py"", line 140, in wrapped; data_to_wrap = f(self, X, *args, **kwargs); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 462, in fit_transform; U, S, Vt = self._fit(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 514, in _fit; return self._fit_truncated(X, n_components, self._fit_svd_solver); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 609, in _fit_truncated; U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/_svds.py"", line 532, in svds; _, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter,; File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py"", line 1697, in eigsh; params.iterate(); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py"", line 573, in iterate; raise ArpackError(self.info, infodict=self.iterate_infodict); scipy.sparse.linalg._eigen.arpack.arpack.ArpackError: ARPACK error -9999: Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated. ```. #### Versions:. <!-- Output of scvi.__version__ -->. > scvi-tools==0.20.3 python==3.9.16 scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==1.5.3 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.4 pynndescent==0.5.10. and macOS 13.2 (intel). > ; <!-- Relevant screenshots -->. Thanks; Patrick",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2473:2693,learn,learn,2693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2473,1,['learn'],['learn']
Usability,"aconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 371 try:; --> 372 pm.run(self.state); 373 if self.state.cr is not None:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 340 patched_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 441 # TODO: Pull this out into the pipeline; --> 442 NativeLowering().run_pass(state); 443 lowered = state['cr']. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 369 metadata=metadata); --> 370 lower.lower(); 371 if not flags.no_cpython_wrapper:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 216 # Materialize LLVM Module; --> 217 self.library.add_ir_module(self.module); 218 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\codegen.py in add_ir_module(self, ir_module); 205 ir = cgutils.normalize_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:3392,Simpl,SimpleTimer,3392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['Simpl'],['SimpleTimer']
Usability,adapted from https://github.com/scverse/anndata/blob/main/.azure-pipelines.yml. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:151,guid,guidelines,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,2,['guid'],"['guide', 'guidelines']"
Usability,added some quotes to make the code copy-able in the docs. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2646:129,guid,guidelines,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2646,2,['guid'],"['guide', 'guidelines']"
Usability,addresses #2506. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2507:88,guid,guidelines,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2507,2,['guid'],"['guide', 'guidelines']"
Usability,"age pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package xz conflicts for:; python=3.6 -> xz[version='>=5.2.3,<6.0a0|>=5.2.4,<6.0a0']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:1844,learn,learn,1844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012,4,['learn'],['learn']
Usability,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973:466,clear,clear,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973,2,['clear'],['clear']
Usability,"all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```. ### Versions. ```; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.8.0; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 7.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; google NA; h5py 3.7.0; ipykernel 5.3.0; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.0; joblib 1.1.0; kiwisolver 1.2.0; llvmlite 0.38.1; matplotlib 3.3.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; netifaces 0.11.0; numba 0.55.2; numexpr 2.8.3; numpy 1.20.0; packaging 21.3; pandas 1.1.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.9.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 3.0.8; pytz 2022.1; ruamel NA; scipy 1.7.0; seaborn 0.11.2; setuptools 62.1.0; simplejson 3.17.6; six 1.16.0; sklearn 1.0.1; statsmodels 0.13.2; storemagic NA; tables 3.7.0; threadpoolctl 3.1.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 19.0.1; -----; IPython 7.15.0; jupyter_client 6.1.3; jupyter_core 4.6.3; jupyterlab 2.1.4; notebook 6.1.5; -----; Python 3.7.13 (default, Aug 6 2022, 00:43:34) [GCC 9.2.0]; Linux-4.18.0-348.12.2.el8_5.x86_64-x86_64-with-centos-8.7-Green_Obsidian; 192 logical CPU cores, x86_64; -----; Session information updated at 2023-08-22 16:49; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:4543,simpl,simplejson,4543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['simpl'],['simplejson']
Usability,"all very good points, and I don't think I have a clear solution, it's more that we have to decide how to go about this:; > Question for this, what heuristics have you tried? My guess would be that min(distances_between_points) / 3 should be fine for an upper bound. this indeed could be solved, but we still would have to set this heuristics differently according to the spatial data type in question. E.g. for visium `size=1` is correct, because coordinates are in pixel measure. In the dataset I have now (seqFISH) the coordinates are essentially z-score and so would have to change for instance to the one you proposed. However, why would we want to have `circle` at all in that t case, and not just scatterplot? Since there is no real notion of size, I think a scatterplot is actually more appropriate. We discussed this already but back then we didn't have this example. > Second, I think this logic is a little convoluted, and I don't know that library_id will always be associated with visium only. Would a better check be for [""metadata""][""software_version""] or something like that?. It definitely is, there might be a better solution but I couldn't come up with it. The problem stems in the fact that we have a `library_id` key in `adata.uns[""spatial""]`. In `spatial` we also put this; ```python; {'connectivities_key': 'spatial_connectivities',; 'distances_key': 'spatial_distances',; 'params': {'n_neighbors': 6, 'coord_type': None, 'radius': None}}; ``` ; this is needed for plotting. The default in `sc.pl.spatial` now is that if library_id is `_empty`, then it iteratively search for some keys in `adata.uns[""spatial""]`.; ```python; try: # check if key is empty; spatial_data = adata.uns['spatial']; library_id = next(; (; i; for i in spatial_data.keys(); if i not in [""connectivities_key"", ""distances_key""]; ); ); except (KeyError, StopIteration) as e:; 	library_id = None; ```; The point is that it should only assign whatever key it finds that is not `[""connectivities_key"", ""distance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626:49,clear,clear,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626,2,['clear'],['clear']
Usability,allowed the function to use specified var_names. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] #1549; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2765:120,guid,guidelines,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2765,2,['guid'],"['guide', 'guidelines']"
Usability,"and I could not track it down yet.; The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results; Bottom = old=agando expected results. The old environment has:. ```; scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins-base=1.14.0=h8213a91_2; - gstreamer=1.14.0=h28cd5cc_2; - icu=58.2=he6710b0_3; - importlib_metadata=2.0.0=1; - ipykernel=5.3.4=py37h5ca1d4c_0; - ipython=7.20.0=py37hb070fc8_1; - ipython_genutils=0.2.0=pyhd3eb1b0_1; - ipywidgets=7.6.3=pyhd3eb1b0_1; - jedi=0.17.0=py37_0; - ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:1273,learn,learn,1273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['learn'],['learn']
Usability,"anpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1104,learn,learn,1104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,4,['learn'],['learn']
Usability,"anpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.13.0); Requirement already satisfied: numpy>=1.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.17.4); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:8611,learn,learn,8611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['learn'],['learn']
Usability,"anpy, and so I may be performing this incorrectly. I encountered an error when trying to create a backed AnnData object from an h5ad file, and then logarithmizing the data matrix within the object using scanpy.pp.log1p. However I get an error within the AnnData object code because the preprocessing/_simple.py script is not passing a filename in the copy() function. Right now my current workaround is to create the AnnData object as non-backed, do the log1p, and then create a ""filename"" property to the AnnData object afterwards to make it backed for other scanpy functions. ### Example; ```python; import scanpy as sc. dataset_path = ""/path/to/test/data.h5ad"" # Subbing out actual filenames for data; adata = sc.read_h5ad(dataset_path, backed='r'); print(adata) # To ensure there is a backed filepath. adata.raw = sc.pp.log1p(adata, copy=True) # Error is here; ```. #### Error output; ```pytb; # I printed the AnnData object to ensure it was backed; AnnData object with n_obs × n_vars = 4166 × 16852 backed at '/tmp/1b12dde9-1762-7564-8fbd-1b07b750505f.h5ad'; obs: 'cell_type', 'barcode', 'tSNE_1', 'tSNE_2', 'replicate', 'louvain', 'n_genes', 'percent_mito', 'n_counts'; var: 'gene_symbol', 'n_cells'; obsm: 'X_tsne'. # Actual error after calling log1p; Traceback (most recent call last):; File ""log1p_test.cgi"", line 129, in <module>; main(); File ""log1p_test.cgi"", line 81, in main; adata.raw = sc.pp.log1p(adata, copy=True); File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 292, in log1p; data = data.copy(); File ""/opt/Python-3.7.3/lib/python3.7/site-packages/anndata/_core/anndata.py"", line 1457, in copy; ""To copy an AnnData object in backed mode, ""; ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`.; ```. #### Versions:; scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.16.3 scipy==1.4.1 pandas==0.24.2 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1153:1954,learn,learn,1954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1153,1,['learn'],['learn']
Usability,"anpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:1223,clear,clear,1223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['clear'],['clear']
Usability,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:2001,feedback,feedback,2001,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,2,['feedback'],['feedback']
Usability,"arger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks abo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3057,learn,learning,3057,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['learn'],['learning']
Usability,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/809:3890,simpl,simply,3890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809,1,['simpl'],['simply']
Usability,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:85,simpl,simply,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896,2,['simpl'],['simply']
Usability,"ase.py"",; > line 635, in *init*; > self._init_as_view(X, oidx, vidx); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",; > line 661, in _init_as_view; > var_sub = adata_ref.var.iloc[vidx_normalized]; > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1478, in *getitem*; > return self._getitem_axis(maybe_callable, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 2087, in _getitem_axis; > return self._getbool_axis(key, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1494, in _getbool_axis; > inds, = key.nonzero(); > ValueError: too many values to unpack (expected 1); >; > I've tried several variations of this yet I don't see why your command; > wouldn't work, it seems like it should do what you intend ..; >; > Note however, I ran :; >; > print(np.any(adata.X.sum(axis=0) == 0)) # True; > print(np.any(adata.X.sum(axis=1) == 0)) # False; >; > right after loading the dataset and it still shows True and False, yet if; > I were to regress out WITHOUT removing cell types via:; >; > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]; > adata = adata[Temp,:]; >; > or; >; > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells; >; > ... the regression will work. However, once I remove, it won't. I could; > try to remove the 0 columns with R, unless you have another suggestion?; >; > Thank you for any feedback.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475:2552,feedback,feedback,2552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475,2,['feedback'],['feedback']
Usability,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; adata = sc.datasets.blobs(); sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'); ```. ```pytb; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-11-1415b8dea7b8> in <module>; 5 adata.obs['boolean'] = True; 6 ; ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs); 727 if not annotate_var_explained:; 728 return embedding(; --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs; 730 ); 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 257 if sort_order is True and value_to_plot is not None and categorical is False:; 258 # Higher values plotted on top, null values on bottom; --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]; 260 elif sort_order and categorical:; 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead.; ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646:2102,learn,learn,2102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646,1,['learn'],['learn']
Usability,"at's going on here? I can't do a ""pip3.6 install scanpy"" on our linux cluster:. <details>. ```; Collecting scanpy; Using cached scanpy-0.4.3.tar.gz; Requirement already up-to-date: anndata>=0.5 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: matplotlib==2.0.0 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: pandas>=0.21 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scipy in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: seaborn in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: psutil in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: h5py in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:1043,learn,learn,1043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['learn'],['learn']
Usability,"ata' object has no attribute 'is_view'`.; The reason is that the function name changed in version 0.7rc1 from `isview` -> `is_view`. I propose two possible solutions:; **Solution A**: Change requirements to `anndata>=0.7rc1`; **Solution B**: Add function to anndata:; ```python; def isview(self):; return self.is_view(); ```; I think solution B is preferable as it provides back-compatibility of anndata. ---; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; import scanpy as sc; adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-2-59eff31dcd22> in <module>; 1 get_ipython().system('pip install git+https://github.com/theislab/scanpy.git@spatial'); 2 import scanpy as sc; ----> 3 adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'). /opt/conda/lib/python3.7/site-packages/scanpy/datasets/__init__.py in visium_sge(sample_id); 368 ; 369 # read h5 file; --> 370 adata = read_10x_h5(files['counts']); 371 adata.var_names_make_unique(); 372 . /opt/conda/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only); 169 if gex_only:; 170 adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]; --> 171 if adata.is_view:; 172 return adata.copy(); 173 else:. AttributeError: 'AnnData' object has no attribute 'is_view'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post3.dev17+g09b9856 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027:2121,learn,learn,2121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027,1,['learn'],['learn']
Usability,"ate); 339 (self.pipeline_name, pass_desc); 340 patched_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 ; 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 330 pass_inst = _pass_registry.get(pss).pass_inst; 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:; 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 289 mutated |= check(pss.run_initialization, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:; 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 262 ; 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):; 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 90 % (state.func_id.func_name,)):; 91 # Type inference; ---> 92 typemap, return_type, calltypes = type_inference_stage(; 93 state.typingctx,; 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors); 68 ; 69 infer.build_constraint(); ---> 70 infer.propagate(raise_errors=raise_errors); 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors); 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:6942,Simpl,SimpleTimer,6942,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['Simpl'],['SimpleTimer']
Usability,"b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; astor 0.8.1; astunparse 1.6.3; bottleneck 1.3.4; cached_property 1.5.2; certifi 2021.10.08; cffi 1.15.0; chardet 3.0.4; cloudpickle 1.3.0; cvxopt 1.2.7; cycler 0.10.0; cython_runtime NA; dask 2.12.0; dateutil 2.8.2; debugpy 1.0.0; decorator 4.4.2; dill 0.3.4; flatbuffers 2.0; gast 0.5.3; google NA; google_auth_httplib2 NA; googleapiclient NA; h5py 3.1.0; httplib2 0.17.4; idna 2.10; igraph 0.9.9; ipykernel 4.10.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.4; jaxlib 0.3.2; joblib 1.1.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.34.0; matplotlib 3.2.2; mpl_toolkits NA; natsort 5.5.0; numba 0.51.2; numexpr 2.8.1; numpy 1.21.5; oauth2client 4.1.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.5; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; portpicker NA; prompt_toolkit 1.0.18; psutil 5.4.8; ptyprocess 0.7.0; pyarrow 6.0.1; pyasn1 0.4.8; pyasn1_modules 0.2.8; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.0.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot_ng 2.0.0; pygments 2.6.1; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2018.9; requests 2.23.0; rsa 4.8; scipy 1.4.1; seaborn 0.11.2; session_info 1.0.0; simplegeneric NA; sitecustomize NA; six 1.15.0; sklearn 1.0.2; socks 1.7.1; sphinxcontrib NA; statsmodels 0.10.2; storemagic NA; tblib 1.7.0; tensorboard 2.8.0; tensorflow 2.8.0; tensorflow_probability 0.16.0; termcolor 1.1.0; texttable 1.6.4; threadpoolctl 3.1.0; toolz 0.11.2; tornado 5.1.1; tqdm 4.63.0; traitlets 5.1.1; tree 0.1.6; typing_extensions NA; umap 0.5.2; uritemplate 3.0.1; urllib3 1.24.3; wcwidth 0.2.5; wrapt 1.14.0; yaml 3.13; zipp NA; zmq 22.3.0. IPython 5.5.0; jupyter_client 5.3.5; jupyter_core 4.9.2; notebook 5.3.1. Python 3.7.13 (default, Mar 16 2022, 17:37:17) [GCC 7.5.0]; Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic. Session information updated at 2022-04-04 17:56. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:4949,simpl,simplegeneric,4949,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['simpl'],['simplegeneric']
Usability,"b\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index); 160 obs, var = unpack_index(packed_index); 161 obs = _normalize_index(obs, self._adata.obs_names); --> 162 var = _normalize_index(var, self.var_names); 163 return obs, var; 164 . D:\anaconda\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 73 return indexer; 74 elif isinstance(indexer, str):; ---> 75 return index.get_loc(indexer) # int; 76 elif isinstance(indexer, (Sequence, np.ndarray, pd.Index, spmatrix, np.matrix)):; 77 if hasattr(indexer, ""shape"") and (. D:\anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 3629 return self._engine.get_loc(casted_key); 3630 except KeyError as err:; -> 3631 raise KeyError(key) from err; 3632 except TypeError:; 3633 # If we have a listlike key, _check_indexing_error will raise. KeyError: 'CST3'. #### Versions; scanpy==1.9.2 anndata==0.8.0 umap==0.5.3 numpy==1.21.6 scipy==1.9.1 pandas==1.4.4 scikit-learn==1.0.2 statsmodels==0.13.2 python-igraph==0.10.4 pynndescent==0.5.8; <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.2.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fsspec 2022.7.1; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.38.0; lz4 3.1.3; markupsafe 2.1.2; matplotlib 3.5.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numexpr 2.8.3; numpy 1.21.6; packaging 21.3; pandas 1.4.4; parso 0.8.3; patsy 0.5.2; pi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2430:4202,learn,learn,4202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430,1,['learn'],['learn']
Usability,"ba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. ""; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:7093,Simpl,SimpleTimer,7093,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['Simpl'],['SimpleTimer']
Usability,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:1626,user-friendly,user-friendly,1626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,1,['user-friendly'],['user-friendly']
Usability,"bors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata). # Running on a machine with 16 CPUs, evaluate the differences between the results first from the arpack solver; adata8 = sc.read('test8.h5ad'); adata16 = sc.read('test16.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); print((adata8.uns['neighbors']['connectivities'] != adata16.uns['neighbors']['connectivities']).sum()); sc.tl.leiden(adata8, random_state=14); sc.tl.leiden(adata16, random_state=14); display(adata8.obs['leiden'].value_counts()); display(adata16.obs['leiden'].value_counts()). # Running on a machine with 16 CPUs, evaluate the differences between the results from the randomized solver; adata8 = sc.read('test8_randomized.h5ad'); adata16 = sc.read('test16_randomized.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); print((adata8.uns['neighbors']['connectivities'] != adata16.uns['neighbors']['connectivities']).sum()); sc.tl.leiden(adata8, random_state=14); sc.tl.leiden(adata16, random_state=14); display(adata8.obs['leiden'].value_counts()); display(adata16.obs['leiden'].value_counts()). ```; This outputs the following. ```; 0; 134513; 37696; 0 659; 1 605; 2 398; 3 352; 4 342; 5 174; 6 118; 7 41; 8 11; Name: leiden, dtype: int64; 0 527; 1 484; 2 398; 3 324; 4 320; 5 301; 6 174; 7 109; 8 52; 9 11; Name: leiden, dtype: int64. 0; 134127; 37278; 0 646; 1 617; 2 382; 3 362; 4 334; 5 173; 6 129; 7 46; 8 11; Name: leiden, dtype: int64; 0 646; 1 631; 2 408; 3 349; 4 334; 5 170; 6 106; 7 45; 8 11; Name: leiden, dtype: int64. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions(). -->; scanpy==1.4.4.post1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187:4514,learn,learn,4514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187,1,['learn'],['learn']
Usability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:192,simpl,simplification,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,2,['simpl'],['simplification']
Usability,"c neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration fact",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1979,learn,learning,1979,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['learn'],['learning']
Usability,"c.obs['cell_types'] = pbmc.obs['phase']; pbmc.obs['cell_types'].cat.categories = new_cluster_names; sc.pl.umap(pbmc, color=['cell_types']). pbmc.rename_categories('phase', new_cluster_names); sc.pl.umap(pbmc, color=['phase']). ```. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-82-890b788bf078> in <module>; ----> 1 sc.pl.umap(pbmc, color=['phase']). ~/miniconda3/envs/single_cell_181/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs); 657 tl.umap; 658 """"""; --> 659 return embedding(adata, 'umap', **kwargs); 660 ; 661 . ~/miniconda3/envs/single_cell_181/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 255 color_source_vector,; 256 palette=palette,; --> 257 na_color=na_color,; 258 ); 259 . ~/miniconda3/envs/single_cell_181/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in _color_vector(adata, values_key, values, palette, na_color); 1275 # Set color to 'missing color' for all missing values; 1276 if color_vector.isna().any():; -> 1277 color_vector = color_vector.add_categories([to_hex(na_color)]); 1278 color_vector = color_vector.fillna(to_hex(na_color)); 1279 return color_vector, True. AttributeError: 'Float64Index' object has no attribute 'add_categories'; ```. #### Versions. scanpy==1.8.1 anndata==0.7.6 umap==0.5.1 numpy==1.21.1 scipy==1.7.0 pandas==1.3.1 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.6 pynndescent==0.5.4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1975:2404,learn,learn,2404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975,1,['learn'],['learn']
Usability,c3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.6.0; tokenizers 0.14.0; toml 0.10.2; tomli 2.0.1; tomlkit 0.12.1; toolz 0.12.0; toposort 1.10; tornado 6.3.3; tqdm 4.66.1; traitlets 5.11.2; transformers 4.34.0; truststore 0.8.0; Twisted 22.10.0; types-python-dateutil 2.8.19.14; typing_extensions 4.8.0; typing-utils 0.1.0; tzdata 2023.3; uc-micro-py 1.0.1; ujson 5.8.0; umap-learn 0.5.4; uri-template 1.3.0; urllib3 1.26.15; virtualenv 20.24.5; w3lib 2.1.2; watchdog 3.0.0; wcwidth 0.2.8; webcolors 1.13; webencodings 0.5.1; websocket-client 1.6.4; Werkzeug 3.0.0; whatthepatch 1.0.5; wheel 0.38.4; widgetsnbextension 4.0.9; wrapt 1.15.0; wurlitzer 3.0.3; xarray 2023.9.0; xxhash 3.4.1; xyzservices 2023.10.0; yapf 0.24.0; yarl 1.9.2; yte 1.5.1; zict 3.0.0; zipp 3.17.0; zope.interface 6.1; zstandard 0.21.0. ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:10329,learn,learn,10329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['learn'],['learn']
Usability,ca | 1.0.3; pynndescent | 0.5.5 | pynndescent | 0.5.5 | pynndescent | 0.5.5; pyOpenSSL | 21.0.0 | pyOpenSSL | 21.0.0 | pyOpenSSL | 21.0.0; pyparsing | 3.0.4 | pyparsing | 3.0.4 | pyparsing | 3.0.4; pyrsistent | 0.18.0 | pyrsistent | 0.18.0 | pyrsistent | 0.18.0; pyscenic | 0.11.2 | pyscenic | 0.11.2 | pyscenic | 0.11.2; PySocks | 1.7.1 | PySocks | 1.7.1 | PySocks | 1.7.1; python-dateutil | 2.8.2 | python-dateutil | 2.8.2 | python-dateutil | 2.8.2; python-igraph | 0.9.9 | python-igraph | 0.9.9 | python-igraph | 0.9.9; python-utils | 3.1.0 | python-utils | 3.1.0 | python-utils | 3.1.0;   |   | pytoml | 0.1.21 |   |  ; pytz | 2021.3 | pytz | 2021.3 | pytz | 2021.3; pywin32 | 302 | pywin32 | 302 | pywin32 | 302; pywinpty | 0.5.7 | pywinpty | 0.5.7 | pywinpty | 0.5.7; PyYAML | 6 | PyYAML | 6 | PyYAML | 6; pyzmq | 22.3.0 | pyzmq | 22.3.0 | pyzmq | 22.3.0; requests | 2.27.1 | requests | 2.27.1 | requests | 2.27.1; scanpy | 1.8.2 | scanpy | 1.8.2 | scanpy | 1.8.2; scikit-learn | 1.0.2 | scikit-learn | 1.0.2 | scikit-learn | 1.0.2;   |   | scikit-misc | 0.1.4 |   |  ; scipy | 1.7.3 | scipy | 1.7.3 | scipy | 1.7.3; scvelo | 0.2.4 | scvelo | 0.2.4 | scvelo | 0.2.4; seaborn | 0.11.2 | seaborn | 0.11.2 | seaborn | 0.11.2; Send2Trash | 1.8.0 | Send2Trash | 1.8.0 | Send2Trash | 1.8.0; setuptools | 58.0.4 | setuptools | 58.0.4 | setuptools | 58.0.4;   |   | setuptools-scm | 6.3.2 |   |  ; sinfo | 0.3.4 | sinfo | 0.3.4 | sinfo | 0.3.4; six | 1.16.0 | six | 1.16.0 | six | 1.16.0; sniffio | 1.2.0 | sniffio | 1.2.0 | sniffio | 1.2.0; sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0; statsmodels | 0.13.1 | statsmodels | 0.13.1 | statsmodels | 0.13.1; stdlib-list | 0.8.0 | stdlib-list | 0.8.0 | stdlib-list | 0.8.0; tables | 3.6.1 | tables | 3.6.1 | tables | 3.6.1; tblib | 1.7.0 | tblib | 1.7.0 | tblib | 1.7.0; terminado | 0.9.4 | terminado | 0.9.4 | terminado | 0.9.4; testpath | 0.5.0 | testpath | 0.5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:13179,learn,learn,13179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['learn'],['learn']
Usability,can't reproduce the same result between sc.tl.tsne and scikit-learn.TSNE,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1759:62,learn,learn,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759,2,['learn'],['learn']
Usability,"canpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arrays):; ```. </details>. Versions of my modules:; scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. I have downgraded pandas to 0.23.4, however, it not works. But I figured out where the problem lies in. ```py; adata.X /= adata.obs['size_factors'].values[:,None]; ```. This step transform the adata.X to a structure of matrix.; Before the adata.X is. ```; <6242x15065 sparse matrix of type '<class 'numpy.float32'>'; with 19234986 stored elements in Compressed Sparse Row format>; ```. But after performing this step, the adata.X is; This is my adata.X looks like right now:. ```py; matrix([[0. , 0. , 0. , ..., 0. , 0. , 0. ],; [0. , 0. , 1.203, ..., 0. , 0. , 0. ],; [0. , 1.096, 0. , ..., 0. , 0. , 0. ],; ...,; [0. , 0. , 2.042, ..., 0. , 0. , 0. ],; [0. , 0. , 0. , ..., 0.926, 0. , 0. ],; [0. , 0. , 2.951, ..., 0. , 0. , 0. ]]),; ```. And this format of adata.X caused error of sc.pp.highly_variable_genes. But I don't know how to fix it. Looking forward your response!; Thank you !",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:2129,learn,learn,2129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['learn'],['learn']
Usability,"cc: @mumichae about our conversation the other day. I've been thinking that a good entry point here could just be a notebook that demonstrates using these packages on single cell data that we could point to on https://scverse.org/learn (hosted on https://github.com/scverse/scverse-tutorials). This could be a good starting point for anyone who wants to jump in to investigate further. And, for completeness, I would also want to point out https://github.com/brianhie/geosketch as another promising subsampling method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862#issuecomment-2024910951:230,learn,learn,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862#issuecomment-2024910951,2,['learn'],['learn']
Usability,"cgutils.py?line=192) index=index)); [194](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/cgutils.py?line=193) self._builder.store(value, ptr). TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); e:\Drosophila\ST\04.yao\E8-10_b_analysis\E8-10_b_analysis\8_cellbin_clustering\cellbin_scsq.ipynb Cell 10' in <cell line: 1>(); ----> [1](vscode-notebook-cell:/e%3A/Drosophila/ST/04.yao/E8-10_b_analysis/E8-10_b_analysis/8_cellbin_clustering/cellbin_scsq.ipynb#ch0000016?line=0) from umap import UMAP. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\__init__.py:2, in <module>; [1](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=0) from warnings import warn, catch_warnings, simplefilter; ----> [2](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=1) from .umap_ import UMAP; [4](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=3) try:; [5](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=4) with catch_warnings():. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\umap_.py:41, in <module>; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=33) from umap.utils import (; [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=34) submatrix,; [36](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=35) ts,; [37](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=36) csr_unique,; [38](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=37) fast_knn_indices,; [39",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:8685,simpl,simplefilter,8685,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,2,['simpl'],['simplefilter']
Usability,"ckages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package libstdcxx-ng conflicts for:; python=3.7 -> libstdcxx-ng[version='>=7.2.0|>=7.3.0']; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11789,learn,learn,11789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,4,['learn'],['learn']
Usability,"ckages\anndata\_core\anndata.py in __getitem__(self, index); 1114 def __getitem__(self, index: Index) -> ""AnnData"":; 1115 """"""Returns a sliced view of the object.""""""; -> 1116 oidx, vidx = self._normalize_indices(index); 1117 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1118 . ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in _normalize_indices(self, index); 1095 ; 1096 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1097 return _normalize_indices(index, self.obs_names, self.var_names); 1098 ; 1099 # TODO: this is not quite complete... ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\index.py in _normalize_indices(index, names0, names1); 34 ax0, ax1 = unpack_index(index); 35 ax0 = _normalize_index(ax0, names0); ---> 36 ax1 = _normalize_index(ax1, names1); 37 return ax0, ax1; 38 . ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 99 if np.any(positions < 0):; 100 not_found = indexer[positions < 0]; --> 101 raise KeyError(; 102 f""Values {list(not_found)}, from {list(indexer)}, ""; 103 ""are not valid obs/ var names or indices."". KeyError: 'Values [nan, nan, nan, nan, nan, nan, True,... nan, nan, True], are not valid obs/ var names or indices.'. ```. ```pytb; KeyError: 'Values [nan, nan, nan, nan, nan, nan, True,... nan, nan, True], are not valid obs/ var names or indices.'; ```. #### Versions. <details>. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.3 pandas==1.3.5 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5; scvelo==0.2.4 scanpy==1.8.2 anndata==0.7.8 loompy==3.0.6 numpy==1.20.3 scipy==1.7.3 matplotlib==3.5.1 sklearn==1.0.1 pandas==1.3.5 ; cellrank==1.5.0 scanpy==1.8.2 anndata==0.7.8 numpy==1.20.3 numba==0.54.1 scipy==1.7.3 pandas==1.3.5 pygpcca==1.0.2 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 scvelo==0.2.4 pygam==0.8.0 matplotlib==3.5.1 seaborn==0.11.2. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:3804,learn,learn,3804,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,2,['learn'],['learn']
Usability,"clear and concise description of what the bug is: -->; I'm using the ""scvelo"" https://scvelo.readthedocs.io/getting_started.html for scRNA data analysis. It underlying called ""scanpy"" function ""umap"" for calculating the coordinates. I tried the release version ""scanpy-1.4.4.post1-py_0"". It can not be imported to Python. Error message: ""ImportError: cannot import name '_Metric' from 'scanpy.neighbors' (/Users/shuzhe/anaconda3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py)"". I finally switch to the developing version for ""scanpy"". When I run ""umap"", it gives me the error message below. I'm wondering what the "".obsp"" is and how it is generated.; ; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; scv.tl.umap(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-391fc8667646> in <module>; ----> 1 scv.tl.umap(adata). ~/scanpy/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 125 start = logg.info('computing UMAP'); 126 ; --> 127 neighbors = NeighborsView(adata, neighbors_key); 128 ; 129 if ('params' not in neighbors. ~/scanpy/scanpy/_utils.py in __init__(self, adata, key); 667 self._dists_key = self._neighbors_dict['distances_key']; 668 ; --> 669 if self._conns_key in adata.obsp:; 670 self._connectivities = adata.obsp[self._conns_key]; 671 if self._dists_key in adata.obsp:. AttributeError: 'AnnData' object has no attribute 'obsp'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.7.dev26+gc255fa10 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.21.2 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125:1950,learn,learn,1950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125,1,['learn'],['learn']
Usability,"close #1698 . this is the implementation as proposed here: https://github.com/theislab/squidpy/pull/304; Explicit design choices compared to first proposal in squidpy:; - no joblib present, with low n permutation is fine and saw that you don't even do it in gearys C (which btw, makes a lot of sense in this setting, should consider to skip permutation entirely as well ); - only working on genes. technically it could work on continuos covariates as well, should I add that option?; - I think it could be worth it to add a row wise normalization of the weights (standard in pysal). @ivirshup would be good to have feedback on those points, I will then add tests. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740:615,feedback,feedback,615,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740,1,['feedback'],['feedback']
Usability,code sample. ```python; sc.pp.normalize_total(adata); sc.pp.log1p(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.9.1; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 9.1.0; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; brotli NA; certifi 2022.12.07; cffi 1.15.0; charset_normalizer 2.0.12; cloudpickle 2.0.0; cycler 0.10.0; cython_runtime NA; dask 2022.8.1; dateutil 2.8.2; debugpy 1.6.0; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.4; fastjsonschema NA; fsspec 2022.7.1; google NA; h5py 3.4.0; idna 3.3; igraph 0.9.6; ipykernel 6.4.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.11.0; jupyterlab_server 2.8.1; kiwisolver 1.3.2; leidenalg 0.8.7; llvmlite 0.38.0; louvain 0.7.0; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; mpmath 1.3.0; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.55.1; numexpr 2.7.3; numpy 1.21.6; nvfuser NA; opt_einsum v3.3.0; packaging 23.1; pandas 1.5.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.31; psutil 5.9.0; ptyprocess 0.7.0; pvectorc NA; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pyparsing 2.4.7; pyrsistent NA; pytz 2022.2.1; requests 2.28.1; scipy 1.10.1; send2trash NA; setuptools 67.8.0; simplejson 3.17.6; six 1.16.0; sklearn 1.2.2; sniffio 1.2.0; socks 1.7.1; sparse 0.14.0; storemagic NA; sympy 1.12; tables 3.6.1; tblib 1.7.0; terminado 0.12.1; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.12.0; toolz 0.12.0; torch 2.0.1+cu117; tornado 6.1; tqdm 4.62.2; traitlets 5.1.0; typing_extensions NA; unicodedata2 NA; urllib3 1.26.9; wcwidth 0.2.5; websocket 1.3.2; yaml 6.0; zipp NA; zmq 22.3.0; zstandard 0.18.0; -----; IPython 7.34.0; jupyter_client 7.3.0; jupyter_core 4.10.0; jupyterlab 3.1.11; notebook 6.4.11; -----; Python 3.8.13 | packaged by conda-forge ; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2556:2326,simpl,simplejson,2326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556,1,['simpl'],['simplejson']
Usability,"colorama 0.4.6; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; exceptiongroup 1.2.2; fastjsonschema NA; fqdn NA; google NA; h5py 3.11.0; idna 3.3; importlib_resources NA; ipykernel 6.5.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; isoduration NA; jedi 0.18.0; jinja2 3.0.3; joblib 1.4.2; json5 0.9.25; jsonpointer 3.0.0; jsonschema 4.23.0; jsonschema_specifications NA; jupyter_events 0.10.0; jupyter_server 2.14.2; jupyterlab_server 2.27.3; kiwisolver 1.4.7; legacy_api_wrap NA; llvmlite 0.43.0; markupsafe 2.0.1; matplotlib 3.9.2; mpl_toolkits NA; natsort 8.4.0; nbformat 5.10.4; nt NA; ntsecuritycon NA; numba 0.60.0; numpy 1.26.4; overrides NA; packaging 24.1; pandas 2.2.2; parso 0.8.2; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.1; pooch v1.7.0; prometheus_client NA; prompt_toolkit 3.0.22; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pyparsing 3.1.4; pythoncom NA; pythonjsonlogger NA; pytz 2024.1; pywin32_bootstrap NA; pywin32_system32 NA; pywintypes NA; referencing NA; requests 2.32.3; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.13.1; send2trash NA; session_info 1.0.0; setuptools_scm NA; simplejson 3.19.2; six 1.16.0; sklearn 1.5.1; sniffio 1.3.1; storemagic NA; threadpoolctl 3.5.0; torch 1.10.1+cpu; tornado 6.4.1; tqdm 4.66.5; traitlets 5.14.3; typing_extensions NA; uri_template NA; urllib3 1.26.7; wcwidth 0.2.5; webcolors 24.8.0; websocket 1.8.0; win32api NA; win32com NA; win32con NA; win32security NA; win32trace NA; winerror NA; yaml 6.0; zipp NA; zmq 26.2.0; zoneinfo NA; -----; IPython 7.29.0; jupyter_client 7.4.9; jupyter_core 5.7.2; jupyterlab 4.2.5; notebook 7.2.2; -----; Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]; Windows-10-10.0.22621-SP0; -----; Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3228:9716,simpl,simplejson,9716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228,1,['simpl'],['simplejson']
Usability,conda-forge; protobuf 4.23.4 pypi_0 pypi; psutil 5.9.5 py311h2582759_0 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pydantic 1.10.11 pypi_0 pypi; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pypi_0 pypi; pynndescent 0.5.10 pypi_0 pypi; pyparsing 3.0.9 pypi_0 pypi; pyro-api 0.1.2 pypi_0 pypi; pyro-ppl 1.8.5 pypi_0 pypi; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.11.4 hab00c5b_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-editor 1.0.4 pypi_0 pypi; python-igraph 0.10.6 pypi_0 pypi; python-levenshtein 0.21.1 pypi_0 pypi; python-multipart 0.0.6 pypi_0 pypi; python_abi 3.11 3_cp311 conda-forge; pytorch-lightning 2.0.5 pypi_0 pypi; pytz 2023.3 pypi_0 pypi; pyyaml 6.0.1 pypi_0 pypi; pyzmq 25.1.0 py311h75c88c4_0 conda-forge; rapidfuzz 3.1.2 pypi_0 pypi; readchar 4.0.5 pypi_0 pypi; readline 8.2 h8228510_1 conda-forge; requests 2.28.1 pypi_0 pypi; rich 13.4.2 pypi_0 pypi; scanpy 1.9.3 pypi_0 pypi; scikit-learn 1.3.0 pypi_0 pypi; scipy 1.11.1 py311h64a7726_0 conda-forge; scirpy 0.13.0 pypi_0 pypi; scmisc 0.0.1 pypi_0 pypi; scvi-tools 1.0.2 pypi_0 pypi; seaborn 0.12.2 pypi_0 pypi; session-info 1.0.0 pypi_0 pypi; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; singlecellhaystack 0.0.5 pypi_0 pypi; six 1.16.0 pyh6c4a22f_0 conda-forge; sniffio 1.3.0 pypi_0 pypi; soupsieve 2.4.1 pypi_0 pypi; sparse 0.14.0 pypi_0 pypi; squarify 0.4.3 pypi_0 pypi; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pypi_0 pypi; starsessions 1.3.0 pypi_0 pypi; statsmodels 0.14.0 pypi_0 pypi; stdlib-list 0.9.0 pypi_0 pypi; sympy 1.11.1 pypi_0 pypi; tensorstore 0.1.40 pypi_0 pypi; texttable 1.6.7 pypi_0 pypi; threadpoolctl 3.2.0 pypi_0 pypi; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pypi_0 pypi; torch 2.0.1+cu118 pypi_0 pypi; torchaudio 2.0.2+cu118 pypi_0 pypi; torchmetrics 1.0.1 pypi_0 pypi; torchvision 0.15.2+cu118 pypi_0 pypi; tornado 6.3.2 py311h459d7ec_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:6680,learn,learn,6680,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,2,['learn'],['learn']
Usability,"conda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 152 save_raw=save_raw,; 153 n_jobs=n_jobs,; --> 154 **kwargs,; 155 ); 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,; 125 compute_angle=compute_angle, mnn_order=mnn_order,; --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs); 127 print('Packing AnnData object...'); 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 180 print(' Computing correction vectors...'); 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,; --> 182 new_batch_in, sigma); 183 if not same_set:; 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type; ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]); ```; #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/974:2373,learn,learn,2373,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974,1,['learn'],['learn']
Usability,"core/compiler_lock.py?line=31) @functools.wraps(func); [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=32) def _acquire_compile_lock(*args, **kwargs):; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=33) with self:; ---> [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=34) return func(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); [294](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=293) mutated |= check(pss.run_initialization, internal_state); [295](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=294) with SimpleTimer() as pass_time:; --> [296](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=295) mutated |= check(pss.run_pass, internal_state); [297](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=296) with SimpleTimer() as finalize_time:; [298](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=297) mutated |= check(pss.run_finalizer, internal_state). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:269, in PassManager._runPass.<locals>.check(func, compiler_state); [268](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=267) def check(func, compiler_state):; --> [269](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=268) mangled = func(compiler_state); [270](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/sit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:25063,Simpl,SimpleTimer,25063,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['Simpl'],['SimpleTimer']
Usability,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4098,clear,clear,4098,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,6,"['clear', 'learn', 'simpl']","['clear', 'learn', 'simply']"
Usability,"cted components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually test umap’s pynndescent codepath at all (just the fast `precomputed` path for small data); - umap’s `precomputed` code does some weird things to its knn `indices` array, which we don’t test for: ; ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`; - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one; - if not, run `umap.nearest_neighbors`.; - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`; - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path); - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:3571,simpl,simply,3571,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,1,['simpl'],['simply']
Usability,"ctive/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 13 numba.config.THREADING_LAYER = ""workqueue""; 14 ; ---> 15 __version__ = pkg_resources.get_distribution(""pynndescent"").version. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in get_distribution(dist); 464 dist = Requirement.parse(dist); 465 if isinstance(dist, Requirement):; --> 466 dist = get_provider(dist); 467 if not isinstance(dist, Distribution):; 468 raise TypeError(""Expected string, Requirement, or Distribution"", dist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:2230,simpl,simplefilter,2230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,1,['simpl'],['simplefilter']
Usability,"cumentation is good, certain parameters are not described in detail, which might lead to ambiguity in their application. Notably:. - **Parameters like `use_raw`, `log`, `num_categories`, `categories_order`, etc.**: The existing documentation does not provide enough context or explanation about what each of these parameters does, their expected data types, default values, and how they influence the behavior of the plot. - **Complex Parameters**: Parameters that involve more complex concepts or data structures, such as `var_names`, `groupby`, `var_group_positions`, and `values_df`, would benefit significantly from more detailed descriptions and examples. - **Method `style` and Its Parameters**: The `style` method within the `MatrixPlot` class modifies plot visual parameters, but the implications and use cases of changing parameters like `cmap`, `edge_color`, and `edge_lw` are not well-explained. ### Suggested Improvements; To address these issues, I recommend the following enhancements:. 1. **Detailed Parameter Explanations**: Expand on the description of each parameter, especially those that are complex or not self-explanatory. This should include the type of data expected, default values, and a clear explanation of the parameter’s role and impact. 2. **Include Examples and Use Cases**: For complex parameters, providing examples or typical use cases can be extremely helpful. This could be in the form of small code snippets or scenarios illustrating when and how to use these parameters effectively. 3. **Consistency in Documentation Style**: Ensure that the documentation style is consistent across different parameters, making it easier for users to read and understand. ### Conclusion; Enhancing the documentation of the `MatrixPlot` class will improve the library's usability and user experience. . I am new to open-source contribution and I am eager to contribute to this enhancement, and welcome any additional input or guidance from the project maintainers and community.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:1750,clear,clear,1750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,4,"['clear', 'guid', 'usab', 'user experience']","['clear', 'guidance', 'usability', 'user experience']"
Usability,"d: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:3855,learn,learn,3855,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['learn'],['learn']
Usability,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells.; I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109:341,simpl,simplified,341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109,2,['simpl'],['simplified']
Usability,"data.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1622,simpl,simplefilter,1622,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,2,['simpl'],['simplefilter']
Usability,"de a directory like `scanpy/tests/_images`. Tests generate a new file with the same name as the reference image inside `scanpy/tests/figures`. These files are compared. If they're not similar enough, a file with a similar name, appended with `failed-diff` is written to the figures folder. There are a few annoyances with this setup. ### Opening the relevant files to investigate a failed test. Its not hard to open up the generated file, reference, and diff to see what changed, but it's harder than it should be. You'd have to write a function to make this at all easy. ### It's not obvious from the paths which file is the reference and which file was generated. E.g. given the paths:. ```; scanpy/tests/_images/pca.png; scanpy/tests/figures/pca.png; ```. which one is the reference?. ### Shared file names cause ambiguity. <img width=""771"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113106711-cda24400-924e-11eb-8e00-743f178e40b8.png"">. ~~This is more of a hunch.~~ There should be three images here, but only two show up. I suspect this has to do with the missing image having a duplicated name. Update: Tested this in #1773, all images show up now (@gokceneraslan):. <img width=""364"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113120191-0f39eb80-925d-11eb-9c99-b2630e829ecf.png"">. ## This PR. This PR addresses the three points from above. Now each reference image gets its own directory. The reference image is named `expected.png`, and is stored in git. When a test runs the generated image is stored as `actual.png`, and compared with `expected.png`. A failing diff is also written to this directory if the comparison fails. * Opening all images is now as simple as `open scanpy/tests/_images/{fig_name}/*`; * Naming conventions are obvious; * Per test, each file has a unique name. ## TODO. - [x] Dev docs; - [ ] Decide on fixture api; - [ ] Decide how generated outputs are handled (`.gitignore`-d, deleted each time that test is run, both?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1772:1936,simpl,simple,1936,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1772,1,['simpl'],['simple']
Usability,"does not recompute, simply saves the filtered data under; adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be; tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>; wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups; > result? Or does it recompute? The former would not alleviate the multiple; > testing burden.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471643748:20,simpl,simply,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471643748,2,['simpl'],['simply']
Usability,"done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:1130,learn,learn,1130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,4,['learn'],['learn']
Usability,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 601 **kwds); 602 ; --> 603 output[asheetname] = parser.read(nrows=nrows); 604 ; 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows); 1137 def read(self, nrows=None):; 1138 nrows = _validate_integer('nrows', nrows); -> 1139 ret = self._engine.read(nrows); 1140 ; 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows); 2419 columns, data = self._do_date_conversions(columns, data); 2420 ; -> 2421 data = self._convert_data(data); 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow); 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data); 2485 return self._convert_to_ndarrays(data, clean_na_values,; 2486 clean_na_fvalues, self.verbose,; -> 2487 clean_conv, clean_dtypes); 2488 ; 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes); 1703 # invalid input to is_bool_dtype; 1704 pass; -> 1705 cvals = self._cast_types(cvals, cast_type, c); 1706 ; 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column); 1809 except ValueError:; 1810 raise ValueError(""Unable to convert column %s to ""; -> 1811 ""type %s"" % (column, cast_type)); 1812 return values; 1813 . ValueError: Unable to convert column Cell to type float32; ```; ```python; sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:6106,learn,learn,6106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,1,['learn'],['learn']
Usability,"e first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:1279,learn,learned,1279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884,2,['learn'],['learned']
Usability,"e pipeline (step: %s)"" % \; 351 (self.pipeline_name, pass_desc); 352 patched_exception = self._patch_error(msg, e); --> 353 raise patched_exception. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:341, in PassManager.run(self, state); 339 pass_inst = _pass_registry.get(pss).pass_inst; 340 if isinstance(pass_inst, CompilerPass):; --> 341 self._runPass(idx, pass_inst, state); 342 else:; 343 raise BaseException(""Legacy pass in use""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs); 32 @functools.wraps(func); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); 294 mutated |= check(pss.run_initialization, internal_state); 295 with SimpleTimer() as pass_time:; --> 296 mutated |= check(pss.run_pass, internal_state); 297 with SimpleTimer() as finalize_time:; 298 mutated |= check(pss.run_finalizer, internal_state). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:269, in PassManager._runPass.<locals>.check(func, compiler_state); 268 def check(func, compiler_state):; --> 269 mangled = func(compiler_state); 270 if mangled not in (True, False):; 271 msg = (""CompilerPass implementations should return True/False. ""; 272 ""CompilerPass with name '%s' did not.""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/typed_passes.py:306, in ParforPass.run_pass(self, state); 295 assert state.func_ir; 296 parfor_pass = _parfor_ParforPass(state.func_ir,; 297 state.typemap,; 298 state.calltypes,; (...); 304 state.metadata,; 305 state.parfor_diagnostics); --> 306 parfor_pass.run(); 308 # check the parfor pass worked and warn if it didn't; 309 has_parfor = False. File ~/miniconda3/e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:8073,Simpl,SimpleTimer,8073,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['Simpl'],['SimpleTimer']
Usability,"e two different values for the ""names"" column. Whereas Spider gives the expected adata.var_names (e.g. Rpl5; see below), Jupyter gives a numerical code (15721, which is not included in adata.var). ### Minimal code sample (that we can copy&paste without having any data). In Spider:; ```python; de_df.head(5); ```; ```pytb; Out[34]: ; scores names logfoldchanges pvals pvals_adj; 0 9.194006 Rpl15 0.815534 3.784770e-20 1.006711e-15; 1 8.427418 Rps28 0.653911 3.533771e-17 4.699739e-13; 2 7.989542 Rps21 0.676462 1.354418e-15 1.200872e-11; 3 7.871397 Rps27 0.483027 3.507037e-15 2.055341e-11; 4 7.859277 Rps24 0.507071 3.863569e-15 2.055341e-11; ```. In Jupyter:; ```python; de_df.head(5); ```; ```pytb; Out[34]: ;   | scores | names | logfoldchanges | pvals | pvals_adj; 9.194006 | 15721 | 0.815534 | 3.784770e-20 | 1.006711e-15; 8.427418 | 23746 | 0.653911 | 3.533771e-17 | 4.699739e-13; 7.989542 | 3910 | 0.676462 | 1.354418e-15 | 1.200872e-11; 7.871397 | 5571 | 0.483027 | 3.507037e-15 | 2.055341e-11; 7.859277 | 15774 | 0.507071 | 3.863569e-15 | 2.055341e-11. In both cases, Spider and Jupyter; ```python; adata.var_names; ```; ```pytb; Index(['Xkr4', 'Gm1992', 'Gm37381', 'Rp1', 'Sox17', 'Gm37323', 'Mrpl15',; 'Lypla1', 'Gm37988', 'Tcea1',; ```. If I try to specify a different column in Jupyter I get this. ```python; de_df = sc.get.rank_genes_groups_df(database, group=groupA, gene_symbols=""symbol""); ```; ```pytb; You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat; ```. #### Versions; [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; In Spider:; scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 leidenalg==0.7.0. In Jupyter:; scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 leidenalg==0.7.0; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1426:2164,learn,learn,2164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1426,2,['learn'],['learn']
Usability,"eady satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:3725,learn,learn,3725,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['learn'],['learn']
Usability,"ealised that when I used the groups separation the legend has 2 elements: the selected sample and NA. In the older version it was only the name selected in groups. I also tried to get rid afterwards, but it modifies the whole legend... ### Code:. ```python; scanpy.pl.umap(adata,color='Sample_ID',groups='1M_curiox'); ```. ```pytb; ![out](https://user-images.githubusercontent.com/59560120/131350267-f6c2f0b2-209a-4c3c-8f88-6df0ab001333.png); ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 8.0.1; anndata 0.7.5; annoy NA; attr 20.3.0; bbknn NA; cached_property 1.5.1; cairo 1.20.0; cffi 1.14.4; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; joblib 0.17.0; jsonschema 3.2.0; kaleido 0.2.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.34.0; louvain 0.6.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; nbformat 5.1.3; numba 0.51.2; numexpr 2.7.1; numpy 1.19.4; packaging 20.4; pandas 1.1.4; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 4.14.3; prompt_toolkit 1.0.15; psutil 5.8.0; ptyprocess 0.6.0; pvectorc NA; pycparser 2.20; pygments 2.7.2; pyparsing 2.4.7; pyrsistent NA; pytz 2020.4; retrying NA; scanpy 1.7.2; scipy 1.5.3; scvi 0.6.8; seaborn 0.11.0; setuptools_scm NA; simplegeneric NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; solo 0.1; sphinxcontrib NA; statsmodels 0.12.1; storemagic NA; tables 3.6.1; texttable 1.6.3; torch 1.8.1+cu102; tornado 6.1; tqdm 4.54.0; traitlets 5.0.5; typing_extensions NA; umap 0.4.6; wcwidth 0.2.5; zipp NA; zmq 20.0.0; -----; IPython 5.8.0; jupyter_client 6.1.7; jupyter_core 4.7.0; -----; Python 3.7.8 | packaged by conda-forge | (default, Nov 27 2020, 19:24:58) [GCC 9.3.0]; Linux-4.9.0-16-amd64-x86_64-with-debian-9.13; 8 logical CPU cores; -----; Session information updated at 2021-08-30 15:50. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1988:1769,simpl,simplegeneric,1769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988,1,['simpl'],['simplegeneric']
Usability,"ease be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Hi,; We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec)|; | -----------| ----- |; | Original | 297|; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Numbe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110:1025,simpl,simplefilter,1025,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110,1,['simpl'],['simplefilter']
Usability,"ecify that it should look in a column of `var` for `var_names` rather than look for them in the index, the underlying `_prepare_dataframe` function tries to find the `var_names` in `adata.var` rather than `adata.raw.var`, even when looking for the data itself in raw. For example, this code:. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; adata.var['varnames'] = list(adata.var.index); adata.raw.var['varnames'] = list(adata.raw.var.index); 'ENSGALG00000048305' in adata.raw.var['varnames'] # returns true; sc.pl.heatmap(; adata,; var_names=marker_genes_table.iloc[:, :5].values.flatten(),; groupby='cluster_anno',; show_gene_labels=True,; swap_axes=True,; gene_symbols='varnames',; save=True,; use_raw=True,; ); ```; produces this error:; <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ERROR: Gene symbol 'ENSGALG00000048305' not found in given gene_symbols column: 'varnames'. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-40-80ce653c9d2e> in <module>; ----> 1 sc.pl.heatmap(; 2 adata,; 3 var_names=marker_genes_table.iloc[:, :5].values.flatten(),; 4 groupby='cluster_anno',; 5 show_gene_labels=True,. ~/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds); 1413 ); 1414 ; -> 1415 categories, obs_tidy = _prepare_dataframe(; 1416 adata,; 1417 var_names,. TypeError: cannot unpack non-iterable NoneType object; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; ```; scanpy==1.4.6 anndata==0.7.1 umap==0.4.3 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.23.0 statsmodels==0.11.1 python-igraph==0.8.2; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1277:2006,learn,learn,2006,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277,1,['learn'],['learn']
Usability,ed1ab_0 conda-forge; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pyhd8ed1ab_0 conda-forge; starsessions 1.3.0 pyhd8ed1ab_0 conda-forge; statsmodels 0.14.0 py310h278f3c1_1 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; suitesparse 5.10.1 h9e50725_1 conda-forge; sympy 1.12 pypyh9d50eac_103 conda-forge; tbb 2021.9.0 hf52228f_0 conda-forge; texttable 1.6.7 pyhd8ed1ab_0 conda-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.12 h27826a3_0 conda-forge; tomli 2.0.1 pyhd8ed1ab_0 conda-forge; tomlkit 0.11.8 pyha770c72_0 conda-forge; toolz 0.12.0 pyhd8ed1ab_0 conda-forge; torchaudio 2.0.2 py310_cu118 pytorch; torchmetrics 0.11.4 pyhd8ed1ab_0 conda-forge; torchtriton 2.0.0 py310 pytorch; torchvision 0.15.2 py310_cu118 pytorch; tornado 6.3.2 py310h2372a71_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.9.0 pyhd8ed1ab_0 conda-forge; trove-classifiers 2023.7.6 pyhd8ed1ab_0 conda-forge; typing 3.10.0.0 pyhd8ed1ab_0 conda-forge; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023c h71feb2d_0 conda-forge; umap-learn 0.5.3 py310hff52083_1 conda-forge; unicodedata2 15.0.0 py310h5764c6d_0 conda-forge; urllib3 1.26.15 pyhd8ed1ab_0 conda-forge; uvicorn 0.23.1 py310hff52083_0 conda-forge; virtualenv 20.24.1 pyhd8ed1ab_0 conda-forge; wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge; webencodings 0.5.1 py_1 conda-forge; websocket-client 1.6.1 pyhd8ed1ab_0 conda-forge; websockets 11.0.3 py310h2372a71_0 conda-forge; wheel 0.40.0 pyhd8ed1ab_1 conda-forge; widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge; xarray 2023.7.0 pyhd8ed1ab_0 conda-forge; xlrd 1.2.0 pyh9f0ad1d_1 conda-forge; xorg-libxau 1.0.11 hd590300_0 conda-forge; xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; yaml 0.2.5 h7f98852_2 conda-forge; zeromq 4.3.4 h9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge; zlib 1.2.13 hd590300_5 conda-forge; zstd 1.5.2 hfc55251_7 conda-forge. </p>; </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:21080,learn,learn,21080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,2,['learn'],['learn']
Usability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4823,learn,learn,4823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,8,['learn'],"['learn', 'learn-']"
Usability,"em, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:1941,clear,clear,1941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,2,['clear'],['clear']
Usability,"empty dictionary, `{}`). I have gone through my workflow and confirmed that `adata.uns['log1p']` is the expected `{'base': None}` after each preprocessing step, so I don't think the issue is with any of preprocessing code. However, when I save my adata object to a .h5ad file using the `.write()` function and then read my .h5ad file using the `sc.read()` function, when I check `adata.uns['log1p']` it is an empty dictionary - so maybe the issue is either in the writing or reading function? I am able to manually set `adata.uns['log1p']` to `{'base': None}` after reading the file, and can then run downstream functions like `tl.rank_genes_groups` without issue. I have not had this problem previously when reading .h5ad files (into either the same Jupyter notebook or into a new Jupyter notebook). Since I can manually set `adata.uns['log1p']` to `{'base': None}`, I don't think this issue is pressing. It's just a little strange to me. Thank you for any help/advice!. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). This can all be run in one Jupyter notebook and should produce the issue (unless it's something exclusively on my end; I've been able to reproduce the error with my own data and one of the scanpy built-in test datasets). Sorry the code chunks are broken up/a little long; I am using the scran normalization approach outlined in the [single cell tutorial](https://github.com/theislab/single-cell-tutorial). ```python; adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_cells = 1). # scran normalization; adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after = 1e6); sc.pp.log1p(adata_pp); sc.pp.pca(adata_pp, n_comps = 15); sc.pp.neighbors(adata_pp); sc.tl.leiden(adata_pp, key_added = 'groups', resolution = 0.5); input_groups = adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2181:1365,guid,guide,1365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2181,1,['guid'],['guide']
Usability,ent 0.5.12 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 7.4.4 pypi_0 pypi; pytest-cov 5.0.0 pypi_0 pypi; pytest-mock 3.14.0 pypi_0 pypi; pytest-nunit 1.0.7 pypi_0 pypi; pytest-xdist 3.5.0 pypi_0 pypi; python 3.12.2 hab00c5b_0_cpython conda-forge; python-dateutil 2.9.0.post0 pypi_0 pypi; pytz 2024.1 pypi_0 pypi; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h8228510_1 conda-forge; scanpy 1.10.0rc2.dev33+g9c8c095d pypi_0 pypi; scikit-image 0.22.0 pypi_0 pypi; scikit-learn 1.4.1.post1 pypi_0 pypi; scipy 1.13.0 pypi_0 pypi; seaborn 0.13.2 pypi_0 pypi; session-info 1.0.0 pypi_0 pypi; setuptools 69.2.0 pyhd8ed1ab_0 conda-forge; setuptools-scm 8.0.4 pypi_0 pypi; six 1.16.0 pypi_0 pypi; statsmodels 0.14.1 pypi_0 pypi; stdlib-list 0.10.0 pypi_0 pypi; texttable 1.7.0 pypi_0 pypi; threadpoolctl 3.4.0 pypi_0 pypi; tifffile 2024.2.12 pypi_0 pypi; tk 8.6.13 noxft_h4845f30_101 conda-forge; toolz 0.12.1 pypi_0 pypi; tqdm 4.66.2 pypi_0 pypi; typing-extensions 4.11.0 pypi_0 pypi; tzdata 2024.1 pypi_0 pypi; umap-learn 0.5.6 pypi_0 pypi; virtualenv 20.25.1 pypi_0 pypi; wheel 0.43.0 pyhd8ed1ab_1 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zarr 2.17.2 pypi_0 pypi; ```. </details>. <details>; <summary> My failing env </summary>. ```; # packages in environment at /mnt/workspace/mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-compat 1.6 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 hd590300_5 conda-forge; ca-certificates 2024.2.2 hbcca054_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.1 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.4.1 pypi_0 pypi; dask-expr 1.0.10 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.51.0 pypi_0 pypi; fsspec 2024.3.1 pyp,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:28584,learn,learn,28584,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['learn'],['learn']
Usability,ent 0.5.12 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 8.1.1 pypi_0 pypi; pytest-cov 5.0.0 pypi_0 pypi; pytest-mock 3.14.0 pypi_0 pypi; pytest-nunit 1.0.7 pypi_0 pypi; pytest-xdist 3.5.0 pypi_0 pypi; python 3.12.2 hab00c5b_0_cpython conda-forge; python-dateutil 2.9.0.post0 pypi_0 pypi; pytz 2024.1 pypi_0 pypi; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h8228510_1 conda-forge; scanpy 1.10.0rc2.dev33+g9c8c095d pypi_0 pypi; scikit-image 0.22.0 pypi_0 pypi; scikit-learn 1.4.1.post1 pypi_0 pypi; scipy 1.13.0 pypi_0 pypi; seaborn 0.13.2 pypi_0 pypi; session-info 1.0.0 pypi_0 pypi; setuptools 69.2.0 pyhd8ed1ab_0 conda-forge; setuptools-scm 8.0.4 pypi_0 pypi; six 1.16.0 pypi_0 pypi; statsmodels 0.14.1 pypi_0 pypi; stdlib-list 0.10.0 pypi_0 pypi; texttable 1.7.0 pypi_0 pypi; threadpoolctl 3.4.0 pypi_0 pypi; tifffile 2024.2.12 pypi_0 pypi; tk 8.6.13 noxft_h4845f30_101 conda-forge; toolz 0.12.1 pypi_0 pypi; tqdm 4.66.2 pypi_0 pypi; typing-extensions 4.11.0 pypi_0 pypi; tzdata 2024.1 pypi_0 pypi; umap-learn 0.5.6 pypi_0 pypi; virtualenv 20.25.1 pypi_0 pypi; wheel 0.43.0 pyhd8ed1ab_1 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zarr 2.17.2 pypi_0 pypi; ```. </details>. Luke's environment: MacOS Ventura 13.4.1. Intel MacBook pro. <details>; <summary> Luke's failing env </summary>. ```; # packages in environment at /Users/luke.zappia/miniconda3/envs/scanpy-dev:; #; # Name Version Build Channel; anndata 0.10.6 pypi_0 pypi; array-api-compat 1.4.1 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 h10d778d_5 conda-forge; ca-certificates 2024.2.2 h8857fd0_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.0 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.3.0 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.49.0 pypi_0 pypi; fsspec 2024.2.0 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; ident,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:31949,learn,learn,31949,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['learn'],['learn']
Usability,"epodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts for:; scanpy -> natsort; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1769,learn,learn,1769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,4,['learn'],['learn']
Usability,erage of 3 probes per gene. Additionally around 1/3-rd of our probes span splice junctions and can be treated differently from other probes for various downstream analyses like gDNA characterization and RNA-velocity. This matrix will enable users to use the more fine grained probe-barcode matrix for downstream analyses. This PR does the following:; - Adds an example `raw_probe_barcode_matrix.h5` to `tests/_data/visium_data/2.1.0/raw_probe_bc_matrix.h5` (this is a probe barcode matrix downsampled to 1000 features to reduce the size); - The structure of a probe barcode h5 file is; ```; /matrix Group; /matrix/barcodes Dataset {4987}; /matrix/data Dataset {17581240/Inf}; /matrix/features Group; /matrix/features/feature_type Dataset {21178}; /matrix/features/filtered_probes Dataset {21178}; /matrix/features/gene_id Dataset {21178}; /matrix/features/gene_name Dataset {21178}; /matrix/features/genome Dataset {21178}; /matrix/features/id Dataset {21178}; /matrix/features/name Dataset {21178}; /matrix/features/probe_region Dataset {21178}; /matrix/features/target_sets Group; /matrix/features/target_sets/Visium\ Mouse\ Transcriptome\ Probe\ Set Dataset {19779}; /matrix/filtered_barcodes Dataset {4987}; /matrix/indices Dataset {17581240/Inf}; /matrix/indptr Dataset {4988}; /matrix/shape Dataset {2}; ```; - Enables `_read_v3_10x_h5` to read all the metadata that is in the output H5 files into an anndata. This is done by reading all the metadata in the `h5` files into the anndata. This changes the default behaviour of while reading 10x H5 files to read in all the metadata (the code currently reads all the metadata we usually put in - this will read any additional fields if we put them in too). ; - Adds a test to make sure the reader works correctly.; <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2470:2188,guid,guidelines,2188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2470,2,['guid'],"['guide', 'guidelines']"
Usability,es 3.0.1; Pygments 2.16.1; PyJWT 2.8.0; pylint 2.17.5; pylint-venv 3.0.2; pyls-spyder 0.4.0; pynndescent 0.5.10; pyodbc 4.0.39; pyOpenSSL 23.2.0; pyparsing 3.1.1; PyQt5-sip 12.11.0; PySocks 1.7.1; pytest 7.4.2; python-dateutil 2.8.2; python-dotenv 1.0.0; python-json-logger 2.0.7; python-lsp-black 1.3.0; python-lsp-jsonrpc 1.1.2; python-lsp-server 1.7.2; python-slugify 8.0.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz_comms 3.0.0; PyWavelets 1.4.1; pyxdg 0.28; PyYAML 6.0.1; pyzmq 25.1.1; QDarkStyle 3.1; qstylizer 0.2.2; QtAwesome 1.2.3; qtconsole 5.4.4; QtPy 2.4.0; queuelib 1.6.2; referencing 0.30.2; regex 2023.10.3; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; reretry 0.11.8; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.6.0; tokenizers 0.14.0; toml 0.10.2; tomli 2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:9040,learn,learn-intelex,9040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['learn'],['learn-intelex']
Usability,"escription of what the bug is: -->; I am calculating custom connectivities using hsnw on rep 'X', I don't want to calculate PCA, I want to compute UMAP using these connectivities. ; sc.tl.umap falls back to pca in:; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_umap.py#L153; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L23. how to get sc.tl.umap to run on the precomputed 'X 'connectivities?; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; from scvelo.pp import neighbors; adata; #AnnData object with n_obs × n_vars = 4329 × 192; #obs: 'BARCODE', 'sample', 'detectable.features'; #var: 'gene_ids', 'feature_types'; #layers: 'normalized.counts'. neighbors(adata, n_neighbors = 20, use_rep = ""X"",knn = True,random_state = 0,method = 'hnsw',metric = ""euclidean"",metric_kwds = {""M"":20,""ef"":200,""ef_construction"":200},num_threads=1). adata.uns[""neighbors""]['params']; #{'n_neighbors': 20, 'method': 'hnsw', 'metric': 'euclidean', 'n_pcs': None}. sc.tl.umap(adata). #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.19.0 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.1; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1318:1960,learn,learn,1960,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318,1,['learn'],['learn']
Usability,"esired e.g. he want to focus one cell type etc. So we should support; weights generally rather specifically. Thanks,; Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,; >; > But i will suggest to just support weights instead of coreset, may be user; > want to sample data with some other weighting technique. So we should ask; > them to just put the weights for observations, then we need to modify PCA; > as well and i think my code will support most of plots and marker genes,; > but not PCA, because my input is PCA matrix with weights for each; > observations.; >; > Thanks,; > Khalid; >; > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>; > wrote:; >; >> Long-term, we should think about the design here: Specifying weights all; >> the time is possible, but not very nice for users. So a few questions come; >> to mind:; >>; >> Should we add scanpy.pp.coreset, which would create a sampling and add; >> adata.obs['coreset_weights'] or simply adata.obs['weights']?; >>; >> If we do that or plan to in the future, how should the added weights; >> parameter to all these functions work?; >>; >> I think it might default to 'coreset_weights', and the functions would; >> automatically use that .obs column if it exists. Users should also still; >> be able to specify weights manually as in this PR.; >>; >> So the type of the parameter would be Union[str, pd.DataFrame,; >> Sequence[Union[float, int]]].; >> ------------------------------; >>; >> All of that doesn’t really affect this PR, as we can merge it as it is; >> and include anndata-stored weights later.; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,; >> or mute the thread; >> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004:1457,simpl,simply,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004,2,['simpl'],['simply']
Usability,"et things a bit into order here, as at the moment some wrong impressions are around I think:. **1. Incorrect comparisons done here**; To my current knowledge,; - `sc.pp.highly_variable_genes(…, flavor=“seurat”)` mimics `FindVariableFeatures(…, method=“mean.var.plot”)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(…, flavor=“seurat_v3”)` mimics `FindVariableFeatures(…, method=“vst”)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:988,Guid,Guidance,988,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,1,['Guid'],['Guidance']
Usability,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:31,simpl,simple,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252,2,['simpl'],['simple']
Usability,"fixes #1642 . I'll reply to the comments here @adamgayoso . > shouldn't both not be Optional? . I removed optional `type` from both span and `check_values`. > On another note, in this current state check_nonnegative_integers is an unused import. Do you guys check for this with flake8?. we don't have pre-commit in place, we are discussing it here #1563 . I do check flake8 but clearly didn't do it this time. > I think a bug has been introduced @ivirshup @giovp. Namely, the call to check_nonnegative_integers(X) was removed (should be here on line 61) if returns False, raise the warning. this should be fixed now. Sorry again for very sloppy handling of this, should be ready to review",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1679:378,clear,clearly,378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679,1,['clear'],['clearly']
Usability,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:428,simpl,simple,428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,1,['simpl'],['simple']
Usability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2522,simpl,simple,2522,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,2,['simpl'],['simple']
Usability,"forge; matplotlib-base 3.8.2 py311h54ef318_0 conda-forge; munkres 1.1.4 pyh9f0ad1d_0 conda-forge; natsort 8.4.0 pyhd8ed1ab_0 conda-forge; ncurses 6.4 h59595ed_2 conda-forge; networkx 3.2.1 pyhd8ed1ab_0 conda-forge; numba 0.58.1 py311h96b013e_0 conda-forge; numpy 1.26.3 py311h64a7726_0 conda-forge; openjpeg 2.5.0 h488ebb8_3 conda-forge; openssl 3.2.0 hd590300_1 conda-forge; packaging 23.2 pyhd8ed1ab_0 conda-forge; pandas 2.1.4 py311h320fe9a_0 conda-forge; patsy 0.5.6 pyhd8ed1ab_0 conda-forge; pillow 10.2.0 py311ha6c5da5_0 conda-forge; pip 23.3.2 pyhd8ed1ab_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; pynndescent 0.5.11 pyhca7485f_0 conda-forge; pyparsing 3.1.1 pyhd8ed1ab_0 conda-forge; python 3.11.7 hab00c5b_1_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.4 pyhd8ed1ab_0 conda-forge; python_abi 3.11 4_cp311 conda-forge; pytz 2023.3.post1 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; scanpy 1.9.6 pyhd8ed1ab_1 conda-forge; scikit-learn 1.3.2 py311hc009520_2 conda-forge; scipy 1.11.4 py311h64a7726_0 conda-forge; seaborn 0.13.1 hd8ed1ab_0 conda-forge; seaborn-base 0.13.1 pyhd8ed1ab_0 conda-forge; session-info 1.0.0 pyhd8ed1ab_0 conda-forge; setuptools 69.0.3 pyhd8ed1ab_0 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; statsmodels 0.14.1 py311h1f0f07a_0 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; tbb 2021.11.0 h00ab1b0_0 conda-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.13 noxft_h4845f30_101 conda-forge; tqdm 4.66.1 pyhd8ed1ab_0 conda-forge; tzdata 2023d h0c530f3_0 conda-forge; umap-learn 0.5.5 py311h38be061_0 conda-forge; wheel 0.42.0 pyhd8ed1ab_0 conda-forge; xorg-libxau 1.0.11 hd590300_0 conda-forge; xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zstd 1.5.5 hfc55251_0 conda-forge; ```. 2) I imported the scanpy, seaborn, pandas, numpy and matplotlib libraries. Then I called the `read_10x_mtx()` function. The code is given below. ```pycon; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:4703,learn,learn,4703,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['learn'],['learn']
Usability,"fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels ❓ . And thanks all for the ongoing discussion, still learning things here and also getting new perspectives on the general topic here 👍🏻",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:2117,learn,learning,2117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,2,['learn'],['learning']
Usability,"fusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python; import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc); sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)); ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python; sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]); ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python; import numpy as np; pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""); # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms; # Wall time: 74.9 ms; to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]; sc.pl.umap(pbmc, color=to_plot, ncols=2); ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915:1819,feedback,feedback,1819,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915,1,['feedback'],['feedback']
Usability,"g intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1795,learn,learn,1795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['learn'],['learn']
Usability,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:2571,simpl,simple,2571,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,2,['simpl'],['simple']
Usability,"gend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 447 return fig; 448 axs = axs if grid else ax; --> 449 _utils.savefig_or_show(basis, show=show, save=save); 450 if show is False:; 451 return axs. ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in savefig_or_show(writekey, show, dpi, ext, save); 310 show = settings.autoshow if show is None else show; 311 if save:; --> 312 savefig(writekey, dpi=dpi, ext=ext); 313 if show:; 314 pl.show(). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in savefig(writekey, dpi, ext); 280 else:; 281 dpi = rcParams['savefig.dpi']; --> 282 settings.figdir.mkdir(parents=True, exist_ok=True); 283 if ext is None:; 284 ext = settings.file_format_figs. AttributeError: 'str' object has no attribute 'mkdir'; ```. #### Versions. scanpy==1.8.1 anndata==0.7.6 umap==0.5.1 numpy==1.18.5 scipy==1.6.2 pandas==1.1.5 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.4 louvain==0.7.0 pynndescent==0.5.2. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; clou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:3036,learn,learn,3036,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['learn'],['learn']
Usability,"genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cell = X[:, gene_subset].sum(1); 175 else:; --> 176 counts_per_cell = X.sum(1); 177 start = logg.info(msg); 178 counts_per_cell = np.ravel(counts_per_cell). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```; And when I run the command:; `type(adata_orig.X)`; I get the output as:; `anndata._core.sparse_dataset.SparseDataset`. After reading your comment, I feel that earlier the scanpy module was expecting the sparse dataset as the input, but you have changed it to expect the dense format , and maybe that's the reason for this error? I am just two days into the world of scanpy and any help would be highly appreciated, in order to make this error go away. Also, to help you in debugging, I'd like to mention that the raw data in this dataset is present in the layer, which has the name 'raw' and the adata_orig.raw is set to null as of now. and when i try to run : `adata_orig.layers['raw'].sum(1)` it runs with no error. While running: `adata_orig.X.sum(1)` gives me the error as : ; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-23-951a31c71c45> in <module>; ----> 1 adata_orig.X.sum(1). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```. PS: I downloaded this` .h5ad` file from a published research paper to perform some analysis over it, would be happy to provide you the link to same if required. . Also this is the environment that I am working in:; `scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.1 scipy==1.6.2 pandas==1.2.4 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.6`. Hello @LuckyMD, tagging you for just in case you might be knowing the resolution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:3583,learn,learn,3583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['learn'],['learn']
Usability,"gmean', 'sct_residual_mean', 'sct_residual_variance', 'sct_variable', 'sct_variance'; uns: 'Biological replicate_colors', 'ImmGen_colors', 'State_colors', 'leiden', 'neighbors', 'state'; obsm: 'X_pca', 'X_umap'; varm: 'pca_feature_loadings'; layers: 'norm_data', 'scale_data'; obsp: 'connectivities', 'distances'; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-208-9f15be957dd9> in <module>; 1 sc.external.exporting.spring_project(adata, '/Users/mariusmessemaker/Documents/Project/mempel/SPRING', 'X_umap', subplot_name='Mempel', cell_groupings=['State', 'ImmGen', 'Biological replicate'], ; ----> 2 custom_color_tracks=None, total_counts_key='nCount_RNA', neighbors_key='neighbors', overwrite=False). ~/miniconda3/envs/py36-sc/lib/python3.6/site-packages/scanpy/external/exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, neighbors_key, overwrite); 179 ; 180 # Write graph in two formats for backwards compatibility; --> 181 edges = _get_edges(adata, neighbors_key); 182 _write_graph(subplot_dir / 'graph_data.json', E.shape[0], edges); 183 _write_edges(subplot_dir / 'edges.csv', edges). ~/miniconda3/envs/py36-sc/lib/python3.6/site-packages/scanpy/external/exporting.py in _get_edges(adata, neighbors_key); 217 ; 218 def _get_edges(adata, neighbors_key=None):; --> 219 neighbors = NeighborsView(adata, neighbors_key); 220 if 'distances' in neighbors: # these are sparse matrices; 221 matrix = neighbors['distances']. NameError: name 'NeighborsView' is not defined; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.0 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1260:2864,learn,learn,2864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1260,1,['learn'],['learn']
Usability,h code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igraph 0.11.4; imageio 2.34.0; importlib-metadata 7.0.1; importlib-resources 6.1.1; iniconfig 2.0.0; intervaltree 3.1.0; Jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; llvmlite 0.42.0; locket 1.0.0; magic-impute 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; msgpack 1.0.7; multipledispatch 1.0.0; natsort 8.4.0; networkx 3.2.1; nodeenv 1.8.0; numba 0.59.0; numcodecs 0.12.1; numpy 1.26.4; packaging 23.2; pandas 2.0.3; partd 1.4.1; patsy 0.5.6; pbr 6.0.0; pillow 10.2.0; pip 24.0; platformdirs 4.2.0; pluggy 1.4.0; pre-commit 3.6.2; profimp 0.1.0; psutil 5.9.8; PyGSP 0.5.1; pynndescent 0.5.11; pyparsing 3.1.1; pytest 8.0.1; pytest-mock 3.12.0; pytest-nunit 1.0.6; python-dateutil 2.8.2; pytz 2024.1; PyYAML 6.0.1; scanorama 1.7.4; scanpy 1.10.0.dev220+g534145f6; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; scprep 1.2.3; seaborn 0.13.2; session-info 1.0.0; setuptools 58.1.0; setuptools-scm 8.0.4; six 1.16.0; sortedcontainers 2.4.0; sparse 0.15.1; statsmodels 0.14.1; stdlib-list 0.10.0; tasklogger 1.2.0; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tomli 2.0.1; toolz 0.12.1; tornado 6.4; tqdm 4.66.2; typing_extensions 4.9.0; tzdata 2024.1; umap-learn 0.5.5; urllib3 2.2.1; virtualenv 20.25.0; wheel 0.42.0; wrapt 1.16.0; zarr 2.17.0; zict 3.0.0; zipp 3.17.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:8308,learn,learn,8308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,2,['learn'],['learn']
Usability,"h_error(msg, e); --> 353 raise patched_exception. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:341, in PassManager.run(self, state); 339 pass_inst = _pass_registry.get(pss).pass_inst; 340 if isinstance(pass_inst, CompilerPass):; --> 341 self._runPass(idx, pass_inst, state); 342 else:; 343 raise BaseException(""Legacy pass in use""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs); 32 @functools.wraps(func); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); 294 mutated |= check(pss.run_initialization, internal_state); 295 with SimpleTimer() as pass_time:; --> 296 mutated |= check(pss.run_pass, internal_state); 297 with SimpleTimer() as finalize_time:; 298 mutated |= check(pss.run_finalizer, internal_state). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:269, in PassManager._runPass.<locals>.check(func, compiler_state); 268 def check(func, compiler_state):; --> 269 mangled = func(compiler_state); 270 if mangled not in (True, False):; 271 msg = (""CompilerPass implementations should return True/False. ""; 272 ""CompilerPass with name '%s' did not.""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/typed_passes.py:306, in ParforPass.run_pass(self, state); 295 assert state.func_ir; 296 parfor_pass = _parfor_ParforPass(state.func_ir,; 297 state.typemap,; 298 state.calltypes,; (...); 304 state.metadata,; 305 state.parfor_diagnostics); --> 306 parfor_pass.run(); 308 # check the parfor pass worked and warn if it didn't; 309 has_parfor = False. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:2926, in ParforPass.run(self); 2924 #",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:8167,Simpl,SimpleTimer,8167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['Simpl'],['SimpleTimer']
Usability,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:2093,simpl,simple,2093,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884,2,['simpl'],['simple']
Usability,"he full dataset) with ```sc.pp.filter_genes(adata, min_counts=1)``` and now the zero variance genes are gone but still same warning, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/aues",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1097,learn,learn,1097,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,2,['learn'],['learn']
Usability,"he user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibility but we should probably stick with it. hence, another argument for simply using a dataframe and putting it in the unstructured annotation `.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:1311,simpl,simple,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458,4,['simpl'],"['simple', 'simply']"
Usability,"hed_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 ; 343 def dependency_analysis(self):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 330 pass_inst = _pass_registry.get(pss).pass_inst; 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:; 334 raise BaseException(""Legacy pass in use""). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 289 mutated |= check(pss.run_initialization, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:; 293 mutated |= check(pss.run_finalizer, internal_state). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 262 ; 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):; 266 msg = (""CompilerPass implementations should return True/False. "". ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 440 ; 441 # TODO: Pull this out into the pipeline; --> 442 NativeLowering().run_pass(state); 443 lowered = state['cr']; 444 signature = typing.signature(state.return_type, *state.args). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 368 lower = lowering.Lower(targetctx, library, fndesc, interp,; 369 metadata=metadata); --> 370 lower.lower(); 371 if not flags.no_cpython_wrapper:; 372 lower.create_cpython_wrapper(flags.release_gil). ~\AppDat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:10680,Simpl,SimpleTimer,10680,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['Simpl'],['SimpleTimer']
Usability,"hey @WeilerP ,; > umap-learn>=0.5.1 should work (see here). I think this approach would be best, since numba>=0.53 supports python>=3.9. I agree this is the best solution, and don't think there is any drawback from scanpy side. Can @Koncopd @ivirshup comment on this? If so, I think it would be easy to inlcude it in 1.8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-845733520:23,learn,learn,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845733520,2,['learn'],['learn']
Usability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:20,feedback,feedback,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,2,['feedback'],['feedback']
Usability,"hi @yotamcons ,. thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561:38,feedback,feedback,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561,2,['feedback'],['feedback']
Usability,"hi Alex; here my list :) thanks a lot! and I might expand it.... 'needed'. - [x] concatenate mulitple data sets; - [ ] scale up sc.pp.regress_out, add some function for batch correction?; - [x] additional heatmap annotation for cells/genes (e.g. .smp); - [ ] aga-graph: labels in pie charts are misleading (sometimes switched), maybe better to have just a legend with the colors than labeling every node. 'nice to have'; - [x] In scatterplot showing gene expression: plot points ordered by expression, i.d. cells with higher expression on top of cells with lower expression; - [x] log transform (let user choose the base in sc.pp.log1p (natural log., log2 or log10); - [x] cell cycle scoring (and scoring for any other gene list); - [x] function and plots for basic qc metrics (n_counts, n_genes, CV, %drop out) ; - [ ] sc.pl.scatter also for genes (adata.var, e.g. to plot e.g. mean expression vs dropout rate); - [x] table for high scoring genes, in addtition to sc.pl.rank_genes_groups; - [x] additional differential expression test; - [x] Heatmap for genes per cluster/sample/condition (not just along a path in pseuodtime order) including custom sample and gene annotation; - [ ] Maybe also an option for simplified visualization with just mean expression per cluster.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45:1210,simpl,simplified,1210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45,1,['simpl'],['simplified']
Usability,"hmm, we could just make the upload conditional to the file existing and merge this into `main`. Then we can debug things as soon as the problem occurs somewhere, and finally undo this PR when commiting the actual fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3069#issuecomment-2202643573:174,undo,undo,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3069#issuecomment-2202643573,2,['undo'],['undo']
Usability,hon-fastjsonschema=2.18.0; - python-igraph=0.11.3; - python-json-logger=2.0.7; - python-kaleido=0.2.1; - python-tzdata=2023.3; - python_abi=3.11; - pytorch=2.0.1; - pytz=2023.3; - pyvcf3=1.0.3; - pyyaml=6.0.1; - pyzmq=25.1.1; - radian=0.6.7; - rchitect=0.4.1; - readline=8.2; - referencing=0.30.2; - requests=2.31.0; - rfc3339-validator=0.1.4; - rfc3986-validator=0.1.1; - rpds-py=0.9.2; - scanpy=1.10.1; - scikit-learn=1.3.0; - scipy=1.11.2; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.2; - session-info=1.0.0; - setuptools=68.1.2; - simplejson=3.19.2; - six=1.16.0; - snappy=1.1.10; - sniffio=1.3.0; - soupsieve=2.3.2.post1; - stack_data=0.6.2; - statsmodels=0.14.0; - stdlib-list=0.10.0; - svt-av1=1.6.0; - sympy=1.12; - tbb=2021.11.0; - tenacity=8.2.3; - terminado=0.17.1; - texttable=1.7.0; - threadpoolctl=3.2.0; - tinycss2=1.2.1; - tk=8.6.12; - tomli=2.0.1; - torchvision=0.15.2; - tornado=6.3.3; - traitlets=5.9.0; - typing_extensions=4.8.0; - typing_utils=0.1.0; - tzdata=2023c; - umap-learn=0.5.5; - uri-template=1.3.0; - wcwidth=0.2.6; - webcolors=1.13; - webencodings=0.5.1; - websocket-client=1.6.2; - wheel=0.41.2; - x264=1!164.3095; - x265=3.5; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2.22.0; - google-auth-oauthlib==1.0.0; - google-pasta==0.2.0; - grpcio==1.57.0; - imageio==2.34.1; - keras==2.13.1; - lazy-loader==0.4; - libclang==16.0.6; - louvain==0.8.2; - markdown==3.4.4; - numpy==1.24.3; - oauthlib==3.2.2; - opt-einsum==3.3.0; - protobuf==4.24.1; - pyasn1==0.5.0; - pyasn1-modules==0.3.0; - requests-oauthlib==1.3.1; - rsa==4.9; - scikit-image==0.24.0; - tensorboard==2.13.0; - tensorboard-data-server==0.7.1; - tensorflow==2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:14279,learn,learn,14279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['learn'],['learn']
Usability,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py; import numba; import warnings. with warnings.catch_warnings():; warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):; do_thing(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/688#issuecomment-504451555:162,simpl,simplefilter,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688#issuecomment-504451555,2,['simpl'],['simplefilter']
Usability,"https://github.com/scverse/scanpy/blob/master/.azure-pipelines.yml#L14. https://learn.microsoft.com/en-us/answers/questions/1181262/what-happens-to-azure-vms-with-ubuntu-18-04-lts-af. and our CI already cries about it. Let's up this to a more recent version, maybe even 22.04?. While we're at it, I'd also remove 3.7 from the CI because it'll lose support very soon. We could add one of the more recent Python versions instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2446:80,learn,learn,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2446,1,['learn'],['learn']
Usability,https://github.com/theislab/scanpy/blob/0caaa9d2e684e2aa76acdf6672d71d7ac38b33cf/scanpy/preprocessing/simple.py#L822; it is not possible to calculate mean for 'SparseDataset' because it is not implemented.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/89:102,simpl,simple,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/89,1,['simpl'],['simple']
Usability,i just learned that OSX sends its locale per default when connecting to a server. so is it a local ubuntu or on a server?. what does `locale` (executed from a terminal) return?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344015034:7,learn,learned,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344015034,2,['learn'],['learned']
Usability,"id some digging (=a lot of digging) and found that `scanpy.pl._tools.paga._compute_pos` uses igraph layouts, i.e. `pos_list = g.layout(...)`. Apparently, igraph uses python's built-in RNG, which has not been seeded. I therefore did the following, which fixed my irreproducibility problem:. ```python; import random; random.seed(0). sc.tl.paga(adata); sc.pl.paga(adata, color='degree_solid'). print(adata.uns['paga']['pos']) # Exact same result each run; ```. I think it would be nice if this could be included wherever igraph layouts are used to improve reproducibility. I'm not sure, but perhaps issue #1418 occurs due to this problem. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.2.0; anndata 0.7.5; cairo 1.20.0; cffi 1.14.5; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 5.0.6; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.34.0; matplotlib 3.3.4; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; nt NA; ntsecuritycon NA; numba 0.51.2; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.4; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 1.0.15; psutil 5.8.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pythoncom NA; pytz 2021.1; pywintypes NA; scanpy 1.7.2; scipy 1.6.2; setuptools_scm NA; simplegeneric NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; texttable 1.6.3; tornado 6.1; traitlets 5.0.5; typing_extensions NA; umap 0.4.6; wcwidth 0.2.5; win32api NA; win32com NA; win32security NA; zipp NA; zmq 20.0.0; -----; IPython 5.8.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.3.0; -----; Python 3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.19041-SP0; 8 logical CPU cores, Intel64 Family 6 Model 142 Stepping 12, GenuineIntel; -----. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1859:2271,simpl,simplegeneric,2271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859,1,['simpl'],['simplegeneric']
Usability,"ies'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:2512,learn,learn,2512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,4,['learn'],['learn']
Usability,"iled in object mode without forceobj=True. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); Traceback (most recent call last):. File ""<ipython-input-2-111b3b404a99>"", line 7, in <module>; batch_categories=['1','2','3','4','5','6','7','8','9','10','11','12']). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py"", line 154, in mnn_correct; **kwargs,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/mnn.py"", line 126, in mnn_correct; svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/mnn.py"", line 182, in mnn_correct; new_batch_in, sigma). IndexError: arrays used as indices must be of integer (or boolean) type; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; >scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:28289,learn,learn,28289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,1,['learn'],['learn']
Usability,ion; ------------------- ---------; anndata 0.7.8; anyio 2.2.0; argon2-cffi 20.1.0; async-generator 1.10; attrs 21.2.0; Babel 2.9.1; backcall 0.2.0; bleach 4.1.0; Bottleneck 1.3.2; brotlipy 0.7.0; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.4; colorama 0.4.4; cryptography 36.0.0; cycler 0.11.0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fonttools 4.25.0; h5py 3.6.0; idna 3.3; igraph 0.9.9; importlib-metadata 4.8.2; ipykernel 6.4.1; ipython 7.29.0; ipython-genutils 0.2.0; jedi 0.18.0; Jinja2 3.0.2; joblib 1.1.0; json5 0.9.6; jsonschema 3.2.0; jupyter-client 7.1.0; jupyter-core 4.9.1; jupyter-server 1.4.1; jupyterlab 3.2.1; jupyterlab-pygments 0.1.2; jupyterlab-server 2.10.2; kiwisolver 1.3.1; leidenalg 0.8.8; llvmlite 0.37.0; MarkupSafe 2.0.1; matplotlib 3.5.0; matplotlib-inline 0.1.2; mistune 0.8.4; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; mock 4.0.3; munkres 1.1.4; natsort 8.0.2; nbclassic 0.2.6; nbclient 0.5.3; nbconvert 6.1.0; nbformat 5.1.3; nest-asyncio 1.5.1; networkx 2.6.3; notebook 6.4.6; numba 0.54.1; numexpr 2.8.1; numpy 1.20.3; olefile 0.46; packaging 21.3; pandas 1.3.5; pandocfilters 1.4.3; parso 0.8.3; patsy 0.5.2; pickleshare 0.7.5; Pillow 8.4.0; pip 21.2.2; prometheus-client 0.12.0; prompt-toolkit 3.0.20; pycparser 2.21; Pygments 2.10.0; pynndescent 0.5.5; pyOpenSSL 21.0.0; pyparsing 3.0.4; pyrsistent 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; python-igraph 0.9.9; pytz 2021.3; pywin32 302; pywinpty 0.5.7; pyzmq 22.3.0; requests 2.27.1; scanpy 1.8.2; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; Send2Trash 1.8.0; setuptools 58.0.4; sinfo 0.3.4; sip 4.19.13; six 1.16.0; sniffio 1.2.0; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; terminado 0.9.4; testpath 0.5.0; texttable 1.6.4; threadpoolctl 2.2.0; tornado 6.1; tqdm 4.62.3; traitlets 5.1.1; umap-learn 0.5.2; urllib3 1.26.7; wcwidth 0.2.5; webencodings 0.5.1; wheel 0.37.1; win-inet-pton 1.1.0; wincertstore 0.2; xlrd 1.2.0; zipp 3.7.0. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:4757,learn,learn,4757,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,2,['learn'],['learn']
Usability,"ional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2627,learn,learning,2627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['learn'],['learning']
Usability,"it's not clear what the problem here sorry, can you copy the error and report a reproducible example? thank you!; I'll close this for now, feel free to reopen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437:9,clear,clear,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437,2,['clear'],['clear']
Usability,"itle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds); 930 dot_color_df=dot_color_df,; 931 ax=ax,; --> 932 **kwds,; 933 ); 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds); 142 layer=layer,; 143 ax=ax,; --> 144 **kwds,; 145 ); 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds); 111 num_categories,; 112 layer=layer,; --> 113 gene_symbols=gene_symbols,; 114 ); 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 1837 # translate the column names to the symbol names; 1838 obs_tidy.rename(; -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},; 1840 inplace=True,; 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0); 1837 # translate the column names to the symbol names; 1838 obs_tidy.rename(; -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},; 1840 inplace=True,; 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope; ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1636:2838,learn,learn,2838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636,1,['learn'],['learn']
Usability,"ix | 1.16.0 | six | 1.16.0; sniffio | 1.2.0 | sniffio | 1.2.0 | sniffio | 1.2.0; sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0; statsmodels | 0.13.1 | statsmodels | 0.13.1 | statsmodels | 0.13.1; stdlib-list | 0.8.0 | stdlib-list | 0.8.0 | stdlib-list | 0.8.0; tables | 3.6.1 | tables | 3.6.1 | tables | 3.6.1; tblib | 1.7.0 | tblib | 1.7.0 | tblib | 1.7.0; terminado | 0.9.4 | terminado | 0.9.4 | terminado | 0.9.4; testpath | 0.5.0 | testpath | 0.5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:14637,learn,learn,14637,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['learn'],['learn']
Usability,"kages/anndata/base.py"", line 1205, in __getitem__; return self._getitem_view(index); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__; self._init_as_view(X, oidx, vidx); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view; var_sub = adata_ref.var.iloc[vidx_normalized]; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__; return self._getitem_axis(maybe_callable, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis; return self._getbool_axis(key, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis; inds, = key.nonzero(); ValueError: too many values to unpack (expected 1); ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py; keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]; adata = adata[keep_cells, :]; ```. or . ```py; adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells ; ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion?. Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297:2256,feedback,feedback,2256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297,2,['feedback'],['feedback']
Usability,"kwds); 577 edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist]); 578 ; --> 579 if not cb.iterable(width):; 580 lw = (width,); 581 else:. AttributeError: module 'matplotlib.cbook' has no attribute 'iterable'; ```; ![Screenshot 2020-10-16 at 16 10 20](https://user-images.githubusercontent.com/32264060/96275744-ff3d9080-0fc9-11eb-8706-d398e3b08c79.png). #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; atomicwrites 1.3.0; attr 20.2.0; backcall 0.2.0; cffi 1.14.3; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; defusedxml 0.6.0; future_fstrings NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; importlib_metadata 0.23; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.14.0; kiwisolver 1.1.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.6.1+2.g8073db7; matplotlib 3.3.2; more_itertools NA; mpl_toolkits NA; natsort 6.0.0; networkx 2.3; numba 0.51.2; numexpr 2.7.0; numpy 1.19.1; packaging 20.4; pandas 1.1.2; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pluggy 0.13.0; prompt_toolkit 3.0.7; psutil 5.7.2; ptyprocess 0.6.0; py 1.8.0; pycparser 2.20; pygments 2.7.1; pyparsing 2.4.2; pytest 5.2.1; pytz 2019.2; scanpy 1.6.0; scipy 1.5.2; setuptools_scm NA; simplejson 3.17.2; sinfo 0.3.1; six 1.15.0; sklearn 0.21.3; sphinxcontrib NA; storemagic NA; tables 3.5.2; texttable 1.6.3; tornado 6.0.4; tqdm 4.36.1; traitlets 4.3.3; typing_extensions NA; umap 0.3.10; wcwidth 0.2.5; xlrd 1.2.0; yaml 5.1.2; zipp NA; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:25:08) [GCC 7.5.0]; Linux-4.4.0-190-generic-x86_64-with-debian-buster-sid; 24 logical CPU cores, x86_64; -----; Session information updated at 2020-10-16 15:10. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1459:3286,simpl,simplejson,3286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1459,1,['simpl'],['simplejson']
Usability,"later development versions will then again show `0.1+NUMCOMMITS.gHASH` and it will be clear for the user which kind of version she/he uses. i think it makes sense, do you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/15#issuecomment-298315116:86,clear,clear,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298315116,2,['clear'],['clear']
Usability,"layed around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed corr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:3551,simpl,simply,3551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,2,['simpl'],['simply']
Usability,"le_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 552 if title[icolor] is not None:; 553 axs[icolor].set_title(title[icolor]); --> 554 sct = _paga_graph(; 555 adata,; 556 axs[icolor],. ~/github/scanpy/scanpy/plotting/_tools/paga.py in _paga_graph(adata, ax, solid_edges, dashed_edges, adjacency_solid, adjacency_dashed, transitions, threshold, root, colors, labels, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, normalize_to_color, title, pos, cmap, frameon, min_edge_width, max_edge_width, export_to_gexf, colorbar, use_raw, cb_kwds, single_component, arrowsize); 820 with warnings.catch_warnings():; 821 warnings.simplefilter(""ignore""); --> 822 nx.draw_networkx_edges(; 823 nx_g_solid, pos, ax=ax, width=widths, edge_color='black'; 824 ). /usr/local/lib/python3.8/site-packages/networkx/drawing/nx_pylab.py in draw_networkx_edges(G, pos, edgelist, width, edge_color, style, alpha, arrowstyle, arrowsize, edge_cmap, edge_vmin, edge_vmax, ax, arrows, label, node_size, nodelist, node_shape, connectionstyle, min_source_margin, min_target_margin); 654 ; 655 # set edge positions; --> 656 edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist]); 657 ; 658 # Check if edge_color is an array of floats and map to edge_cmap. /usr/local/lib/python3.8/site-packages/networkx/drawing/nx_pylab.py in <listcomp>(.0); 654 ; 655 # set edge positions; --> 656 edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist]); 657 ; 658 # Check if edge_color is an array of floats and map to edge_cmap. KeyError: 5; ```. </details>. In this PR:. ![image](https://user-images.githubusercontent.com/8238804/123222118-f5820a80-d512-11eb-97",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1898:2782,simpl,simplefilter,2782,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1898,2,['simpl'],['simplefilter']
Usability,lite 0.34.0 ; markdown-it-py 0.5.6 ; MarkupSafe 1.1.1 ; matplotlib 3.3.3 ; mccabe 0.6.1 ; mistune 0.8.4 ; mypy-extensions 0.4.3 ; natsort 7.0.1 ; nbclient 0.5.1 ; nbconvert 6.0.7 ; nbdime 2.1.0 ; nbformat 5.0.8 ; nbresuse 0.3.6 ; nest-asyncio 1.4.3 ; networkx 2.5 ; notebook 6.1.5 ; numba 0.51.2 ; numexpr 2.7.1 ; numpy 1.19.4 ; packaging 20.4 ; pandas 1.1.4 ; pandocfilters 1.4.3 ; parso 0.7.1 ; path 15.0.0 ; pathspec 0.8.1 ; pathtools 0.1.2 ; patsy 0.5.1 ; peepdis 0.1.13 ; pexpect 4.8.0 ; pickleshare 0.7.5 ; Pillow 8.0.1 ; pip 20.0.2 ; plotly 4.12.0 ; pluggy 0.13.1 ; prometheus-client 0.8.0 ; prompt-toolkit 3.0.8 ; psutil 5.7.3 ; ptvsd 4.3.2 ; ptyprocess 0.6.0 ; py 1.9.0 ; pycodestyle 2.6.0 ; pycparser 2.20 ; pydocstyle 5.1.1 ; pyflakes 2.2.0 ; Pygments 2.7.2 ; PyGObject 3.36.0 ; pylint 2.6.0 ; pymongo 3.11.0 ; pyparsing 2.4.7 ; pyrsistent 0.17.3 ; pytest 6.1.2 ; python-apt 2.0.0+ubuntu0.20.4.1 ; python-dateutil 2.8.1 ; python-debian 0.1.36ubuntu1 ; python-jsonrpc-server 0.4.0 ; python-language-server 0.36.1 ; pytoml 0.1.21 ; pytz 2020.4 ; PyYAML 5.3.1 ; pyzmq 20.0.0 ; regex 2020.11.13 ; requests 2.22.0 ; requests-unixsocket 0.2.0 ; retrying 1.3.3 ; rich 9.2.0 ; rope 0.18.0 ; scikit-learn 0.23.2 ; scikit-misc 0.1.3 ; scipy 1.5.4 ; scriptedforms 0.10.1 ; scvi-tools 0.7.1 ; seaborn 0.11.0 ; Send2Trash 1.5.0 ; setuptools 50.3.2 ; setuptools-scm 4.1.2 ; sinfo 0.3.1 ; six 1.14.0 ; smmap 3.0.4 ; snowballstemmer 2.0.0 ; SQLAlchemy 1.3.20 ; statsmodels 0.12.1 ; stdlib-list 0.7.0 ; tables 3.6.1 ; termcolor 1.1.0 ; terminado 0.9.1 ; testpath 0.4.4 ; threadpoolctl 2.1.0 ; toml 0.10.2 ; torch 1.7.0 ; tornado 6.1 ; tqdm 4.51.0 ; traitlets 5.0.5 ; typed-ast 1.4.1 ; typeguard 2.10.0 ; typing-extensions 3.7.4.3 ; ujson 4.0.1 ; umap-learn 0.4.6 ; unattended-upgrades 0.1 ; urllib3 1.25.8 ; watchdog 0.10.3 ; wcwidth 0.2.5 ; webencodings 0.5.1 ; Werkzeug 1.0.1 ; wheel 0.35.1 ; widgetsnbextension 3.5.1 ; wrapt 1.12.1 ; xeus-python 0.8.3 ; xlrd 1.2.0 ; yapf 0.30.0 ; zipp 3.4.0. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:6198,learn,learn,6198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,2,['learn'],['learn']
Usability,"lkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3836,learn,learn,3836,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,2,['learn'],['learn']
Usability,"ll of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. It’s a big improvement to no longer have ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:1030,learn,learn,1030,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,6,['learn'],['learn']
Usability,"ll the LOBPCG code. Moreover, if one calls the LOBPCG algorithm for 5k > n, it would likely break internally, so the code calls the standard function eigh instead. It is not that n should be large for the LOBPCG to work, but rather the ratio n / k should be large. It you call LOBPCG with k=1 and n=10, it works though n is small. The method is intended for extremely large n / k. The workaround was to rerun failed tests until the fixture randomly returned another solver, which isn’t great. I therefore simply `xfail` that test for now to not block PRs in https://github.com/scverse/scanpy/pull/2745. ### Minimal code sample. ```bash; # try one of the following to reproduce (if necessary multiple times); pytest --runxfail 'scanpy/tests/test_pca.py::test_pca_warnings[scipy_csr-zero_center-valid]'; pytest --runxfail 'scanpy/tests/test_pca.py::test_pca_warnings[scipy_csc-zero_center-valid]'; ```. ### Error output. (note that since the code is run with `warnings.simplefilter('error')`, the below is an error that fails the test). ```pytb; UserWarning: The problem size 5 minus the constraints size 0 is too small relative to the block size 4. Using a dense eigensolver instead of LOBPCG iterations.No output of the history of the iterations.; ```. ### Versions. <details>. ```; -----; anndata 0.10.2; scanpy 1.10.0.dev156+gd1a2c8f8.d20231110; -----; PIL 10.0.1; asciitree NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2023.10.0; dateutil 2.8.2; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jinja2 3.1.2; joblib 1.3.2; kiwisolver 1.4.5; leidenalg 0.10.1; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.0; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.1; packaging 23.2; pandas 2.1.1; pluggy 1.3.0; psutil 5.9.6; py NA; pyparsing 3.1.1; pytest 7.4.3; pytz 2023.3.post1; scipy 1.11.3; session_info 1.0.0; setuptools 68.2.2; setuptools_scm NA; six 1.16.0; sklearn 1.3.2; sparse 0.14.0; tblib 3.0.0; texttable 1.7.0; threadpo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2744:1710,simpl,simplefilter,1710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2744,1,['simpl'],['simplefilter']
Usability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:2344,simpl,simply,2344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,2,['simpl'],['simply']
Usability,"log transformation applied after normalisation seems to upset this relationship, example below. Why is this not problematic?. Incidentally, I first noticed this on my real biological dataset, not the toy example below. Edit: [relevant paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6215955/). > We can show, mathematically, that if we normalize expression profiles to have the same mean across cells, the mean after the equation [log] transformation used for RNA-Seq data will not be the same, and it will depend on the detection rate... And this [one](https://www.biorxiv.org/content/10.1101/404962v1.full):. > One issue of particular interest is that the mean of the log-counts is not generally the same as the log-mean count [1]. This is problematic in scRNA-seq contexts where the log-transformation is applied to normalized expression data. ---. ### Minimal code sample. ```python; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import numpy as np; >>> adata = AnnData(np.array([[3, 3, 3, 6, 6],[1, 1, 1, 2, 2],[1, 22, 1, 2, 2], ])); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; >>> X_norm_log = np.log1p(X_norm); >>> X_norm_again = np.expm1(X_norm_log); >>> adata.X.sum(axis=1); array([21., 7., 28.], dtype=float32) # Different counts for each cell; >>> X_norm.sum(axis=1); array([1., 1., 1.], dtype=float32) # Normalisation means same counts for each cell; >>> X_norm_log.sum(axis=1); array([0.90322304, 0.90322304, 0.7879869 ], dtype=float32) # <<< Interested in this! Different counts for each cell; >>> X_norm_again.sum(axis=1); array([1., 1., 1.], dtype=float32) # Counts the same again; ```. #### Versions. I'm not using the latest scanpy and anndata verions, but i don't think this will be different on the master branch. <details>. >>> sc.logging.print_versions(); scanpy==1.4.5.post2 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.2.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364:2380,learn,learn,2380,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364,1,['learn'],['learn']
Usability,"looks really great, I'd say Yes! to pretty much everything, thanks for looking into this.; > Getting the spot size even if the image isn't used. so this is something we are going back and forth a lot, I still think it has pros and cons, and also agree with the point below re having a separate argument `spot_size`. ; If `sc.pl.spatial` *always* plot `circles` and not `scatter`, then the size of the radius needs to be inferred from the data: this makes me a bit worried for the different cases that could arise. I understand that is much nicer that the function returns always the same type of plot (circles), but it might be a bit forcing in this context (given the heterogeneity of the data). What I would agree instead is to pass the `size_spot` and use circles, otherwise simply use scatter (and then set size). What do you think? . > Using ax.set_aspect(""equal"") when there is no image, so that the aspect ratio is equivalent (coordinates are assumed to be pixel space); this is really nice, I'd say yes in principle, would like to try it out though for couple of plots. > If crop_coords is not passed, use the cropping matplotlib would have used if there was no image. This is done by getting the axis limits before the image is added. this is also fine and probably cleaner than having an heuristic for the offset. > I feel like it would make sense for these to crop to the same part of the image or embedding:. missed this, ok yes it makes sense, then metric of `crop_coord` is the same as `adata.obsm[""spatial""]`. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-756087364:778,simpl,simply,778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-756087364,2,['simpl'],['simply']
Usability,"lorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.1; matplotlib 3.3.2; matplotlib_inline NA; ml_collections NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.1; multipledispatch 0.6.0; nats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3765,learn,learn,3765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,2,['learn'],['learn']
Usability,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(); ```; ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```; def style(; self,; cmap: str = DEFAULT_COLORMAP,; color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,; dot_max: Optional[float] = DEFAULT_DOT_MAX,; dot_min: Optional[float] = DEFAULT_DOT_MIN,; .....; ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1633:1554,clear,clearly,1554,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633,1,['clear'],['clearly']
Usability,"lysis and yield no errors. However, obviously, subsequent call to ```sc.highly_variable_genes()``` will result in disaster. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/aues",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1175:1356,learn,learn,1356,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1175,1,['learn'],['learn']
Usability,"manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understand this from the coding and engineering perspective, very ugly and violates several principles of good design. But on the other hand, it makes the life of many practitioners easier by setting the plotting config early on in a ""Scanpy notebook session"", which is clearly created to do research on single-cell genomics with scanpy, without rerunning things several times. For example, I use plotnine sometimes for publications too, but I hate writing `theme(text=element_text(family='Arial'))` every time I make a figure. plotnine is a general-purpose plotting library so it's not their problem, but people use Scanpy for both data exploration AND publications. So I think we can do better than that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:2263,clear,clearly,2263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,2,['clear'],['clearly']
Usability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3269,clear,clear,3269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,2,['clear'],['clear']
Usability,"mbedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": {; ""library1"": {...},; ""library2"": {...},; ...; },; ""spatial_neighbors"": {; ""library_ids"": [""library1"", ...],; ""connectivities_key"": ...,; ""distances_key"": ...,; ""params"": {...},; },; }; ```. yes indeed, I will change all occurrences in squidpy so that `sc.pl.spatial` can simply work as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:1876,clear,clearer,1876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,4,"['clear', 'simpl']","['clearer', 'simply']"
Usability,"me, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7202,Simpl,SimpleTimer,7202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['Simpl'],['SimpleTimer']
Usability,"med this bug exists on the main branch of scanpy. ### What happened?. When running highly_variable_genes with batch_key input, and the data contains batch with only one sample (due to earlier filtering) the computation failed, returning ""division by zero"" error. ; The expected behavior is to ignore this batch (a single sample). ### Minimal code sample. ```python; #Create dummy anndata with batch key. Batch 3 contains 1 sample.; test_ad = ad.AnnData(X=np.random.randn(10,100), ; obs=pd.DataFrame({'batch':pd.Categorical([1,1,1,1,2,2,2,2,2,3])})); #This works fine; sc.pp.highly_variable_genes(test_ad). #This returns division by zero error; #sc.pp.highly_variable_genes(test_ad, batch_key='batch'); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; ZeroDivisionError Traceback (most recent call last); Cell In[41], line 11; 9 with warnings.catch_warnings(): #ignore future_warning in groupby; 10 warnings.simplefilter(action='ignore', category=FutureWarning); ---> 11 sc.pp.highly_variable_genes(adata_ct, batch_key='sample_id', n_top_genes=hvg_count); 12 sc.pl.highly_variable_genes(adata_ct). File ~/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:469, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 465 filt = filter_genes(adata_subset, min_cells=1, inplace=False)[0]; 467 adata_subset = adata_subset[:, filt]; --> 469 hvg = _highly_variable_genes_single_batch(; 470 adata_subset,; 471 layer=layer,; 472 min_disp=min_disp,; 473 max_disp=max_disp,; 474 min_mean=min_mean,; 475 max_mean=max_mean,; 476 n_top_genes=n_top_genes,; 477 n_bins=n_bins,; 478 flavor=flavor,; 479 ); 481 # Add 0 values for genes that were filtered out; 482 missing_hvg = pd.DataFrame(; 483 np.zeros((np.sum(~filt), len(hvg.columns))),; 484 columns=hvg.columns,; 485 ). File ~/lib/python3.10/site-packages/scanpy/preprocessin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3103:1186,simpl,simplefilter,1186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3103,1,['simpl'],['simplefilter']
Usability,"mg = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} cannot be converted to ""; --> 685 ""float"".format(self._A.dtype)); 686 ; 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float; ```; Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; ```; scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```; >",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:5712,learn,learn,5712,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['learn'],['learn']
Usability,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1656,clear,clearer,1656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,2,['clear'],['clearer']
Usability,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4926,learn,learn,4926,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,2,['learn'],['learn']
Usability,"mulated data. The script I am running is the following:; ```python; adata = sc.tl.sim('toggleswitch',; nrRealizations=5,; tmax=200,; branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.diffmap(adata); adata.uns['iroot'] = 0; sc.tl.dpt(adata). sc.tl.umap(adata); sc.pl.umap(adata,; edges=True,; edges_width=1,; color=['louvain', 'dpt_pseudotime'],; legend_loc='on data'); ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:; ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png); and ; ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:; - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis.; - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/854:1899,intuit,intuition,1899,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854,1,['intuit'],['intuition']
Usability,"n Pycharm. Hence, there is no gain in the annotations, by contrast, the function annotations simply look super complicated and it's no longer feasible to grasp at first sight what's going on. This also regards the output of the help in Jupyter Lab and Notebooks.; > ; > So, while I think that for AnnData and everything in the background, type annotations may make sense for a few developers (not for me, as I'm doing everything on remote servers using emacs), it doesn't make sense for the Scanpy user.; > ; > Also, all the other big packages I work with all the time simply don't have it (numpy, seaborn, pandas, tensorflow) and it makes it harder and lengthier for contributors to contribute if they need to go through it.; > ; > Finally, I'm still not happy about how the automatically generated docs from the type annotation look:; > ![image](https://user-images.githubusercontent.com/16916678/48796750-6ebb8000-ecce-11e8-9cdc-33b6056d8957.png); > which is from; > ![image](https://user-images.githubusercontent.com/16916678/48796824-a0344b80-ecce-11e8-8570-e4754f4ccd96.png); > Clearly, the automatically generated line with `Union[...]` is just way too complicated for a human to make sense of. The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous.; > ; > So, please let's stay away from having more type annotations and corresponding docstrings at this stage and let's simply continue imitating what all the major packages are doing.; > ; > Also: regarding your comment about the use of '``' vs. '`' in the docs: again, I think it leads to an inhomogeneous appearance to have *two* types of markup for code-related things. I agree that the read-the-docs italicized default style for '`' might be supoptimal, and I'll work on that if there is some time. But in general, I think there should be essentially one markup for code, as it's done in the tensorflow docs and a couple of other examples.; > ; > Happy to also discuss offline, @flying-sheep ;).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373:1379,Clear,Clearly,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373,2,"['Clear', 'simpl']","['Clearly', 'simply']"
Usability,"n read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 542 return key, value; 543 ; --> 544 key, value = postprocess_reading(key, value); 545 d[key_write] = value; 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value); 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))); 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]; --> 541 value = value.astype(new_dtype); 542 return key, value; 543 . ValueError: invalid shape in fixed-type tuple.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:4785,learn,learn,4785,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,1,['learn'],['learn']
Usability,"n reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed at nopython (nopython frontend); Unknown attribute 'partition' of type Module(<module 'numpy' from '/n/app/python/3.7.4-ext/lib/python3.7/site-packages/numpy/__init__.py'>). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. [1] During: typing of get attribute at /home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ... [Version](url) of the packages in path : ; scanpy 1.4.4.post1; anndata 0.6.22.post1; anndata2ri 1.0.1; umap-learn 0.3.10; numpy 1.16.5; scipy 1.3.1; pandas 1.0.1; scikit-learn 0.21.3; statsmodels 0.10.1; python-igraph 0.7.1.post6; louvain 0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193:4624,learn,learn,4624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193,2,['learn'],['learn']
Usability,"n wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from sta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:1959,simpl,simple,1959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,1,['simpl'],['simple']
Usability,"n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'\n uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'\n obsm: 'X_pca', 'X_umap'\n varm: 'PCs'\n layers: 'raw'\n obsp: 'distances', 'connectivities', vals=index\nAAAGCCTGGCTAAC-1 0.023856\nAAATTCGATGCACA-1 0.027458\nAACACGTGGTCTTT-1 0.016819\nAAGTGCACGTGCTA-1 0.011797\nACACGAACGGAGTG-1 0.017277\n ... \nTGGCACCTCCAACA-8 0.008840\nTGTGAGTGCTTTAC-8 0.022068\nTGTTACTGGCGATT-8 0.012821\nTTCAGTACCGGGAA-8 0.014169\nTTGAGGTGGAGAGC-8 0.010886\nName: percent_mito, Length: 700, dtype: float32); E + where <function morans_i at 0x7f354779d9d0> = <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'>.morans_i; E + where <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'> = sc.metrics. scanpy/tests/test_metrics.py:78: AssertionError; ```. ### Versions. <details>. ```; Package Version; ------------------- --------------------; anndata 0.10.2; array-api-compat 1.4; docutils 0.20.1; exceptiongroup 1.1.3; fonttools 4.43.1; h5py 3.10.0; importlib-resources 6.1.0; iniconfig 2.0.0; joblib 1.3.2; kiwisolver 1.4.5; llvmlite 0.41.0; matplotlib 3.8.0; mypy-extensions 1.0.0; natsort 8.4.0; networkx 3.1; numba 0.58.0; numpy 1.25.2; packaging 23.2; pandas 2.1.1; pathspec 0.11.2; patsy 0.5.3; pbr 5.11.1; Pillow 10.1.0; pip 23.2.1; platformdirs 3.11.0; pluggy 1.3.0; profimp 0.1.0; pynndescent 0.5.10; pyparsing 3.1.1; pytest 7.4.2; pytest-nunit 1.0.4; python-dateutil 2.8.2; pytz 2023.3.post1; scanpy 1.9.6.dev4+g6c7dd46e; scikit-learn 1.3.1; scipy 1.11.3; seaborn 0.12.2; session-info 1.0.0; setuptools 58.1.0; setuptools-scm 8.0.4; six 1.16.0; statsmodels 0.14.0; stdlib-list 0.9.0; tbb 2021.10.0; threadpoolctl 3.2.0; tomli 2.0.1; tqdm 4.66.1; typing_extensions 4.8.0; tzdata 2023.3; umap-learn 0.5.4; wheel 0.41.2; zipp 3.17.0; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2688:4112,learn,learn,4112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2688,2,['learn'],['learn']
Usability,"ndex.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'CD14'; >>> adata.raw.var['CD14']; Traceback (most recent call last):; File ""/Users/marcellp/Code/biomage/worker/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2646, in get_loc; return self._engine.get_loc(key); File ""pandas/_libs/index.pyx"", line 111, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'CD14'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/marcellp/Code/biomage/worker/venv/lib/python3.7/site-packages/pandas/core/frame.py"", line 2800, in __getitem__; indexer = self.columns.get_loc(key); File ""/Users/marcellp/Code/biomage/worker/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2648, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 111, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'CD14'; ```. #### Versions:; ```; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338:5576,learn,learn,5576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338,1,['learn'],['learn']
Usability,"ne 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:3267,learn,learn,3267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['learn'],['learn']
Usability,"nfirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi!. I am new to scanpy and I am facing some trouble reading my data in an appropriate way. I noticed that anndata objects in memory require roughly 4x the space they require on disk, so working with large datasets (>50GB on disk) is prohibitive in most scenarios. The USP of h5 files, however, is that you can index and slice them on disk as if they were in memory. This way I could greatly reduce the data size before loading it into memory. However, when I attempt to filter on a backed anndata object, I encounter a TypeError. The case of gene filtering should be just a column-sum, comparing it against a threshold and then saving it as a boolean index mask. It seems like the case that the data is backed and not in memory - which should be the default when dealing with h5 files - is not considered in the scanpy API. Am I simply missing something here?. ### Minimal code sample. ```python; from urllib.request import urlretrieve; import scanpy as sc. # We are downloading a small dataset here, 43MB. url = ""https://datasets.cellxgene.cziscience.com/7fb8b010-50bd-4238-a466-7c598f16d061.h5ad""; filename = ""testfile.h5ad"". urlretrieve(url, filename). adata = sc.read_h5ad(filename, backed=""r+""). sc.pp.filter_genes(adata, min_cells=100); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""/home/ubuntu/test_scanpy.py"", line 11, in <module>; sc.pp.filter_genes(adata, min_cells=100); File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 237, in filter_genes; filter_genes(; File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 258, in filter_genes; X if min_cells is None and max_cells is None else X > 0, axis=0; ^^^^^; TypeError: '>' not supported between instances of 'CSRDataset' and 'int'; ```. ###",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894:1119,simpl,simply,1119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894,1,['simpl'],['simply']
Usability,"ng and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:2273,simpl,simply,2273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,2,['simpl'],['simply']
Usability,"ng with the `svd_solver='arpack'` parameter. I attach some examples:. ```python; >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,; 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,; 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],; dtype=float32),; 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,; 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,; 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],; dtype=float32)}. >>> sc.tl.pca(adata_h); Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,; 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,; 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,; 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/264#issuecomment-423300225:1224,learn,learn,1224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264#issuecomment-423300225,2,['learn'],['learn']
Usability,ning build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/highly_variable_genes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/magic.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/dca.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/mnn_correct.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/qc.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/simple.py -> build/lib/scanpy/preprocessing; creating build/lib/scanpy/datasets; copying scanpy/datasets/__init__.py -> build/lib/scanpy/datasets; copying scanpy/datasets/api_without_datasets.py -> build/lib/scanpy/datasets; creating build/lib/scanpy/queries; copying scanpy/queries/__init__.py -> build/lib/scanpy/queries; creating build/lib/scanpy/api; copying scanpy/api/__init__.py -> build/lib/scanpy/api; copying scanpy/api/datasets.py -> build/lib/scanpy/api; copying scanpy/api/export_to.py -> build/lib/scanpy/api; copying scanpy/api/tl.py -> build/lib/scanpy/api; copying scanpy/api/pl.py -> build/lib/scanpy/api; copying scanpy/api/queries.py -> build/lib/scanpy/api; copying scanpy/api/logging.py -> build/lib/scanpy/api; copying scanpy/api/pp.py -> build/lib/scanpy/api; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/phate.py -> build/lib/scanpy/tools; copying scanpy/tools/diffmap.py -> build/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:4914,simpl,simple,4914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['simpl'],['simple']
Usability,"ning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:1852,learn,learning,1852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,2,['learn'],['learning']
Usability,"npointer 2.4; jsonschema 4.19.1; jsonschema_specifications NA; jupyter_events 0.8.0; jupyter_server 2.8.0; jupyterlab_server 2.25.0; kiwisolver 1.4.5; kneed 0.8.5; legacy_api_wrap NA; leidenalg 0.9.1; llvmlite 0.40.1; louvain 0.8.0; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.6.2; matplotlib_inline 0.1.6; mpl_toolkits NA; msgpack 1.0.6; mudata 0.2.3; muon 0.1.6; natsort 8.4.0; nbformat 5.9.2; numba 0.57.1; numcodecs 0.12.1; numexpr 2.8.7; numpy 1.24.4; overrides NA; packaging 23.2; pandas 2.2.1; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.11.0; plotly 5.17.0; prometheus_client NA; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 10.0.1; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.16.1; pynndescent 0.5.10; pyparsing 3.1.1; pysam 0.20.0; pythonjsonlogger NA; pytz 2023.3.post1; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rich NA; rpds NA; scipy 1.11.3; seaborn 0.13.0; send2trash NA; session_info 1.0.0; setuptools 68.2.2; setuptools_scm NA; simplejson 3.19.2; six 1.16.0; sklearn 1.3.1; sniffio 1.3.0; socks 1.7.1; sortedcontainers 2.4.0; sphinxcontrib NA; stack_data 0.6.2; statsmodels 0.14.0; tblib 2.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.2; tomli 2.0.1; toolz 0.12.0; tornado 6.3.3; tqdm 4.66.1; traitlets 5.9.0; typing_extensions NA; umap 0.5.4; upsetplot 0.8.0; uri_template NA; urllib3 1.26.18; wcwidth 0.2.8; webcolors 1.13; websocket 1.6.4; xxhash NA; yaml 6.0.1; zarr 2.17.2; zipp NA; zmq 25.1.1; zoneinfo NA; zope NA; zstandard 0.21.0; -----; IPython 8.16.1; jupyter_client 8.4.0; jupyter_core 5.4.0; jupyterlab 4.1.4; notebook 7.1.1; -----; Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0]; Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28; -----; Session information updated at 2024-06-13 10:40; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3106:3595,simpl,simplejson,3595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3106,1,['simpl'],['simplejson']
Usability,"nt, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obvi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2775,learn,learning,2775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['learn'],['learning']
Usability,"nto two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1290,simpl,simple,1290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,2,['simpl'],['simple']
Usability,"numba\core\compiler_machinery.py in run(self, state); 339 (self.pipeline_name, pass_desc); 340 patched_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 ; 343 def dependency_analysis(self):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 330 pass_inst = _pass_registry.get(pss).pass_inst; 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:; 334 raise BaseException(""Legacy pass in use""). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 289 mutated |= check(pss.run_initialization, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:; 293 mutated |= check(pss.run_finalizer, internal_state). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 262 ; 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):; 266 msg = (""CompilerPass implementations should return True/False. "". ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 440 ; 441 # TODO: Pull this out into the pipeline; --> 442 NativeLowering().run_pass(state); 443 lowered = state['cr']; 444 signature = typing.signature(state.return_type, *state.args). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 368 lower = lowering.Lower(targetctx, library, fndesc, interp,; 369 metadata=metadata); --> 370 lower.lower(); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:10586,Simpl,SimpleTimer,10586,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['Simpl'],['SimpleTimer']
Usability,"number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2952,learn,learning,2952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['learn'],['learning']
Usability,"odule>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['Sample', 'n_counts', 'n_genes', 'percent_mito', 'log_counts',; 'louvain'],; dtype='object'). ```. adata.var contains the column ""Symbol"" and ""Tnnt2"" is present:. `adata.var[adata.var['Symbol'] == 'Tnnt2']`. . Symbol | type | highly_variable | means | dispersions | dispersions_norm; -- | -- | -- | -- | -- | --; Tnnt2 | protein_coding | True | 0.923869 | 4.090601 | 11.370244. run with:; `scanpy==1.3.7 anndata==0.6.17 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455:2491,learn,learn,2491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455,1,['learn'],['learn']
Usability,"oh, and versions:; ```; scanpy==1.4.3+116.g0075c62 ; anndata==0.6.22.post2.dev80+g72c2bde ; umap==0.3.9 ; numpy==1.17.2 ; scipy==1.3.0 ; pandas==0.24.1 ; scikit-learn==0.21.3 ; statsmodels==0.10.0rc2 ; ython-igraph==0.7.1 ; louvain==0.6.1; ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/887#issuecomment-545561793:161,learn,learn,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561793,2,['learn'],['learn']
Usability,"ok that's great, thank you! The `radius_neighbors` have very clear applications in fish-like data, and we are assmebling a tutorial to show exactly that. The n-rings as well especially in the context of cell-cell communication (although did not check that systematically yet). So shall we add this functionality to spatial-tools ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707604861:61,clear,clear,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707604861,2,['clear'],['clear']
Usability,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:530,clear,clear,530,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112,2,['clear'],['clear']
Usability,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:64,learn,learn,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006,4,['learn'],['learn']
Usability,"om numba import config; 724 tb = sys.exc_info()[2] if config.FULL_TRACEBACKS else None; --> 725 six.reraise(type(newerr), newerr, tb); 726 ; 727 . ~\anaconda3\lib\site-packages\numba\six.py in reraise(tp, value, tb); 667 if value.__traceback__ is not tb:; 668 raise value.with_traceback(tb); --> 669 raise value; 670 ; 671 else:. LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. File ""..\..\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.260, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (399)>, 400: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (405)>, 402: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (406)>, 276: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (403)>, 318: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (404)>}Var(parfor_index.260, _qc.py:399)"" at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (399); ```. Unlikely to be related, but this was after I had issues installing scanpy from conda (as in #1142), which I got around by installing through pip. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:14549,learn,learn,14549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['learn'],['learn']
Usability,"ompiler_machinery.py in run(self, state); 345 (self.pipeline_name, pass_desc); 346 patched_exception = self._patch_error(msg, e); --> 347 raise patched_exception; 348 ; 349 def dependency_analysis(self):. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in run(self, state); 336 pass_inst = _pass_registry.get(pss).pass_inst; 337 if isinstance(pass_inst, CompilerPass):; --> 338 self._runPass(idx, pass_inst, state); 339 else:; 340 raise BaseException(""Legacy pass in use""). ~\anaconda3\lib\site-packages\numba\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in _runPass(self, index, pss, internal_state); 300 mutated |= check(pss.run_initialization, internal_state); 301 with SimpleTimer() as pass_time:; --> 302 mutated |= check(pss.run_pass, internal_state); 303 with SimpleTimer() as finalize_time:; 304 mutated |= check(pss.run_finalizer, internal_state). ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in check(func, compiler_state); 273 ; 274 def check(func, compiler_state):; --> 275 mangled = func(compiler_state); 276 if mangled not in (True, False):; 277 msg = (""CompilerPass implementations should return True/False. "". ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 405 ; 406 # TODO: Pull this out into the pipeline; --> 407 NativeLowering().run_pass(state); 408 lowered = state['cr']; 409 signature = typing.signature(state.return_type, *state.args). ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 347 lower = lowering.Lower(targetctx, library, fndesc, interp,; 348 metadata=metadata); --> 349 lower.lower(); 350 if not flags.no_cpython_wrapper:; 351 lower.create_cpython_wrapper(flags.release_gil). ~\anaconda3\lib\site-packages\numba\lowering.py in lower(self); 193 if self.generator_info is N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:10371,Simpl,SimpleTimer,10371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['Simpl'],['SimpleTimer']
Usability,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-795155685:34,simpl,simple,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795155685,2,['simpl'],['simple']
Usability,"on5 0.9.24; jsonpointer 2.4; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.9.0; jupyter_server 2.13.0; jupyterlab_server 2.25.4; kiwisolver 1.4.5; leidenalg 0.10.2; llvmlite 0.42.0; markupsafe 2.1.5; matplotlib 3.6.2; mpl_toolkits NA; msgpack 1.0.8; mudata 0.2.3; muon 0.1.5; mygene 3.2.2; natsort 8.4.0; nbformat 5.10.3; networkx 3.2.1; numba 0.59.1; numexpr 2.9.0; numpy 1.26.4; optree 0.10.0; optuna 3.6.0; overrides NA; packaging 24.0; pandas 1.5.3; pandas_flavor NA; parso 0.8.3; patsy 0.5.6; pingouin 0.5.4; pkg_resources NA; platformdirs 4.2.0; plotly 5.20.0; prometheus_client NA; prompt_toolkit 3.0.43; psutil 5.9.8; pure_eval 0.2.2; pyBigWig 0.3.22; pyarrow 15.0.2; pychromvar 0.0.4; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 2.0.0; pyfaidx 0.8.1.1; pygments 2.17.2; pyjaspar 3.0.0; pynndescent 0.5.11; pyparsing 3.1.2; pysam 0.22.0; pythonjsonlogger NA; pytz 2024.1; ray 2.10.0; referencing NA; requests 2.31.0; requests_cache 1.2.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rich NA; rpds NA; scipy 1.12.0; seaborn 0.13.2; send2trash NA; session_info 1.0.0; setproctitle 1.2.2; simplejson 3.19.2; sitecustomize NA; six 1.16.0; sklearn 1.4.1.post1; sniffio 1.3.1; stack_data 0.6.3; statsmodels 0.14.1; swig_runtime_data4 NA; tabulate 0.9.0; tensorboard 2.16.2; texttable 1.7.0; threadpoolctl 3.4.0; torch 2.2.1+cu121; torchgen NA; tornado 6.4; tqdm 4.66.2; traitlets 5.14.2; typing_extensions NA; umap 0.5.5; uri_template NA; url_normalize 1.4.3; urllib3 2.2.1; uvloop 0.19.0; wcwidth 0.2.13; webcolors 1.13; websocket 1.7.0; wget 3.2; xarray 2024.2.0; yaml 6.0.1; zmq 25.1.2; zoneinfo NA; -----; IPython 8.22.2; jupyter_client 8.6.1; jupyter_core 5.7.2; jupyterlab 4.1.5; notebook 7.1.2; -----; Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]; Linux-6.5.0-27-generic-x86_64-with-glibc2.35; -----; Session information updated at 2024-04-18 18:58; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3014:3698,simpl,simplejson,3698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3014,1,['simpl'],['simplejson']
Usability,"or = 'total_counts',; 5 spot_size = 120; 6 ). File /home/shared/.conda/envs/Spatial/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:988, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs); 936 """"""\; 937 Scatter plot in spatial coordinates.; 938 ; (...); 985 Tutorial on spatial analysis.; 986 """"""; 987 # get default image params if available; --> 988 library_id, spatial_data = _check_spatial_data(adata.uns, library_id); 989 img, img_key = _check_img(spatial_data, img, img_key, bw=bw); 990 spot_size = _check_spot_size(spatial_data, spot_size). File /home/shared/.conda/envs/Spatial/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1291, in _check_spatial_data(uns, library_id); 1289 if library_id is _empty:; 1290 if len(spatial_mapping) > 1:; -> 1291 raise ValueError(; 1292 ""Found multiple possible libraries in `.uns['spatial']. Please specify.""; 1293 f"" Options are:\n\t{list(spatial_mapping.keys())}""; 1294 ); 1295 elif len(spatial_mapping) == 1:; 1296 library_id = list(spatial_mapping.keys())[0]. ValueError: Found multiple possible libraries in `.uns['spatial']. Please specify. Options are:; 	['1', '10', '100', '101', '102', '103', '104', '105', '106'...; ```. Plotting individual FOV's by specifying singular library keys generates plots. If I do an approach similar to the workflow; in this SquidPy [tutorial](https://squidpy.readthedocs.io/en/stable/external_tutorials/tutorial_nanostring.html) where I use the `sq.pl.spatial_segment()`, or `sq.pl.spatial_scatter()` and specify a list of library keys, it states that the format of the library keys argument is of an unhashable type. #### Versions. <details>. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.22.4 scipy==1.9.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.14.0rc0 python-igraph==0.10.4 pynndescent==0.5.10. </details>. I would much appreciate if anyone could advise. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2486:3392,learn,learn,3392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2486,1,['learn'],['learn']
Usability,"or:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package xz conflicts for:; python=3.7 -> xz[version='>=5.2.4,<6.0a0']; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package pytables conflicts for:; scanpy -> pytables",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:2023,learn,learn,2023,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,4,['learn'],['learn']
Usability,"ore/projects/0c3b7785-f74d-4091-8616-a68757e4c2a8/m/project-matrices). ```python; import scanpy; loomdata = scanpy.read_loom(""path/bone-marrow-myeloma-human-hematopoeitic-10XV2.loom""). #I also tried:; loomdata=scanpy.read_loom(""path/bone-marrow-myeloma-human-hematopoeitic-10XV2.loom"", obs_names='CellID', var_names='ensembl_ids'. ```. ```pytb; scanpy.read_loom(""/Users/acastanza/Downloads/bone-marrow-myeloma-human-hematopoeitic-10XV2.loom""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/local/anaconda3/lib/python3.8/site-packages/anndata/compat/__init__.py"", line 253, in inner_f; return f(*args, **kwargs); File ""/usr/local/anaconda3/lib/python3.8/site-packages/anndata/_io/read.py"", line 261, in read_loom; with connect(filename, ""r"", **kwargs) as lc:; File ""/usr/local/anaconda3/lib/python3.8/site-packages/loompy/loompy.py"", line 1140, in connect; return LoomConnection(filename, mode, validate=validate, spec_version=spec_version); File ""/usr/local/anaconda3/lib/python3.8/site-packages/loompy/loompy.py"", line 84, in __init__; raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'ensembl_ids' dtype object is not allowed; Row attribute 'gene_names' dtype object is not allowed; Column attribute 'CellID' dtype object is not allowed; Column attribute 'cell_names' dtype object is not allowed; Column attribute 'input_id' dtype object is not allowed; For help, see http://linnarssonlab.org/loompy/format/; /Users/acastanza/Downloads/bone-marrow-myeloma-human-hematopoeitic-10XV2.loom does not appead to be a valid Loom file according to Loom spec version '2.0.1'; ```. #### Versions. <details>. scanpy==1.8.1 anndata==0.7.6.dev49+g19ba44d umap==0.5.1 numpy==1.19.2 scipy==1.7.1 pandas==1.2.4 scikit-learn==0.24.2 statsmodels==0.11.1 python-igraph==0.9.6 pynndescent==0.5.4. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2040:2489,learn,learn,2489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2040,1,['learn'],['learn']
Usability,orkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001; pip=20.0.2=py37_1; pixman=0.38.0=h516909a_1003; prometheus_client=0.7.1=py_0; prompt-toolkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3391,learn,learn,3391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,2,['learn'],['learn']
Usability,"ors(adata_sub, n_neighbors = 30); sc.tl.umap(adata_sub); sc.tl.ingest(CODEX_sub, adata_sub, obs='leiden', embedding_method='umap'); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-214-01a03312d3df> in <module>; ----> 1 sc.tl.ingest(CODEX_sub, adata_sub, obs='leiden', embedding_method='umap'). ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 124 labeling_method = labeling_method * len(obs); 125 ; --> 126 ing = Ingest(adata_ref, neighbors_key); 127 ing.fit(adata); 128 . ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in __init__(self, adata, neighbors_key); 383 ; 384 if neighbors_key in adata.uns:; --> 385 self._init_neighbors(adata, neighbors_key); 386 else:; 387 raise ValueError(. ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in _init_neighbors(self, adata, neighbors_key); 349 else:; 350 self._neigh_random_state = neighbors['params'].get('random_state', 0); --> 351 self._init_pynndescent(neighbors['distances']); 352 ; 353 def _init_pca(self, adata):. ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in _init_pynndescent(self, distances); 284 ; 285 first_col = np.arange(distances.shape[0])[:, None]; --> 286 init_indices = np.hstack((first_col, np.stack(distances.tolil().rows))); 287 ; 288 self._nnd_idx = NNDescent(. <__array_function__ internals> in stack(*args, **kwargs). ~\anaconda3\envs\scenv\lib\site-packages\numpy\core\shape_base.py in stack(arrays, axis, out); 424 shapes = {arr.shape for arr in arrays}; 425 if len(shapes) != 1:; --> 426 raise ValueError('all input arrays must have the same shape'); 427 ; 428 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape. ```. #### Versions. scanpy==1.7.0 anndata==0.7.6 umap==0.5.1 numpy==1.21.1 scipy==1.7.0 pandas==1.2.5 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.8. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085:2667,learn,learn,2667,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085,1,['learn'],['learn']
Usability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1168,user experience,user experience,1168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,2,['user experience'],['user experience']
Usability,"pandas transparently handles gzipping. We could also simply add support for gzipped files:. ```py; def maybe_zipped_path(path: Path) -> Path:; zipped_path = Path(f'{path}.gz'); if zipped_path.is_file(): return zipped_path; return path; ```. And then we use it:. https://github.com/scverse/scanpy/blob/a38a22ab85074f17788b8d1effa89c1373e0c978/scanpy/readwrite.py#L488. to. ```py; genefile_exists = maybe_zipped_path(path / f'{prefix}genes.tsv').is_file(); ```. and . https://github.com/scverse/scanpy/blob/a38a22ab85074f17788b8d1effa89c1373e0c978/scanpy/readwrite.py#L525. to. ```py; genes = pd.read_csv(maybe_zipped_path(path / f'{prefix}genes.tsv'), header=None, sep='\t'); ```. and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916#issuecomment-1102500257:53,simpl,simply,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1102500257,2,['simpl'],['simply']
Usability,"pd.concat(results_dict, ignore_index=True). marker_df = marker_df.sort_values(by=['scores'], ascending=False); # Make dataframe of the top 3 markers per cluster; marker_df_plt = marker_df.groupby('cluster').head(3); ; # here sc.tl.dendrogram will fail; _ = sc.pl.dotplot(; adata,; var_names=marker_df_plt['names'],; groupby='leiden',; dendrogram=True,; use_raw=False,; show=False,; color_map='Blues'; save='{}.png'.format('test'); ); ```. ```pytb; /lib/python3.6/site-packages/scanpy/tools/_dendrogram.py in dendrogram(adata, groupby, n_pcs, use_rep, var_names, use_raw, cor_method, linkage_method, optimal_ordering, key_added, inplace); 130 corr_matrix, method=linkage_method, optimal_ordering=optimal_ordering; 131 ); --> 132 dendro_info = sch.dendrogram(z_var, labels=categories, no_plot=True); 133; 134 # order of groupby categories. /lib/python3.6/site-packages/scipy/cluster/hierarchy.py in dendrogram(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, no_plot, no_labels, leaf_font_size, leaf_rotation, leaf_label_func, show_contracted, link_color_func, ax, above_threshold_color); 3275 ""'bottom', or 'right'""); 3276; -> 3277 if labels and Z.shape[0] + 1 != len(labels):; 3278 raise ValueError(""Dimensions of Z and labels must be consistent.""); 3279. /lib/python3.6/site-packages/pandas/core/indexes/base.py in __nonzero__(self); 2148 def __nonzero__(self):; 2149 raise ValueError(; -> 2150 f""The truth value of a {type(self).__name__} is ambiguous. ""; 2151 ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""; 2152 ). ValueError: The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().; ```. #### Versions:; scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.17.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.1. Conda environment is attached. ; [environment.txt](https://github.com/theislab/scanpy/files/4857757/environment.txt)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1300:2996,learn,learn,2996,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1300,1,['learn'],['learn']
Usability,"peline_name, pass_desc); 367 patched_exception = self._patch_error(msg, e); --> 368 raise patched_exception; 369 ; 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 354 pass_inst = _pass_registry.get(pss).pass_inst; 355 if isinstance(pass_inst, CompilerPass):; --> 356 self._runPass(idx, pass_inst, state); 357 else:; 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 ; C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 309 mutated |= check(pss.run_initialization, internal_state); 310 with SimpleTimer() as pass_time:; --> 311 mutated |= check(pss.run_pass, internal_state); 312 with SimpleTimer() as finalize_time:; 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 271 ; 272 def check(func, compiler_state):; --> 273 mangled = func(compiler_state); 274 if mangled not in (True, False):; 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 392 lower = lowering.Lower(targetctx, library, fndesc, interp,; 393 metadata=metadata); --> 394 lower.lower(); 395 if not flags.no_cpython_wrapper:; 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 166 if self.generator_info is None:; 167 self.genlower = None; --> 168 self.lower_normal_function(self.fndesc); 169 else:; 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:9605,Simpl,SimpleTimer,9605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['Simpl'],['SimpleTimer']
Usability,"pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 157 # here, we replaced the implementation based on name_idx with this. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in name_idx(i); 140 raise IndexError(; 141 'Key ""{}"" is not valid observation/variable name/index.'; --> 142 .format(i)); 143 i = i_found[0]; 144 return i; ```. I don't understand why anndata thinks that . IndexError: Key ""n_counts"" is not valid observation/variable name/index. even though it's clearly in adata.obs... any suggestions what to do? add print statements to the various functions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728:2450,clear,clearly,2450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728,1,['clear'],['clearly']
Usability,"ponent, solid_edges, dashed_edges, transitions, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 541 single_component=single_component,; 542 arrowsize=arrowsize,; --> 543 pos=pos,; 544 ); 545 if colorbars[icolor]:. /usr/local/lib/python3.6/dist-packages/scanpy/plotting/_tools/paga.py in _paga_graph(adata, ax, solid_edges, dashed_edges, adjacency_solid, adjacency_dashed, transitions, threshold, root, colors, labels, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, normalize_to_color, title, pos, cmap, frameon, min_edge_width, max_edge_width, export_to_gexf, colorbar, use_raw, cb_kwds, single_component, arrowsize); 756 with warnings.catch_warnings():; 757 warnings.simplefilter(""ignore""); --> 758 nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black'); 759 # draw directed edges; 760 else:. /usr/local/lib/python3.6/dist-packages/networkx/drawing/nx_pylab.py in draw_networkx_edges(G, pos, edgelist, width, edge_color, style, alpha, arrowstyle, arrowsize, edge_cmap, edge_vmin, edge_vmax, ax, arrows, label, node_size, nodelist, node_shape, **kwds); 609 # value globally, since the user can instead provide per-edge alphas; 610 # now. Only set it globally if provided as a scalar.; --> 611 if cb.is_numlike(alpha):; 612 edge_collection.set_alpha(alpha); 613 . AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'. ```. I've been searching online and found some related threads like [this](https://github.com/palash1992/GEM/issues/51) and [this](https://stackoverflow.com/questions/53421905/matplotlib-attributeerror-module-matplotlib-cbook-has-no-attribute-define-a). Is there a solution that doesn't require me to downgrade my matplotlib",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1227:1596,simpl,simplefilter,1596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227,1,['simpl'],['simplefilter']
Usability,"portError:. ImportError: cannot import name 'GProfiler'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-383-c1b09359d1a1> in <module>; 14 ; 15 #get gene set enrichment; ---> 16 print(sc.queries.enrich(this_adata, org='hsapiens', group='malignant', key='malignantvshealthy', pval_cutoff=0.01, log2fc_min=np.log2(1.5))); 17 ; 18 #plot volcano (makes a sep df along the way, should consolidate with above). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in _enrich_anndata(adata, group, org, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols, gprofiler_kwargs); 305 else:; 306 gene_list = list(de[""names""].dropna()); --> 307 return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 266 except ImportError:; 267 raise ImportError(; --> 268 ""This method requires the `gprofiler-official` module to be installed.""; 269 ); 270 gprofiler = GProfiler(user_agent=""scanpy"", return_dataframe=True). ImportError: This method requires the `gprofiler-official` module to be installed.; ```. #### Versions. gprofiler-official bioconda/noarch::gprofiler-official-1.0.0-py_0. scanpy==1.7.1 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.3 pandas==1.1.5 scikit-learn==0.19.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1896:2666,learn,learn,2666,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896,1,['learn'],['learn']
Usability,"ported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; # Introduction. Hi,. so this is a weird one and I could not track it down yet.; The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results; Bottom = old=agando expected results. The old environment has:. ```; scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins-base=1.14.0=h8213a91_2; - gstreamer=1.14.0=h28cd5cc_2; - i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:1060,learn,learn,1060,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['learn'],['learn']
Usability,"print(time.time() - t0); ```. **Output**: ; ```Writing subplot to SPRING\all; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-59-9c683583ff59> in <module>; 1 import time; 2 t0 = time.time(); ----> 3 sc.external.exporting.spring_project(adata, './SPRING',; 4 'umap', subplot_name='all', overwrite=True, cell_groupings=['leiden'],; 5 custom_color_tracks=['total_counts']). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, neighbors_key, overwrite); 179 ; 180 # Write graph in two formats for backwards compatibility; --> 181 edges = _get_edges(adata, neighbors_key); 182 _write_graph(subplot_dir / 'graph_data.json', E.shape[0], edges); 183 _write_edges(subplot_dir / 'edges.csv', edges). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in _get_edges(adata, neighbors_key); 217 ; 218 def _get_edges(adata, neighbors_key=None):; --> 219 neighbors = NeighborsView(adata, neighbors_key); 220 if 'distances' in neighbors: # these are sparse matrices; 221 matrix = neighbors['distances']. NameError: name 'NeighborsView' is not defined; ```. #### AnnData: ; ```AnnData object with n_obs × n_vars = 2638 × 1838; obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'; var: 'gene_ids', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'connectivities', 'distances'; ```. #### Versions:; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.3.2 pandas==1.0.4 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1285:2389,learn,learn,2389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285,1,['learn'],['learn']
Usability,prometheus_client=0.17.1; - prompt-toolkit=3.0.39; - prompt_toolkit=3.0.39; - psutil=5.9.5; - pthread-stubs=0.4; - ptyprocess=0.7.0; - pure_eval=0.2.2; - py-cpuinfo=9.0.0; - pycparser=2.21; - pyct=0.5.0; - pyfaidx=0.8.1.1; - pygments=2.16.1; - pymde=0.1.18; - pymongo=4.5.0; - pynndescent=0.5.11; - pyobjc-core=9.2; - pyobjc-framework-cocoa=9.2; - pyparsing=3.0.9; - pysocks=1.7.1; - pytables=3.8.0; - python=3.11.4; - python-dateutil=2.8.2; - python-fastjsonschema=2.18.0; - python-igraph=0.11.3; - python-json-logger=2.0.7; - python-kaleido=0.2.1; - python-tzdata=2023.3; - python_abi=3.11; - pytorch=2.0.1; - pytz=2023.3; - pyvcf3=1.0.3; - pyyaml=6.0.1; - pyzmq=25.1.1; - radian=0.6.7; - rchitect=0.4.1; - readline=8.2; - referencing=0.30.2; - requests=2.31.0; - rfc3339-validator=0.1.4; - rfc3986-validator=0.1.1; - rpds-py=0.9.2; - scanpy=1.10.1; - scikit-learn=1.3.0; - scipy=1.11.2; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.2; - session-info=1.0.0; - setuptools=68.1.2; - simplejson=3.19.2; - six=1.16.0; - snappy=1.1.10; - sniffio=1.3.0; - soupsieve=2.3.2.post1; - stack_data=0.6.2; - statsmodels=0.14.0; - stdlib-list=0.10.0; - svt-av1=1.6.0; - sympy=1.12; - tbb=2021.11.0; - tenacity=8.2.3; - terminado=0.17.1; - texttable=1.7.0; - threadpoolctl=3.2.0; - tinycss2=1.2.1; - tk=8.6.12; - tomli=2.0.1; - torchvision=0.15.2; - tornado=6.3.3; - traitlets=5.9.0; - typing_extensions=4.8.0; - typing_utils=0.1.0; - tzdata=2023c; - umap-learn=0.5.5; - uri-template=1.3.0; - wcwidth=0.2.6; - webcolors=1.13; - webencodings=0.5.1; - websocket-client=1.6.2; - wheel=0.41.2; - x264=1!164.3095; - x265=3.5; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:13819,simpl,simplejson,13819,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['simpl'],['simplejson']
Usability,"python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7108,Simpl,SimpleTimer,7108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['Simpl'],['SimpleTimer']
Usability,"python; _, axs = pl.subplots(ncols=3, figsize=(6, 2.5), gridspec_kw={'wspace': 0.05, 'left': 0.12}); pl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2); for ipath, (descr, path) in enumerate(paths):; _, data = sc.pl.paga_path(; adata, path, gene_names,; show_node_names=False,; ax=axs[ipath],; ytick_fontsize=8,; left_margin=0.15,; n_avg=50,; annotations=['distance'],; show_yticks=True if ipath==0 else False,; show_colorbar=False,; color_map='Greys',; groups_key='clusters',; color_maps_annotations={'distance': 'viridis'},; title='{} path'.format(descr),; return_data=True,; show=False); data.to_csv('./write/paga_path_{}.csv'.format(descr)); pl.savefig('./figures/paga_path_panglao.pdf'); pl.show(); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); <ipython-input-8-86ecf06e6589> in <module>(); 18 title='{} path'.format(descr),; 19 return_data=True,; ---> 20 show=False); 21 data.to_csv('./write/paga_path_{}.csv'.format(descr)); 22 pl.savefig('./figures/paga_path_panglao.pdf'). 5 frames; <__array_function__ internals> in cumsum(*args, **kwargs). /usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.18.5 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.2 leidenalg==0.8.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295:2374,learn,learn,2374,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295,1,['learn'],['learn']
Usability,"r, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 541 single_component=single_component,; 542 arrowsize=arrowsize,; --> 543 pos=pos,; 544 ); 545 if colorbars[icolor]:. ~/.local/lib/python3.6/site-packages/scanpy/plotting/_tools/paga.py in _paga_graph(adata, ax, solid_edges, dashed_edges, adjacency_solid, adjacency_dashed, transitions, threshold, root, colors, labels, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, normalize_to_color, title, pos, cmap, frameon, min_edge_width, max_edge_width, export_to_gexf, colorbar, use_raw, cb_kwds, single_component, arrowsize); 756 with warnings.catch_warnings():; 757 warnings.simplefilter(""ignore""); --> 758 nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black'); 759 # draw directed edges; 760 else:. ~/.conda/envs/single_cell/lib/python3.6/site-packages/networkx/drawing/nx_pylab.py in draw_networkx_edges(G, pos, edgelist, width, edge_color, style, alpha, arrowstyle, arrowsize, edge_cmap, edge_vmin, edge_vmax, ax, arrows, label, node_size, nodelist, node_shape, **kwds); 609 # value globally, since the user can instead provide per-edge alphas; 610 # now. Only set it globally if provided as a scalar.; --> 611 if cb.is_numlike(alpha):; 612 edge_collection.set_alpha(alpha); 613 . AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'; ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.4.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1094:2536,simpl,simplefilter,2536,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094,2,"['learn', 'simpl']","['learn', 'simplefilter']"
Usability,r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath 1.0.1 pyhd3eb1b0_0 ; sphinxcontrib-qthelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-serializinghtml 1.1.4 pyhd3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi;,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:16192,simpl,simplegeneric,16192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['simpl'],['simplegeneric']
Usability,"r['gene_symbol'].astype('object'); ```. ### Minimal code sample. ```python; import scanpy as sc. <anndata object with a categorical adata.var.gene_symbol column>. sc.pl.highest_expr_genes(adata, n_top=20, gene_symbols='gene_symbol', show=True, save="".png""); ```. I also tried this with the same results. ```python; import scanpy as sc. <anndata object with a categorical adata.var.gene_symbol column>. adata.var.index = adata.var.gene_symbol; sc.pl.highest_expr_genes(adata, n_top=20, show=True, save="".png""); ```. ### Error output. ![349265437-b0a6e963-5d56-40e6-9922-5e4a543c08cf](https://github.com/user-attachments/assets/478c6a20-817f-4e39-92a3-62f5c2a62ed0). Above is a boxplot from `sc.pl.highest_expr_genes` that shows all the Categorical genes in addition to the top-20 as specified in the function argument. <img width=""1077"" alt=""Screenshot 2024-07-17 at 1 23 27 PM"" src=""https://github.com/user-attachments/assets/cfbdb40d-57f5-4da6-bf69-b4f4f3c489cc"">. Above is the correct boxplot, after my hack was applied to force the adata.var.gene_symbols to be mixed-object datatype instead of Categorical. ### Versions. <details>. python-3-10-4. ```; aiohttp==3.8.3; anndata==0.10.6; biocode==0.10.0; biopython==1.79; cairosvg==2.7.1; dash-bio==1.0.2; #diffxpy==0.7.4; Flask==3.0.0; Flask-RESTful==0.3.9; gunicorn; h5py==3.10.0; itsdangerous==2.1.2 # See -> https://stackoverflow.com/a/71206978; jupyterlab==4.0.5; jupyter==1.0.0; kaleido==0.2.1; leidenalg==0.10.2; llvmlite==0.41.1; matplotlib==3.9.0; mod-wsgi==4.9.4; more_itertools==9.0.0; mysql-connector-python==8.4.0; numba==0.58.1; numexpr==2.8.4; numpy==1.26.0; opencv-python==4.5.5.64; openpyxl==3.1.5; pandas==2.2.1; Pillow==10.2.0; pika==1.3.1; plotly==5.6.0; python-dotenv==0.20.0; requests==2.31.0; rpy2==3.5.1 # 3.5.2 and up gives errors with rpy2py and py2rpy; sanic; scanpy==1.10.1; scikit-learn==1.0.2; scipy==1.11.04; seaborn==0.13.2; SQLAlchemy==1.4.32; tables==3.9.2 # Read hdf5 files into pandas; xlrd==1.2.0; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3158:3438,learn,learn,3438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3158,1,['learn'],['learn']
Usability,"rank_genes_groups ""groups"" argument ignored or not clearly explained",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519:51,clear,clearly,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519,2,['clear'],['clearly']
Usability,"re(self); 380 if is_final_pipeline:; --> 381 raise e; 382 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 371 try:; --> 372 pm.run(self.state); 373 if self.state.cr is not None:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 340 patched_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 441 # TODO: Pull this out into the pipeline; --> 442 NativeLowering().run_pass(state); 443 lowered = state['cr']. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 369 metadata=metadata); --> 370 lower.lower(); 371 if not flags.no_cpython_wrapper:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 216 # Materialize LLVM Module; --> 217 self.library.add_ir_module(self.module); 218 . ~\AppData\Local\Continuum\anaconda3\lib\site-p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:3298,Simpl,SimpleTimer,3298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['Simpl'],['SimpleTimer']
Usability,"reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1299,simpl,simple,1299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['simpl'],['simple']
Usability,rmation of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leid,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1101,learn,learn,1101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['learn'],['learn']
Usability,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`).; ### Minimal code sample (that we can copy&paste without having any data). ```python; analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False); ```. ```pytb; /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide; residuals = diff / np.sqrt(mu + mu**2 / theta); ```; `analytic_pearson[""X""]` ; Output:; ```py; array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,; -0.08709449, -0.16926342],; [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,; -0.09342913, -0.18157157],; [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,; 12.02085262, 6.06603257],; ...,; [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,; -0.03033674, -0.05896328],; [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,; -0.03077326, -0.05981169],; [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,; -0.03157547, -0.06137084]]); ```. ```py; adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]; adata.layers[""analytic_pearson_residuals""].sum(1); ```; Output:; `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2496:2095,learn,learn,2095,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496,1,['learn'],['learn']
Usability,"rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 fro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:1946,simpl,simplefilter,1946,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,2,['simpl'],['simplefilter']
Usability,rty | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0.9.4; testpath | 0.5.0; texttable | 1.6.4; threadpoolctl | 3.0.0; tornado | 6.1; tqdm | 4.62.3; traitlets | 4.3.3; typing-extensions | 3.10.0.2; **umap-learn | 0.5.2**; urllib3 | 1.26.7; wcwidth | 0.2.5; webencodings | 0.5.1; wheel | 0.37.0; win-inet-pton | 1.1.0; wincertstore | 0.2; xlrd | 1.2.0; zipp | 3.6.0. </body>. </html>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:3507,learn,learn,3507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,4,['learn'],['learn']
Usability,"ry familiar with Scanpy (which seems like a fantastic library!), so please bear with me if some of the things I mention are not relevant to Scanpy. PyMDE (documentation here: https://pymde.org/) has a few benefits:; - as @adamgayoso mentioned, PyMDE supports computing embeddings on GPU. This makes it possible to compute very large embeddings quickly (often 4-10x faster than CPU).; - PyMDE is a very general embedding library. It is based on a general framework for embedding, and this framework includes many well-known methods --- such as UMAP, PCA, Laplacian embedding, multi-dimensional scaling, and more --- as special cases. This makes it easy to compare different methods using a single framework.; - PyMDE also supports creating entirely new types of embeddings, as custom instances of our framework.; - PyMDE provides ways to reason about how much an embedding distorts the original neighborhood graph. There are some comparisons to UMAP & openTSNE in the third part of our manuscript, which has been published in Foundations & Trends in Machine Learning and is available here: https://web.stanford.edu/~boyd/papers/pdf/min_dist_emb.pdf; - on CPU, UMAP and PyMDE are comparable in speed, with UMAP often having a slight edge; on GPU PyMDE can be much faster; - unlike UMAP/openTSNE, PyMDE allows users to fit constrained embeddings. Right now the supported constraints are standardization (zero mean, unit covariance; this forces embeddings to spread out, but not too much, and as a result standardized embeddings are typically similarly scaled), centering, and anchoring (pre-specifying the coordinates of a subset of the items); - PyMDE allows for more types of embeddings, in addition to UMAP-style embeddings. On the other hand, PyMDE is young software. If you do depend on it, I would recommend including it as an optional dependency, not a required one. Happy to chat more, to answer any questions, and to help with integration, if that is something you are ultimately interested in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627:1181,Learn,Learning,1181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627,1,['Learn'],['Learning']
Usability,"s code block (if applicable, else delete the block): -->; ```pytb; computing neighbors; using 'X_pca' with n_pcs = 30; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-38-e2dd1fe70ab9> in <module>; 6 sc.settings.n_jobs = 15; 7 with parallel_backend('threading', n_jobs=20):; ----> 8 sc.pp.neighbors(adata, n_neighbors=15, n_pcs=30); 9 ; 10 #sc.settings.n_jobs = 15. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. > ...; Could you please help me resolve it? I looked up older issues but could not find solution to this. Thank you very much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154:2335,learn,learn,2335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154,1,['learn'],['learn']
Usability,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:2436,simpl,simple,2436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,2,['simpl'],['simple']
Usability,"s on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; Hi all, I am wondering if anyone has had similar situation as mine. ; After data normalization, batch correction with combat, and work through the pipeline on my own data, I was having issues generating rank gene groups. The error is as below. I understand that there are issues with using highly_variable_genes after combat, and this can be resolved after converting raw data back to sparse matrix using "" adata.X = scipy.sparse.csr_matrix(adata.X) "", but this method does not address my error. . Look forward to your response, thanks a lot! . **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.tl.rank_genes_groups(all_case,groupby='louvain',method='wilcoxon'). ```. ```pytb; ranking genes; ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <ipython-input-16-961d52bd7e16> in <module>(); ----> 1 sc.tl.rank_genes_groups(all_case,groupby='louvain',method='wilcoxon'). 7 frames; <__array_function__ internals> in matrix_power(*args, **kwargs). /usr/local/lib/python3.6/dist-packages/numpy/linalg/linalg.py in _assert_stacked_square(*arrays); 211 m, n = a.shape[-2:]; 212 if m != n:; --> 213 raise LinAlgError('Last 2 dimensions of the array must be square'); 214 ; 215 def _assert_finite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square; ```. #### Versions. <details>; scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.4.1 pandas==1.1.2 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.2; [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1467:1910,learn,learn,1910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1467,1,['learn'],['learn']
Usability,"s, kwargs)); 751 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 752 reraise(type(newerr), newerr, tb); 753 ; 754 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\utils.py in reraise(tp, value, tb); 79 if value.__traceback__ is not tb:; 80 raise value.with_traceback(tb); ---> 81 raise value; 82 ; 83 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4079:36: error: '%.2747' defined with type 'i64' but expected 'i32'; %"".2748"" = icmp eq i32 %"".2746"", %"".2747""; ^. File ""..\..\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 412:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. During: lowering ""id=13[LoopNest(index_variable = parfor_index.264, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (412)>, 400: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (418)>, 402: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (419)>, 276: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (416)>, 318: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (417)>}Var(parfor_index.264, _qc.py:412)"" at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (412); ```. **The package version is as follows:**. scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1+5.3b99dbf6 leidenalg==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:15138,learn,learn,15138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['learn'],['learn']
Usability,"s. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()). In [2]: sc.pp.highly_variable_genes(adata); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-2-7727f5f928cd>"", line 1, in <module>; sc.pp.highly_variable_genes(adata). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, nan, nan]).; You can drop duplicate edges by setting the 'duplicates' kwarg; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172:3361,learn,learn,3361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172,1,['learn'],['learn']
Usability,"s. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a); 544 signature = 'D->D' if isComplexType(t) else 'd->d'; 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular); --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj); 547 return wrap(ainv.astype(result_t, copy=False)); 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag); 86 ; 87 def _raise_linalgerror_singular(err, flag):; ---> 88 raise LinAlgError(""Singular matrix""); 89 ; 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix; ```; #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:3258,learn,learn,3258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,1,['learn'],['learn']
Usability,s_client=0.20.0; - prompt-toolkit=3.0.47; - prompt_toolkit=3.0.47; - protobuf=4.25.3; - psutil=5.9.8; - pthread-stubs=0.4; - ptyprocess=0.7.0; - pure_eval=0.2.2; - py-cpuinfo=9.0.0; - pycparser=2.22; - pyfaidx=0.8.1.1; - pygments=2.18.0; - pymde=0.1.18; - pymongo=4.7.3; - pynndescent=0.5.12; - pyobjc-core=10.2; - pyobjc-framework-cocoa=10.2; - pyparsing=3.1.2; - pysocks=1.7.1; - pytables=3.9.2; - python=3.11.4; - python-dateutil=2.9.0; - python-fastjsonschema=2.19.1; - python-igraph=0.11.5; - python-json-logger=2.0.7; - python-kaleido=0.2.1; - python-tzdata=2024.1; - python_abi=3.11; - pytorch=2.2.2; - pytz=2024.1; - pyvcf3=1.0.3; - pyyaml=6.0.1; - pyzmq=26.0.3; - radian=0.6.12; - rchitect=0.4.6; - readline=8.2; - referencing=0.35.1; - requests=2.32.3; - rfc3339-validator=0.1.4; - rfc3986-validator=0.1.1; - rpds-py=0.18.1; - scanpy=1.10.1; - scikit-learn=1.5.0; - scipy=1.13.1; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.3; - session-info=1.0.0; - setuptools=70.0.0; - simplejson=3.19.2; - six=1.16.0; - snappy=1.2.0; - sniffio=1.3.1; - soupsieve=2.5; - stack_data=0.6.2; - statsmodels=0.14.2; - stdlib-list=0.10.0; - sympy=1.12; - tbb=2021.12.0; - tenacity=8.3.0; - terminado=0.18.1; - texttable=1.7.0; - threadpoolctl=3.5.0; - tinycss2=1.3.0; - tk=8.6.13; - tomli=2.0.1; - torchvision=0.17.2; - tornado=6.4.1; - tqdm=4.66.4; - traitlets=5.14.3; - types-python-dateutil=2.9.0.20240316; - typing-extensions=4.12.2; - typing_extensions=4.12.2; - typing_utils=0.1.0; - tzdata=2024a; - umap-learn=0.5.5; - uri-template=1.3.0; - urllib3=2.2.1; - wcwidth=0.2.13; - webcolors=24.6.0; - webencodings=0.5.1; - websocket-client=1.8.0; - wheel=0.43.0; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.5; - zipp=3.19.2; - zlib-ng=2.0.7; - zstd=1.5.6; - pip:; - absl-py==2.1.0; - astunparse==1.6.3; - bcbio-gff==0.7.1; - flatbuffers==24.3.25; - gast==0.5.4; - google-pasta==0.2.0; - grpcio==1.64.1; - keras==3.3.3; - libclang==18.1.1; -,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:7224,simpl,simplejson,7224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['simpl'],['simplejson']
Usability,"s_with_different_data_types():; tree1 = MockTree(hyperplanes=[[1.0, 2.0]], offsets=[3], children=[[4, 5]], indices=[[6, 7]]); tree2 = MockTree(hyperplanes=[[8, 9]], offsets=[10], children=[[11, 12]], indices=[[13, 14]]); forest = [tree1, tree2]; result = _make_forest_dict(forest); assert result[""hyperplanes""][""data""].dtype == np.float64. # Test with trees that have properties with NaN or inf values; @pytest.mark.parametrize(""value"", [np.nan, np.inf, -np.inf]); def test_trees_with_special_values(value):; tree = MockTree(hyperplanes=[[value, value]], offsets=[value], children=[[value, value]], indices=[[value, value]]); forest = [tree]; result = _make_forest_dict(forest); assert np.isnan(result[""hyperplanes""][""data""]).all() or np.isinf(result[""hyperplanes""][""data""]).all(). # Test with a large number of trees; def test_large_number_of_trees():; trees = [MockTree(hyperplanes=[[i, i+1]], offsets=[i+2], children=[[i+3, i+4]], indices=[[i+5, i+6]]) for i in range(1000)]; forest = trees; result = _make_forest_dict(forest); assert len(result[""hyperplanes""][""start""]) == 1000; assert result[""hyperplanes""][""data""].shape == (2000, 2). # Test with trees missing one of the expected properties; def test_trees_missing_properties():; class IncompleteTree:; def __init__(self, hyperplanes, children, indices):; self.hyperplanes = np.array(hyperplanes); self.children = np.array(children); self.indices = np.array(indices); tree = IncompleteTree(hyperplanes=[[1, 2]], children=[[3, 4]], indices=[[5, 6]]); forest = [tree]; with pytest.raises(AttributeError):; _make_forest_dict(forest); ```; </details>. This optimization was discovered by [Codeflash AI](https://codeflash.ai) ⚡️. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [X] Tests included",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2971:4602,guid,guidelines,4602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2971,2,['guid'],"['guide', 'guidelines']"
Usability,"sc.pl.scatter() is a wrapper for _scatter_obs(). It checks to make sure; the variable names the caller is requesting to plot exist in var and/or; obs, but does not take into account whether it should look in raw based; on the use_raw flag, as _scatter_obs() does. This leads to errors when a; user asks to plot variables that are in the raw but not the filtered; matrix of adata. This commit fixes that bug. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027:479,guid,guidelines,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027,2,['guid'],"['guide', 'guidelines']"
Usability,"sc.pl.spatial plots vertically flipped spots when `sc.pl.spatial(..., img_key=None)`. Try comparing these in your lymph node demo notebook.; ```python; # Plot orientation correct; sc.pl.spatial(adata, img_key = ""hires"", cmap='magma',; color=['total_counts', 'n_genes_by_counts']). # Now spots are flipped; sc.pl.spatial(adata, img_key = None, cmap='magma',; color=['total_counts', 'n_genes_by_counts']); ```. #### Versions:; scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1148:516,learn,learn,516,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1148,1,['learn'],['learn']
Usability,"sc.pp.scale is giving me an error when I run it for the first time, then running fine on the second. This occurs when I run on an object generated using .copy() . ![Screen Shot 2019-07-08 at 10 55 34 AM](https://user-images.githubusercontent.com/26631928/60820560-a8fb6400-a16f-11e9-9915-7d808561af69.png). In terms of the numbers at the end--I have 2176 cells and 1600 highly variable genes. . If I run on an object not generated using copy, I get ""Trying to set attribute `.obs` of view, making a copy."" but it finishes on first run. . If I try to regress out counts first, I get. ![Screen Shot 2019-07-08 at 11 12 57 AM](https://user-images.githubusercontent.com/26631928/60821330-58850600-a171-11e9-9a50-666694bf2c1c.png). One additional oddity--if I run sc.pphighly_variable_genes with flavor = 'seurat' instead of flavor = 'cell_ranger' and call sc.pp.regress_out(Bcell, 'n_counts') prior to running sc.pp.scale(Bcell, max_value = 10) I don't get any error. If I don't run regress_counts but have the 'seurat' flavor I get ; ![Screen Shot 2019-07-08 at 11 22 04 AM](https://user-images.githubusercontent.com/26631928/60822004-9f273000-a172-11e9-814d-dfc83155b488.png). Really not sure what's happening but figured I should let you know. Thanks!. sc.settings.verbosity = 3. scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731:1370,learn,learn,1370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731,1,['learn'],['learn']
Usability,scRNA-seq CRISPR data: sc.read_10x_h5 does not include guide sequence in the count matrix and .var,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2398:55,guid,guide,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398,2,['guid'],['guide']
Usability,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:22,usab,usable,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395,2,['usab'],['usable']
Usability,"scale_spot is a float to scale the size of the spots from users. We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Regarding examples, I've made a tutorial that I will push to scanpy-tutorial. I'm waiting for @Mirkazemi read function that I need to complete the notebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580139642:166,simpl,simply,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580139642,2,['simpl'],['simply']
Usability,"scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/813:50,learn,learning,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813,1,['learn'],['learning']
Usability,"scanpy version 1.3.7. NOTE!: During typing this issue out i figured out how to solve it and save figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using yo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/418:887,simpl,simply,887,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418,1,['simpl'],['simply']
Usability,"sed https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, you’re locked into the threading backend for that session. * Make sure the threading layer is workqueue after pynndescent is imported, but make pynndescent import fast. A kinda simpler solution would be to require pynndescent 0.5.3, import it at the top level (boosts total import time by about ~0.2 seconds out of 3.3 total), and set the threading layer right there. This also has the advantage of letting users specify the threading layer after import scanpy and not having us interfere. This also requires that pynndescent is installed, which hits the reproducibility issues again.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:2177,simpl,simpler,2177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['simpl'],['simpler']
Usability,seems like the `--deps` flag can be used to select dependencies. Default is all dependencies. ```bash; $ beni --deps production pyproject.toml; channels:; - conda-forge; dependencies:; - pip:; - flit; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; name: scanpy; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811:376,learn,learn,376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811,4,['learn'],['learn']
Usability,"self._engine.get_loc(self._maybe_cast_indexer(key)); 2649 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2650 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'SPP1'. ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.4.0; backcall 0.1.0; bottleneck 1.2.1; cffi 1.11.5; cloudpickle 0.5.3; colorama 0.3.9; cycler 0.10.0; cython_runtime NA; cytoolz 0.9.0.1; dask 0.17.5; dateutil 2.7.3; decorator 4.3.0; fa2 NA; flaskext NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.9.7; ipykernel 4.8.2; ipython_genutils 0.2.0; ipywidgets 7.2.1; jedi 0.12.0; joblib 0.13.2; kiwisolver 1.0.1; legacy_api_wrap 1.2; leidenalg 0.8.8; llvmlite 0.34.0; louvain 0.6.1; lxml NA; matplotlib 3.3.4; mpl_toolkits NA; natsort 6.0.0; networkx 2.5.1; numba 0.51.2; numexpr 2.6.5; numpy 1.19.5; packaging 21.0; pandas 1.1.5; parso 0.2.0; pexpect 4.5.0; pickleshare 0.7.4; pkg_resources NA; prompt_toolkit 1.0.15; psutil 5.4.5; ptyprocess 0.5.2; pycparser 2.18; pygments 2.2.0; pynndescent 0.5.0; pyparsing 2.2.0; pytz 2018.4; ruamel NA; scipy 1.4.1; scvelo 0.2.4; setuptools_scm NA; simplegeneric NA; six 1.11.0; sklearn 0.24.2; sphinxcontrib NA; storemagic NA; tables 3.4.3; texttable 1.6.2; toolz 0.9.0; tornado 5.0.2; tqdm 4.32.1; traitlets 4.3.2; typing_extensions NA; umap 0.4.6; wcwidth NA; yaml 5.1.2; zipp NA; zmq 17.0.0; -----; IPython 6.4.0; jupyter_client 5.2.3; jupyter_core 4.4.0; jupyterlab 0.32.1; notebook 5.5.0; -----; Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) [GCC 7.2.0]; Linux-3.10.0-957.21.3.el7.x86_64-x86_64-with-centos-7.6.1810-Core; 120 logical CPU cores, x86_64; -----. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2018:5101,simpl,simplegeneric,5101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2018,1,['simpl'],['simplegeneric']
Usability,simplify travis,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360:0,simpl,simplify,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360,2,['simpl'],['simplify']
Usability,"sion of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python; import scanpy; scanpy.logging.print_versions(); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file); 178 print_versions(); 179 else:; --> 180 session_info.show(; 181 dependencies=True,; 182 html=False,; 183 excludes=[; 184 'builtins',; 185 'stdlib_list',; 186 'importlib_metadata',; 187 # Special module present if test coverage being calculated; 188 # https://gitlab.com/joelostblom/session_info/-/issues/10; 189 ""$coverage"",; 190 ],; 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 207 for mod_name in clean_modules:; 208 mod_names.append(mod_name); --> 209 mod = sys.modules[mod_name]; 210 # Since modules use different attribute names to store version info,; 211 # try the most common ones.; 212 try:. KeyError: 'numcodecs'; ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```; scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2580:2088,learn,learn,2088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580,1,['learn'],['learn']
Usability,sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1557:46,learn,learn,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557,3,['learn'],['learn']
Usability,"so idk about you, but i think [CFFI] is pretty painless for interacting with C/C++ code:. * you simply compile a `.so` file, use `ffi.dlopen('….so')`, `ffi.cdef('void myfunc(…)')` and are able to call it from python.; * it works with PyPy; * it can be used with numpy ([example](https://gist.github.com/arjones6/5533938)). we just have to think if we need to pass complex data structures, but i assume a few dense and sparse matrices is all you need. we just have to figure that out beforehand, and how to create a sparse matrix from raw memory (AFAIK everything under the sun supports at least column compressed layout, we might want to switch `csr_matrix` → `csc_matrix`). [CFFI]: https://cffi.readthedocs.io/en/latest",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/19:96,simpl,simply,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19,1,['simpl'],['simply']
Usability,"some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1110,learn,learn,1110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['learn'],['learn']
Usability,son-logger=2.0.7; - python-kaleido=0.2.1; - python-tzdata=2024.1; - python_abi=3.11; - pytorch=2.2.2; - pytz=2024.1; - pyvcf3=1.0.3; - pyyaml=6.0.1; - pyzmq=26.0.3; - radian=0.6.12; - rchitect=0.4.6; - readline=8.2; - referencing=0.35.1; - requests=2.32.3; - rfc3339-validator=0.1.4; - rfc3986-validator=0.1.1; - rpds-py=0.18.1; - scanpy=1.10.1; - scikit-learn=1.5.0; - scipy=1.13.1; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.3; - session-info=1.0.0; - setuptools=70.0.0; - simplejson=3.19.2; - six=1.16.0; - snappy=1.2.0; - sniffio=1.3.1; - soupsieve=2.5; - stack_data=0.6.2; - statsmodels=0.14.2; - stdlib-list=0.10.0; - sympy=1.12; - tbb=2021.12.0; - tenacity=8.3.0; - terminado=0.18.1; - texttable=1.7.0; - threadpoolctl=3.5.0; - tinycss2=1.3.0; - tk=8.6.13; - tomli=2.0.1; - torchvision=0.17.2; - tornado=6.4.1; - tqdm=4.66.4; - traitlets=5.14.3; - types-python-dateutil=2.9.0.20240316; - typing-extensions=4.12.2; - typing_extensions=4.12.2; - typing_utils=0.1.0; - tzdata=2024a; - umap-learn=0.5.5; - uri-template=1.3.0; - urllib3=2.2.1; - wcwidth=0.2.13; - webcolors=24.6.0; - webencodings=0.5.1; - websocket-client=1.8.0; - wheel=0.43.0; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.5; - zipp=3.19.2; - zlib-ng=2.0.7; - zstd=1.5.6; - pip:; - absl-py==2.1.0; - astunparse==1.6.3; - bcbio-gff==0.7.1; - flatbuffers==24.3.25; - gast==0.5.4; - google-pasta==0.2.0; - grpcio==1.64.1; - keras==3.3.3; - libclang==18.1.1; - markdown==3.6; - markdown-it-py==3.0.0; - mdurl==0.1.2; - ml-dtypes==0.3.2; - namex==0.0.8; - opt-einsum==3.3.0; - optree==0.11.0; - rich==13.7.1; - tensorboard==2.16.2; - tensorboard-data-server==0.7.2; - tensorflow==2.16.1; - tensorflow-io-gcs-filesystem==0.37.0; - termcolor==2.4.0; - werkzeug==3.0.3; - wrapt==1.16.0; ```. The virtual environment on my laptop (successful case):; ```; channels:; - pytorch; - bioconda; - conda-forge; dependencies:; - adjusttext=1.0.4; - anndata=0.10.5.post1; - anyio=,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:7743,learn,learn,7743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['learn'],['learn']
Usability,"source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed correctly and modified, as well as a scale_basis argument to match the image coordinate if needed. That's it, looking forward to hear what are your thoughts and if you agree with current behaviour I'll go on and changing the docs as well as writing more tests (especially for first example where `groups` is used, where the background color is different)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:4451,simpl,simply,4451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,2,['simpl'],['simply']
Usability,"ssion"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1285,simpl,simply,1285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,2,['simpl'],['simply']
Usability,"suddenly I have this problem, maybe related to an anndata upgrade. pip says all requirements are satisfied:. scanpy==1.4.3 anndata==0.6.22rc1 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1 ; ```; ; adata ; AnnData object with n_obs × n_vars = 466 × 28685 ; obs: 'GEO_Sample_age', 'age', 'age_unit', 'biosample_source_life_stage', 'biosample_source_gender', 'sample_category', 'biosample_cell_type', 'n_genes', 'n_counts', 'percent_mito'; var: 'n_cells'. fig1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728:203,learn,learn,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728,1,['learn'],['learn']
Usability,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:267,simpl,simply,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457,4,"['clear', 'simpl']","['clear', 'simply']"
Usability,"symbols instead of ensemblID (index column) I use the gene_symbols parameter:. sc.pl.dotplot(adata=adata, var_names = ['ENSG00000104814','ENSG00000043462'], gene_symbols='symbol'). But I get the following error:. Error: Gene symbol 'ENSG00000104814' not found in given gene_symbols column: 'symbol'; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-58-6d92e2cc2451> in <module>; 4 sc.pl.dotplot(adata, myg, groupby=condition,dot_min=0,dot_max=0.2,vmin=0,vmax=0.2, save=title+'_'+myg_geneID+'.png'); 5 if type(myg_geneID_orig) == list:; ----> 6 sc.pl.dotplot(adata, myg, groupby=condition,dot_min=0,dot_max=0.2,vmin=0,vmax=0.2, gene_symbols='symbol', save=title+'_multiple_genes'+'.png'). /pstore/apps/bioinfo/scseq/modules/software/Scanpy/1.4.1-foss-2018b-Python-3.7.1-2018.12/lib/python3.7/site-packages/scanpy-1.4.1-py3.7.egg/scanpy/plotting/_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, figsize, dendrogram, gene_symbols, var_group_positions, standard_scale, smallest_dot, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1383 var_names = [var_names]; 1384 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,; -> 1385 layer=layer, gene_symbols=gene_symbols); 1386 ; 1387 # for if category defined by groupby (if any) compute for each var_name. TypeError: cannot unpack non-iterable NoneType object. My understanding is that it should search for 'ENSG00000104814' in the index column and return the corresponding value in 'symbols', but it seems that is directly searching 'ENSG00000104814' in the 'symbols' column. . Thanks for helping, find below the version I am using: . #### Versions. scanpy==1.4.1 anndata==0.6.22.post1 numpy==1.15.4 scipy==1.1.0 pandas==0.25.2 scikit-learn==0.20.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1412:2808,learn,learn,2808,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412,1,['learn'],['learn']
Usability,"t this ```adata``` object is a subset of all cells created by keeping only the cells that express the gene Crabp1, as follows:; ```adata = adata_full[adata_full.obs['Crabp1_cell'] == 'True']```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.pp.regress_out(adata, ['n_counts']); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; sc.pp.regress_out(adata, ['n_counts']); regressing out ['n_counts']; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:179: RuntimeWarning: invalid value encountered in true_divide; return np.sum(resid_dev * freq_weights * var_weights / scale); Traceback (most recent call last):. File ""<ipython-input-6-4693dee26417>"", line 1, in <module>; sc.pp.regress_out(adata, ['n_counts']). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 841, in regress_out; res = list(map(_regress_out_chunk, tasks)). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 867, in _regress_out_chunk; result = sm.GLM(data_chunk[:, col_index], regres, family=sm.families.Gaussian()).fit(). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1027, in fit; cov_kwds=cov_kwds, use_t=use_t, **kwargs). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1142, in _fit_irls; raise ValueError(""The first guess on the deviance function "". ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1171:2461,learn,learn,2461,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1171,1,['learn'],['learn']
Usability,t-SNE optimization using scikit-learn-intelex,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:32,learn,learn-intelex,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,4,['learn'],['learn-intelex']
Usability,"t: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`?. ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these?. ```python; sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns; sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]); ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:2912,intuit,intuitive,2912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,2,['intuit'],['intuitive']
Usability,"ta analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the fol",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:1205,intuit,intuitive,1205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,2,['intuit'],['intuitive']
Usability,tconsole 5.0.3 pyhd3eb1b0_0 ; qtpy 1.9.0 py_0 ; r 3.6.0 r36_0 r; r-base 3.6.1 haffb61f_2 r; r-boot 1.3_20 r36h6115d3f_0 r; r-class 7.3_15 r36h96ca727_0 r; r-cluster 2.0.8 r36ha65eedd_0 r; r-codetools 0.2_16 r36h6115d3f_0 r; r-foreign 0.8_71 r36h96ca727_0 r; r-kernsmooth 2.23_15 r36ha65eedd_4 r; r-lattice 0.20_38 r36h96ca727_0 r; r-mass 7.3_51.3 r36h96ca727_0 r; r-matrix 1.2_17 r36h96ca727_0 r; r-mgcv 1.8_28 r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:15790,learn,learn,15790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['learn'],['learn']
Usability,"te without warnings; * DotPlot and MatrixPlot docs are only rebuilt if I update them. ## Separate tracked and generated content. I find it difficult to navigate the rst api docs when there is a large amount of auto generated files mixed in with manually curated ones. This becomes more of a problem since the autogenerated files are not removed by `make clean` and will still generate `html` files and generally be a nuisance while they are still around. Now all autogenerated API files are put in a `generated` directory. This is also what [xarray](https://github.com/pydata/xarray/blob/e6168ed6b771b7377e89acb47cc7e5aa9f21a574/doc/api.rst), [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_range_finder.html#sklearn.utils.extmath.randomized_range_finder), and [seaborn](https://github.com/mwaskom/seaborn/blob/ba4bd0fa0a90b2bd00cb62c2b4a5e38013a73ac6/doc/api.rst) do. Now `make clean` also deletes all auto generated `rst` files. This also dramatically simplifies our `.gitignore`. ## Consolidation. I've also consolidated the managed api docs to a smaller number of files. Namely, instead of `api/*.rst` there's now just `api.rst`. Instead of `external/*.rst` there's just `external.rst`. ## Plotting functions API docs. In the current api docs, some plotting functions have a plot displayed inline instead of a one line description. This is nice because it provides a visual reference for what the plotting function does. It's less nice because:. * It takes up a huge amount of space; * It takes up a different amount of space for each function; * These plots are manually generated, and are often quite out of date. <details>; <summary> example </summary>. <img width=""899"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111894736-5657fd80-8a61-11eb-8077-97bcb5c54765.png"">. </details>. I think the reference docs should be handled in the same way for each of the modules. Example plots make more sense in the user guide/ tutor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1753:1680,simpl,simplifies,1680,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753,1,['simpl'],['simplifies']
Usability,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:3028,intuit,intuitively,3028,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,2,['intuit'],['intuitively']
Usability,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:64,clear,clear,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756,4,"['clear', 'simpl']","['clear', 'simple']"
Usability,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:430,simpl,simple,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119,10,"['clear', 'simpl']","['clear', 'clearer', 'simple', 'simply']"
Usability,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it?. enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:497,clear,clear,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,2,['clear'],['clear']
Usability,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON; old_to_new = dict(; old_cluster1='new_cluster1',; old_cluster2='new_cluster1',; old_cluster3='new_cluster2',; ); adata.obs['new_clusters'] = (; adata.obs['old_clusters']; .map(old_to_new); .astype('category'); ); ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-555069647:17,simpl,simply,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-555069647,2,['simpl'],['simply']
Usability,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:40,learn,learn,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671,2,['learn'],['learn']
Usability,"the problem is that at some point the code calls np.argsort on a; pandas.Series and this returns -1 for NaN values. I will submit a PR to fix; this. Meanwhile, simpy plot as follows: sc.pl.pca(adata, color='property',; size=50, sort_order=False). On Tue, Mar 19, 2019 at 11:45 AM Fabian Rost <notifications@github.com>; wrote:. > I just add the versions I used:; >; > sc.logging.print_versions(); >; > scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/536#issuecomment-474301705>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b3l_xKITwuZQ-XpENUHSioDqLWLks5vYL_TgaJpZM4b6BYA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/536#issuecomment-474344606:479,learn,learn,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536#issuecomment-474344606,2,['learn'],['learn']
Usability,"the runtime checks would be too costly or impossible. to test for `List[int]`, you’d have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just can’t test it at all – if you’d iterate the thing to check the objects it yields, you exhaust it and it’s no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064:298,usab,usable,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064,2,['usab'],['usable']
Usability,"the sc.pl.dendrogram function doesn't include height, which I wanted to include in the dendrogram I am producing that will inform subsequent modifications. I wanted to use the output from adata.uns[""dendrogram_bulk_labels""][""dendrogram_info""] for subsequent analysis (e.g. keys `""linkage""` and `""ivl""`), but the labels in the `""ivl""` key do not agree with the order of the leaves on the dendrogram. . ```; import numpy as np; import pandas as pd; import scanpy as sc. from scipy.cluster import hierarchy; from scipy.cluster.hierarchy import dendrogram, linkage; import matplotlib.pyplot as plt. # import adata; adata = sc.datasets.pbmc68k_reduced(); sc.pp.normalize_total(adata, target_sum = 1e4); sc.pp.log1p(adata); groupCat = ""bulk_labels""; sc.tl.dendrogram(adata, groupby = groupCat). zLab = ""dendrogram_"" + groupCat; Z = adata.uns[zLab][""linkage""]. ### scanpy dendrogram; sc.pl.dendrogram(adata, groupby = groupCat,; orientation = ""right"",; dendrogram_key = zLab). scanpyIVL =adata.uns[zLab][""dendrogram_info""][""ivl""]. print(scanpyIVL) # output: ['CD14+ Monocyte', 'Dendritic', 'CD19+ B', 'CD34+', 'CD4+/CD45RA+/CD25- Naive T', 'CD8+/CD45RA+ Naive Cytotoxic', 'CD4+/CD25 T Reg', 'CD4+/CD45RO+ Memory', 'CD8+ Cytotoxic T', 'CD56+ NK']. ```. in this case, it's the reverse order of the leaves. for different adata objects I've tried, I get completely shuffled lists.; I tried checking if the order is guided based on the index of 'leaves' and I did not find it to be for the objects ive tried.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1944:1404,guid,guided,1404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1944,1,['guid'],['guided']
Usability,"this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689; - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3267:149,learn,learn,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267,7,"['learn', 'simpl']","['learn', 'simplifies']"
Usability,"thod=“mean.var.plot”)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(…, flavor=“seurat_v3”)` mimics `FindVariableFeatures(…, method=“vst”)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is; - a) only a question currently open (I am really not particularly an expert in R with limited bandwidth to check things there so waiting for their answer).; - b) Even if true, this does not affect our examples here. It comes into play when we t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:1322,clear,clear,1322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,2,['clear'],['clear']
Usability,"ties' is deprecated. It has been moved to .obsp[connectivities], and will not be accesible here in a future version of anndata.; after removing the cwd from sys.path. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-28-2906b54049c5> in <module>; 3 sc.pp.neighbors(adata, n_neighbors=1); 4 print('Connectivities:\n', adata.uns['neighbors']['connectivities'].A); ----> 5 sc.tl.umap(adata). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 194 neigh_params.get('metric', 'euclidean'),; 195 neigh_params.get('metric_kwds', {}),; --> 196 verbose=settings.verbosity > 3,; 197 ); 198 elif method == 'rapids':. /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, output_metric, output_metric_kwds, euclidean_output, parallel, verbose); 1022 n_epochs = 200; 1023 ; -> 1024 graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0; 1025 graph.eliminate_zeros(); 1026 . /opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where); 37 def _amax(a, axis=None, out=None, keepdims=False,; 38 initial=_NoValue, where=True):; ---> 39 return umr_maximum(a, axis, None, out, keepdims, initial, where); 40 ; 41 def _amin(a, axis=None, out=None, keepdims=False,. ValueError: zero-size array to reduction operation maximum which has no identity. ```. #### Versions; For me, scanpy.logging.print_versions() crashes due to the importlib_metadata issue.; <details>. scanpy==1.7.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706:2897,learn,learn,2897,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706,1,['learn'],['learn']
Usability,"tigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal tSNE embeddings. The same about initialization: UMAP smartly uses Laplacian Eigenmaps to initialize, but sklearn/multicore tSNE use random init, which is simply a bad choice (as again shown in that paper). openTSNE now uses PCA init by default which is much more sensible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:1733,learn,learning,1733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833,6,"['learn', 'simpl']","['learning', 'simply']"
Usability,"tion with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3857,simpl,simply,3857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,2,['simpl'],['simply']
Usability,"tion/pca.py in _fit(self, X); 380 ; 381 X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,; --> 382 copy=self.copy); 383 ; 384 # Handle n_components==None. ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator); 556 "" a minimum of %d is required%s.""; 557 % (n_features, array.shape, ensure_min_features,; --> 558 context)); 559 ; 560 if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:. ValueError: Found array with 0 feature(s) (shape=(44495, 0)) while a minimum of 1 is required.; ```. The `pca` code doesn't error here, because `highly_variable_intersection` makes `'highly_variable' in adata.var.keys()` evaluate to `True`:; ```; if use_highly_variable is True and 'highly_variable' not in adata.var.keys():; raise ValueError('Did not find adata.var[\'highly_variable\']. '; 'Either your data already only consists of highly-variable genes '; 'or consider running `pp.highly_variable_genes` first.'); if use_highly_variable is None:; use_highly_variable = True if 'highly_variable' in adata.var.keys() else False; if use_highly_variable:; logg.info(' on highly variable genes'); adata_comp = adata[:, adata.var['highly_variable']] if use_highly_variable else adata. ```; ```pytb; adata.var.keys(); Index(['mito', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts',; 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts',; 'n_cells', 'highly_variable', 'means', 'dispersions',; 'dispersions_norm', 'highly_variable_nbatches',; 'highly_variable_intersection'],; dtype='object'); ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post2 anndata==0.7.1 umap==0.3.10 numpy==1.17.0 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.3 statsmodels==0.11.0dev0+630.g4565348 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032:3785,learn,learn,3785,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032,1,['learn'],['learn']
Usability,tisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.13.0); Requirement already satisfied: numpy>=1.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.17.4); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:8123,learn,learn,8123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['learn'],['learn']
Usability,"to make formatting easier, you should do `pre-commit install` like mentioned in the contributor guide: https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#pre-commit. Please also fill out the checklist:. - if there is an issue closed by this, please link it; - you can check the box for tests, since this function already has tests. I added the `benchmark` label so we see that it actually makes things faster; - what’s missing is a release note entry: just edit `1.10.2.md` and add a line there please",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949:96,guid,guide,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949,2,['guid'],['guide']
Usability,trust 0.2.0; conda_index 0.3.0; conda-libmamba-solver 23.7.0; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; constantly 15.1.0; contourpy 1.0.5; cookiecutter 1.7.3; cryptography 41.0.3; cssselect 1.1.0; cycler 0.11.0; Cython 3.0.3; cytoolz 0.12.0; daal4py 2023.1.1; dask 2023.6.0; datasets 2.12.0; datashader 0.15.2; datashape 0.5.4; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; diff-match-patch 20200713; dill 0.3.6; distributed 2023.6.0; docstring-to-markdown 0.11; docutils 0.18.1; entrypoints 0.4; et-xmlfile 1.1.0; executing 0.8.3; fastjsonschema 2.16.2; filelock 3.9.0; flake8 6.0.0; Flask 2.2.2; fonttools 4.25.0; frozenlist 1.3.3; fsspec 2023.4.0; future 0.18.3; gensim 4.3.0; glob2 0.7; gmpy2 2.1.2; greenlet 2.0.1; h5py 3.9.0; HeapDict 1.0.1; holoviews 1.17.1; huggingface-hub 0.15.1; hvplot 0.8.4; hyperlink 21.0.0; idna 3.4; imagecodecs 2023.1.23; imageio 2.31.1; imagesize 1.4.1; imbalanced-learn 0.10.1; importlib-metadata 6.0.0; incremental 21.3.0; inflection 0.5.1; iniconfig 1.1.1; intake 0.6.8; intervaltree 3.1.0; ipykernel 6.25.0; ipython 8.15.0; ipython-genutils 0.2.0; ipywidgets 8.0.4; isort 5.9.3; itemadapter 0.3.0; itemloaders 1.0.4; itsdangerous 2.0.1; jaraco.classes 3.2.1; jedi 0.18.1; jeepney 0.7.1; jellyfish 1.0.1; Jinja2 3.1.2; jinja2-time 0.2.0; jmespath 0.10.0; joblib 1.2.0; json5 0.9.6; jsonpatch 1.32; jsonpointer 2.1; jsonschema 4.17.3; jupyter 1.0.0; jupyter_client 7.4.9; jupyter-console 6.6.3; jupyter_core 5.3.0; jupyter-events 0.6.3; jupyter-server 1.23.4; jupyter_server_fileid 0.9.0; jupyter_server_ydoc 0.8.0; jupyter-ydoc 0.2.4; jupyterlab 3.6.3; jupyterlab-pygments 0.1.2; jupyterlab_server 2.22.0; jupyterlab-widgets 3.0.5; kaleido 0.2.1; keyring 23.13.1; kiwisolver 1.4.4; lazy_loader 0.2; lazy-object-proxy 1.6.0; libarchive-c 2.9; libmambapy 1.5.1; linkify-it-py 2.0.0; llvmlite 0.40.0; lmdb 1.4.1; locket 1.0.0; lxml 4.9.3; lz4 4.3.2; Markdown 3.4,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:3331,learn,learn,3331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['learn'],['learn']
Usability,"ts.visium_sge(sample_id=""V1_Human_Lymph_Node""); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/site-packages/scanpy/datasets/_datasets.py"", line 367, in visium_sge; _download_visium_dataset(sample_id); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/site-packages/scanpy/datasets/_datasets.py"", line 325, in _download_visium_dataset; _utils.check_presence_download(; File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/site-packages/scanpy/_utils.py"", line 604, in check_presence_download; _download(backup_url, filename); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/site-packages/scanpy/readwrite.py"", line 905, in _download; urlretrieve(url, str(path), reporthook=update_to); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/urllib/request.py"", line 247, in urlretrieve; with contextlib.closing(urlopen(url, data)) as fp:; File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, timeout); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/urllib/request.py"", line 531, in open; response = meth(req, response); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/urllib/request.py"", line 640, in http_response; response = self.parent.error(; File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/urllib/request.py"", line 569, in error; return self._call_chain(*args); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/urllib/request.py"", line 502, in _call_chain; result = func(*args); File ""/data/lu/anaconda3/envs/spatial_t/lib/python3.8/urllib/request.py"", line 649, in http_error_default; raise HTTPError(req.full_url, code, msg, hdrs, fp); urllib.error.HTTPError: HTTP Error 403: Forbidden. Process finished with exit code 1. #### Versions. <details>; scanpy==1.5.0 anndata==0.7.1 umap==0.4.2 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.1 statsmodels==0.11.0. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1714:2773,learn,learn,2773,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1714,1,['learn'],['learn']
Usability,"typo, `variabes->variables`. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [X] Tests included or not required because:; small change in documentation; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:; minor change in documentation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2793:100,guid,guidelines,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2793,2,['guid'],"['guide', 'guidelines']"
Usability,"u have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Hi all!. My use case for scanpy is analysis of whole-body data from a weird marine annelid. We sort of have an idea of what to expect, but a lot of the analysis is exploratory, and my main job is helping canalize the knowledge that is available in the lab into making sense of the data. In this context, dotplots are our best friend, as it provides a very nice summary of gene expression over the whole (clustered) dataset. However, yesterday we noticed a confusing edge case: let’s say gene $g$ is expressed in the same number of cells in two clusters, 4 and 23. Cluster 4 has many, many more cells than 23, therefore on the dotplot it will look like $g$; is barely expressed in 4, but a great marker for 23. Of course, combining a dotplot with a feature plot helps you see that, but you get no sense of how many cells those are (more/less/the same). To alleviate this I am proposing an extension of dotplots: instead of circles, boxes, that have a height proportional to $log(#cells_{cluster})$, are filled proportionally to how many cells express gene $g$, and are colored according to the average expression. I think this works better than violinplots. Sadly I see no good way to multiplex this and plot multiple genes at once. I am really interested in feedback - maybe I am overlooking something super simple/basic?. ![image](https://user-images.githubusercontent.com/1651067/149312386-fbabade5-fdbe-4a72-a627-599bd103a9a9.png). the corresponding dotplot:. ![image](https://user-images.githubusercontent.com/1651067/149316899-eb34a0f6-5261-4234-94a4-59ee6d566090.png). The domino plot without log scale:. ![image](https://user-images.githubusercontent.com/1651067/149316905-334deb6a-aeda-46be-8261-84e09ba7ad54.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107:1727,feedback,feedback,1727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107,2,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:14,learn,learn,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609,4,['learn'],['learn']
Usability,umap embedding not working as expected in umap-learn 0.5.2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2026:47,learn,learn,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2026,2,['learn'],['learn']
Usability,"umap(adata, color, use_raw, edges, edges_width, edges_color, arrows, arrows_kwds, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, size, title, show, save, ax); 290 show=False,; 291 save=False,; --> 292 ax=ax); 293 if edges: utils.plot_edges(axs, adata, basis, edges_width, edges_color); 294 if arrows: utils.plot_arrows(axs, adata, basis, arrows_kwds). ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 104 show=show,; 105 save=save,; --> 106 ax=ax); 107 elif x is not None and y is not None:; 108 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 409 raise ValueError('""' + name + '"" is invalid!'; 410 + ' specify valid name, one of '; --> 411 + str(adata.obs[key].cat.categories)); 412 else:; 413 iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]. ValueError: ""Z"" is invalid! specify valid name, one of Index(['Zero', '1', '2', '3', '4'], dtype='object'); ```; The last call `sc.pl.umap` gives and error but I would expect it to work. It seems that scanpy iterates over the string `'Zero'` in the last call of `sc.pl.umap`. Of course, it is easy to work around by explicitly passing a list with one element as in the second call, but it took me a while to figure this out. `sc.logging.print_versions()` prints `scanpy==1.2.2+96.g28f5034 anndata==0.6.4 numpy==1.15.0 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/231:2855,learn,learn,2855,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231,1,['learn'],['learn']
Usability,"un(self.state); 365 if self.state.cr is not None:; 366 break. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in run(self, state); 345 (self.pipeline_name, pass_desc); 346 patched_exception = self._patch_error(msg, e); --> 347 raise patched_exception; 348 ; 349 def dependency_analysis(self):. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in run(self, state); 336 pass_inst = _pass_registry.get(pss).pass_inst; 337 if isinstance(pass_inst, CompilerPass):; --> 338 self._runPass(idx, pass_inst, state); 339 else:; 340 raise BaseException(""Legacy pass in use""). ~\anaconda3\lib\site-packages\numba\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in _runPass(self, index, pss, internal_state); 300 mutated |= check(pss.run_initialization, internal_state); 301 with SimpleTimer() as pass_time:; --> 302 mutated |= check(pss.run_pass, internal_state); 303 with SimpleTimer() as finalize_time:; 304 mutated |= check(pss.run_finalizer, internal_state). ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in check(func, compiler_state); 273 ; 274 def check(func, compiler_state):; --> 275 mangled = func(compiler_state); 276 if mangled not in (True, False):; 277 msg = (""CompilerPass implementations should return True/False. "". ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 405 ; 406 # TODO: Pull this out into the pipeline; --> 407 NativeLowering().run_pass(state); 408 lowered = state['cr']; 409 signature = typing.signature(state.return_type, *state.args). ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 347 lower = lowering.Lower(targetctx, library, fndesc, interp,; 348 metadata=metadata); --> 349 lower.lower(); 350 if not flags.no_cpython_wrapper:; 351 lower.create_cpython_wrapper(flags.release_gi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:10277,Simpl,SimpleTimer,10277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['Simpl'],['SimpleTimer']
Usability,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:1611,clear,clear,1611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,4,['clear'],['clear']
Usability,"ups``` function. Specifically, I specify a reference level with ```reference = ``` argument but it is ignored. The table that this function produces in the ```.uns``` object indicates the reference as ```rest``` (the default) when I have indicated otherwise. . ```python; print(set(noncycling_adult.obs.class_1)); #{'krt', 'dendritic', 'eccrine', 'T-cell', 'mel'}. sc.tl.rank_genes_groups(noncycling_adult, groupby = 'class_1', groups = ['eccrine', 'krt', 'T-cell', 'dendritic'], reference = 'mel', method = 'wilcoxon'). print(full_adata.uns['rank_genes_groups']). """"""{'params': {'groupby': 'class_1', 'reference': 'rest', 'method': 'wilcoxon', 'use_raw': True, 'corr_method': 'benjamini-hochberg'}, 'scores': rec.array([(8.494621 ,), (8.326364 ,), (8.24139 ,), (7.382108 ,),; (7.340947 ,), (7.25889 ,), (7.2148457,), (7.0626616,),; (6.991276 ,), (6.952865 ,)],; dtype=[('T-cell', '<f4')]), 'names': rec.array([('IL32',), ('CD52',), ('CORO1A',), ('CD3D',), ('IL2RG',),; ('PTPRCAP',), ('RAC2',), ('CD2',), ('LTB',), ('S100A4',)],; dtype=[('T-cell', '<U50')]), 'logfoldchanges': rec.array([(10.175177 ,), (12.354224 ,), (11.05518 ,), (14.337216 ,),; (11.3317585,), ( 9.758805 ,), ( 8.825092 ,), (14.170704 ,),; (10.144425 ,), ( 5.6517367,)],; dtype=[('T-cell', '<f4')]), 'pvals': rec.array([(1.98579427e-17,), (8.33632215e-17,), (1.70221006e-16,),; (1.55802204e-13,), (2.12087430e-13,), (3.90279912e-13,),; (5.39952731e-13,), (1.63343167e-12,), (2.72397796e-12,),; (3.57940624e-12,)],; dtype=[('T-cell', '<f8')]), 'pvals_adj': rec.array([(4.86400449e-13,), (1.02094937e-12,), (1.38979777e-12,),; (9.54054799e-10,), (1.03897390e-09,), (1.59325269e-09,),; (1.88937174e-09,), (5.00115941e-09,), (7.41345735e-09,),; (8.76739764e-09,)],; dtype=[('T-cell', '<f8')])}; """"""; ```. Thanks for your help. #### Versions. <details>. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.2 pandas==1.1.3 scikit-learn==0.22 statsmodels==0.12.0 python-igraph==0.7.1 louvain==0.6.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1485:2212,learn,learn,2212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1485,1,['learn'],['learn']
Usability,"ut your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2282, in <module>; switch_backend(rcParams[""backend""]); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/miniconda3/envs/path/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/backends/backend_wxagg.py"", line 1, in <module>; import wx; ModuleNotFoundError: No module named 'wx'; ```. The solution is simple, install `wxPython` https://pypi.org/project/wxPython/. However, it would be nice if scanpy could handle this OS-specific dependancy. #### Versions:; The latest scanpy version (1.5.1) installed via conda- of course I cannot print the versions since the scanpy import fails, other details;. ```; >>> import sys; print(sys.version); 3.7.6 | packaged by conda-forge | (default, Jun 1 2020, 18:33:30) ; [Clang 9.0.1 ]; >>> import platform; print(platform.python_implementation()); print(platform.platform()); CPython; Darwin-17.7.0-x86_64-i386-64bit; ```; ...; ```; $ sw_vers; ProductName:	Mac OS X; ProductVersion:	10.13.6; BuildVersion:	17G11023; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:1654,simpl,simple,1654,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,1,['simpl'],['simple']
Usability,"ut-21-3dabe52b6132> in <module>; 1 import scanpy as sc; 2 adata = sc.datasets.paul15(); ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 208 expected_doublet_rate=expected_doublet_rate,; 209 stdev_doublet_rate=stdev_doublet_rate,; --> 210 random_state=random_state,; 211 ); 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose); 349 ; 350 if mean_center and normalize_variance:; --> 351 sl.pipeline_zscore(scrub); 352 elif mean_center:; 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self); 62 def pipeline_zscore(self):; 63 gene_means = self._E_obs_norm.mean(0); ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)); 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)); 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis); 153 ''' variance across the specified axis '''; 154 ; --> 155 mean_gene = E.mean(axis=axis).A.squeeze(); 156 tmp = E.copy(); 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'; ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1645:3079,learn,learn,3079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645,1,['learn'],['learn']
Usability,"ve confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Since updating to scanpy 1.8 and pandas 1.3 I am getting an error with sc.tl.rank_genes_groups that my reference needs to be one of groupby=[cats], but as you can see in the traceback below, it appears identical. It has something to do with the data types, I've been dealing with this for a few weeks now and on some data sets I am eventually able to figure it out and get it to run, on some I am not. I think this is more of a pandas issue than scanpy, so I am wondering what version of pandas you recommend for anndata 0.7.6? Or if you know a simple workaround to make sure that the datatypes match? This has been a massive headache. I use this list(zip()) syntax in my code frequently, have never had an issue with datatypes inside of a zipped object... so this may be a simple pythonic question, but if there is a different method of accomplishing the same thing thing that doesn't introduce this error I would be happy to hear that as well. In the example below, in my adata.obs I have a column 'condition' that is a categorical variable of biological condition for differential expression, and so all cells belong to either group 0 or 1. I have tried seemingly every combination of having these, and the leiden clusters column be integers, strings, objects, before converting to the categorical dtype in line 7 below, always get the same error, and the reference = item always appears identical to the first item in the groupby = list. ### Minimal code sample (that we can copy&paste without having any data). ```python; cluster_method='leiden'; n_genes=1000; g1n='Control'; adata.obs['condition']=adata.obs['condition'].astype('category'); adata.obs[cluster_method]=adata.obs[cluster_method].astype('category'); pairs = list(zip(adata.obs['condition'], adata.obs[cluster_method])); adata.obs['pairs_'+cluster_method]=pairs; adata.obs['pairs_'+c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:1003,simpl,simple,1003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['simpl'],['simple']
Usability,"ve figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using your plotting functions? @fidelram tagging you here as I assume you might be able to know this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/418:1735,clear,clear,1735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418,1,['clear'],['clear']
Usability,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/170#issuecomment-398737615:165,learn,learn,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170#issuecomment-398737615,2,['learn'],['learn']
Usability,"what’s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(…)` to `categories, obs_tidy, _ = _prepare_dataframe(…)`. Other than that, there’s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so!; 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace “xyz” with whatever you want):. ```py; def test_xyz(image_comparer):; save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); […]; sc.pl.xyz(adata, …); save_and_compare_images('xyz'); ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesn’t exist. You need to copy the pngs from `scanpy/tests/figures`→`scanpy/test/_images` and `git commit` them.; 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144; 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data?. @Khalid-Usman I’m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you won’t regret doing this. You’re learning good coding practices here that will come in handy in the future, I promise!. Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412:1958,learn,learning,1958,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412,2,['learn'],['learning']
Usability,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/826:846,learn,learn,846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826,1,['learn'],['learn']
Usability,"with self:; ---> [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=34) return func(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); [294](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=293) mutated |= check(pss.run_initialization, internal_state); [295](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=294) with SimpleTimer() as pass_time:; --> [296](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=295) mutated |= check(pss.run_pass, internal_state); [297](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=296) with SimpleTimer() as finalize_time:; [298](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=297) mutated |= check(pss.run_finalizer, internal_state). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:269, in PassManager._runPass.<locals>.check(func, compiler_state); [268](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=267) def check(func, compiler_state):; --> [269](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=268) mangled = func(compiler_state); [270](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=269) if mangled not in (True, False):; [271](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=270) msg = (""CompilerPass implementations should return True/False. ""; [272](file:///d%3A/Users/xiangrong1/Miniconda3/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:25385,Simpl,SimpleTimer,25385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['Simpl'],['SimpleTimer']
Usability,wow. I was thinking...maybe the PAGA graph can be used as the simplified graph for color assignment. But will try your solution here!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-763156349:62,simpl,simplified,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763156349,2,['simpl'],['simplified']
Usability,y 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; diff-match-patch 20230430; dill 0.3.7; distlib 0.3.7; distributed 2023.9.3; docopt 0.6.2; docstring-to-markdown 0.12; docutils 0.20.1; dpath 2.1.6; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.1.3; executing 1.2.0; fastjsonschema 2.18.1; filelock 3.12.4; flake8 6.0.0; Flask 3.0.0; fonttools 4.43.1; fqdn 1.5.1; frozenlist 1.4.0; fsspec 2023.6.0; future 0.18.3; gensim 4.3.2; gitdb 4.0.10; GitPython 3.1.36; gmpy2 2.1.2; greenlet 3.0.0; h5py 3.9.0; holoviews 1.17.1; huggingface-hub 0.17.3; humanfriendly 10.0; hvplot 0.8.4; hyperlink 21.0.0; idna 3.4; igraph 0.10.4; imagecodecs 2023.1.23; imageio 2.31.1; imagesize 1.4.1; imbalanced-learn 0.11.0; importlib-metadata 6.8.0; importlib-resources 6.1.0; incremental 22.10.0; inflection 0.5.1; iniconfig 2.0.0; intake 0.7.0; intervaltree 3.1.0; ipykernel 6.25.2; ipython 8.16.1; ipython-genutils 0.2.0; ipywidgets 8.1.1; isoduration 20.11.0; isort 5.12.0; itemadapter 0.8.0; itemloaders 1.1.0; itsdangerous 2.1.2; jaraco.classes 3.3.0; jedi 0.18.1; jeepney 0.8.0; jellyfish 1.0.1; Jinja2 3.1.2; jmespath 1.0.1; joblib 1.3.2; json5 0.9.14; jsonpatch 1.33; jsonpointer 2.4; jsonschema 4.19.1; jsonschema-specifications 2023.7.1; jupyter 1.0.0; jupyter_client 8.3.1; jupyter-console 6.6.3; jupyter_core 5.3.2; jupyter-events 0.7.0; jupyter-lsp 2.2.0; jupyter_server 2.7.3; jupyter_server_terminals 0.4.4; jupyterlab 4.0.6; jupyterlab-pygments 0.2.2; jupyterlab_server 2.25.0; jupyterlab-widgets 3.0.9; keyring 24.2.0; kiwisolver 1.4.5; lazy_loader 0.3; lazy-object-proxy 1.9.0; leidenalg 0.9.1; libarchive-c 5.0; libmambapy 1.5.1; linkify-it-py 2.0.0; llvmlite 0.40.1; locket 1.0.0; lxml ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:5662,learn,learn,5662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['learn'],['learn']
Usability,y>Installed scanpy on jupyter notebook/ anaconda: </summary>. ```; pip install scanpy. Requirement already satisfied: scanpy in c:\users\charles\anaconda3\lib\site-packages (1.7.2); Requirement already satisfied: numba>=0.41.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\anaconda3\lib\site-p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:1034,learn,learn,1034,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,2,['learn'],['learn']
Usability,"yeah, a fix could simply be. ```diff; -return np.nan_to_num(dispersion_norm) >= disp_cut_off; +return np.nan_to_num(dispersion_norm, nan=-np.inf) >= disp_cut_off; ```. but I have a hard time coming up with a test. Doing something like this doesn’t work, as it crashes earlier,; with something like “ValueError: cannot specify integer `bins` when input data contains infinity”. ```py; @pytest.mark.parametrize(""flavor"", [""seurat"", ""cell_ranger""]); def test_no_filter_genes(flavor):; """"""Test that even with 0 columns in the data, n_top_genes is respected.""""""; adata = pbmc3k(); means, _ = _get_mean_var(adata.X); assert (means == 0).any(); sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000); assert adata.var[""highly_variable""].sum() == 10000; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157#issuecomment-2252862491:18,simpl,simply,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157#issuecomment-2252862491,2,['simpl'],['simply']
Usability,"yes, I know, that's non-ideal... the sparseness issue is circumvented by only returning top-scoring genes... I see that you make suggestions for how the user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:414,intuit,intuitive,414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458,2,['intuit'],['intuitive']
Usability,ygpcca | 1.0.3 | pygpcca | 1.0.3; pynndescent | 0.5.5 | pynndescent | 0.5.5 | pynndescent | 0.5.5; pyOpenSSL | 21.0.0 | pyOpenSSL | 21.0.0 | pyOpenSSL | 21.0.0; pyparsing | 3.0.4 | pyparsing | 3.0.4 | pyparsing | 3.0.4; pyrsistent | 0.18.0 | pyrsistent | 0.18.0 | pyrsistent | 0.18.0; pyscenic | 0.11.2 | pyscenic | 0.11.2 | pyscenic | 0.11.2; PySocks | 1.7.1 | PySocks | 1.7.1 | PySocks | 1.7.1; python-dateutil | 2.8.2 | python-dateutil | 2.8.2 | python-dateutil | 2.8.2; python-igraph | 0.9.9 | python-igraph | 0.9.9 | python-igraph | 0.9.9; python-utils | 3.1.0 | python-utils | 3.1.0 | python-utils | 3.1.0;   |   | pytoml | 0.1.21 |   |  ; pytz | 2021.3 | pytz | 2021.3 | pytz | 2021.3; pywin32 | 302 | pywin32 | 302 | pywin32 | 302; pywinpty | 0.5.7 | pywinpty | 0.5.7 | pywinpty | 0.5.7; PyYAML | 6 | PyYAML | 6 | PyYAML | 6; pyzmq | 22.3.0 | pyzmq | 22.3.0 | pyzmq | 22.3.0; requests | 2.27.1 | requests | 2.27.1 | requests | 2.27.1; scanpy | 1.8.2 | scanpy | 1.8.2 | scanpy | 1.8.2; scikit-learn | 1.0.2 | scikit-learn | 1.0.2 | scikit-learn | 1.0.2;   |   | scikit-misc | 0.1.4 |   |  ; scipy | 1.7.3 | scipy | 1.7.3 | scipy | 1.7.3; scvelo | 0.2.4 | scvelo | 0.2.4 | scvelo | 0.2.4; seaborn | 0.11.2 | seaborn | 0.11.2 | seaborn | 0.11.2; Send2Trash | 1.8.0 | Send2Trash | 1.8.0 | Send2Trash | 1.8.0; setuptools | 58.0.4 | setuptools | 58.0.4 | setuptools | 58.0.4;   |   | setuptools-scm | 6.3.2 |   |  ; sinfo | 0.3.4 | sinfo | 0.3.4 | sinfo | 0.3.4; six | 1.16.0 | six | 1.16.0 | six | 1.16.0; sniffio | 1.2.0 | sniffio | 1.2.0 | sniffio | 1.2.0; sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0; statsmodels | 0.13.1 | statsmodels | 0.13.1 | statsmodels | 0.13.1; stdlib-list | 0.8.0 | stdlib-list | 0.8.0 | stdlib-list | 0.8.0; tables | 3.6.1 | tables | 3.6.1 | tables | 3.6.1; tblib | 1.7.0 | tblib | 1.7.0 | tblib | 1.7.0; terminado | 0.9.4 | terminado | 0.9.4 | terminado | 0.9.4; testpath | 0.5.0 | testpath | 0.5.0 | testpath | 0.5.0; texttable | 1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:13156,learn,learn,13156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['learn'],['learn']
Usability,yhd8ed1ab_0 conda-forge; python-editor 1.0.4 py_0 conda-forge; python-igraph 0.10.6 py310h33b8572_0 conda-forge; python-installer 0.7.0 pyhd8ed1ab_0 conda-forge; python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge; python_abi 3.10 3_cp310 conda-forge; pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch; pytorch-cuda 11.8 h7e8668a_5 pytorch; pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge; pytorch-mutex 1.0 cuda pytorch; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyyaml 6.0 py310h5764c6d_5 conda-forge; pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge; rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge; re2 2023.03.02 h8c504da_0 conda-forge; readchar 4.0.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; referencing 0.30.0 pyhd8ed1ab_0 conda-forge; requests 2.31.0 pyhd8ed1ab_0 conda-forge; requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge; rich 13.4.2 pyhd8ed1ab_0 conda-forge; rpds-py 0.9.2 py310hcb5633a_0 conda-forge; scanpy 1.9.3 pyhd8ed1ab_0 conda-forge; scikit-learn 1.3.0 py310hf7d194e_0 conda-forge; scipy 1.11.1 py310ha4c1d20_0 conda-forge; scvi-tools 1.0.2 pyhd8ed1ab_0 conda-forge; seaborn 0.12.2 hd8ed1ab_0 conda-forge; seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge; secretstorage 3.3.3 py310hff52083_1 conda-forge; session-info 1.0.0 pyhd8ed1ab_0 conda-forge; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; shellingham 1.5.1 pyhd8ed1ab_0 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; sleef 3.5.1 h9b69904_2 conda-forge; sniffio 1.3.0 pyhd8ed1ab_0 conda-forge; soupsieve 2.3.2.post1 pyhd8ed1ab_0 conda-forge; sparse 0.14.0 pyhd8ed1ab_0 conda-forge; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pyhd8ed1ab_0 conda-forge; starsessions 1.3.0 pyhd8ed1ab_0 conda-forge; statsmodels 0.14.0 py310h278f3c1_1 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; suitesparse 5.10.1 h9e50725_1 conda-forge; sympy 1.12 pypyh9d50eac_103 conda-forge; tbb 2021.9.0 hf52228f_0 conda-forge; texttable 1.6.7 pyhd8ed1ab_0 conda-forge; threadp,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:19387,learn,learn,19387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,2,['learn'],['learn']
Usability,"you wanted to say “implicit” right?. and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations!. also, there’s no “technical restriction”. it’s about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441244349:401,simpl,simpler,401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441244349,2,['simpl'],['simpler']
Usability,ypi_0 pypi; libexpat 2.6.2 h73e2aa4_0 conda-forge; libffi 3.4.2 h0d85af4_5 conda-forge; libsqlite 3.45.2 h92b6c6a_0 conda-forge; libzlib 1.2.13 h8a1eda9_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.3 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4 h93d8f39_2 conda-forge; networkx 3.2.1 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.0 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd75f5a5_0 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.2.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre-commit 3.7.0 pypi_0 pypi; profimp 0.1.0 pypi_0 pypi; pynndescent 0.5.11 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 8.1.1 pypi_0 pypi; pytest-cov 4.1.0 pypi_0 pypi; pytest-mock 3.12.0 pypi_0 pypi; pytest-nunit 1.0.7 pypi_0 pypi; pytest-xdist 3.5.0 pypi_0 pypi; python 3.12.2 h9f0c242_0_cpython conda-forge; python-dateutil 2.9.0.post0 pypi_0 pypi; pytz 2024.1 pypi_0 pypi; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h9e318b2_1 conda-forge; scanpy 1.10.0rc2.dev16+g60aa7180 pypi_0 pypi; scikit-image 0.22.0 pypi_0 pypi; scikit-learn 1.4.1.post1 pypi_0 pypi; scipy 1.12.0 pypi_0 pypi; seaborn 0.13.2 pypi_0 pypi; session-info 1.0.0 pypi_0 pypi; setuptools 69.2.0 pyhd8ed1ab_0 conda-forge; setuptools-scm 8.0.4 pypi_0 pypi; six 1.16.0 pypi_0 pypi; statsmodels 0.14.1 pypi_0 pypi; stdlib-list 0.10.0 pypi_0 pypi; texttable 1.7.0 pypi_0 pypi; threadpoolctl 3.3.0 pypi_0 pypi; tifffile 2024.2.12 pypi_0 pypi; tk 8.6.13 h1abcd95_1 conda-forge; toolz 0.12.1 pypi_0 pypi; tqdm 4.66.2 pypi_0 pypi; typing-extensions 4.11.0 pypi_0 pypi; tzdata 2024.1 pypi_0 pypi; umap-learn 0.5.5 pypi_0 pypi; virtualenv 20.25.1 pypi_0 pypi; wheel 0.42.0 pyhd8ed1ab_0 conda-forge; xz 5.2.6 h775f41a_0 conda-forge; zarr 2.17.1 pypi_0 pypi; ```. </details>. ## TODO:. - [ ] Unpin pytest,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:34456,learn,learn,34456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,2,['learn'],['learn']
Usability,"ypi_0 pypi; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; singlecellhaystack 0.0.5 pypi_0 pypi; six 1.16.0 pyh6c4a22f_0 conda-forge; sniffio 1.3.0 pypi_0 pypi; soupsieve 2.4.1 pypi_0 pypi; sparse 0.14.0 pypi_0 pypi; squarify 0.4.3 pypi_0 pypi; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pypi_0 pypi; starsessions 1.3.0 pypi_0 pypi; statsmodels 0.14.0 pypi_0 pypi; stdlib-list 0.9.0 pypi_0 pypi; sympy 1.11.1 pypi_0 pypi; tensorstore 0.1.40 pypi_0 pypi; texttable 1.6.7 pypi_0 pypi; threadpoolctl 3.2.0 pypi_0 pypi; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pypi_0 pypi; torch 2.0.1+cu118 pypi_0 pypi; torchaudio 2.0.2+cu118 pypi_0 pypi; torchmetrics 1.0.1 pypi_0 pypi; torchvision 0.15.2+cu118 pypi_0 pypi; tornado 6.3.2 py311h459d7ec_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.9.0 pyhd8ed1ab_0 conda-forge; triton 2.0.0 pypi_0 pypi; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023.3 pypi_0 pypi; umap-learn 0.5.3 pypi_0 pypi; urllib3 1.26.13 pypi_0 pypi; uvicorn 0.23.1 pypi_0 pypi; wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge; websocket-client 1.6.1 pypi_0 pypi; websockets 11.0.3 pypi_0 pypi; wheel 0.41.0 pyhd8ed1ab_0 conda-forge; widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge; xarray 2023.7.0 pypi_0 pypi; xz 5.2.6 h166bdaf_0 conda-forge; yamlordereddictloader 0.4.0 pypi_0 pypi; yarl 1.9.2 pypi_0 pypi; zeromq 4.3.4 h9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:7884,learn,learn,7884,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,2,['learn'],['learn']
Usability,"| 0.1.2; jupyterlab-server | 2.8.2 | 2.8.2; kiwisolver | 1.3.1 | 1.3.1; legacy-api-wrap | 1.2 | 1.2; leidenalg | 0.8.8 | 0.8.8; llvmlite | 0.36.0 | 0.36.0; MarkupSafe | 2.0.1 | 2.0.1; matplotlib | 3.3.4 | 3.3.4; mistune | 0.8.4 | 0.8.4; **natsort | 7.1.1 | 8.0.0**; nbclassic | 0.2.6 | 0.2.6; nbclient | 0.5.3 | 0.5.3; nbconvert | 6.0.7 | 6.0.7; nbformat | 5.1.3 | 5.1.3; nest-asyncio | 1.5.1 | 1.5.1; networkx | 2.5.1 | 2.5.1; notebook | 6.4.3 | 6.4.3; numba | 0.53.1 | 0.53.1; numexpr | 2.7.3 | 2.7.3; numpy | 1.19.5 | 1.19.5; packaging | 21 | 21; pandas | 1.1.5 | 1.1.5; pandocfilters | 1.4.3 | 1.4.3; parso | 0.8.2 | 0.8.2; patsy | 0.5.2 | 0.5.2; pickleshare | 0.7.5 | 0.7.5; Pillow | 8.4.0 | 8.4.0; pip | 21.0.1 | 21.2.2; prometheus-client | 0.11.0 | 0.11.0; prompt-toolkit | 3.0.20 | 3.0.20; pycparser | 2.2 | 2.2; Pygments | 2.10.0 | 2.10.0; pynndescent | 0.5.5 | 0.5.5; pyOpenSSL | 20.0.1 | 21.0.0; **pyparsing | 2.4.7 | 3.0.4**; pyrsistent | 0.17.3 | 0.17.3; PySocks | 1.7.1 | 1.7.1; python-dateutil | 2.8.2 | 2.8.2; python-igraph | 0.9.8 | 0.9.8; pytz | 2021.3 | 2021.3; pywin32 | 228 | 228; pywinpty | 0.5.7 | 0.5.7; pyzmq | 22.2.1 | 22.2.1; requests | 2.26.0 | 2.26.0; scanpy | 1.7.2 | 1.7.2; scikit-learn | 0.24.2 | 0.24.2; scipy | 1.5.4 | 1.5.4; seaborn | 0.11.2 | 0.11.2; Send2Trash | 1.8.0 | 1.8.0; setuptools | 58.0.4 | 58.0.4; sinfo | 0.3.4 | 0.3.4; six | 1.16.0 | 1.16.0; sniffio | 1.2.0 | 1.2.0; statsmodels | 0.12.2 | 0.12.2; stdlib-list | 0.8.0 | 0.8.0; tables | 3.6.1 | 3.6.1; terminado | 0.9.4 | 0.9.4; testpath | 0.5.0 | 0.5.0; texttable | 1.6.4 | 1.6.4; threadpoolctl | 3.0.0 | 3.0.0; tornado | 6.1 | 6.1; tqdm | 4.62.3 | 4.62.3; traitlets | 4.3.3 | 4.3.3; typing-extensions | 3.10.0.2 | 3.10.0.2; **umap-learn | 0.5.1 | 0.5.2**; urllib3 | 1.26.7 | 1.26.7; wcwidth | 0.2.5 | 0.2.5; webencodings | 0.5.1 | 0.5.1; wheel | 0.37.0 | 0.37.0; win-inet-pton | 1.1.0 | 1.1.0; wincertstore | 0.2 | 0.2; xlrd | 1.2.0 | 1.2.0; zipp | 3.6.0 | 3.6.0. </body>. </html>. Thanks!; Best,; YJ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045:5507,learn,learn,5507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045,2,['learn'],['learn']
Usability,"| 0.3.4 | sinfo | 0.3.4; six | 1.16.0 | six | 1.16.0 | six | 1.16.0; sniffio | 1.2.0 | sniffio | 1.2.0 | sniffio | 1.2.0; sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0 | sortedcontainers | 2.4.0; statsmodels | 0.13.1 | statsmodels | 0.13.1 | statsmodels | 0.13.1; stdlib-list | 0.8.0 | stdlib-list | 0.8.0 | stdlib-list | 0.8.0; tables | 3.6.1 | tables | 3.6.1 | tables | 3.6.1; tblib | 1.7.0 | tblib | 1.7.0 | tblib | 1.7.0; terminado | 0.9.4 | terminado | 0.9.4 | terminado | 0.9.4; testpath | 0.5.0 | testpath | 0.5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta na",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:14595,learn,learn,14595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['learn'],['learn']
