quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"	--master-machine-type=n1-highmem-8 --worker-machine-type=n1-highmem-8 \; 	--num-workers=300	--num-secondary-workers=0 \; 	--worker-boot-disk-size=1000 \; 	--properties=dataproc:dataproc.logging.stackdriver.enable=true,dataproc:dataproc.monitoring.stackdriver.enable=true; ```; We are currently receiving a spark error when using this cluster for our larger dataset. ```; [Stage 10:=====> (69 + 656) / 729]; raise err; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 98, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:1463,failure,failure,1463,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['failure'],['failure']
Availability,"	G,*,AG,CG	596	PASS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7327,Error,ErrorHandling,7327,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Error'],['ErrorHandling']
Availability,	at is.hail.expr.ApplyMethod$$anonfun$toIR$12$$anonfun$apply$58.apply(AST.scala:844); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12$$anonfun$apply$58.apply(AST.scala:844); 	at scala.Option.map(Option.scala:146); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12.apply(AST.scala:844); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12.apply(AST.scala:842); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ApplyMethod.toIR(AST.scala:842); 	at is.hail.expr.StructConstructor$$anonfun$toIR$5.apply(AST.scala:410); 	at is.hail.expr.StructConstructor$$anonfun$toIR$5.apply(AST.scala:410); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.StructConstructor.toIR(AST.scala:410); 	at is.hail.table.Table.select(Table.scala:512); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-a870693; Error summary: ClassCastException: java.lang.Integer cannot be cast to java.lang.Double; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3436:6797,Error,Error,6797,https://hail.is,https://github.com/hail-is/hail/issues/3436,1,['Error'],['Error']
Availability,	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.Resiza,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:9191,Error,ErrorHandling,9191,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Error'],['ErrorHandling']
Availability,	at is.hail.rvd.RVD$class.take(RVD.scala:251); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:32); 	at is.hail.table.Table.take(Table.scala:637); 	at is.hail.table.Table.showString(Table.scala:673); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils.R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:8462,error,error,8462,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['error'],['error']
Availability,"	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.stats.HistogramCombiner.merge(HistogramCombiner.scala:42); 	at is.hail.annotations.aggregators.RegionValueHistogramAggregator.seqOp(RegionValueHistogramAggregator.scala:31); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:809); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:808); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:551); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:550); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:550); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:547); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:10371,Error,ErrorHandling,10371,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['Error'],['ErrorHandling']
Availability,	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12687,Error,Error,12687,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['Error']
Availability,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:118368,Error,Error,118368,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['Error'],['Error']
Availability,	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1011); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RowStore.scala:1071); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:42); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:27); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:8271,Error,ErrorHandling,8271,https://hail.is,https://github.com/hail-is/hail/issues/4096,1,['Error'],['ErrorHandling']
Availability,	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:7684,Error,ErrorHandling,7684,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Error'],['ErrorHandling']
Availability," 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 	at is.hail.utils.package$.using(package.scala:609); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.53-e245ebe1d9a6; Error summary: HailException: Index 5 is out of bounds for axis 0 with size 2; ```. Numpy:; ```sh; In [22]: hl.eval(ndarray_t[5,0:1000]) ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-22-0643c9661056> in <module>; ----> 1 hl.eval(ndarray_t[5,0:1000]). IndexError: index 5 is out of bounds for axis 0 with size 2; ```. The stack trace is uninformative, and the typical Hail user will not want to see it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226:4774,Error,Error,4774,https://hail.is,https://github.com/hail-is/hail/issues/9226,1,['Error'],['Error']
Availability," ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:35,400	job.py	schedule_job:473	error while scheduling job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:7355,ERROR,ERROR,7355,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,364	job.py	schedule_job:473	error while scheduling job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:11856,ERROR,ERROR,11856,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,390	job.py	schedule_job:473	error while scheduling job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:13861,ERROR,ERROR,13861,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,435	job.py	schedule_job:473	error while scheduling job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:15866,ERROR,ERROR,15866,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,447	job.py	schedule_job:473	error while scheduling job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:17872,ERROR,ERROR,17872,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:39,193	job.py	schedule_job:473	error while scheduling job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:22021,ERROR,ERROR,22021,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:39,204	job.py	schedule_job:473	error while scheduling job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:24026,ERROR,ERROR,24026,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," ### What went wrong (all error messages here, including the full java stack trace): HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ:Array[Float64],AS_MQRankSum:Array[Float64],AS_QD:Float64,AS_ReadPosRankSum:Array[Float64],AS_SOR:Array[Float64],BaseQRankSum:Float64,DP:Int32,DS:Boolean,ExcessHet:Float64,FS:Float64,HRun:Int32,HaplotypeScore:Float64,InbreedingCoeff:Float64,LikelihoodRankSum:Float64,MLEAC:Array[Int32],MLEAF:Array[Float64],MQ:Float64,MQ0:Int32,MQRankSum:Float64,OND:Float64,QD:Float64,RPA:Array[Int32],RU:String,ReadPosRankSum:Float64,ReverseComplementedAlleles:Boolean,SOR:Float64,STR:Boolean,SwappedAlleles:Boolean},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{AB:Float64,AD:Array[+Int32],DP:Int32,GQ:Int32,GT:Call,MQ0:Int32,PL:Array[+Int32],SB:Array[+Int32]}}; after: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4827:405,error,error,405,https://hail.is,https://github.com/hail-is/hail/issues/4827,1,['error'],['error']
Availability," ### What you did: ; Ran hail's maximal independent set method with the following code:. ```; related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); ```. where related pairs is structured as:. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'i': struct {; data_type: str, ; s: str; } ; 'j': struct {; data_type: str, ; s: str; } ; 'id1_rank': struct {; id: struct {; data_type: str, ; s: str; }, ; rank: int32; } ; 'id2_rank': struct {; id: struct {; data_type: str, ; s: str; }, ; rank: int32; } ; ----------------------------------------; Key: ['i', 'j']; ----------------------------------------; ```. and tie_breaker is :. ```; def tie_breaker(l, r):; return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError Traceback (most recent call last); <ipython-input-220-88e7ce1066ed> in <module>; 11 return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); 12 ; ---> 13 related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); 14 #return related_samples_to_drop_ranked.select(**related_samples_to_drop_ranked.node.id).key_by('data_type', 's'). <decorator-gen-1024> in maximal_independent_set(i, j, keep, tie_breaker). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4857:965,error,error,965,https://hail.is,https://github.com/hail-is/hail/issues/4857,1,['error'],['error']
Availability," ### What you did:. copied from gitter:. I'm using hail 0.1 on dataproc and trying to import a vcf from the local filesystem; run(""wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh{0}/clinvar.vcf.gz -O /tmp/clinvar.vcf.gz"".format(args.genome_version)); run(""ls -l /tmp/clinvar.vcf.gz""); vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); The output is; ls -l /tmp/clinvar.vcf.gz; -rw-r--r-- 1 root root 16218805 Jun 14 20:21 /tmp/clinvar.vcf.gz; Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:1264,failure,failure,1264,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['failure'],['failure']
Availability," && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:2047,avail,available,2047,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['avail'],['available']
Availability," &{map[""apiVersion"":""v1"" ""kind"":""Namespace"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods"" ""namespace"":""""]]}; from server for: ""deployment.yaml"": namespaces ""batch-pods"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get namespaces in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=serviceaccounts"", GroupVersionKind: ""/v1, Kind=ServiceAccount""; Name: ""batch-svc"", Namespace: ""batch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:1301,Error,Error,1301,https://hail.is,https://github.com/hail-is/hail/issues/4609,2,"['Error', 'error']","['Error', 'error']"
Availability," (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/dateutil/dateutil/blob/master/NEWS"">python-dateutil's changelog</a>.</em></p>; <blockquote>; <h1>Version 2.8.2 (2021-07-08)</h1>; <h2>Data updates</h2>; <ul>; <li>Updated tzdata version to 2021a. (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1128"">#1128</a>)</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Fixed a bug in the parser where non-<code>ValueError</code> exceptions would be raised; during exception handling; this would happen, for example, if an; <code>IllegalMonthError</code> was raised in <code>dateutil</code> code. Fixed by Mark Bailey.; (gh issue <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/981"">#981</a>, pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/987"">#987</a>).</li>; <li>Fixed the custom <code>repr</code> for <code>dateutil.parser.ParserError</code>, which was not; defined due to an indentation error. (gh issue <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/991"">#991</a>, gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/993"">#993</a>)</li>; <li>Fixed a bug that caused <code>b'</code> prefixes to appear in parse_isodate exception; messages. Reported and fixed by Paul Brown (<a href=""https://github.com/pawl""><code>@​pawl</code></a>) (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1122"">#1122</a>)</li>; <li>Make <code>isoparse</code> raise when trying to parse times with inconsistent use of; <code>:</code> separator. Reported and fixed by <a href=""https://github.com/mariocj89""><code>@​mariocj89</code></a> (gh pr <a href=""https://github-redirect.dependabot.com/dateutil/dateutil/issues/1125"">#1125</a>).</li>; <li>Fixed <code>tz.gettz()</code> not returning local time when passed an empty string.; Reported by <a href=""https://github.com/labrys""><code>@​labrys</code><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11518:5416,error,error,5416,https://hail.is,https://github.com/hail-is/hail/pull/11518,1,['error'],['error']
Availability," - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiOGQwNmE2Yi00MTg0LTRhMzAtOGMxYi0wYzNhZDVkZDk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14045:1288,avail,available,1288,https://hail.is,https://github.com/hail-is/hail/pull/14045,1,['avail'],['available']
Availability," --------------- end of error -------------------. The above is for running the GWAS tutorial. While running my own GWAS, I have encountered a similar error when running GWAS with the line :. gwas = hl.agg.linreg(hl.float(mt.phenos.height),; [1.0, mt.hapcounts0.x, mt.anc1dos.x, mt.anc2dos.x]). The error message is as follows:. -------------- start of error ------------------. ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:8712,ERROR,ERROR,8712,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['ERROR'],['ERROR']
Availability," / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; 24/01/17 20:59:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.2; SparkUI available at http://523bc6a27b69:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-bb535cd096c5; LOGGING: writing to /cromwell_root/hail-20240117-2059-0.2.127-bb535cd096c5.log; 2024-01-17 21:01:32.019 Hail: INFO: Found 34523 samples in fam file.; 2024-01-17 21:01:32.020 Hail: INFO: Found 18377527 variants in bim file.; 2024-01-17 21:02:45.920 Hail: INFO: Found 34523 samples in fam file.; 2024-01-17 21:02:45.920 Hail: INFO: Found 18377527 variants in bim file.; Traceback (most recent call last):; File ""<stdin>"", line 38, in <module>; File ""<decorator-gen-1366>"", line 2, in write; File ""/usr/local/lib/python3.10/dist-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/lib/python3.10/dist-packages/hail/matrixtable.py"", line 2807, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/usr/local/lib/python3.10/dist-packages/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:11300,avail,available,11300,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['avail'],['available']
Availability, /batch/990e17d5209d429196c84ce010acab9d; 2024-11-05 02:43:37.202 JVMEntryway: INFO: 3: /batch/990e17d5209d429196c84ce010acab9d/log; 2024-11-05 02:43:37.202 JVMEntryway: INFO: 4: gs://hail-query-daaf463550/jars/4c60fddb171a52c21f41a81995c53a28e375c26b.jar; 2024-11-05 02:43:37.202 JVMEntryway: INFO: 5: driver; 2024-11-05 02:43:37.202 JVMEntryway: INFO: 6: execute(...); 2024-11-05 02:43:37.202 JVMEntryway: INFO: 7: gs://cpg-bioheart-hail/batch-tmp/tmp/hail/sRjJqvkZ3l9nmKuUErfNZv/jHpWQ6lemx/in; 2024-11-05 02:43:37.202 JVMEntryway: INFO: 8: gs://cpg-bioheart-hail/batch-tmp/tmp/hail/sRjJqvkZ3l9nmKuUErfNZv/jHpWQ6lemx/out; 2024-11-05 02:43:37.202 JVMEntryway: INFO: Yielding control to the QoB Job.; 2024-11-05 02:43:37.206 ServiceBackendAPI$: INFO: BatchClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Th,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2160,ERROR,ERROR,2160,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['ERROR'],['ERROR']
Availability," /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:1868,recover,recover,1868,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['recover'],['recover']
Availability," /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 times, most recent failure: Lost task 15.3 in stage 16.0 (TID 178, ip-172-31-1-20.ec2.internal, executor 4): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:5371,error,error,5371,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['error'],['error']
Availability," 0.20.1.; notebook 6.5.6 has requirement pyzmq<25,>=17, but you have pyzmq 25.1.2.; aiohttp-devtools 1.1 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **444/1000** <br/> **Why?** Has a fix available, CVSS 4.6 | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:1619,avail,available,1619,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability," 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:1434,failure,failure,1434,https://hail.is,https://github.com/hail-is/hail/issues/3901,1,['failure'],['failure']
Availability," 19.15.1 requires aiohttp, which is not installed.; aiohttp-session 2.12.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5NGM3N2YwYy0xN2JkLTRkMzQtYmJhOS1iNzBiNmVhMDl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14039:1479,avail,available,1479,https://hail.is,https://github.com/hail-is/hail/pull/14039,1,['avail'],['available']
Availability," 656) / 729]; raise err; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 98, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:1857,failure,failure,1857,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['failure'],['failure']
Availability," 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:2777,Error,Error,2777,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['Error'],['Error']
Availability," : INFO: RegionPool: REPORT_THRESHOLD: 257.0K allocated (129.0K blocks / 128.0K chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.486 : INFO: RegionPool: REPORT_THRESHOLD: 577.0K allocated (193.0K blocks / 384.0K chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.486 GoogleStorageFS$: INFO: createNoCompression: gs://neale-bge/foo.ht/rows/parts/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79; 2023-09-22 19:11:12.625 GoogleStorageFS$: INFO: close: gs://neale-bge/foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index; 2023-09-22 19:11:12.656 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9221,ERROR,ERROR,9221,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['ERROR'],['ERROR']
Availability," </code></pre>; <p>The Apache Commons Codec package contains simple encoder and decoders for; various formats such as Base64 and Hexadecimal. In addition to these; widely used encoders and decoders, the codec package also maintains a; collection of phonetic encoding utilities.</p>; <p>Feature and fix release.</p>; <p>Changes in this version include:</p>; <p>New features:; o CODEC-290: Base16Codec and Base16Input/OutputStream. Thanks to Adam Retter.; o CODEC-291: Hex encode/decode with existing arrays. Thanks to Adam Retter.</p>; <p>Fixed Bugs:; o CODEC-264: MurmurHash3: Ensure hash128 maintains the sign extension bug.; Thanks to Andy Seaborne.</p>; <p>Changes:; o CODEC-280: Base32/Base64/BCodec: Added strict decoding property to control; handling of trailing bits. Default lenient mode discards them; without error. Strict mode raise an exception.; o CODEC-289: Base32/Base64 Input/OutputStream: Added strict decoding property; to control handling of trailing bits. Default lenient mode; discards them without error. Strict mode raise an exception.; o Update tests from JUnit 4.12 to 4.13. Thanks to Gary Gregory.; o Update actions/checkout from v1 to v2.3.2 <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/50"">#50</a>, <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/56"">#56</a>.; Thanks to Dependabot.; o Update actions/setup-java from v1.4.0 to v1.4.1 <a href=""https://github-redirect.dependabot.com/apache/commons-codec/issues/57"">#57</a>.; Thanks to Dependabot.</p>; <p>For complete information on Apache Commons Codec, including instructions on how; to submit bug reports, patches, or suggestions for improvement, see the; Apache Commons Codec website:</p>; <p><a href=""https://commons.apache.org/proper/commons-codec/"">https://commons.apache.org/proper/commons-codec/</a></p>; <p>Download page: <a href=""https://commons.apache.org/proper/commons-codec/download_codec.cgi"">https://commons.apache.org/proper/commons-codec/download",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12385:1385,error,error,1385,https://hail.is,https://github.com/hail-is/hail/pull/12385,1,['error'],['error']
Availability," <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1791"">jupyter/nbconvert#1791</a></li>; <li>Fix fonts overriden by user stylesheet by inheriting styles by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1793"">jupyter/nbconvert#1793</a></li>; <li>Fix lab template output alignment by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1795"">jupyter/nbconvert#1795</a></li>; <li>Add qtpdf and qtpng exporters by <a href=""https://github.com/davidbrochart""><code>@​davidbrochart</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1611"">jupyter/nbconvert#1611</a></li>; <li>Fix linters by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1825"">jupyter/nbconvert#1825</a></li>; <li>Remove downloaded CSS from repository by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1827"">jupyter/nbconvert#1827</a></li>; <li>escape_html: prevent escaping quotes on widgets JSON reprs (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1829"">#1829</a>) by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1830"">jupyter/nbconvert#1830</a></li>; <li>Remove tests from bdist by <a href=""https://github.com/TiagodePAlves""><code>@​TiagodePAlves</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1822"">jupyter/nbconvert#1822</a></li>; <li>Encode SVG image data as UTF-8 before calling lxml cleaner (fixes <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1836"">#1836</a>) by <a href=""https://github.com/emarsden""><code>@​e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12126:3073,down,downloaded,3073,https://hail.is,https://github.com/hail-is/hail/pull/12126,1,['down'],['downloaded']
Availability," <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.1/whatsnew/v1.4.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.0/whatsnew/v1.4.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install -c conda-forge pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.4.0. If all goes well, we'll release pandas 1.4.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/whatsnew/v1.4.0.html"">whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.4.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.4.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.3.5</h2>; <!-- raw HTM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11539:1312,avail,available,1312,https://hail.is,https://github.com/hail-is/hail/pull/11539,1,['avail'],['available']
Availability," <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14211:2415,Error,Error,2415,https://hail.is,https://github.com/hail-is/hail/pull/14211,1,['Error'],['Error']
Availability," <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `40.5.0 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:2485,Error,Error,2485,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['Error'],['Error']
Availability," <li>; <p>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a>; [radarhere]</p>; </li>; <li>; <p>Fixed saving TIFF multiframe images with LONG8 tag types <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7078"">#7078</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/6e28ed1f36d0eb74053af54e1eddc9c29db698cd""><code>6e28ed1</code></a> 10.0.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/c827f3b30f50bf04fd65daeeba6bbfd56fc7b50e""><code>c827f3b</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7246"">#7246</a> from radarhere/deallocate</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/39a3b1d83edcf826c3864e26bedff5b4e4dd331b""><code>39a3b1d</code></a> Fixed deallocating mask images</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/8c1dc819fd91471825da01976ac0e0bc8789590f""><code>8c1dc81</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/e37b25087d39bd54495380a9898c8c7a2a4698d1""><code>e37b250</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7244"">#7244</a> from radarhere/imagefont_max_string_length</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/d398fedb9d5af22316c715d2066176d15031d439""><code>d398fed</code></a> Added underscores for readability</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/1fe1bb49c452b0318cad12ea9d97c3bef188e9a7""><code>1fe1bb4</code></a> Added ImageFont.MAX_STRING_LENGTH</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/7c945f5131cf8596084b32af582f90a43b090540""><code>7c945f5</code></a> Merge pull request <a href=""https://redirect.gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:13027,mask,mask,13027,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['mask'],['mask']
Availability," <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <p>Fixed urllib3 exception leak, wrapping <code>urllib3.exceptions.InvalidHeader</code> with; <code>requests.exceptions.InvalidHeader</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5914"">#5914</a>)</p>; </li>; <li>; <p>Fixed bug where two Host headers were sent for chunked requests. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5391"">#5391</a>)</p>; </li>; <li>; <p>Fixed regression in Requests 2.26.0 where <code>Proxy-Authorization</code> was; incorrectly stripped from all requests sent with <code>Session.send</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed performance regression in 2.26.0 for hosts with a large number of; proxies available in the environment. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <code>requests.exceptions.JSONDecodeError</code> to unify JSON exceptions between; Python 2 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:3198,avail,available,3198,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['avail'],['available']
Availability," <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <p>Fixed urllib3 exception leak, wrapping <code>urllib3.exceptions.InvalidHeader</code> with; <code>requests.exceptions.InvalidHeader</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5914"">#5914</a>)</p>; </li>; <li>; <p>Fixed bug where two Host headers were sent for chunked requests. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5391"">#5391</a>)</p>; </li>; <li>; <p>Fixed regression in Requests 2.26.0 where <code>Proxy-Authorization</code> was; incorrectly stripped from all requests sent with <code>Session.send</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed performance regression in 2.26.0 for hosts with a large number of; proxies available in the environment. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5924"">#5924</a>)</p>; </li>; <li>; <p>Fixed idna exception leak, wrapping <code>UnicodeError</code> with; <code>requests.exceptions.InvalidURL</code> for URLs with a leading dot (.) in the; domain. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5414"">#5414</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/psf/requests/commit/31a89d9c8463c3394ca00f408f4b86d814421a09""><code>31a89d9</code></a> v2.27.1</li>; <li><a href=""https://github.com/psf/requests/commit/8fa9724398c4f44090997ff430a1dd3e935a9057""><code>8fa9724</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a> from nateprewitt/prox_auth_fix</li>; <li><a href=""https://github.com/psf/requests/commit/38f3f8ecb93cadfac03a6b7b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:6309,avail,available,6309,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['avail'],['available']
Availability," <li>; <p>increases the weight of user specifiable priorities.; The weights of following priority plugins are increased</p>; <ul>; <li><code>TaintTolerations</code> to 3 - as leveraging node tainting to group nodes in the cluster is becoming a widely-adopted practice</li>; <li><code>NodeAffinity</code> to 2</li>; <li><code>InterPodAffinity</code> to 2</li>; </ul>; </li>; <li>; <p>Won't have <code>HealthzBindAddress</code>, <code>MetricsBindAddress</code> fields (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104251"">kubernetes/kubernetes#104251</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</p>; </li>; </ul>; </li>; <li>Introduce v1beta2 for Priority and Fairness with no changes in API spec. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104399"">kubernetes/kubernetes#104399</a>, <a href=""https://github.com/tkashem""><code>@​tkashem</code></a>)</li>; <li>JSON log output is configurable and now supports writing info messages to stdout and error messages to stderr. Info messages can be buffered in memory. The default is to write both to stdout without buffering, as before. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104873"">kubernetes/kubernetes#104873</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>)</li>; <li>JobTrackingWithFinalizers graduates to beta. Feature is enabled by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105687"">kubernetes/kubernetes#105687</a>, <a href=""https://github.com/alculquicondor""><code>@​alculquicondor</code></a>)</li>; <li>Kube-apiserver: Fixes handling of CRD schemas containing literal null values in enums. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104969"">kubernetes/kubernetes#104969</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-apiserver: The <code>rbac.authorization.k8s.io/v1alp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:6967,error,error,6967,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['error'],['error']
Availability," <li><a href=""https://github.com/jupyter/jupyter_client/commit/51e071a973b232deaefd4cbf11f003f112c24f1a""><code>51e071a</code></a> Automated Changelog Entry for 7.3.4 on main</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/ca4cb2d6a4b95a6925de85a47b323d2235032c74""><code>ca4cb2d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/804"">#804</a> from blink1073/fix-docs-build</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/2c545599e1da419c096abffcd81f922fb709e239""><code>2c54559</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/803"">#803</a> from ccordoba12/fix-threaded-client</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/30ce7539778e2a25ff5e6eba4ccb6c08b8a0fe20""><code>30ce753</code></a> fix sphinx 5.0 support</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/a2e90574645052320de861bb84ba1752e25ef2dd""><code>a2e9057</code></a> ignore type error</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/3c6fc38e8dda754aba4a1217733eb1a0146b4c57""><code>3c6fc38</code></a> Run qtconsole test suite as a another downstream project</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/dcb45960b337fb089e04b0c3dde880e8f0f10ae5""><code>dcb4596</code></a> Revert changes related to _handle_recv in ThreadedZMQSocketChannel</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/01bfdd18c2eb8ea34cbb9915cb2bc7d9806f81a4""><code>01bfdd1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/799"">#799</a> from jupyter/pre-commit-ci-update-config</li>; <li>Additional commits viewable in <a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...v7.3.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:9009,error,error,9009,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['error'],['error']
Availability," = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.alleles[1] == variants.ss.nt2)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt1) & ; (flip_text(variants.alleles[1]) == variants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:2802,error,errors,2802,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['error'],['errors']
Availability, Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:244); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:77); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at __C1310collect_distributed_array_table_native_writer.apply_region1_87(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:6073,Error,ErrorHandling,6073,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Error'],['ErrorHandling']
Availability," Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/table.py:2112, in Table.persist(self, storage_level); 2076 @typecheck_method(storage_level=storage_level); 2077 def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; 2078 """"""Persist this table in memory or on disk.; 2079 ; 2080 Examples; (...); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self). File ~/projects/hail/hail/python/hail/backend/backend.py:208, in Backend.persist(self, dataset); 206 from hail.context import TemporaryFilename; 207 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 208 persisted = dataset.checkpoint(tempfile.__enter__()); 209 self._persisted_locations[persisted] = (tempfile, dataset); 210 return persisted. File <decorator-gen-1232>:2, in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). File ~/projects/hail/hail/python/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/table.py:1331, in Table.checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1328 hl.current_backend().validate_file(output); 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. File <decorator-gen-1234>:2, in write(self, output, overwrite, stage_locally, _codec_spec). File ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13788:2566,checkpoint,checkpoint,2566,https://hail.is,https://github.com/hail-is/hail/issues/13788,1,['checkpoint'],['checkpoint']
Availability," Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgrad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14236:1246,avail,available,1246,https://hail.is,https://github.com/hail-is/hail/pull/14236,1,['avail'],['available']
Availability," Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **444/1000** <br/> **Why?** Has a fix available, CVSS 4.6 | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **429/1000** <br/> **Why?** Has a fix available, CVSS 4.3 | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **501/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.3 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""mediu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:3151,Error,Error,3151,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['Error'],['Error']
Availability," Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3YjUwZjkzNy0zZjY4LTRkZjItYjliMC0zZjRiYzUyNmIwNWIiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14244:1833,avail,available,1833,https://hail.is,https://github.com/hail-is/hail/pull/14244,1,['avail'],['available']
Availability," Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215:1670,error,errors,1670,https://hail.is,https://github.com/hail-is/hail/pull/5215,1,['error'],['errors']
Availability," File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:2966,error,error,2966,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['error'],['error']
Availability," File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 57, in wrapped; return await fun(request, userdata, *args, **kwargs);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8307:1865,error,errorclass,1865,https://hail.is,https://github.com/hail-is/hail/pull/8307,1,['error'],['errorclass']
Availability," INFO: linear_regression_rows: running on 250 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:44859); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-40-d6d936b012db>"", line 3, in <module>; covariates=[1.0]); File ""<decorator-gen-1697>"", line 2, in linear_regression_rows; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/methods/statgen.py"", line 370, in linear_regression_rows; return ht_result.persist(); File ""<decorator-gen-1111>"", line 2, in persist; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:1883,error,error,1883,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['error'],['error']
Availability," In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:1087,error,error,1087,https://hail.is,https://github.com/hail-is/hail/pull/9120,1,['error'],['error']
Availability," Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-WHEEL-3180413](https://snyk.io/vuln/SNYK-PYTHON-WHEEL-3180413) | `wheel:` <br> `0.30.0 -> 0.38.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2NTQzMzZlYi02MmRmLTQ0ODAtOTFkOS0xZDg4N2FmNmQwMTUiLCJl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:10210,avail,available,10210,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability," Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-WHEEL-3180413](https://snyk.io/vuln/SNYK-PYTHON-WHEEL-3180413) | `wheel:` <br> `0.30.0 -> 0.38.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3YmFjNzAzOC00ZmQzLTQ3YmItOGUwMy0yNjRmYTUxNDRlNGQiLCJl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:9194,avail,available,9194,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['avail'],['available']
Availability," TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; From is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$20(StorageImpl.java:610); 	at com.google.api.gax.retrying.DirectRetryingExecuto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:4033,down,download,4033,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['down'],['download']
Availability," WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 2022-10-06 15:56:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:208); at is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:135); at is.hail.expr.ir.GenericTableValue.getLTVCoercer(GenericTableValue.scala:137); at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1798); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:717); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:697); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:903); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:467); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:472); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:73); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); at is.hail.expr.ir.lowering.LowerToDistributedArrayPas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:1968,Error,Error,1968,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['Error'],['Error']
Availability," [NaN, Infinity, -Infinity]}; line: 1, column: 17]; 	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581); 	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533); 	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1602); 	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:689); 	at org.json4s.jackson.JValueDeserializer.deserialize(JValueDeserializer.scala:26); 	at org.json4s.jackson.JValueDeserializer.deserialize(JValueDeserializer.scala:42); 	at org.json4s.jackson.JValueDeserializer.deserialize(JValueDeserializer.scala:35); 	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3736); 	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2726); 	at org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:20); 	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:50); 	at is.hail.table.Table.annotateGlobalJSON(Table.scala:476); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2d80e80610df; Error summary: JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3785:7147,Error,Error,7147,https://hail.is,https://github.com/hail-is/hail/issues/3785,1,['Error'],['Error']
Availability," ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:18023,error,error,18023,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['error'],['error']
Availability," all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annotations will be updated.</p>; <p>All new gRPC related APIs are annotated with <code>@BetaApi</code> to denote they are in preview and the possibility of breaking change is present. At this time, opting to use any of the gRPC transport mode means you are okay with the possibility of a breaking change happening. When the APIs are out of preview, we will remove the <code>@BetaApi</code> annotation to signal they are now considered stable and will not break outside a major version.</p>; <p><strong><em>NOTICE</em></strong>: Using the gRPC transport is exclusive. Any operations which have not yet been implemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview su",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:3255,error,error,3255,https://hail.is,https://github.com/hail-is/hail/pull/12456,4,['error'],['error']
Availability," an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: ‘class simdpp::arch_avx2::uint64<2>’ declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:62:35: required from ‘simdpp::arch_avx2::int16<8>& simdpp::arch_avx2::int16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:45:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int16<8>’ with ‘private’ member ‘simdpp::arch_avx2::int16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:15592,Mask,MaskCastOverride,15592,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2758,down,download-task,2758,https://hail.is,https://github.com/hail-is/hail/pull/12332,2,"['Mainten', 'down']","['Maintenance', 'download-task']"
Availability," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:15607,reliab,reliable,15607,https://hail.is,https://github.com/hail-is/hail/pull/5162,2,"['avail', 'reliab']","['available', 'reliable']"
Availability," args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/table.py in aggregate(self, expr, _localize); 1138 ; 1139 if _localize:; -> 1140 return Env.backend().execute(agg_ir); 1141 else:; 1142 return construct_expr(agg_ir, expr.dtype). /home/hail/hail.zip/hail/backend/backend.py in execute(self, ir); 91 return ir.typ._from_json(; 92 Env.hail().backend.spark.SparkBackend.executeJSON(; ---> 93 self._to_java_ir(ir))); 94 ; 95 def value_type(self, ir):. /usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 226 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 227 'Hail version: %s\n'; --> 228 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:1755,Error,Error,1755,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['Error'],['Error']
Availability," array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: ‘class simdpp::arch_avx2::uint64<4>’ declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:58:37: required from ‘simdpp::arch_avx2::float64<2>& simdpp::arch_avx2::float64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle2x2.h:342:31: required from ‘simdpp::arch_avx2::uint64<2> simdpp::arch_avx2::detail::insn::i_shuffle2x2(const simdpp::arch_avx2::uint64<2>&, const simdpp::arch_avx2::uint64<2>&) [with unsigned int s0 = 1; unsigned int s1 = 2]’; libsimdpp-2.0-rc2/simdpp/core/shuffle1.h:58:47: required from ‘typename simdpp::arch_avx2::detail::get_expr2_nomask<V1, V2>::empty simdpp::arch_avx2::shuffle1(const simdpp::arch_avx2::any_vec64<N, V1>&, const simdpp::arc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:144657,Mask,MaskCastOverride,144657,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:114:36: required from ‘simdpp::arch_avx2::uint32<8>& simdpp::arch_avx2::uint32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:136:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:37556,Mask,MaskCastOverride,37556,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: ‘class simdpp::arch_avx2::uint64<2>’ declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:114:36: required from ‘simdpp::arch_avx2::uint64<4>& simdpp::arch_avx2::uint64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:471:8: required from ‘void simdpp::arch_avx2::detail::insn::v_mem_unpack4_impl16_128(T&, T&, T&, T&) [with T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/mem_unpack.h:585:29: required from ‘void simdpp::arch_avx2::detail::insn::mem_unpack4(simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&, simdpp::arch_avx2::uint16<N>&) [with unsigned int N =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:142142,Mask,MaskCastOverride,142142,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:115:37: required from ‘simdpp::arch_avx2::uint16<16>& simdpp::arch_avx2::uint16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::uint16<16> >, simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::uint32<8> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/unzip_lo.h:107:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<16>::d_’ from an array of ‘const class simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:33745,Mask,MaskCastOverride,33745,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:115:37: required from ‘simdpp::arch_avx2::uint16<16>& simdpp::arch_avx2::uint16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint64<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:488:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:54135,Mask,MaskCastOverride,54135,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," array of ‘const class simdpp::arch_avx2::uint64<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:37,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32.h:86:7: note: ‘class simdpp::arch_avx2::uint32<16>’ declared here; class uint32<N, void> : public any_int32<N, uint32<N,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4>]’; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:58:37: required from ‘simdpp::arch_avx2::float64<2>& simdpp::arch_avx2::float64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:590:8: required from ‘void simdpp::arch_avx2::detail::insn::v_sse_transpose32x4(V&, V&, V&, V&) [with V = simdpp::arch_avx2::float32<4>; D = simdpp::arch_avx2::float64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:549:63: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float64<",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:100020,Mask,MaskCastOverride,100020,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," array of ‘const class simdpp::arch_avx2::uint8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:33:7: note: ‘class simdpp::arch_avx2::int16<16>’ declared here; class int16<16, void> : public any_int16<16, int16<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from ‘simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int16<16>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:91:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<32>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<32>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:20972,Mask,MaskCastOverride,20972,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," at org.apache.spark.rdd.RDD.treeReduce(RDD.scala:1037); at is.hail.methods.SampleQC$.results(SampleQC.scala:206); at is.hail.methods.SampleQC$.apply(SampleQC.scala:221); at is.hail.methods.SampleQC.apply(SampleQC.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:7447,Error,ErrorHandling,7447,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['Error'],['ErrorHandling']
Availability," audit log versions, deprecated since 1.13, have been removed. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108092"">kubernetes/kubernetes#108092</a>, <a href=""https://github.com/carlory""><code>@​carlory</code></a>)</li>; <li>Kube-apiserver: the <code>metadata.selfLink</code> field can no longer be populated by kube-apiserver; it was deprecated in 1.16 and has not been populated by default since 1.20+. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107527"">kubernetes/kubernetes#107527</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>)</li>; <li>Kubelet external Credential Provider feature is moved to Beta. Credential Provider Plugin and Credential Provider Config API's updated from v1alpha1 to v1beta1 with no API changes. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108847"">kubernetes/kubernetes#108847</a>, <a href=""https://github.com/adisky""><code>@​adisky</code></a>)</li>; <li>Make STS available replicas optional again. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109241"">kubernetes/kubernetes#109241</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>MaxUnavailable for StatefulSets, allows faster RollingUpdate by taking down more than 1 pod at a time. The number of pods you want to take down during a RollingUpdate is configurable using maxUnavailable parameter. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/82162"">kubernetes/kubernetes#82162</a>, <a href=""https://github.com/krmayankk""><code>@​krmayankk</code></a>)</li>; <li>Non-graceful node shutdown handling is enabled for stateful workload failovers (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108486"">kubernetes/kubernetes#108486</a>, <a href=""https://github.com/sonasingh46""><code>@​sonasingh46</code></a>)</li>; <li>Omit enum declarations from the static opena",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:9479,avail,available,9479,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['avail'],['available']
Availability," available. <code>+k8s:conversion-gen</code> tags can be used with the <code>k8s.io/code-generator</code> component to generate conversions. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/90018"">kubernetes/kubernetes#90018</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG API Machinery, Apps and Testing]</li>; <li>Kube-proxy: add <code>--bind-address-hard-fail</code> flag to treat failure to bind to a port as fatal (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89350"">kubernetes/kubernetes#89350</a>, <a href=""https://github.com/SataQiu""><code>@​SataQiu</code></a>) [SIG Cluster Lifecycle and Network]</li>; <li>Kubebuilder validation tags are set on metav1.Condition for CRD generation (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92660"">kubernetes/kubernetes#92660</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG API Machinery]</li>; <li>Kubelet's --runonce option is now also available in Kubelet's config file as <code>runOnce</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89128"">kubernetes/kubernetes#89128</a>, <a href=""https://github.com/vincent178""><code>@​vincent178</code></a>) [SIG Node]</li>; <li>Kubelet: add '--logging-format' flag to support structured logging (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91532"">kubernetes/kubernetes#91532</a>, <a href=""https://github.com/afrouzMashaykhi""><code>@​afrouzMashaykhi</code></a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Node]</li>; <li>Kubernetes is now built with golang 1.15.0-rc.1.; <ul>; <li>The deprecated, legacy behavior of treating the CommonName field on X.509 serving certificates as a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:9820,avail,available,9820,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['avail'],['available']
Availability," basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/80f04c6a46fe7df053ac55bcfc6f90ff74c4b873""><code>80f04c6</code></a> Bump up version number to 5.1.3</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will reb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:4640,down,download-task,4640,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability," call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-3cf3108; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3173:2578,Error,Error,2578,https://hail.is,https://github.com/hail-is/hail/issues/3173,1,['Error'],['Error']
Availability," come with 2 CPU and 7.5 GB each, but not all of that is allocatable to our pods. In reality, somewhere between 5.7-5.9GB ([GCP Docs](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture) say 5.7, GKE console says 5.9) of memory and 1.9CPU are available for us to use. Some of our Big services request 1 CPU and 3.75GB, but then we can never fit two big pods on one node. This is an attempt to standardize our requests so their easier to reason about while hopefully getting better packing. . | resource | Big | Medium | Small |; | --- | --- | --- | --- |; | CPU | 600m | 100m | 20m |; | Memory | 2G | 200M | 20M |. The intentions here are:; - always be able to comfortably get 2 Big pods on a node; - medium pods shouldn't have to force new nodes to spin up just because there's a Big pod there already; - small pods take up minimal resources; - there's ample room for small pods (which are mostly on HPA) to scale up considering most nodes shouldn't be at their medium pod capacity. The ratios don't match exactly, because I didn't want to assign CPU lower than 20m to prevent HPA thrashing we saw with auth and see a bit now with router. Setting it to 20m should hopefully convince k8s that idle small apps don't need to be scaled up under normal fluctuation. ## Big; - query; - batch-driver; - shuffler; - memory. ## Medium; - grafana; - ukbb-browser; - ukbb-static; - blog; - ci; - internal-gateway. ## Small; - amundsen; - router; - gateway; - site; - batch; - address; - atgu; - router-resolver; - ci/test statefulset & deployment; - auth-driver; - echo; - benchmark; - image-fetcher. ### Fun surprises I found along the way; - CI test statefulsets and deployment are getting .5GB and .5 CPU each; - We run a lot of image fetchers because a daemon set gets added per PR namespace. EDIT: It seems that discrepancy between GCP docs and GKE console is the console counts kube-system pods in ""Allocatable Memory"", and it really does take 2GB to run the Kubernetes Engine 🙃",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10117:1594,echo,echo,1594,https://hail.is,https://github.com/hail-is/hail/pull/10117,1,['echo'],['echo']
Availability," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:7859,down,downsampled,7859,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['down'],['downsampled']
Availability," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10919:1141,toler,toleration,1141,https://hail.is,https://github.com/hail-is/hail/pull/10919,4,"['redundant', 'toler']","['redundant', 'toleration', 'tolerations']"
Availability," deep record data; Python: AVRO-2914 Drop Python 2 support; Python: AVRO-3004 Drop Python 3.5 support; Ruby: AVRO-3108 Drop Ruby 2.5 support</p>; <p>For the first time, the 1.11.0 release includes experimental support for; Rust. Work is continuing on this donated SDK, but we have not versioned and; published official artifacts for this release.</p>; <p>Python: The avro package fully supports Python 3. We will no longer publish a; separate avro-python3 package</p>; <p>And of course upgraded dependencies to latest versions, CVE fixes and more:; <a href=""https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0"">https://issues.apache.org/jira/issues/?jql=project%3DAVRO%20AND%20fixVersion%3D1.11.0</a></p>; <p>The link to all fixed JIRA issues and a brief summary can be found at:; <a href=""https://github.com/apache/avro/releases/tag/release-1.11.0"">https://github.com/apache/avro/releases/tag/release-1.11.0</a></p>; <p>In addition, language-specific release artifacts are available:</p>; <ul>; <li>C#: <a href=""https://www.nuget.org/packages/Apache.Avro/1.11.0"">https://www.nuget.org/packages/Apache.Avro/1.11.0</a></li>; <li>Java: from Maven Central,</li>; <li>Javascript: <a href=""https://www.npmjs.com/package/avro-js/v/1.11.0"">https://www.npmjs.com/package/avro-js/v/1.11.0</a></li>; <li>Perl: <a href=""https://metacpan.org/release/Avro"">https://metacpan.org/release/Avro</a></li>; <li>Python 3: <a href=""https://pypi.org/project/avro/1.11.0"">https://pypi.org/project/avro/1.11.0</a></li>; <li>Ruby: <a href=""https://rubygems.org/gems/avro/versions/1.11.0"">https://rubygems.org/gems/avro/versions/1.11.0</a></li>; </ul>; <p>Thanks to everyone for contributing!</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/apache/avro/compare/release-1.10.0...release-1.11.0"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11475:2076,avail,available,2076,https://hail.is,https://github.com/hail-is/hail/pull/11475,1,['avail'],['available']
Availability," deployment query; ```; - When the new pod is created (seen with `kubectl --namespace $NAMESPACE get pod`), issue the second request to the wait method.; - If all goes well, you should have:; - termination logs like those below,; - the first request successfully fulfilled with the response of env value being None (filled by the first pod); - The second request successfully filled, but has the value of the environment value, the one you set in the deploy.yaml (it got scheduled to the new node). Termination logs:. ```; {""severity"": ""INFO"", ""levelname"": ""INFO"", ""asctime"": ""2021-02-24 23:22:40,472"", ""filename"": ""query.py"", ""funcNameAndLine"": ""on_shutdown:253"", ""message"": ""On shutdown request received, with 2 tasks left"", ""hail_log"": 1}; ++ term; ++ kill -TERM 7; + true; + '[' no == yes ']'; + trap - SIGTERM SIGINT; + wait 7; {""severity"": ""INFO"", ""levelname"": ""INFO"", ""asctime"": ""2021-02-24 23:23:26,004"", ""filename"": ""hail_logging.py"", ""funcNameAndLine"": ""log:40"", ""message"": ""https GET /michaelfranklin/query/api/v1alpha/wait done in 50.029999999998836s: 200"", ""remote_address"": ""10.28.127.3"", ""request_start_time"": ""[24/Feb/2021:23:22:35 +0000]"", ""request_duration"": 50.029999999998836, ""response_status"": 200, ""x_real_ip"": ""124.170.20.28"", ""hail_log"": 1}; {""severity"": ""INFO"", ""levelname"": ""INFO"", ""asctime"": ""2021-02-24 23:23:26,005"", ""filename"": ""query.py"", ""funcNameAndLine"": ""on_shutdown:255"", ""message"": ""Tasks have all completed."", ""hail_log"": 1}; ```. Test duration endpoint; ```python; @routes.get('/api/v1alpha/wait'); async def wait_seconds(request):; """"""; Wait query.duration seconds before returning the request.; """"""; duration = request.query.get('duration'); try:; duration = int(duration); except Exception as e:; return web.json_response({; 'error': f'Invalid parameter duration ""{duration}"": {e}',; }, status=422). await asyncio.sleep(int(duration)); e = os.getenv(""TEST_VALUE"", ""None""); return web.json_response({""d"": f""You waited '{duration}' seconds!!"", ""env"": e}); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106:2785,error,error,2785,https://hail.is,https://github.com/hail-is/hail/pull/10106,1,['error'],['error']
Availability," devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:1260,failure,failure,1260,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['failure'],['failure']
Availability," discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-e60bdb1a125a. ### What you did:; ```; def get_counts_agg_expr2(mt: hl.MatrixTable):; return (hl.case(missing_false=True); #0x; .when(hl.is_missing(mt.gt1) & ~mt.missing1,; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[1,0,0,0,0,0,0,0,0]); .when(mt.gt2.is_het(), [0,1,0,0,0,0,0,0,0]); .when(mt.gt2.is_hom_var(), [0,0,1,0,0,0,0,0,0]); .default([0,0,0,0,0,0,0,0,0])); #1x; .when(mt.gt1.is_het(),; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[0,0,0,1,0,0,0,0,0]); .when(mt.gt2.is_het(), [0,0,0,0,1,0,0,0,0]); .when(mt.gt2.is_hom_var(), [0,0,0,0,0,1,0,0,0]); .default([0,0,0,0,0,0,0,0,0])); #2x; .when(mt.gt1.is_hom_var(),; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[0,0,0,0,0,0,1,0,0]); .when(mt.gt2.is_het(), [0,0,0,0,0,0,0,1,0]); .when(mt.gt2.is_hom_var(), [0,0,0,0,0,0,0,0,1]); .default([0,0,0,0,0,0,0,0,0])); .default([0,0,0,0,0,0,0,0,0])); mt = mt.annotate_rows(gt_counts=hl.agg.array_sum(get_counts_agg_expr2(mt))); ```; ### What went wrong (all error messages here, including the full java stack trace):; `gt_counts` is `[]` everywhere:; ```; mt.show(); +---------------+------------+---------------+------------+--------------+; | locus2 | alleles2 | locus1 | alleles1 | gt_counts |; +---------------+------------+---------------+------------+--------------+; | locus<GRCh37> | array<str> | locus<GRCh37> | array<str> | array<int64> |; +---------------+------------+---------------+------------+--------------+; | 1:69173 | [""A"",""T""] | 1:69166 | [""G"",""T""] | [] |; | 1:69946 | [""G"",""A""] | 1:69359 | [""G"",""A""] | [] |; | 1:69947 | [""A"",""G""] | 1:69359 | [""G"",""A""] | [] |; | 1:69735 | [""A"",""G""] | 1:69438 | [""T"",""C""] | [] |; | 1:69496 | [""G"",""A""] | 1:69453 | [""G"",""A""] | [] |; +---------------+------------+---------------+------------+--------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4757:1291,error,error,1291,https://hail.is,https://github.com/hail-is/hail/issues/4757,1,['error'],['error']
Availability," documentation</li>; <li>Add integration tests for Gradle 8.0.1</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4c983ed5cd229fa64912294737c858c2ba8486d6""><code>4c983ed</code></a> Bump up version number to 5.4.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/cc20442ab67bf37687c08e67af7e7de3a21c8fbe""><code>cc20442</code></a> Add integration tests for Gradle 8.0.2</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/472920e572e4cf45d321868874ced50ad8d1e2d5""><code>472920e</code></a> Add possibility to set request method and body</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/82e70cae2a8d48b4f5165a9b543d4e65bb793d88""><code>82e70ca</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86a15f1c16eb729dc71b6caf30237d07b8e0bb01""><code>86a15f1</code></a> Fix compiler warnings and deprecations</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86363072c8239330b28976109a622bdd073507b6""><code>8636307</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/gradle-download-task/issues/295"">#295</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/c1e212c0fb41b3ea9185a9ea463fb1ea7142f748""><code>c1e212c</code></a> Add integration tests for Gradle 8.0 and 8.0.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:1446,down,download-task,1446,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download-task']
Availability," execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:2886,Error,ErrorHandling,2886,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['ErrorHandling']
Availability," extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:2919,Mainten,Maintenance,2919,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['Mainten'],['Maintenance']
Availability," extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2200,Mainten,Maintenance,2200,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['Mainten'],['Maintenance']
Availability," figure out what the tests are doing. We pay about 2 USD per PR test, which is not very high but still higher than I would like. I investigated why and looked at eight recent PR tests (all after Daniel's QoB test reduction):. 1. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1695065243258&to=1695070128973&var-namespace=pr-13643-default-w484n4ke6oeg&orgId=1; 2. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1694628837271&to=1694632585473&var-namespace=pr-12468-default-y8okmle5k65x&orgId=1; 3. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1695078157872&to=1695080578446&var-namespace=pr-13644-default-phtb7scq3qln&orgId=1; 4. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1694729270680&to=1694730449193&var-namespace=pr-13376-default-biulo4i0wohp&orgId=1; 5. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1694628896138&to=1694632029521&var-namespace=pr-12468-default-y8okmle5k65x&orgId=1; 6. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1695077856969&to=1695080563275&var-namespace=pr-13644-default-phtb7scq3qln&orgId=1; 7. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1694625081745&to=1694626754800&var-namespace=pr-13430-default-hf2v0q29kgqy&orgId=1; 8. https://grafana.hail.is/d/8Kldmmynk/job-analytics?from=1694729252321&to=1694730683820&var-namespace=pr-13376-default-biulo4i0wohp&orgId=1. In every case, we spin up 32 cores of highcpu VMs but, apparently, never use them. They are live for 20-40 minutes depending on the tests. For the non-preemptible VM, thats about 0.40 USD for 30 minutes. We use 16 highmem cores once for about two minutes but we otherwise let them run idle the whole time. This PR accepts that we will wait 2-3min for a highmem to start when we need it. In exchange, we save about a dollar per PR (50%). I am also investigating why we seem to keep 80 cores alive for about 10 minutes despite being unused. My best guess is fragmentation, probably not much to do about that. cc: @daniel-goldstein, @jigold.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13667:1972,alive,alive,1972,https://hail.is,https://github.com/hail-is/hail/pull/13667,1,['alive'],['alive']
Availability," fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; jupyter 1.0.0 requires qtconsole, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **461/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.5 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3MThjYjgyZC1jNGU3LTRlNWEtODgzZi02NjQ0NjlmYzA4MGEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjcxOG",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14070:1278,Error,Error,1278,https://hail.is,https://github.com/hail-is/hail/pull/14070,1,['Error'],['Error']
Availability," from an array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:32:7: note: ‘class simdpp::arch_avx2::int8<32>’ declared here; class int8<32, void> : public any_int8<32, int8<32,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::int64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::int64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::int64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:62:35: required from ‘simdpp::arch_avx2::int32<4>& simdpp::arch_avx2::int32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int64<2>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:260:13: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ with ‘private’ member ‘simdpp::arch_avx2::int32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::int64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:67105,Mask,MaskCastOverride,67105,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint32<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:37,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32.h:31:7: note: ‘class simdpp::arch_avx2::int32<16>’ declared here; class int32<N, void> : public any_int32<N, int32<N,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:55:35: required from ‘simdpp::arch_avx2::int32<8>& simdpp::arch_avx2::int32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:48:73: required from ‘simdpp::arch_avx2::int32<8>::int32(const simdpp::arch_avx2::uint32<8, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:73:69: required from ‘simdpp::arch_avx2::int32<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int32<N, E>&, const simdpp::arch_avx2::int32<N, E2>&) [with unsigned int N = 4; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int32.h:188:26: required from here; libsimdpp-2.0-rc2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:130737,Mask,MaskCastOverride,130737,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:33:7: note: ‘class simdpp::arch_avx2::int32<4>’ declared here; class int32<4, void> : public any_int32<4, int32<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from ‘simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/detail/extract128.h:40:82: required from ‘simdpp::arch_avx2::int32x4 simdpp::arch_avx2::detail::extract128(const int32x8&) [with unsigned int s = 0; simdpp::arch_avx2::int32x4 = simdpp::arch_avx2::int32<4>; simdpp::arch_avx2::int32x8 = simdpp::arch_avx2::int32<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_add.h:356:49: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<32>’ ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:120259,Mask,MaskCastOverride,120259,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:32:7: note: ‘class simdpp::arch_avx2::int32<8>’ declared here; class int32<8, void> : public any_int32<8, int32<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:114:36: required from ‘simdpp::arch_avx2::uint64<4>& simdpp::arch_avx2::uint64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:107:73: required from ‘simdpp::arch_avx2::uint64<4>::uint64(const simdpp::arch_avx2::int64<4, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:79:49: required from ‘simdpp::arch_avx2::int64<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int64<N, E>&, const simdpp::arch_avx2::int64<N, E2>&) [with unsigned int N = 4; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int64.h:70:26: required from here; libsimdpp-2.0-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:132936,Mask,MaskCastOverride,132936,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:33:7: note: ‘class simdpp::arch_avx2::int32<4>’ declared here; class int32<4, void> : public any_int32<4, int32<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:129:36: required from ‘simdpp::arch_avx2::uint64<2>& simdpp::arch_avx2::uint64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int32<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:272:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<2>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::int32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:74206,Mask,MaskCastOverride,74206,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:33:7: note: ‘class simdpp::arch_avx2::int64<2>’ declared here; class int64<2, void> : public any_int64<2, int64<2,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<4>; T = simdpp::arch_avx2::uint64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int64<4>; T = simdpp::arch_avx2::uint64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int64<4>; T = simdpp::arch_avx2::uint64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:55:35: required from ‘simdpp::arch_avx2::int64<4>& simdpp::arch_avx2::int64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:48:73: required from ‘simdpp::arch_avx2::int64<4>::int64(const simdpp::arch_avx2::uint64<4, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:307:25: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int64<4>’ with ‘private’ member ‘simdpp::arch_avx2::int64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:110039,Mask,MaskCastOverride,110039,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:32:7: note: ‘class simdpp::arch_avx2::int32<8>’ declared here; class int32<8, void> : public any_int32<8, int32<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int8<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:133:36: required from ‘simdpp::arch_avx2::uint8<16>& simdpp::arch_avx2::uint8<16>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int8<16, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg_trunc.h:64:25: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::int8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:83325,Mask,MaskCastOverride,83325,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:32:7: note: ‘class simdpp::arch_avx2::int64<4>’ declared here; class int64<4, void> : public any_int64<4, int64<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int8<32>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:114:36: required from ‘simdpp::arch_avx2::uint8<32>& simdpp::arch_avx2::uint8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int8<32>]’; libsimdpp-2.0-rc2/simdpp/core/bit_xor.h:38:8: required from ‘typename simdpp::arch_avx2::detail::get_expr2<V1, V2>::empty simdpp::arch_avx2::bit_xor(const simdpp::arch_avx2::any_vec<N, V>&, const simdpp::arch_avx2::any_vec<N, V2>&) [with unsigned int N = 32; V1 = simdpp::arch_avx2::int8<32>; V2 = simdpp::arch_avx2::uint8<32>; typename simdpp::arch_avx2::detail::get_expr2<V1, V2>::empty = simdpp::arch_avx2::uint8<32, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg.h:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:111987,Mask,MaskCastOverride,111987,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:33:7: note: ‘class simdpp::arch_avx2::int8<16>’ declared here; class int8<16, void> : public any_int8<16, int8<16,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int8<32>; T = simdpp::arch_avx2::uint8<32>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int8<32>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int8<32>; T = simdpp::arch_avx2::uint8<32>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:55:35: required from ‘simdpp::arch_avx2::int8<32>& simdpp::arch_avx2::int8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_zbytes16.h:87:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int8<32>’ with ‘private’ member ‘simdpp::arch_avx2::int8<32>::d_’ from an array of ‘const class simdpp::arch_avx2::uint8<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:90499,Mask,MaskCastOverride,90499,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:33:7: note: ‘class simdpp::arch_avx2::int16<8>’ declared here; class int16<8, void> : public any_int16<8, int16<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from ‘simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:51:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::int16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:17389,Mask,MaskCastOverride,17389,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," from an array of ‘const class simdpp::arch_avx2::uint8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:33:7: note: ‘class simdpp::arch_avx2::int32<4>’ declared here; class int32<4, void> : public any_int32<4, int32<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from ‘simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/detail/extract128.h:42:82: required from ‘simdpp::arch_avx2::int64x2 simdpp::arch_avx2::detail::extract128(const int64x4&) [with unsigned int s = 0; simdpp::arch_avx2::int64x2 = simdpp::arch_avx2::int64<2>; simdpp::arch_avx2::int64x4 = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_reduce_max.h:478:40: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<32>’ ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:124373,Mask,MaskCastOverride,124373,https://hail.is,https://github.com/hail-is/hail/issues/3955,2,['Mask'],['MaskCastOverride']
Availability," has requirement docutils<0.20,>=0.14, but you have docutils 0.20.1.; sphinx-rtd-theme 1.3.0 has requirement docutils<0.19, but you have docutils 0.20.1. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; !",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:1462,avail,available,1462,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['avail'],['available']
Availability," href=""https://github.com/pallets/jinja/releases"">jinja2's releases</a>.</em></p>; <blockquote>; <h2>3.1.3</h2>; <p>This is a fix release for the 3.1.x feature branch.</p>; <ul>; <li>Fix for <a href=""https://github.com/pallets/jinja/security/advisories/GHSA-h5c8-rqwp-cp95"">GHSA-h5c8-rqwp-cp95</a>. You are affected if you are using <code>xmlattr</code> and passing user input as attribute keys.</li>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-3"">https://jinja.palletsprojects.com/en/3.1.x/changes/#version-3-1-3</a></li>; <li>Milestone: <a href=""https://github.com/pallets/jinja/milestone/15?closed=1"">https://github.com/pallets/jinja/milestone/15?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/main/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.3</h2>; <p>Released 2024-01-10</p>; <ul>; <li>Fix compiler error when checking if required blocks in parent templates are; empty. :pr:<code>1858</code></li>; <li><code>xmlattr</code> filter does not allow keys with spaces. GHSA-h5c8-rqwp-cp95</li>; <li>Make error messages stemming from invalid nesting of <code>{% trans %}</code> blocks; more helpful. :pr:<code>1918</code></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/d9de4bb215fd1cc8092a410fb834c7c4060b1fc1""><code>d9de4bb</code></a> release version 3.1.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/50124e16561f17f6c1ec85a692f6551418971cdc""><code>50124e1</code></a> skip test pypi</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ea7222ef3f184480be0d0884e30ccfb4172b17b""><code>9ea7222</code></a> use trusted publishing</li>; <li><a href=""https://github.com/pallets/jinja/commit/da703f7aae36b1e88baaa20de334d7ff6378fdde""><code>da703f7</code></a> use trusted publishing</li>; <li><a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14144:1133,error,error,1133,https://hail.is,https://github.com/hail-is/hail/pull/14144,3,['error'],['error']
Availability," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:13267,outage,outages,13267,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['outage'],['outages']
Availability," in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . /databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw); 115 def deco(*a, **kw):; 116 try:; --> 117 return f(*a, **kw); 118 except py4j.protocol.Py4JJavaError as e:; 119 converted = convert_exception(e.java_exception). /databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); 325 if answer[1] == REFERENCE_TYPE:; --> 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value). Py4JJavaError: An error occurred while calling o504.pyPersistTable.; : is.hail.utils.HailException: 1 samples and 12 covariates (including x) implies -11 degrees of freedom.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:11); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.LinearRegressionRowsSingle.execute(LinearRegression.scala:51); 	at is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:51); 	at is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:2936); 	at is.hail.expr.ir.TableIR.analyzeAndExecute(TableIR.scala:57); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:27); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$2(SparkBackend.scala:502); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:46); 	a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413:3808,Error,ErrorHandling,3808,https://hail.is,https://github.com/hail-is/hail/issues/11413,1,['Error'],['ErrorHandling']
Availability," in a dictionary but not finding it. In an ideal future, we'd bolt on some extra mechanism to give types to these errors, and we could throw a proper `IndexError` in the `ArrayRef` case or `KeyError` in the dictionary case. . It feels a little bit messy right now, open to suggestions. I don't love using `-1` as the ""no error"" situation, but I thought it was probably easier than dealing with optionals between python and scala. . To give an example of what it looks like, the error message for this script:. ```; import hail as hl. ht = hl.utils.range_table(10); ht = ht.annotate(foo = hl.nd.array([[1], [2], [3]])); ht = ht.annotate(bar = ht.foo[0:4, 12]); ht.collect(); ```. is. ```; Traceback (most recent call last):; File ""better_error_test.py"", line 6, in <module>; ht.collect(); File ""<decorator-gen-1103>"", line 2, in collect; File ""/Users/johnc/Code/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/johnc/Code/hail/hail/python/hail/table.py"", line 1903, in collect; return Env.backend().execute(e._ir); File ""/Users/johnc/Code/hail/hail/python/hail/backend/spark_backend.py"", line 325, in execute; raise HailUserError(message_and_trace) from None; hail.utils.java.HailUserError: Error summary: HailException: Index 12 is out of bounds for axis 1 with size 1; ------------; Hail stack trace:; File ""better_error_test.py"", line 5, in <module>; ht = ht.annotate(bar = ht.foo[0:4, 12]). File ""<decorator-gen-707>"", line 2, in __getitem__. File ""/Users/johnc/Code/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 3709, in __getitem__; hl.str(""Index "") + hl.str(s) + hl.str(f"" is out of bounds for axis {i} with size "") + hl.str(dlen). File ""<decorator-gen-1299>"", line 2, in or_error. File ""/Users/johnc/Code/hail/hail/python/hail/expr/builders.py"", line 311, in or_error; die_ir.save_error_info(); ```. (Note that ndarray bounds checking internally uses a case builder, which is why this example works).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9398:2089,Error,Error,2089,https://hail.is,https://github.com/hail-is/hail/pull/9398,1,['Error'],['Error']
Availability," in the Hail SSL; Config. The external client context does not load the hail certificate chain. I intend all Hail's HTTP(S) requests to use `httpx.py` (so named to not conflict; with modules named `http`). Again, I have simplified the landscape. We now have; two functions:. - `httpx.client_session`: The constructor for all asynchronous, HTTPS client; sessions.; - `httpx.blocking_client_session`: The constructor for all synchronous, HTTPS; client sessions. Both sessions have the exact same configuration parameters. The API is exactly; the same except the blocking client session replaces asynchronous methods with; synchronous ones. Both sessions accept the `aiohttp.ClientSession` constructor parameters. They; support one new parameter and modify the behavior of one old parameter.; - `retry_transient`: when set to `True` this parameter will retry all transient; errors in all requests made by this session. This defaults to `True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager. Its; `response_coroutine` field is a coroutine that includes the retry and; raise-for-status logic. - The `HailResolver` overrides domain name resolution to first consult the Hail; `address` service. `address` is effectively a domain name server. It watches; kubernetes services and publishes the pod IPs. It supports two name styles:; `service` and `service.namespace`. The former uses the deploy config to; determine in which namespace to find the given service. Currently, the; cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:1531,error,error,1531,https://hail.is,https://github.com/hail-is/hail/pull/9554,1,['error'],['error']
Availability," in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2334,Error,ErrorHandling,2334,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Error'],['ErrorHandling']
Availability," is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): is.hail.utils.HailException: Hail only supports 8-bit probabilities, found 16.; 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:222); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:206); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); 	at scala.collection.TraversableOnce$FlattenOps$$anon$1.hasNext(TraversableOnce.scala:464); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at is.hail.io.RichContextRDDRegionValue$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:4150,failure,failure,4150,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['failure'],['failure']
Availability," issuing a warning as we cannot help users facing issues with implementations other than OpenSSL. (<code>[#3020](https://github.com/urllib3/urllib3/issues/3020) &lt;https://github.com/urllib3/urllib3/issues/3020&gt;</code>__)</li>; <li>Deprecated URLs which don't have an explicit scheme (<code>[#2950](https://github.com/urllib3/urllib3/issues/2950) &lt;https://github.com/urllib3/urllib3/pull/2950&gt;</code>_)</li>; <li>Fixed response decoding with Zstandard when compressed data is made of several frames. (<code>[#3008](https://github.com/urllib3/urllib3/issues/3008) &lt;https://github.com/urllib3/urllib3/issues/3008&gt;</code>__)</li>; <li>Fixed <code>assert_hostname=False</code> to correctly skip hostname check. (<code>[#3051](https://github.com/urllib3/urllib3/issues/3051) &lt;https://github.com/urllib3/urllib3/issues/3051&gt;</code>__)</li>; </ul>; <h1>2.0.2 (2023-05-03)</h1>; <ul>; <li>Fixed <code>HTTPResponse.stream()</code> to continue yielding bytes if buffered decompressed data; was still available to be read even if the underlying socket is closed. This prevents; a compressed response from being truncated. (<code>[#3009](https://github.com/urllib3/urllib3/issues/3009) &lt;https://github.com/urllib3/urllib3/issues/3009&gt;</code>__)</li>; </ul>; <h1>2.0.1 (2023-04-30)</h1>; <ul>; <li>Fixed a socket leak when fingerprint or hostname verifications fail. (<code>[#2991](https://github.com/urllib3/urllib3/issues/2991) &lt;https://github.com/urllib3/urllib3/issues/2991&gt;</code>__)</li>; <li>Fixed an error when <code>HTTPResponse.read(0)</code> was the first <code>read</code> call or when the internal response body buffer was otherwise empty. (<code>[#2998](https://github.com/urllib3/urllib3/issues/2998) &lt;https://github.com/urllib3/urllib3/issues/2998&gt;</code>__)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/56f01e08",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13852:11043,avail,available,11043,https://hail.is,https://github.com/hail-is/hail/pull/13852,1,['avail'],['available']
Availability, issuing a warning as we cannot help users facing issues with implementations other than OpenSSL. (<code>[#3020](https://github.com/urllib3/urllib3/issues/3020) &lt;https://github.com/urllib3/urllib3/issues/3020&gt;</code>__)</li>; <li>Deprecated URLs which don't have an explicit scheme (<code>[#2950](https://github.com/urllib3/urllib3/issues/2950) &lt;https://github.com/urllib3/urllib3/pull/2950&gt;</code>_)</li>; <li>Fixed response decoding with Zstandard when compressed data is made of several frames. (<code>[#3008](https://github.com/urllib3/urllib3/issues/3008) &lt;https://github.com/urllib3/urllib3/issues/3008&gt;</code>__)</li>; <li>Fixed <code>assert_hostname=False</code> to correctly skip hostname check. (<code>[#3051](https://github.com/urllib3/urllib3/issues/3051) &lt;https://github.com/urllib3/urllib3/issues/3051&gt;</code>__)</li>; </ul>; <h1>2.0.2 (2023-05-03)</h1>; <ul>; <li>Fixed <code>HTTPResponse.stream()</code> to continue yielding bytes if buffered decompressed data; was still available to be read even if the underlying socket is closed. This prevents; a compressed response from being truncated. (<code>[#3009](https://github.com/urllib3/urllib3/issues/3009) &lt;https://github.com/urllib3/urllib3/issues/3009&gt;</code>__)</li>; </ul>; <h1>2.0.1 (2023-04-30)</h1>; <ul>; <li>Fixed a socket leak when fingerprint or hostname verifications fail. (<code>[#2991](https://github.com/urllib3/urllib3/issues/2991) &lt;https://github.com/urllib3/urllib3/issues/2991&gt;</code>__)</li>; <li>Fixed an error when <code>HTTPResponse.read(0)</code> was the first <code>read</code> call or when the internal response body buffer was otherwise empty. (<code>[#2998](https://github.com/urllib3/urllib3/issues/2998) &lt;https://github.com/urllib3/urllib3/issues/2998&gt;</code>__)</li>; </ul>; <h1>2.0.0 (2023-04-26)</h1>; <p>Read the <code>v2.0 migration guide &lt;https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html&gt;</code>__ for help upgrading to the latest ver,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13768:10691,avail,available,10691,https://hail.is,https://github.com/hail-is/hail/pull/13768,2,['avail'],['available']
Availability," of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0OWRkZWE4YS05NjJjLTQ4ODktYjgwMC0zZDY0YjgyYTBiMzgiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:4596,avail,available,4596,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability," of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0ZDFlNzI4ZS0yNjljLTQ5YTItYTJkMC1iZjFjMDQ5NjZlMjkiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:3271,avail,available,3271,https://hail.is,https://github.com/hail-is/hail/pull/14026,1,['avail'],['available']
Availability," of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiYzkzNDY4ZC02OGU5LTRmYWMtYTMzNS1mODcyNjE3MDZmNDgiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14211:3860,avail,available,3860,https://hail.is,https://github.com/hail-is/hail/pull/14211,1,['avail'],['available']
Availability," of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `40.5.0 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1MTRiNWVkZS0yNmZhLTQxMDYtODMxMC1jMmNlZWQ3YzA4YTkiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:3930,avail,available,3930,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['avail'],['available']
Availability, org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:185); at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:19); at is.hail.variant.MatrixTable.partitionCounts(MatrixTable.scala:535); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:1128); at is.hail.variant.MatrixTable.count(MatrixTable.scala:1126); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassCastException: null; at . Hail version: devel-e6de08e; Error summary: ClassCastException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3447:3341,Error,Error,3341,https://hail.is,https://github.com/hail-is/hail/issues/3447,1,['Error'],['Error']
Availability," other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.Illeg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1286,error,error,1286,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['error'],['error']
Availability," output_uri: str) -> bytes:; assert self._batch; ; try:; driver_output = await self._async_fs.open(output_uri); except FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: NativeIoException: readAddress(..) failed: Connection reset by peer; E ; E Java stack trace:; E io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer; E 	at ; E ; E ; E ; E Hail version: 0.2.115-330031a5d973; E Error summary: NativeIoException: readAddress(..) failed: Connection reset by peer. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _make_tsm: found 1000 variants after filtering out monomorphic sites.; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:4406,Error,Errors,4406,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['Error'],['Errors']
Availability," own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progress. Changes to datasets and datasets API site:; - Added the `ldsc_baselineLD_annotations`, `ldsc_baselineLD_ldscores`, and `ldsc_baseline_ldscores` datasets to the `annotation_db.json` configuration file. Now accessible via `load_dataset()` and `db.annotate_rows_db()` (for the annotations at least).; - New `.rst` files in `hail/python/hail/docs/datasets` have been generated to reflect the available datasets in the config file, and `hail/python/hail/docs/datasets.rst` has been updated with the new files as well. Future updates:; - In next PR can add the functionality to automatically determine the users region.; - Also considering modifying the `load_datasets()` function a bit to only require one `version` parameter, to be consistent with the way the version strings are formatted in `annotation_db.json`, and to avoid having to check if the version and reference genome are available separately. Something like this:. ```; mt_1kg = hl.experimental.load_dataset(name='1000_Genomes_autosomes',; version='phase_3-GRCh37', ; region='us'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:2494,avail,available,2494,https://hail.is,https://github.com/hail-is/hail/pull/9496,2,['avail'],['available']
Availability," pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240:1175,down,downsampled,1175,https://hail.is,https://github.com/hail-is/hail/issues/8240,1,['down'],['downsampled']
Availability," remove redundant definition of npy_nextafter [wheel build]</li>; </ul>; <h2>Checksums</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/85f38ab180ece5290f64e8ddbd9cf06ad8fa4a5e""><code>85f38ab</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23159"">#23159</a> from charris/prepare-1.24.2-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/124252537f526a059b6a5ee3ac1e3bf1442bbc13""><code>1242525</code></a> REL: Prepare for the NumPy 1.24.2 release</li>; <li><a href=""https://github.com/numpy/numpy/commit/de0ee415e45b09c86d1ddc04f51c11192b1e2fe6""><code>de0ee41</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23161"">#23161</a> from mattip/npy_nextafter</li>; <li><a href=""https://github.com/numpy/numpy/commit/ed09037473581908f6b52ecc3cabc82a414e2a54""><code>ed09037</code></a> BLD: remove redundant definition of npy_nextafter [wheel build]</li>; <li><a href=""https://github.com/numpy/numpy/commit/bc47a5ba6798a942d4a76e38f1089fc38f81f50d""><code>bc47a5b</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23150"">#23150</a> from charris/backport-23144</li>; <li><a href=""https://github.com/numpy/numpy/commit/e5452b91b87523853b2e33c0f7f6788a9a22c1b4""><code>e5452b9</code></a> TYP,MAINT: Add a missing explicit <code>Any</code> parameter to the <code>npt.ArrayLike</code> defi...</li>; <li><a href=""https://github.com/numpy/numpy/commit/2433fe5b66016a640dd9337d9e114d7076f55861""><code>2433fe5</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23149"">#23149</a> from charris/backport-23128</li>; <li><a href=""https://github.com/numpy/numpy/commit/8dfa47db1eb6472490b33d5f380513308f3e1a2d""><code>8dfa47d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/23148"">#23148</a> from c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12898:4494,redundant,redundant,4494,https://hail.is,https://github.com/hail-is/hail/pull/12898,1,['redundant'],['redundant']
Availability," repeatedly getting this error on when attempting to run vep GRCh38 on a dataproc cluster started using hailctl with 25 preemptible nodes, and otherwise default params.; ```; 2019-07-14 20:54:55 TaskSetManager: INFO: Finished task 1611.1 in stage 8.0 (TID 21696) in 49183 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1601/10000); 2019-07-14 20:54:57 TaskSetManager: INFO: Starting task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1, partition 1559, PROCESS_LOCAL, 8800 bytes); 2019-07-14 20:54:57 TaskSetManager: INFO: Finished task 1570.1 in stage 8.0 (TID 21697) in 45412 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1602/10000); 2019-07-14 20:55:04 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 1.; 2019-07-14 20:55:04 DAGScheduler: INFO: Executor lost: 1 (epoch 0); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-07-14 20:55:04 TransportClient: ERROR: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:994,ERROR,ERROR,994,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['ERROR'],['ERROR']
Availability," return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2196,failure,failure,2196,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['failure'],['failure']
Availability, scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.MatrixValue.filterSamples(Relational.scala:156); at is.hail.expr.FilterSamples.execute(Relational.scala:326); at is.hail.variant.VariantSampleMatrix.value$lzycompute(VariantSampleMatrix.scala:490); at is.hail.variant.VariantSampleMatrix.value(VariantSampleMatrix.scala:488); at is.hail.variant.VariantSampleMatrix.x$13$lzycompute(VariantSampleMatrix.scala:493); at is.hail.variant.VariantSampleMatrix.x$13(VariantSampleMatrix.scala:493); at is.hail.variant.VariantSampleMatrix.globalAnnotation$lzycompute(VariantSampleMatrix.scala:493); at is.hail.variant.VariantSampleMatrix.globalAnnotation(VariantSampleMatrix.scala:493); at is.hail.variant.VariantDatasetFunctions$.filterGenotypes$extension(VariantDataset.scala:463); at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```. Hail version: 0.1-20613ed; Error summary: ClassNotFoundException: is.hail.asm4s.AsmFunction2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:16760,Error,Error,16760,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['Error'],['Error']
Availability," set request method and body</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/82e70cae2a8d48b4f5165a9b543d4e65bb793d88""><code>82e70ca</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86a15f1c16eb729dc71b6caf30237d07b8e0bb01""><code>86a15f1</code></a> Fix compiler warnings and deprecations</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86363072c8239330b28976109a622bdd073507b6""><code>8636307</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/gradle-download-task/issues/295"">#295</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/c1e212c0fb41b3ea9185a9ea463fb1ea7142f748""><code>c1e212c</code></a> Add integration tests for Gradle 8.0 and 8.0.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/304f68e25f53633a92a4d2d6ce003a4986929503""><code>304f68e</code></a> Fix type inference issue</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.1...5.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.1&new-version=5.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:2158,down,download-task,2158,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download-task']
Availability," should print a message such as:. ```; The vds ""/users/dking/projects/hail-data/profile225.vds"" cannot be read by this version of hail. It must be regenerated from the VCF source using this version of hail.; ```. Actual behavior:. ```; dking@wmb16-359 # python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2159:1162,Error,Error,1162,https://hail.is,https://github.com/hail-is/hail/issues/2159,1,['Error'],['Error']
Availability," stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3744:2509,Error,Error,2509,https://hail.is,https://github.com/hail-is/hail/issues/3744,1,['Error'],['Error']
Availability," the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 		at com.google.api.client.http.HttpResponseException$Builder.build(HttpResponseException.java:293) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1118) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpStorageRpc.java:1022) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		... 48 more; Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 	at com.google.api.client.http.HttpResponseException$Builder.build(HttpResponseException.java:293) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1118) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpStorageRpc.java:1022) ~[gs:__hail-que",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:27268,error,error,27268,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['error'],['error']
Availability," the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:9387,error,error,9387,https://hail.is,https://github.com/hail-is/hail/issues/9939,2,['error'],['error']
Availability," the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; conc.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37> ; 'alleles': array<str> ; 'a_index': int32 ; 'was_split': bool ; 'concordance': array<struct {; concordance_matrix: struct {; n_discordant: int64, ; concordance: array<array<int64>>; }, ; meta: dict<str, str>; }> ; 'metrics': array<struct {; score: float64, ; positive_train: bool, ; negative_train: bool, ; rank: int64, ; meta: dict<str, str>; }> ; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-143-c80e74dd23d2> in <module>(); ----> 1 conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3729:1078,error,error,1078,https://hail.is,https://github.com/hail-is/hail/issues/3729,1,['error'],['error']
Availability," the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; sphinx 5.3.0 has requirement docutils<0.20,>=0.14, but you have docutils 0.20.1.; sphinx-rtd-theme 1.3.0 has requirement docutils<0.19, but you have docutils 0.20.1.; notebook 6.5.6 has requirement pyzmq<25,>=17, but you have pyzmq 25.1.1.; matplotlib 3.5.3 requires numpy, which is not installed.; matplotlib 3.5.3 requires pillow, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13871:1291,avail,available,1291,https://hail.is,https://github.com/hail-is/hail/pull/13871,1,['avail'],['available']
Availability," the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:. 1. edited build.gradle to it will accept my spark version like this if (sparkVersion == '2.2.0.2.6.3.0-235') {. 2. Then I ran ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip command. ### What went wrong (all error messages here, including the full java stack trace):; bild fails. ```; [luffy@wp-hdp-ctrl03 hail]$ ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip --stacktrace; 1f253167d53c; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3705:1005,FAILURE,FAILURE,1005,https://hail.is,https://github.com/hail-is/hail/issues/3705,1,['FAILURE'],['FAILURE']
Availability," this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires qtconsole, which is not installed.; jupyter 1.0.0 requires notebook, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiohttp-devtools 1.1 requires watchfiles, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14042:1279,avail,available,1279,https://hail.is,https://github.com/hail-is/hail/pull/14042,1,['avail'],['available']
Availability," thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1468,Error,ErrorHandling,1468,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['Error'],['ErrorHandling']
Availability," timed); 97 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad no",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5027,failure,failure,5027,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['failure'],['failure']
Availability," timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; INFO	2022-03-02 19:06:33,503	job_private.py	schedule_jobs_loop_body:142	starting scheduling jobs for jpim job-private; INFO	2022-03-02 19:06:33,533	job_private.py	schedule_jobs_loop_body:186	scheduled 0 jobs for jpim job-private; INFO	2022-03-02 19:06:34,964	pool.py	create_instances:244	pool highcpu n_instances 0 {'pending': 0, 'active': 0, 'inactive': 0, 'deleted': 0} free_cores 0.0 live_free_cores 0.0 ready_cores 0.0; ERROR	2022-03-02 19:06:35,376	job.py	schedule_job:473	error while scheduling job (94, 2) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:5350,ERROR,ERROR,5350,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['ERROR'],['ERROR']
Availability," to <code>*string</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104624"">kubernetes/kubernetes#104624</a>, <a href=""https://github.com/Haleygo""><code>@​Haleygo</code></a>)</li>; <li>Kubernetes is now built using go 1.17. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/103692"">kubernetes/kubernetes#103692</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>)</li>; <li>Performs strict server side schema validation requests via the <code>fieldValidation=[Strict,Warn,Ignore]</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105916"">kubernetes/kubernetes#105916</a>, <a href=""https://github.com/kevindelgado""><code>@​kevindelgado</code></a>)</li>; <li>Promote <code>IPv6DualStack</code> feature to stable.; Controller Manager flags for the node IPAM controller have slightly changed:; <ol>; <li>When configuring a dual-stack cluster, the user must specify both <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code> to set the per-node IP mask sizes, instead of the previous <code>--node-cidr-mask-size</code> flag.</li>; <li>The <code>--node-cidr-mask-size</code> flag is mutually exclusive with <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code>.</li>; <li>Single-stack clusters do not need to change, but may choose to use the more specific flags. Users can use either the older <code>--node-cidr-mask-size</code> flag or one of the newer <code>--node-cidr-mask-size-ipv4</code> or <code>--node-cidr-mask-size-ipv6</code> flags to configure the per-node IP mask size, provided that the flag's IP family matches the cluster's IP family (--cluster-cidr). (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104691"">kubernetes/kubernetes#104691</a>, <a href=""https://github.com/khenidak""><code>@​khenidak</code></a>)</li>; </ol>; </li>; <li>Remove <code>NodeLease</code> feature gate that ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:10612,mask,mask-size-,10612,https://hail.is,https://github.com/hail-is/hail/pull/11957,4,['mask'],"['mask', 'mask-size', 'mask-size-']"
Availability," to establish the interface and behavior. I expect several follow-on PRs:. - Revise the original copy interface proposal and add to dev-docs.; - ~~Parallelizes the transfers concurrently with async and across multiple threads.~~; - ~~After parallizing, copy will involve a lot of paralellism. Throwing an exception on the first error will be very non-deterministic. Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the source, so stat'ing the sources and destinations are all overlapping. This avoids dependencies where I have to e.g. stat the i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:1158,error,error,1158,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['error'],['error']
Availability," transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the informati",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1412,error,error,1412,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['error'],['error']
Availability," typ; self._compute_type(); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/ir/matrix_ir.py"", line 40, in _compute_type; self._type = Env.backend().matrix_type(self); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 121, in matrix_type; jir = self._to_java_ir(mir); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 102, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/ir/base_ir.py"", line 163, in parse; return Env.hail().expr.ir.IRParser.parse_matrix_ir(code, ref_map, ir_map); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: 17 (of class java.lang.Integer). Java stack trace:; scala.MatchError: 17 (of class java.lang.Integer); 	at org.json4s.scalap.scalasig.ClassFileParser$$anonfun$37.apply(ClassFileParser.scala:119); 	at org.json4s.scalap.scalasig.ClassFileParser$$anonfun$37.apply(ClassFileParser.scala:119); 	at org.json4s.scalap.Rule$$anonfun$flatMap$1.apply(Rule.scala:33); 	at org.json4s.scalap.Rule$$anonfun$flatMap$1.apply(Rule.scala:32); 	at org.json4s.scalap.Rule$$anonfun$mapResult$1.apply(Rule.scala:43); 	at org.json4s.scalap.Rule$$anonfun$mapResult$1.apply(Rule.scala:43); 	at org.json4s.scalap.Rules$DefaultRule.apply(Rules.scala:67); 	at org.json4s.scalap.Rules$DefaultRule.apply(Rules.scala:65); 	at org.json4s.scalap.StateRules$class.rep$2(Rules.scala:137); 	at org.json4s.scalap.StateRules$$anonfun$repeatUntil$1.apply(Rules.scala:143); 	at org.json4s.scalap.StateRules$$anonfun$repeatUntil$1.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299:8873,Error,Error,8873,https://hail.is,https://github.com/hail-is/hail/issues/6299,1,['Error'],['Error']
Availability," url, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/common/session.py:105: in request; return await retry_transient_errors(self._request_with_valid_authn, method, url, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:780: in retry_transient_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:796: in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/common/session.py:117: in _request_with_valid_authn; return await self._http_session.request(method, url, **kwargs); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. async def request_and_raise_for_status():; json_data = kwargs.pop('json', None); if json_data is not None:; if kwargs.get('data') is not None:; raise ValueError('data and json parameters cannot be used at the same time'); kwargs['data'] = aiohttp.BytesPayload(; value=orjson.dumps(json_data),; # https://github.com/ijl/orjson#serialize; #; # ""The output is a bytes object containing UTF-8""; encoding=""utf-8"",; content_type=""application/json"",; ); resp = await self.client_session._request(method, url, **kwargs); if raise_for_status:; if resp.status >= 400:; # reason should always be not None for a started response; assert resp.reason is not None; body = (await resp.read()).decode(); await resp.release(); > raise ClientResponseError(; resp.request_info,; resp.history,; status=resp.status,; message=resp.reason,; headers=resp.headers,; body=body,; ); E hailtop.httpx.ClientResponseError: 400, message='Bad Request', url=URL('http://internal.hail/pr-14351-default-yojxd4mck4io/batch/api/v1alpha/batches/321/update-fast') body=""400: error while inserting job group 1 into batch 321: (1213, 'Deadlock found when trying to get lock; try restarting transaction')""; ```. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14413:3740,error,error,3740,https://hail.is,https://github.com/hail-is/hail/issues/14413,1,['error'],['error']
Availability," used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:2167,down,download-task,2167,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability," used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:1448,down,download-task,1448,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability," v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.5.post1</h2>; <h1>0.23.5 (2024-02-09)</h1>; <ul>; <li>Declare compatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; <li>Prevent DeprecationWarning about internal use of <code>asyncio.get_event_loop()</code> from affecting test cases <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/757"">#757</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have as",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:1514,error,errors,1514,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['error'],['errors']
Availability," with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:3888,failure,failure,3888,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['failure'],['failure']
Availability," wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2443, in _annotate_all; analyze(""MatrixTable.annotate_rows"", row_struct, self._row_indices); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/expr/expressions/expression_utils.py"", line 105, in analyze; raise errors[0]; hail.expr.expressions.base_expression.ExpressionException: MatrixTable.annotate_rows expects an expression from source <hail.matrixtable.MatrixTable object at 0x10889c908>, found expression derived from <hail.matrixtable.MatrixTable object at 0x108acbf60>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3763:2881,error,errors,2881,https://hail.is,https://github.com/hail-is/hail/issues/3763,1,['error'],['errors']
Availability,"![Finally.](https://media.giphy.com/media/yIsbuPCEOgNHO/giphy.gif). - update endpoints to handle the ""zen"" that GitHub sends when a web hook is created. - update `make run-local` and friends for the new IP of the `dk-test` micro instance. - remove the unused `refresh_statuses` (this was intended to recover build state from github's commit statuses, but the commit status description is limited to like 120 characters, so I gave up on this a while ago, but never removed the code). - `.strip()` the GitHub token in case there are newlines. - print the SHA being deployed in the log statement. - add `hail-ci-build.sh` to CI, which just invokes `make test-in-cluster`(which in turn runs `test-in-cluster.sh`. - `test-in-cluster.sh` copies the secrets for testing to the expected locations and exposes the pod in which it is running with an internal service, recent changes to `site` [redirect sub URLs of ci.test.is to services named using this scheme](https://github.com/hail-is/hail/blob/master/site/hail.nginx.conf#L38-L41). GitHub uses these URLs to send updates to the CI under test about the watched repositories. - `test-locally.sh` now installs `../batch` into the currently running `pip` before testing (NB: if you edit batch and run the tests without committing the changes you've made to batch, this will pass tests but fail when pushed to a PR!). - `test-locally.sh` activates the `hail-ci` conda environment itself because it was not being propagated from the `Makefile`. I don't know why, but this is a simple fix. - `test-locally.sh` starts the ci after the repository is created. CI will print error messages if a watched repository doesn't exist. - `test/test-ci.py` now uses access tokens for all interaction with GitHub, previously it relied on the latent privileges that I and Cotton had in our environments. - `test/test-ci.py` uses a temporary, but not automatically deleted, directory when the environment variable `IN_CLUSTER` is set to `true` (to which it is set by `test-in-c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4474:300,recover,recover,300,https://hail.is,https://github.com/hail-is/hail/pull/4474,1,['recover'],['recover']
Availability,""" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""prPublicId"":""4d1e728e-269c-49a2-a2d0-bf1c04966e29"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494,539],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:6284,avail,available,6284,https://hail.is,https://github.com/hail-is/hail/pull/14026,1,['avail'],['available']
Availability,""", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhoste",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8390:2089,down,download,2089,https://hail.is,https://github.com/hail-is/hail/issues/8390,1,['down'],['download']
Availability,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13835:10845,avail,available,10845,https://hail.is,https://github.com/hail-is/hail/pull/13835,1,['avail'],['available']
Availability,""",""from"":""0.8.4"",""to"":""2.0.3""},{""name"":""nbconvert"",""from"":""5.6.1"",""to"":""6.3.0b0""},{""name"":""notebook"",""from"":""5.7.16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13866:10871,avail,available,10871,https://hail.is,https://github.com/hail-is/hail/pull/13866,1,['avail'],['available']
Availability,"""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-9-304965820738>"", line 1, in <module>; x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:1546,Error,Error,1546,https://hail.is,https://github.com/hail-is/hail/issues/3516,1,['Error'],['Error']
Availability,"""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:1688,failure,failure,1688,https://hail.is,https://github.com/hail-is/hail/issues/3465,1,['failure'],['failure']
Availability,"""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:06,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:22:42,128"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6707:3834,ERROR,ERROR,3834,https://hail.is,https://github.com/hail-is/hail/issues/6707,1,['ERROR'],['ERROR']
Availability,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7412:5608,down,download,5608,https://hail.is,https://github.com/hail-is/hail/pull/7412,3,"['avail', 'down', 'error']","['available', 'download', 'error']"
Availability,"""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:35,400	job.py	schedule_job:473	error while scheduling job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:7409,error,error,7409,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,"""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,364	job.py	schedule_job:473	error while scheduling job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:11910,error,error,11910,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,"""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,390	job.py	schedule_job:473	error while scheduling job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:13915,error,error,13915,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,"""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:39,193	job.py	schedule_job:473	error while scheduling job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:22075,error,error,22075,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,""":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14257:12275,avail,available,12275,https://hail.is,https://github.com/hail-is/hail/pull/14257,1,['avail'],['available']
Availability,""":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""40.5.0"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.32.2"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14364:12628,avail,available,12628,https://hail.is,https://github.com/hail-is/hail/pull/14364,1,['avail'],['available']
Availability,""":\""chr20\"",\""position\"":17994753},\""alleles\"":[\""A\"",\""<NON_REF>\""]},\""includeStart\"":true,\""includeEnd\"":true},{\""start\"":{\""locus\"":{\""contig\"":\""chr20\"",\""position\"":17994753},\""alleles\"":[\""A\"",\""<NON_REF>\""]},\""end\"":{\""locus\"":{\""cont...""] ; !s20 = ToStream(%29) [False]; !s21 = StreamMap(!s20) { (%elt10) =>; SelectFields(%elt10) [; (partitionCounts distinctlyKeyed firstKey; lastKey)]; }; !37 = ToArray(!s21); !38 = WriteMetadata(!37) [""{\""name\"":\""TableSpecWriter\"",\""path\"":\""/tmp/foo.ht\"",\""typ\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],data:Array[Struct{}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{new_globals:Array[Struct{}]}\""},\""rowRelPath\"":\""rows\"",\""globalRelPath\"":\""globals\"",\""refRelPath\"":\""references\"",\""log\"":true}""]; !39 = Begin(!34, !36, !38); WriteMetadata(!39) [""{\""name\"":\""RelationalWriter\"",\""path\"":\""/tmp/foo.ht\"",\""overwrite\"":true,\""maybeRefs\"":{\""references\"":[\""GRCh38\""]}}""]. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:17); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:15601,Error,ErrorHandling,15601,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,""">#133</a>). Thanks to <a href=""https://github.com/jmsmkn""><code>@​jmsmkn</code></a> and <a href=""https://github.com/adamchainz""><code>@​adamchainz</code></a>!</li>; <li>Fix incorrect documentation for <a href=""https://www.curlylint.org/docs/rules/no_autofocus""><code>no_autofocus</code></a> and <a href=""https://www.curlylint.org/docs/rules/tabindex_no_positive""><code>tabindex_no_positive</code></a>.</li>; </ul>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.13.0"">v0.13.0</a> 2021-04-25</h2>; <p>This release comes with a blog post! Read on <a href=""https://www.curlylint.org/blog/quality-of-life-improvements"">Quality-of-life improvements</a>.</p>; <h3>Added</h3>; <ul>; <li>Implement --template-tags CLI flag (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/25"">#25</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/77"">#77</a>).</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Add more descriptive error message for missing whitespace between HTML attributes (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/23#issuecomment-700622837"">#23 (comment)</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Move development dependencies from extras to separate <code>requirements.txt</code> (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Declare support for Python 3.9.</li>; <li>Tentatively declare support for Python 3.10 (tested with <code>Python 3.10.0a6+</code>).</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix Python 3.10 deprecation warning by importing Iterable from collections.abc (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; </ul>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.12.2"">v0.12.2</a> 2021-03-06</h2>; <h3>Fixed</h3>; <ul>; <li>The <code>image_alt</code> rule no longer crashes when encou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11713:5047,error,error,5047,https://hail.is,https://github.com/hail-is/hail/pull/11713,1,['error'],['error']
Availability,"""><code>8f7f078</code></a> Limit expensive decorator function (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1407"">#1407</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/98280b57b5ed3db8a4d431cb60e21f136f6c70de""><code>98280b5</code></a> Add Position to the nodes.<strong>init</strong> (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1408"">#1408</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/a5bd030bf0c420e6773369dc0125b34b39681496""><code>a5bd030</code></a> Revert &quot;Use importlib instead of pkg_resources for determining namespace pack...</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/a62f37ddae2d6fdb6f8ec0f2e5b6a0a41e5f883e""><code>a62f37d</code></a> Add position attribute for nodes (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1393"">#1393</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/514c832a6957c7589aa3e14973189e2e245de961""><code>514c832</code></a> Fix recursion error for inference of self-referencing class attribute (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1392"">#1392</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/astroid/compare/astroid-version-1.0.0...v2.10.0"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:5494,error,error,5494,https://hail.is,https://github.com/hail-is/hail/pull/11463,2,['error'],['error']
Availability,"""><code>@​mmichilot</code></a>)</li>; <li>Fix Shift + L not working in stdin <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15440"">#15440</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15499"">#15499</a>: Adopt ruff format <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15564"">#15564</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Pin <code>actions/labeler</code> to v4 to fix failing CI action <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15496"">#15496</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix URLs in debugger-extension <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15462"">#15462</a> (<a href=""https://github.com/fcollonval""><code>@​fcollonval</code></a>)</li>; <li>More robust galata/UI tests <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15355"">#15355</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15499"">#15499</a>: Adopt ruff format <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15564"">#15564</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyterlab/jupyterlab/blob/@jupyterlab/lsp@4.0.11/CHANGELOG.md"">jupyterlab's changelog</a>.</em></p>; <blockquote>; <h2>4.0.11</h2>; <!-- raw HTML omitted -->; <!-- raw HTML omitted -->; <h2>4.0.10</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/compare/v4.0.9...b9bc3002b1ab89b9a1c4d2a3007c43275",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:5479,robust,robust,5479,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['robust'],['robust']
Availability,"""><code>@​mmichilot</code></a>)</li>; <li>Fix Shift + L not working in stdin <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15440"">#15440</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15499"">#15499</a>: Adopt ruff format <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15564"">#15564</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Pin <code>actions/labeler</code> to v4 to fix failing CI action <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15496"">#15496</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix URLs in debugger-extension <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15462"">#15462</a> (<a href=""https://github.com/fcollonval""><code>@​fcollonval</code></a>)</li>; <li>More robust galata/UI tests <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15355"">#15355</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15499"">#15499</a>: Adopt ruff format <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15564"">#15564</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/graphs/contributors?from=2023-11-18&amp;to=2023-12-29&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aafshin+updated%3A2023-11-18..2023-12-29&amp;type=Issues""><code>@​afshin</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Abrichet+updated%3A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:8712,robust,robust,8712,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['robust'],['robust']
Availability,""">atomicwrites</a> dependency on windows with [os.replace]{.title-ref}.</li>; </ul>; <h2>7.1.2</h2>; <h1>pytest 7.1.2 (2022-04-23)</h1>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9726"">#9726</a>: An unnecessary <code>numpy</code> import inside <code>pytest.approx</code>{.interpreted-text role=&quot;func&quot;} was removed.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9820"">#9820</a>: Fix comparison of <code>dataclasses</code> with <code>InitVar</code>.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9869"">#9869</a>: Increase <code>stacklevel</code> for the <code>NODE_CTOR_FSPATH_ARG</code> deprecation to point to the; user's code, not pytest.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9871"">#9871</a>: Fix a bizarre (and fortunately rare) bug where the [temp_path]{.title-ref} fixture could raise; an internal error while attempting to get the current user's username.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/4645bcd44915c2fd6043b101626e5bf1a983ac07""><code>4645bcd</code></a> Remove incorrect output in how-to/fixtures.rst</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/fadfb4f3463bc828535f86682300907a30f240e9""><code>fadfb4f</code></a> Prepare release version 7.1.3</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/ab96ea88e829af05e1491c30214b924c9553697b""><code>ab96ea8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10258"">#10258</a> from pytest-dev/backport-10252-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/fc0e024b118fa63e84637bd5c9242b2b382e58fd""><code>fc0e024</code></a> [7.1.x] Fix regendoc</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/8f5088f4126b61ff76ac9809d5eb27cdbc31f0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12187:3521,error,error,3521,https://hail.is,https://github.com/hail-is/hail/pull/12187,1,['error'],['error']
Availability,""">https://pypi.org/project/nbsphinx/0.8.4/</a></p>; <ul>; <li>Support for <code>mathjax3_config</code> (for Sphinx &gt;= 4)</li>; <li>Force loading MathJax on HTML pages generated from notebooks (can be disabled with <code>nbsphinx_assume_equations = False</code>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/spatialaudio/nbsphinx/blob/master/NEWS.rst"">nbsphinx's changelog</a>.</em></p>; <blockquote>; <p>Version 0.8.8 -- 2021-12-31 -- PyPI__ -- diff__</p>; <ul>; <li>Support for the <code>sphinx_codeautolink</code> extension</li>; <li>Basic support for the <code>text</code> builder</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.8/"">https://pypi.org/project/nbsphinx/0.8.8/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.7...0.8.8"">https://github.com/spatialaudio/nbsphinx/compare/0.8.7...0.8.8</a></p>; <p>Version 0.8.7 -- 2021-08-10 -- PyPI__ -- diff__</p>; <ul>; <li>Fix assertion error in LaTeX build with Sphinx 4.1.0+</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.7/"">https://pypi.org/project/nbsphinx/0.8.7/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.6...0.8.7"">https://github.com/spatialaudio/nbsphinx/compare/0.8.6...0.8.7</a></p>; <p>Version 0.8.6 -- 2021-06-03 -- PyPI__ -- diff__</p>; <ul>; <li>Support for Jinja2 version 3</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.6/"">https://pypi.org/project/nbsphinx/0.8.6/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.5...0.8.6"">https://github.com/spatialaudio/nbsphinx/compare/0.8.5...0.8.6</a></p>; <p>Version 0.8.5 -- 2021-05-12 -- PyPI__ -- diff__</p>; <ul>; <li>Freeze Jinja2 version to 2.11 (for now, until a bugfix is found)</li>; <li>Add <code>theme_comparison.py</code> tool for creating multiple versions; (with different HTML themes) of the docs at once</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsph",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:2352,error,error,2352,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['error'],['error']
Availability,"""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488"">#9488</a>: If custom subclasses of nodes like <code>pytest.Item</code>{.interpreted-text role=&quot;class&quot;} override the; <code>__init__</code> method, they should take <code>**kwargs</code>. See; <code>uncooperative-constructors-deprecated</code>{.interpreted-text role=&quot;ref&quot;} for details.</p>; <p>Note that a deprection warning is only emitted when there is a conflict in the; arguments pytest expected to pass. This deprecation was already part of pytest; 7.0.0rc1 but wasn't documented.</p>; </li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <detail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:2938,avail,available,2938,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['avail'],['available']
Availability,"# Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; sphinx 5.3.0 has requirement docutils<0.20,>=0.14, but you have docutils 0.20.1.; sphinx-rtd-theme 1.3.0 has requirement docutils<0.19, but you have docutils 0.20.1.; notebook 6.5.6 has requirement pyzmq<25,>=17, but you have pyzmq 25.1.1.; aiohttp-devtools 1.1 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14043:1258,avail,available,1258,https://hail.is,https://github.com/hail-is/hail/pull/14043,1,['avail'],['available']
Availability,"# Current situation. In several places, we call a method `emitNDArrayStandardStrides`, which has the effect of calling `emitDeforestedNDArray`, since that is known to always emit things in column major order. However, the downside of this is that we were doing unnecessary copies of the data, even when it was already in column major order, by constructing an `NDArrayEmitter` that just emitted an NDArray and then looked up values from it:. ```; case _ =>; val ndt = emit(x); val ndAddress = mb.genFieldThisRef[Long](); val setup = (ndAddress := ndt.value[Long]); val xP = x.pType.asInstanceOf[PNDArray]. val shapeAddress = new Value[Long] {; def get: Code[Long] = xP.shape.load(ndAddress); }; val shapeTuple = new CodePTuple(xP.shape.pType, shapeAddress). val shapeArray = (0 until xP.shape.pType.nFields).map(i => shapeTuple.apply[Long](i)). new NDArrayEmitter[C](nDims, shapeArray,; xP.shape.pType, xP.elementType, setup, ndt.setup, ndt.m) {; override def outputElement(elemMB: EmitMethodBuilder[C], idxVars: IndexedSeq[Value[Long]]): Code[_] =; xP.loadElementToIRIntermediate(idxVars, ndAddress, elemMB); }; ```. # New Situation. We now have `emitNDArrayColumnMajorStrides`, which calls `emit` on an ndarray, checks if the emitted thing is column major, and only does a copy if it needs to. This uses new `LinalgCodeUtils` methods `checkColumnMajor` and `createColumnMajorCode`. Everything else in `LinalgCodeUtils` was unused / old style and I removed them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9428:222,down,downside,222,https://hail.is,https://github.com/hail-is/hail/pull/9428,1,['down'],['downside']
Availability,"# Hail version:. eb5d13fe97fc. ### What you did:. ```; t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). t1.join(t2, how='outer').show(). # or. t1.join(t2, how='right').show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. FatalError: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 24, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils.R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:952,error,error,952,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['error'],['error']
Availability,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:294,avail,available,294,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['avail'],['available']
Availability,"## Bug fixes to enable Azure deployment. Most of these bugs were discovered in deploying the MySQL server from scratch, specifically deploying version 8.0. ; Some were encountered when we hit certificate issues in trying to run the `./bootstrap.sh deploy_unmanaged` step multiple times within 24hrs. Documentation was clarified in order to resolve this issue. - build.yaml; - Step one fails on rerun since the /repo directory exists, -p to fix; - ci/create_database.py; - In MySQL 8 a new error was introduced [4006](https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_cannot_user_referenced_as_definer); - This error gets triggered on the CREATE USER IF NOT EXISTS commands for both user and admin if the user was previously created and set a a definer on any events/triggers.; - Really this statement should be a no-op given that the user exists, but for some reason the error triggers anyway.; - To get around this I added a manual check if the user/admin exists and if they do simply skip the create user command. This fixes the bug and allows the MySQL db deploy to finish properly. - dev-docs/letsencrypt.md; - Debugging was confusing since the revoke command addressed ids we were unable to find.; - After extensive searching I added to the documentation how to find your existing cert IDs if you need to revoke them. - infra/azure/README.md; - Added clarity to the Azure deployment documentation. - infra/azure/bootstrap.sh; - Added the passing of additional flag arguments to terraform; - In our case the passing of the `-upgrade` flag to the terraform init step was required in order to continue. - infra/azure/main.tf, infra/azure/modules/batch/main.tf, infra/azure/modules/batch/variables.tf infra/azure/variables.tf; - Add additional argument for the az_storage_account.; - The name must be globally unique in Azure, so the original argument failed on our deployment since it shared the name with the Hail team's Azure deployment",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12065:489,error,error,489,https://hail.is,https://github.com/hail-is/hail/pull/12065,5,['error'],"['error', 'error-reference', 'errors']"
Availability,"## Change Description. Config and records relating to the appsec deployment instance. Necessary for future maintenance and management of the appsec instance. ## Security Assessment. - This change has no security impact. ### Impact Description. No impact because this relates only to the parallel appsec instance, not the main production instance. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14726:107,mainten,maintenance,107,https://hail.is,https://github.com/hail-is/hail/pull/14726,1,['mainten'],['maintenance']
Availability,"## Change Description. Corrects our gsa-key copying instructions (from #14664) to copy the key contents, not the entire secret metadata. The batch service seems resilient to these badly formed secrets, but the rotate_keys.py script was not. ## Security Assessment. Delete all except the correct answer:; - This change has a low security impact. ### Impact Description. - Not a production change; - Does not add any new information to the secrets, only formats them a useable way. ; - Only in dev namespaces. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14735:161,resilien,resilient,161,https://hail.is,https://github.com/hail-is/hail/pull/14735,1,['resilien'],['resilient']
Availability,"## Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; jupyter 1.0.0 requires qtconsole, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **461/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.5 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14070:1242,avail,available,1242,https://hail.is,https://github.com/hail-is/hail/pull/14070,1,['avail'],['available']
Availability,"## Mill setup. There is a mill wrapper script `millw` checked into the repo in the `hail/` subdirectory. You can either invoke it directly, as we did with `gradlew`, or copy it somewhere on your path and rename it `mill`. This way you can just run `mill ...`, and it will identify the correct version of mill to use for the project you're in and invoke that. For more details on the installation options, see the github for the wrapper script [here](https://github.com/lefou/millw). To verify that it's working, and download the actual mill jar, run `./millw --version` (or `mill --version` if you put it on your path) from the `hail/` subdirectory. It should report version `0.11.6`. ## Mill from the command line. * `mill clean` - delete all output files (in `hail/hail/out`). Or only delete the output of one target, e.g. `mill clean test.compile`; * `mill compile` - compiles the root module (not including tests); * `mill test.compile` - compiles tests (and, transitively, the rest of the root module); * `mill test.test`, or for short `mill test` - run all tests. You can pass options to the test runner (TestNG currently), e.g.* * `mill test -methods is.hail.expr.ir.CallFunctionsSuite.constructors` to run one test, or `mill test -threadcount 4 -parallel classes` to use 4 threads and parallelize over test classes; * `mill test.testOnly is.hail.expr.ir.CallFunctionsSuite` - run all tests in one or more specified classes. You can use `*` to match anything, e.g. `mill test.testOnly ""*.CallFunctionsSuite""`, or `mill test.testOnly ""is.hail.expr.ir.*""`. You can pass options to the test runner (TestNG currently) after a `--`, e.g. `mill test.testOnly ""is.hail.expr.ir.*"" -- -parallel classes`; * `mill __.testCached` - once the codebase is more modularized, will run tests on only modules whose dependencies have changed since the last test run; * `mill reformat` - runs scalafmt on all sources in the root module (currently that's all scala sources, but hopefully not for long). `mill __.ref",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:516,down,download,516,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['down'],['download']
Availability,"### Change Description. Jobs within our CI pipeline often invoke `mill` indirectly through `Makefile` prerequisites.; By staging mill's build area to and from `/derived/{release/debug}/hail/out`, mill will not rebuild artefacts from previous steps.; Exposing `MILLOPTS` in `hail/Makefile` allows us to build in CI without a compilation server. ; Using a compilation server may have been why we experienced intermittent failures between building the jar and copying to its final destination.; Note that the `--no-server` option must be the first argument to `millw`. ### Security Assessment; - [x] This change has no security impact. Description of the security impact and necessary mitigations:. Only derived files from the mill + python build process are staged and unstaged.; No secrets or otherwise sensitive information are contained therein. (Reviewers: please confirm the security impact before approving)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14709:419,failure,failures,419,https://hail.is,https://github.com/hail-is/hail/pull/14709,1,['failure'],['failures']
Availability,"### Change Description. This change exists as part of larger refactoring work. Herein, I've exchanged; hard-coded contextual strings passed to `ExecutionTimer.time` with implict; contexts, drawing inspiration from scalatest. These contexts are now supplied after entering functions like `Compile` and; `Emit` instead of before (see `ExecuteContext.time`). By sprinking calls to ; `time` throughout the codebase after entering functions, we obtain a nice trace; of the timings with `sourcecode.Enclosing`, minus the previous verbosity. See [1] for more information about what pre-built macros are available. We can; always build our own later. See comments in [pull request id] for example output.; Note that `ExectionTimer.time` still accepts a string to support uses like; `Optimise` and `LoweringPass` where those contexts are provided already.; It is also exception-safe now. This change exposed many similarities between the implementations of query; execution across all three backends. I've stopped short of full unification; which is a greater work, I've instead simplified and moved duplicated result; encoding into the various backend api implementations. More interesting changes are to `ExecuteContext`, which now supports; - `time`, as discussed above; - `local`, a generalisation for temporarily overriding properties of an ; `ExecuteContext` (inspired by [2]). While I've long wanted this for testing,; we were doing some questionable things when reporting timings back to python,; for which locally overriding the `timer` of a `ctx` has been very useful.; We also follow this pattern for local regions. [1] https://github.com/com-lihaoyi/sourcecode; [2] https://hackage.haskell.org/package/mtl-2.3.1/docs/Control-Monad-Reader.html#v:local. ### Security Assessment. This change has no security impact as it's confined to refactoring of existing non-security-related code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14683:596,avail,available,596,https://hail.is,https://github.com/hail-is/hail/pull/14683,1,['avail'],['available']
Availability,"### Change Description. `test_dataproc-*` is failing with:; ```; ERROR: (gcloud.dataproc.clusters.create) unrecognized arguments: --public-ip-address (did you mean '--no-address'?) ; ```; Went into the pushed `ci-utils` image, and verified that gcloud didn't have the `--public-ip-address` flag. Version 495 is current and has the flag. ### Security Assessment. - [x] This change has a low security impact",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14707:65,ERROR,ERROR,65,https://hail.is,https://github.com/hail-is/hail/pull/14707,1,['ERROR'],['ERROR']
Availability,"### Description. In this pull request, I add a function to perform a Cochran-Mantel-Haenszel statistical test for association. This pull request closes #13481. ### Testing. I add unit tests. Since I have not used R before (the [associated GitHub issue](https://github.com/hail-is/hail/issues/13481) suggests using R to create test cases), I created the unit tests from examples that I found on the internet. I linked these sources in the code for the unit tests. I built the documentation locally and inspected it to confirm that it matches my expectations. I am having trouble testing the docstring examples locally. When I run `make -C hail doctest-query`, the tests error due to a checksum exception. ### Discussion. ~I have not added an example to the documentation that uses a matrix table yet. (This is an acceptance criteria in #13481.) I wanted to get some advice about the best way to do this. I think ideally, the example would have a binary phenotype, an allele to test for association, and some stratifying variable. I tried to search through the existing code to find suitable example matrix tables in the docstrings, but I didn't find anything promising. I would appreciate help here.~. Update: thanks to @patrick-schultz's recommendation, I have added an example using a matrix table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255:669,error,error,669,https://hail.is,https://github.com/hail-is/hail/pull/14255,1,['error'],['error']
Availability,"### Description. In this pull request, I fix an error in the MatrixTable tutorial. The tutorial shows some genotype data and erroneously states that all the genotypes that are shown are homozygous reference (0/0). In fact, there are also some heterozygous (0/1) and homozygous alternate (1/1) genotypes in the displayed data. In this pull request, I remove the erroneous statement. ### Testing. I ran the notebook to confirm that the notebook displays a mix of genotypes, not just homozygous reference. You can view the erroneous version of the tutorial [here](https://hail.is/docs/0.2/tutorials/07-matrixtable.html#MatrixTable-operations).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14239:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/pull/14239,1,['error'],['error']
Availability,"### Hail version: 0.2.8. ### What you did:. ```python; smt = hl.methods.balding_nichols_model(3, 100, 200); smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); smt.make_table().to_pandas(); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; 2019-01-26 19:03:28 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 200 variants...; 2019-01-26 19:03:28 Hail: INFO: Coerced sorted dataset; 2019-01-26 19:03:29 Hail: INFO: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-59-9addf4eaf59b> in <module>; 1 smt = hl.methods.balding_nichols_model(3, 100, 200); 2 smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); ----> 3 smt.make_table().to_pandas(). <decorator-gen-1366> in to_pandas(self, flatten). ~/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda3/lib/python3.6/site-packages/hail/table.py in to_pandas(self, flatten); 2703 ; 2704 """"""; -> 2705 return Env.spark_backend('to_pandas').to_pandas(self, flatten); 2706 ; 2707 @staticmethod. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_pandas(self, t, flatten); 66 ; 67 def to_pandas(self, t, flatten):; ---> 68 return self.to_spark(t, flatten).toPandas(); 69 ; 70 def from_pandas(self, df, key):. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_spark(self, t, flatten); 63 if flatten:; 64 t = t.flatten(); ---> 65 return pyspark.sql.DataFrame(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5212:236,error,error,236,https://hail.is,https://github.com/hail-is/hail/issues/5212,1,['error'],['error']
Availability,"### Hail version: ; 0.1-74bf1eb. ### What you did: ; Export vcf to local file:// path. ### What went wrong (all error messages here, including the full java stack trace): ; When exporting vcf to path that begins with 'file://', I get the error: `ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found`. I am using Spark 2.2.1 (prebuilt with hadoop2.7) with AWS-Hadoop 2.7.4. I have the following settings in spark config and am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:112,error,error,112,https://hail.is,https://github.com/hail-is/hail/issues/3946,2,['error'],['error']
Availability,"### Hail version: ; 0.2.9-8588a25687af. ### What you did: ; tbl.export(""filename"", header=False, types_file=None). ### What went wrong (all error messages here, including the full java stack trace):; No errors, but two files were written to my working directory; None and .None.crc. The file contains column types as if the 'types_file = None' was interpreted as 'types_file = ""None""'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5266:140,error,error,140,https://hail.is,https://github.com/hail-is/hail/issues/5266,2,['error'],"['error', 'errors']"
Availability,"### Hail version:. 9e4ca83b6b0d. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):. `hl.eval_expr(hl.is_snp(hl.literal('A'), hl.literal('A')))` should return false, i.e. ['A', 'A'] shouldn't be considered a snp.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3712:77,error,error,77,https://hail.is,https://github.com/hail-is/hail/issues/3712,1,['error'],['error']
Availability,"### Hail version:. Irrelevant feature branch based on `abfb656e2`. ### What you did:. 1. `cd batch ; make run`; 2. ; ```; curl -XPOST localhost:5000/jobs/create -H 'Content-Type: application/json' -d '{ ; ""spec"" : {; ""containers"" : [; { ""name"" : ""foobarbaz""; , ""image"" : ""alpine:3.8""; , ""command"": [""/bin/sh"", ""-c"", ""echo hi""] }] } }'; ```; 3. ; ```; curl localhost:5000/jobs; ```; 4. the job never transitions to Complete and the server log shows:; ```; (hail-batch) # make run; BATCH_USE_KUBE_CONFIG=1 python -c 'import batch.server; batch.server.serve()'; INFO	| 2018-11-13 18:12:19,124 	| server.py 	| <module>:25 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-11-13 18:12:19,125 	| server.py 	| <module>:28 | instance_id = 63aeb0cd4fa840a9864cfd909ce7f682; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target kube_event_loop; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target polling_event_loop; INFO	| 2018-11-13 18:12:19,131 	| server.py 	| run_forever:391 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; INFO	| 2018-11-13 18:12:19,168 	| _internal.py 	| _log:88 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); INFO	| 2018-11-13 18:12:20,141 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:12:20,159 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4773:317,echo,echo,317,https://hail.is,https://github.com/hail-is/hail/issues/4773,1,['echo'],['echo']
Availability,"### Hail version:. d5007d8878f2dc330dd2f01c1c0251785f259dac. ### What you did:. ```; ds = hl.import_vcf(resource(""sample.vcf"")); ds_duplicate = ds.annotate_rows(duplicate = [1,2]); ds_duplicate = ds_duplicate.explode_rows(ds_duplicate['duplicate']); result = hl.ld_prune(ds_duplicate.GT); result.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed: absoluteUpperIndexBounds length 197 did not match GridPartition nRows 394; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.methods.UpperIndexBounds$.computeCoverByUpperTriangularBlocks(UpperIndexBounds.scala:69); 	at is.hail.linalg.BlockMatrix.filteredEntriesTable(BlockMatrix.scala:1198); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3835:334,error,error,334,https://hail.is,https://github.com/hail-is/hail/issues/3835,1,['error'],['error']
Availability,"### Hail version:. de61119ceec6. ### What you did:; ```; ht1 = hl.Table.parallelize([; {'k': 'foo', 'b': 1},; {'k': 'bar', 'b': 2},; {'k': 'bar', 'b': 2}],; hl.tstruct(k=hl.tstr, b=hl.tint32),; key='k'). # IllegalArgumentException: requirement failed; ht1.group_by().aggregate(mean_b = hl.agg.mean(ht1.b)).show(). # ClassCastException: is.hail.rvd.UnpartitionedRVD cannot be cast to is.hail.rvd.OrderedRVD; ht1.group_by('k').aggregate(mean_b = hl.agg.mean(ht1.b)).show(); ```; ### What went wrong (all error messages here, including the full java stack trace):. bad error messages",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4070:502,error,error,502,https://hail.is,https://github.com/hail-is/hail/issues/4070,2,['error'],['error']
Availability,"### Hail version:. eb5d13fe97fc. ### What you did:. ```; t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). t1.join(t2, how='outer').show(). # or. t1.join(t2, how='right').show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. FatalError: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 24, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:484,error,error,484,https://hail.is,https://github.com/hail-is/hail/issues/4055,4,"['error', 'failure']","['error', 'failure']"
Availability,"### Hail version:. f2b0dca9f506. ### What you did:; ```; ht = hl.Table.parallelize([; {'a': '1', 'c': .5,'d': 'foo'},; {'a': '1', 'c': .6,'d': 'foo'},; ], hl.tstruct(a=hl.tstr,; c=hl.tfloat32, d=hl.tstr)); mt = ht.to_matrix_table(['a'], ['d']). mt.entries().show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 7, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.endArray(RegionValueBuilder.scala:167); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75$$anonfun$apply$25.apply(MatrixIR.scala:1878); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75$$anonfun$apply$25.apply(MatrixIR.scala:1849); 	at is.hail.utils.FlipbookIterator$$anon$4.<init>(FlipbookIterator.scala:133); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:131); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75.apply(MatrixIR.scala:1849); 	at is.hail.expr.ir.TableToMatrixTable$$anonfun$75.apply(MatrixIR.scala:1840); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$18.apply(ContextRDD.scala:293); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$18.apply(ContextRDD.scala:293); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1014); 	at scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:296,error,error,296,https://hail.is,https://github.com/hail-is/hail/issues/4114,3,"['error', 'failure']","['error', 'failure']"
Availability,"### Hail version:; 0.1 only - this already works in 0.2. ### What you did:; I'm calling kt.export_elasticsearch(..) on this keytable schema:. ```; ...; aIndex: Int,; alt: String,; codingGeneIds: Set[String],; contig: String,; docId: String,; domains: Set[String],; end: Int,; filters: Set[String],; geneIds: Set[String],; pos: Int,; ref: String,; rsid: String,; start: Int,; variantId: String,; vep: Array[Struct{; gene_id: String,; gene_symbol: String,; protein_id: String,; transcript_id: String,; hgvs: String,; major_consequence: String,; major_consequence_rank: Int,; category: String; }],; wasSplit: Boolean,; ```. ### What went wrong (all error messages here, including the full java stack trace):. Having `vep: Array[Struct..]` in the schema leads to . ```; 2018-09-02 07:06:06,821 INFO ==> exporting data to elasticasearch. Write mode: upsert, blocksize: 1000; Config Map(es.batch.size.entries -> 1000, es.index.auto.create -> true, es.mapping.id -> docId, es.write.operation -> upsert, es.port -> 9200, es.nodes -> 10.56.10.4); [Stage 3:> (0 + 100) / 1000]Traceback (most recent call last):; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/load_dataset_to_es.py"", line 734, in <module>; run_pipeline(); File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/load_dataset_to_es.py"", line 726, in run_pipeline; hc, vds = step2_export_to_elasticsearch(hc, vds, args); File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/load_dataset_to_es.py"", line 565, in step2_export_to_elasticsearch; disable_index_for_fields=(""sortedTranscriptConsequences"", ),; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/load_dataset_to_es.py"", line 358, in export_to_elasticsearch; verbose=True,; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/hail_scripts.zip/hail_scripts/v01/utils/elasticsearch_client.py"", line 140, in export_vds_to_elasticsearch; File ""/tmp/ffc9fb0b99f64080b674ab7a07962df9/hail_scripts.zip/hail_scripts/v01/utils/elasticsearch_client.py"", line 287, in export_kt_to_elasticsearch; File ""<decorator-gen-143>"", line 2, in expo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:646,error,error,646,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['error'],['error']
Availability,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3920:767,error,error,767,https://hail.is,https://github.com/hail-is/hail/issues/3920,4,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"### Hail version:; 1f253167d53c; ### What you did:; ```; hl.eval_expr(hl.literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3708:169,error,error,169,https://hail.is,https://github.com/hail-is/hail/issues/3708,1,['error'],['error']
Availability,"### Hail version:; 1f253167d; ### What you did:; ```; import hail as hl; t = hl.utils.range_table(10); hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)) ; ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3706:234,error,error,234,https://hail.is,https://github.com/hail-is/hail/issues/3706,1,['error'],['error']
Availability,"### Hail version:; 2018-06-04; ### What you did:; ### What went wrong (all error messages here, including the full java stack trace):; The docs for [`liftover`](https://hail.is/docs/devel/functions/genetics.html?highlight=liftover#hail.expr.functions.liftover) confusingly use `.value` to convert an expression to a value. Users are apt to copy paste them and not understand what the `.value` is doing. We should uniformly use `hl.eval_expr` which is less surprising.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3709:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/3709,1,['error'],['error']
Availability,"### Hail version:; 6195693b3; ### What you did:; ```mt.filter_cols(hl.literal(set(max_downsample1)).contains(mt.col_idx))```. ### What went wrong (all error messages here, including the full java stack trace):; ```Hail cannot automatically impute type of <class 'numpy.int64'>: 129```. Hail should understand the usual numpy int types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3699:151,error,error,151,https://hail.is,https://github.com/hail-is/hail/issues/3699,1,['error'],['error']
Availability,"### Hail version:; Hail version: 0.1-20613ed; Error summary: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:46,Error,Error,46,https://hail.is,https://github.com/hail-is/hail/issues/2966,3,"['Down', 'Error', 'error']","['Downloaded', 'Error', 'error']"
Availability,"### Hail version:; Latest build of `devel` from https://storage.googleapis.com/hail-common/distributions/devel/Hail-devel-5d0f74cef4f2-Spark-2.2.0.zip. ### What you did:; 1. Import VCF file into MatrixTable.; 2. Annotate VCF file with `output = hl.vep(input, 'vep.properties', csq=True)`.; 3. Attempt to view output with `output.rows().show()`. With `csq=False`, step 3 succeeds. ### What went wrong (all error messages here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:405,error,error,405,https://hail.is,https://github.com/hail-is/hail/issues/3790,1,['error'],['error']
Availability,"### Hail version:; `08224c6ab`; ### What you did:; Tried to load a plink dataset that was split by chromosome.; ```; files = [(; f'gs://fc-9a7c5487-04c9-4182-b3ec-13de7f6b409b/genotype/ukb_cal_chr{i}_v2.bed',; f'gs://fc-9a7c5487-04c9-4182-b3ec-13de7f6b409b/genotype/ukb_snp_chr{i}_v2.bim'; ) for i in range(1,23)]; mts = [hl.import_plink(bed=f[0],bim=f[1],fam=""gs://phenotype_31063/ukb31063.fam"") for f in files]; mt = mts[0].union_rows(*mts[1:]); ```; ### What went wrong (all error messages here, including the full java stack trace):; It loaded each plink file serially rather than in parallel, thus wasting many cores of my cluster (and my time).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3975:478,error,error,478,https://hail.is,https://github.com/hail-is/hail/issues/3975,1,['error'],['error']
Availability,"### Hail version:; `30bc2bcdf2ba`; ### What you did:; ```python; t = hl.import_table('/path/to/thing', ; delimiter='\s+',; impute=True); ```; on a file with two extra new lines at the end.; ### What went wrong (all error messages here, including the full java stack trace):; The task failed 14 times in the background rather than eagerly failing.; ```; is.hail.utils.HailException: pheno_31063_eur_gwas_skin_color.clumped.gz: expected 13 fields, but found 1 offending line: 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20) 	at is.hail.utils.package$.fatal(package.scala:26) 	at is.hail.utils.Context.wrapException(Context.scala:19) 	at is.hail.utils.WithContext.foreach(Context.scala:51) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5$$anonfun$apply$2.apply(TextTableReader.scala:126) 	at scala.collection.Iterator$class.foreach(Iterator.scala:893) 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:126) 	at is.hail.utils.TextTableReader$$anonfun$5.apply(TextTableReader.scala:122) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:108) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused by: is.hail.uti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4100:215,error,error,215,https://hail.is,https://github.com/hail-is/hail/issues/4100,3,"['Error', 'error']","['ErrorHandling', 'error']"
Availability,"### Hail version:; `6d4d50458`; ### What you did:; ```; label_expr = (hl.case(missing_false=True); .when(ht[tp_col] & ~ht[fp_col], ""TP""); .when(ht[fp_col] & ~ht[tp_col], ""FP”)); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; I’m getting an error that reads,; Traceback (most recent call last):; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 567, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 508, in main; run_hash = train_rf(data_type, args) if args.train_rf else args.run_hash; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 422, in train_rf; fp_to_tp=args.fp_to_tp); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 240, in sample_rf_training_examples; train_col: train_expr}); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/table.py"", line 720, in annotate; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in get_annotate_exprs; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in <dictcomp>; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 102, in to_expr; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 93, in impute_type; hail.expr.expressions.base_expression.ExpressionException: Hail cannot automatically impute type of <class 'hail.expr.builders.CaseBuilder'>: <hail.expr.builders.CaseBuilder object at 0x7f2ad03c74e0>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3696:208,error,error,208,https://hail.is,https://github.com/hail-is/hail/issues/3696,2,['error'],['error']
Availability,"### Hail version:; `784ab2796878`; ### What you did:; ```; In [1]: import hail as hl; ...: ; ...: t1kg = hl.balding_nichols_model(3, 100, 100); ...: print(t1kg.describe()); ...: t1kg = t1kg_sm.repartition(500) ; ...: t1kg = t1kg._filter_partitions([1]); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean}},col_key:[sample_idx],col:Struct{sample_idx:Int32,pop:Int32},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],ancestral_af:Float64,af:Array[Float64],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{GT:Call}}; after: Matrix{global:Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean}},col_key:[sample_idx],col:Struct{sample_idx:Int32,pop:Int32},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],ancestral_af:Float64,af:Array[Float64],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{GT:Call}}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527:349,error,error,349,https://hail.is,https://github.com/hail-is/hail/issues/4527,1,['error'],['error']
Availability,"### Hail version:; ```; 0.2.8-590ea4ae3b83; ```; ### What you did:; ```; hl.import_bgen(; ""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen"",; entry_fields=[]; ).count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):; A spark stage was triggered. I expected it to read the number of rows from the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5182:219,error,error,219,https://hail.is,https://github.com/hail-is/hail/issues/5182,1,['error'],['error']
Availability,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3713:104,error,error,104,https://hail.is,https://github.com/hail-is/hail/issues/3713,3,"['error', 'failure']","['error', 'failure']"
Availability,"### Hail version:; `devel-62679fc21687`; ### What you did:; [ran a PRS script](https://gist.github.com/danking/9c9d77afb3ff318adc7bf79bb52d4f7d); Cluster had 100 pre-emptibles.; ![tasks page showing failures due to container exit](https://user-images.githubusercontent.com/106194/44540204-293de580-a6d4-11e8-90b8-1ecaaec45443.png); ![Job 12 page show mapPartitionsWithIndex running](https://user-images.githubusercontent.com/106194/44540205-293de580-a6d4-11e8-9893-af8edc382906.png); ![executors page showing 202 active executors, 100 dead](https://user-images.githubusercontent.com/106194/44540178-15927f00-a6d4-11e8-99e1-78755df3756c.png). This `mapPartitionsWithIndex` call is in the tree aggregate of context rdd. Judging from the fact that it's the first one, this is probably the first step of the aggregation.; ; ### What went wrong (all error messages here, including the full java stack trace):; Workers exceed memory limits, e.g. (from hail.log):; ```; 2018-08-22 17:28:37 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container killed by YARN for exceeding memory limits. 12.2 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; 2018-08-22 17:28:37 YarnScheduler: ERROR: Lost executor 431 on pca-sw-pd61.c.daly-ibd.internal: Container killed by YARN for exceeding memory limits. 12.2 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; 2018-08-22 17:28:37 TaskSetManager: WARN: Lost task 1047.0 in stage 16.0 (TID 20251, pca-sw-pd61.c.daly-ibd.internal, executor 431): ExecutorLostFailure (executor 431 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.2 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; 2018-08-22 17:28:37 TaskSetManager: WARN: Lost task 1046.0 in stage 16.0 (TID 20250, pca-sw-pd61.c.daly-ibd.internal, executor 431): ExecutorLostFailure (executor 431 exited caused by one of the running tasks) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4202:199,failure,failures,199,https://hail.is,https://github.com/hail-is/hail/issues/4202,2,"['error', 'failure']","['error', 'failures']"
Availability,"### Hail version:; devel-10a75bb57a6f; ### What you did:. ```; def test_rekey_correct_partition_key(self):; ht = hl.utils.range_table(5); ht = ht.add_index('a'); ht = ht.key_by('idx', 'a'); ht = ht.annotate(b=ht.idx); ht = ht.key_by('idx', 'b'); self.assertEqual(ht.aggregate(agg.sum(ht.idx)), 10); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDType.<init>(OrderedRVDType.scala:21); 	at is.hail.rvd.OrderedRVDType.copy(OrderedRVDType.scala:121); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2090); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:520); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.table.Table.aggregate(Table.scala:465); 	at is.hail.table.Table.aggregate(Table.scala:446); 	at is.hail.table.Table.aggregateJSON(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3748:329,error,error,329,https://hail.is,https://github.com/hail-is/hail/issues/3748,1,['error'],['error']
Availability,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4718:164,error,error,164,https://hail.is,https://github.com/hail-is/hail/issues/4718,1,['error'],['error']
Availability,"### Hail version:; latest master. ### What you did:; remove `region.clear()` from the `it.map` function inside `RVD.persistRVRDD`. ### What went wrong (all error messages here, including the full java stack trace):. `LDPruneSuite.testNoPrune` fails giving 313 variants instead of 338.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3398:156,error,error,156,https://hail.is,https://github.com/hail-is/hail/issues/3398,1,['error'],['error']
Availability,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001:139,error,error,139,https://hail.is,https://github.com/hail-is/hail/issues/3001,4,"['Down', 'FAILURE', 'error']","['Downloads', 'FAILURE', 'error']"
Availability,"### Hail version:; reported on Jan 11 9:45 by @nikbaya on [zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/import_table.20IncompleteParseError). `00f9b64d8`; ### What you did:. ```; import hail as hl; phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; IncompleteParseError Traceback (most recent call last); <ipython-input-2-ebe253e83367> in <module>(); 1 phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; ----> 2 missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); 3 # phesant_still_more2 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.2.tsv',; 4 # missing='',impute=True,types={'all_sexes$userId':hl.tstr}). <decorator-gen-1108> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1327 delimiter, missing, no_header, impu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5119:539,error,error,539,https://hail.is,https://github.com/hail-is/hail/issues/5119,1,['error'],['error']
Availability,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4938:66,resilien,resiliently,66,https://hail.is,https://github.com/hail-is/hail/pull/4938,4,"['failure', 'resilien']","['failure', 'failures', 'resilient', 'resiliently']"
Availability,"### What happened?. # Executive Summary. We will pin to `orjson<=3.9.11` until https://github.com/ijl/orjson/pull/457 merges and addresses the root cause of these segfaults. This issue is resolved when orjson merges orjson#457, releases a new version, and we upgrade to it. # Details. Tests that use the py4j_backend and thus rely on orjson to (de)serialize data have been intermittently segfaulting:; ```; [2024-02-08 22:36:47] test/hail/matrixtable/test_file_formats.py::test_backward_compatability_ht[/io/resources/backward_compatability/1.6.0/table/6.ht/] Fatal Python error: Segmentation fault. Thread 0x00007fa51d817640 (most recent call first):; File ""/usr/lib/python3.9/selectors.py"", line 416 in select; File ""/usr/lib/python3.9/socketserver.py"", line 232 in serve_forever; File ""/usr/lib/python3.9/threading.py"", line 917 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Thread 0x00007fa5273ff640 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py"", line 58 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Current thread 0x00007fa52bd6b000 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py"", line 217 in _rpc; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/backend.py"", line 212 in table_type; ...; ```. Line 217 only does one thing: call `orjson.dumps`. https://github.com/hail-is/hail/blob/b3df76360f931f54688bb03bf5774643c0b8205e/hail/python/hail/backend/py4j_backend.py#L216-L218. Indeed, `orjson` has had [this issue since 3.9.12](https://github.com/ijl/orjson/issues/452) and we just recently updated orjson from 3.9.10 to 3.9.12:. ```; commit d2615543476bde5d01061499c92f26124b85caf3; Author: Dan King <daniel.zidan.king@gmail.com>; Date: Fri Feb 2 14:21:47 2024 -0500. [dependencies] mass upd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14299:573,error,error,573,https://hail.is,https://github.com/hail-is/hail/issues/14299,2,"['error', 'fault']","['error', 'fault']"
Availability,"### What happened?. **A Motivating Example**. The statement:. `hl.experimental.load_dataset(""gnomad_pca_variant_loadings"", version='2.1', reference_genome='GRCh38')`. raises the following error:. ```; AssertionError Traceback (most recent call last); Cell In[32], line 1; ----> 1 hl.experimental.load_dataset(""gnomad_pca_variant_loadings"", version='2.1', reference_genome='GRCh38'). File ~/.local/lib/python3.8/site-packages/hail/experimental/datasets.py:115, in load_dataset(name, version, reference_genome, region, cloud); 107 raise ValueError(f'Region {repr(region)} not available for dataset'; 108 f' {repr(name)} on cloud platform {repr(cloud)}.\n'; 109 f'Available regions: {regions}.'); 111 path = [dataset['url'][cloud][region]; 112 for dataset in datasets[name]['versions']; 113 if all([dataset['version'] == version,; 114 dataset['reference_genome'] == reference_genome])]; --> 115 assert len(path) == 1; 116 path = path[0]; 117 if path.startswith('s3://'):. AssertionError: ; ```. I'm a new Hail user and don't have the full context here, but it seems like there are at least three problems:. 1. An assert failed in production code, which indicates either the presence of a bug or an incorrect use of assert (e.g. using assert to check for value errors).; 2. The assert has no corresponding error message, so the user learns that something has gone wrong but can't easily tell what.; 3. The assert is bare. Bare asserts can get optimized out of code in ways that are difficult to foresee in advance, and are generally deprecated in favor of the `if error_condition: raise AssertionError(...)` pattern (see: https://discuss.python.org/t/stop-ignoring-asserts-when-running-in-optimized-mode/13132). **The Big Picture**. The bare assert pattern is used over 3k times in Hail. To be fair, many of these usages occur in test directories, where they're fine. But they also occur in application code, and often in the dangerous form `assert(expr1, expr2)` which will never fail (because a tuple wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952:188,error,error,188,https://hail.is,https://github.com/hail-is/hail/issues/12952,3,"['Avail', 'avail', 'error']","['Available', 'available', 'error']"
Availability,"### What happened?. - Tried to import VariantSpark version 0.5.2 into a Google Colab notebook running Python 3.9.16, with Hail version 0.2.112 and Apache Spark version 3.3.2. When trying to import VariantSpark, the following error occurred:; `import hail as hl; import varspark.hail as vshl; vshl.init()`; `using variant-spark jar at '/usr/local/lib/python3.9/dist-packages/varspark/jars/variant-spark_2.12-0.5.2-all.jar'; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); [<ipython-input-6-3d2ff0083f18>](https://localhost:8080/#) in <cell line: 3>(); 1 import hail as hl; 2 import varspark.hail as vshl; ----> 3 vshl.init(). 4 frames; <decorator-gen-1907> in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions). <decorator-gen-1909> in init_spark(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, gcs_requester_pays_configuration). [/usr/local/lib/python3.9/dist-packages/hail/context.py](https://localhost:8080/#) in init_spark(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, gcs_requester_pays_configuration); 425 app_name = app_name or 'Hail'; 426 gcs_requester_pays_project, gcs_requester_pays_buckets = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(gcs_requester_pays_configuration); --> 427 backend = SparkBackend(; 428 idempotent, sc, spark_conf, app_name, master, local, log,; 429 quiet, append, min",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12859:225,error,error,225,https://hail.is,https://github.com/hail-is/hail/issues/12859,1,['error'],['error']
Availability,"### What happened?. - [ ] Document the AsyncFS interface, RouterAsyncFS, and the three clouds.; - [ ] Do a critical pass of the interfaces and make sure public methods don't have underscores and private methods do. When in doubt, make it private.; - [ ] Same as above but for parameters. For this, be particularly aggressive about making parameters private!; - [ ] Downgrade positional arguments to keyword-arguments as much as is feasible without severely degrading UX. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14021:365,Down,Downgrade,365,https://hail.is,https://github.com/hail-is/hail/issues/14021,1,['Down'],['Downgrade']
Availability,### What happened?. 1. Make a copy of https://docs.google.com/document/d/1deV-i3_oMGBwUreDUKEhovdLawBdf0feshcP1h11wR0/edit; 2. Fill it out; 3. Ping #hail-on-terra in Broad Institute slack to determine next steps for publicly rolling out Batch in Azure-Terra. ### Version. 0.2.126. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13953:143,Ping,Ping,143,https://hail.is,https://github.com/hail-is/hail/issues/13953,1,['Ping'],['Ping']
Availability,"### What happened?. > I'm hitting a 500 error when I try to open a notebook on a dataproc cluster started with hail 0.2.116. hailctl dataproc connect mw nb will open the google bucket and I am able to see all ipynb files but when I try opening one, I'm met with the 500. I have no issue with hail 0.2.113. Error from the logs:. ```; May 16 14:17:07 mw116-m python[8309]: [D 14:17:07.035 NotebookApp] Path notebook/css/override.css served from /opt/conda/miniconda3/lib/python3.10/site-packages/notebook/static/notebook/css/override.css; May 16 14:17:07 mw116-m python[8309]: [E 14:17:07.041 NotebookApp] Uncaught exception GET /notebooks/gnomad-mwilson/v4/sample_qc_defs.ipynb (127.0.0.1); May 16 14:17:07 mw116-m python[8309]: HTTPServerRequest(protocol='http', host='localhost:8123', method='GET', uri='/notebooks/gnomad-mwilson/v4/sample_qc_defs.ipynb', version='HTTP/1.1', remote_ip='127.0.0.1'); May 16 14:17:07 mw116-m python[8309]: Traceback (most recent call last):; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/tornado/web.py"", line 1786, in _execute; May 16 14:17:07 mw116-m python[8309]: result = await result; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/tornado/gen.py"", line 786, in run; May 16 14:17:07 mw116-m python[8309]: yielded = self.gen.send(value); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/notebook/handlers.py"", line 95, in get; May 16 14:17:07 mw116-m python[8309]: self.write(self.render_template('notebook.html',; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/base/handlers.py"", line 507, in render_template; May 16 14:17:07 mw116-m python[8309]: return template.render(**ns); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/jinja2/environment.py"", line 1301, in render; May 16 14:17:07 mw116-m python[8309",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13059:40,error,error,40,https://hail.is,https://github.com/hail-is/hail/issues/13059,2,"['Error', 'error']","['Error', 'error']"
Availability,"### What happened?. > In regards to the https vs hail-az, I get an error for https:; > hailctl config set batch/remote_tmpdir https://kahlquisrefsa.blob.core.windows.net/test; > Error: bad value 'https://kahlquisrefsa.blob.core.windows.net/test' for parameter 'batch/remote_tmpdir' should be valid cloud storage URI such as gs://my-bucket/batch-tmp/. ### Version. 0.2.116. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13049:67,error,error,67,https://hail.is,https://github.com/hail-is/hail/issues/13049,2,"['Error', 'error']","['Error', 'error']"
Availability,"### What happened?. > Laura Gauthier: I'm struggling with some DRAGEN data that probably doesn't quite meet the VCF spec. I got the import working, but once I go to split multi-allelics, one of the annotations seems to be the wrong length because I get an array index out of bounds exception. Is there anyway to get more info on the variant that's causing the problem? VCFtool validator found a bunch of issues with FORMAT annotations and I've turned them all into count=1 strings, but there must be something else.; > ...; > Tim Poterba (he/him): yeah, the answer is that this isn't a parse failure, it's a failure of the split_multi_hts method to support haploid sex chromosome calls; > Tim Poterba (he/him): the right plan is to support sex chromosomes The Right Way™ and update all of Hail to infer, track, and use appropriate ploidy but that's not at all what the system looks like right now. ### Version. 0.2.117. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13149:592,failure,failure,592,https://hail.is,https://github.com/hail-is/hail/issues/13149,2,['failure'],['failure']
Availability,"### What happened?. A job that is attempted more than, say, 5 times, is probably going wrong. We should at the least log an error. Users should probably be able to set hard limits on the number of alerts. See also: https://github.com/hail-is/hail/issues/13395. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13927:124,error,error,124,https://hail.is,https://github.com/hail-is/hail/issues/13927,1,['error'],['error']
Availability,"### What happened?. A simple `hl.init()` fails, that used to work. Maybe an error with Spark, not an expert. ### Version. 0.2.108. ### Relevant log output. ```shell; ~ » python3; Python 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; >>> hl.init(); 2023-01-27 17:15:28.940 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1758>"", line 2, in init; File ""/opt/homebrew/lib/python3.10/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/homebrew/lib/python3.10/site-packages/hail/context.py"", line 345, in init; return init_spark(; File ""<decorator-gen-1760>"", line 2, in init_spark; File ""/opt/homebrew/lib/python3.10/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/homebrew/lib/python3.10/site-packages/hail/context.py"", line 424, in init_spark; backend = SparkBackend(; File ""/opt/homebrew/lib/python3.10/site-packages/hail/backend/spark_backend.py"", line 188, in __init__; self._jbackend = hail_package.backend.spark.SparkBackend.apply(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py"", line 1304, in __call__; return_value = get_return_value(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x4d740d85) cannot access class sun.n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12630:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/12630,1,['error'],['error']
Availability,"### What happened?. A user running RHEL 9 [reported an error](https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653) importing movie lens data when running 0.2.126 but succeeded on 0.2.120. This error did not reproduce on MacOS. We should verify which version of hail this error is introduced and whether the hail installation is fully broken or for some reason is just movie lens/a subset of functionality. ### Version. 0.2.126. ### Relevant log output. ```shell; 2023-11-20 18:25:51.813 Hail: WARN: This Hail JAR was compiled for Spark 3.3.0, running with Spark 3.3.3.; Compatibility is not guaranteed.; 2023-11-20 18:25:53.340 Hail: INFO: SparkUI: http://xxxxx:4040; 2023-11-20 18:25:54.037 Hail: INFO: Running Hail version 0.2.126-ee77707f4fab; 2023-11-20 18:27:48.120 Hail: INFO: downloading MovieLens-100k data ...; Source: https://files.grouplens.org/datasets/movielens/ml-100k.zip; 2023-11-20 18:27:50.320 Hail: INFO: importing users table and writing to data/users.ht ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049:55,error,error,55,https://hail.is,https://github.com/hail-is/hail/issues/14049,4,"['down', 'error']","['downloading', 'error']"
Availability,"### What happened?. After I ran the ""make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0"", I get the following error. > Configure project :; WARNING: Hail primarily tested with Spark 3.3.2, use other versions at your own risk. > Task :compileScala; [Error] /gpfs/fs1/home/jl/Hail2/hail/hail/src/main/scala/is/hail/HailContext.scala:127:21: value implOpMulMatrix_DMD_DVD_eq_DVD is not a member of object breeze.linalg.DenseMatrix; one error found. > Task :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; > Run with --info option to get more log output.; > Run with --scan to get full insights. BUILD FAILED in 4m 52s; 2 actionable tasks: 2 executed; make: *** [build/libs/hail-all-spark.jar] Error 1. ### Version. Hail 0.2.13. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14235:148,error,error,148,https://hail.is,https://github.com/hail-is/hail/issues/14235,5,"['Error', 'FAILURE', 'error']","['Error', 'FAILURE', 'error']"
Availability,"### What happened?. After sorting our costs into ""cost of goods"", ""operating expenses"", and ""capital expenses"", I realized there are four ""operating expenses"" that are not tracked and reported with the other expenses. I regressed these costs against the core-hours to estimate the cost per core-hour. resource | intercept (USD) | cost (USD/core-hour); -- | -- | --; GCP Support Variable fee | 3.46403 +- 0.49155 | 0.00123 +- 0.00007; System logs costs SKU#1 | 13.09991 +- 3.13991 | 0.00093 +- 0.00039; System logs costs SKU#2 | 7.87838 +- 0.81695 | 0.00027 +- 0.00012; Job specifications | 5.41150 +- 0.36608 | 0.00025 +- 0.00005; Firewall policy | 0.51216 +- 0.03185 | 0.00012 +- 0.00000. To fully recover the operating expenses at our current revenue, we need an additional 0.005 USD per core-hour (which is 0.002 more than the sum of intercepts). This issue is complete after we add a new product:. resource | cost (USD/core-hour); -- | --; support-logs-specs-and-firewall-fees/1 | 0.005. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13526:699,recover,recover,699,https://hail.is,https://github.com/hail-is/hail/issues/13526,1,['recover'],['recover']
Availability,### What happened?. All batch workers should have the Ops Agent so that we have RAM and disk usage available in GCP Monitoring. This is critical for diagnosing issues on workers. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13903:99,avail,available,99,https://hail.is,https://github.com/hail-is/hail/issues/13903,1,['avail'],['available']
Availability,"### What happened?. Although it is not possible to avoid all cross-region access (and thus costs), there are some obvious preventable misuses. For example, the following pipeline should error:. ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; But the following pipeline should not error:; ```; b = hb.Batch(regions=['us-east1']); x = b.read_input('gs://bucket-in-central1/'); j = b.new_job(f'cat {x}'); j.regions(['us-central1']); ```; The following should error because the job *could* be in us-east1:; ```; b = hb.Batch(regions=['us-east1', 'us-central1']); x = b.read_input('gs://bucket-in-central1/'); b.new_job(f'cat {x}'); b.run(); ```; The following should error:; ```; b = hb.Batch(regions=['us-east1']) # remote_tmpdir is set in config file as a us-centra1 bucket; j = b.new_job(f'echo hi > {j.f}'); j2 = b.new_job(f'cat {j.f}'); b.run(); ```. ### Version. 0.2.119. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13232:186,error,error,186,https://hail.is,https://github.com/hail-is/hail/issues/13232,5,"['echo', 'error']","['echo', 'error']"
Availability,"### What happened?. At time of writing, building hail with `SPARK_VERSION=3.4.0` errors with the following message:. ```; hail/src/main/scala/is/hail/HailContext.scala:119:21: value implOpMulMatrix_DMD_DVD_eq_DVD is not a member of object breeze.linalg.DenseMatrix; ```. This is due to a major version upgrade and breaking change in the Breeze library on which spark and hail depend. The exact error is a rename and refactor. The method `DenseMatrix.implOpMulMatrix_DMD_DVD_eq_DVD` is now `HasOps.impl_OpMulMatrix_DMD_DVD_eq_DVD`. Notice the method name change and the fact that `HasOps` does not exist in the version of Breeze (1.x) that is used in Spark 3.3. Hail should build with Spark 3.4, but since we only officially support one version of Spark (whichever Dataproc currently is running), it would be reasonable to wait to fully upgrade to Spark 3.4 when [Dataproc 2.2](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.2) is GA instead of trying to do something hacky to support both versions of Breeze. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13971:81,error,errors,81,https://hail.is,https://github.com/hail-is/hail/issues/13971,2,['error'],"['error', 'errors']"
Availability,### What happened?. Bad error message. Should be a user-level nice error. https://discuss.hail.is/t/error-while-ld-pruning-variants-hail-utils-java-fatalerror-illegalargumentexception-requirement-failed/3371. ### Version. 0.2.114. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12971:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/12971,3,['error'],"['error', 'error-while-ld-pruning-variants-hail-utils-java-fatalerror-illegalargumentexception-requirement-failed']"
Availability,"### What happened?. Batch workers appear to take ~2 minutes to start up but; ```; time gcloud compute instances create --machine-type n1-standard-16 dk-test --zone us-central1-a; ```. Takes 9.389s. This task is complete when:; 1. We know the average time between a create API call and the worker accepting its first job.; 2. We know, down to 5 second granularity what is blocking the worker from starting.; 3. We have reduced the average total time to 50% of the value in (1). ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13976:334,down,down,334,https://hail.is,https://github.com/hail-is/hail/issues/13976,1,['down'],['down']
Availability,"### What happened?. Below is a high level overview of how the batch driver communicates scheduled jobs to worker nodes. Scheduling loop on the driver:; 1. Select N ready jobs from the database to schedule on available workers; 2. Compute placement of a subset of the jobs in available slots in the worker pool; 3. Concurrently call `/api/v1alpha/batches/jobs/create` on available workers for each placed job. If/when the request completes successfully, the job is marked as scheduled.; 4. Once all requests complete, goto 1. On the worker, what happens inside `/api/v1alpha/batches/jobs/create`:; 1. Read metadata describing the job to schedule from the request body; 2. Using that information, load the full job spec from blob storage; 3. Spawn a task to run the job asynchronously; 4. Respond to the driver with a 200. The key point relevant to this issue is that the driver currently must wait for all the requests to workers in an iteration to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:208,avail,available,208,https://hail.is,https://github.com/hail-is/hail/issues/14456,3,['avail'],['available']
Availability,"### What happened?. Ben W reports that he can reliably cause a batch worker VM to become non-responsive, triggering the driver to kill the VM, and the job to get rescheduled. https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/workers.20which.20suddenly.20stop.20responding/near/400852561. This ticket is complete when:; 1. We have reproduced Ben's behavior on a main commit before or including 06183480d2. ; 2. We have reduced Ben's test case to something we can add as a test. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13992:46,reliab,reliably,46,https://hail.is,https://github.com/hail-is/hail/issues/13992,1,['reliab'],['reliably']
Availability,### What happened?. Ben tried to submit a batch with 0.2.126 and it kept failing with errors due to not able to enter into a task. A quick google search showed this could be a bad nest_asyncio interaction. I recommended downgrading to 0.2.120 before this possibly related PR (#13614) went in and that unblocked Ben. I could not replicate this behavior on 3.9 or 3.11 on my laptop with 0.2.126. This error occurred on a fresh install for Ben for 3.9 and 3.11. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/RuntimeError.3A.20Cannot.20enter.20into.20task/near/404939884. ### Version. 0.2.126. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051:86,error,errors,86,https://hail.is,https://github.com/hail-is/hail/issues/14051,3,"['down', 'error']","['downgrading', 'error', 'errors']"
Availability,"### What happened?. Currently, Query on Batch waits for the full stage of workers to complete before collecting the results even in the event of a failure. Now that Query on Batch uses Job Groups, we can use the job group `cancel_after_n_failures` functionality to cancel remaining jobs in the stage after a certain number have failed. Query on Batch should set `cancel_after_n_failures = 1` so that the user can see the error of the failed partition without waiting for all the partitions to run (and paying for them). ### Version. 0.2.131. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14597:147,failure,failure,147,https://hail.is,https://github.com/hail-is/hail/issues/14597,2,"['error', 'failure']","['error', 'failure']"
Availability,"### What happened?. Currently, interacting with the billing report is a very manual process with lots of room for error. We could improve this with:. - ""This month"" / ""last month"" / ""last week"" auto-filter buttons (alongside the current manual date-entry fields); - Hide `trials`, hide `_tests`, hide `ci`, hide `benchmark`, hide `< $0.01`? (Need to be manually filtered out during a billing export); - ""Copy table contents"" button. Puts the current table contents into the user's clipboard in CSV format. Replacing the current manual ""Drag-select and copy"" action with a single button press. ### Version. Current. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14658:114,error,error,114,https://hail.is,https://github.com/hail-is/hail/issues/14658,1,['error'],['error']
Availability,"### What happened?. Dear developers,. During the process of reading a large set of VCF files from an exome sequencing study into Hail (version 0.2.126-ee77707f4fab run on Terra container terra-jupyter-hail:1.1.8 using Spark 3.3.0), I ran into an unexpected error. Using a script I have been using for years for sequence datasets, I now run into an error during parsing of certain lines from the VCF file. Specifically, the Hail outputs the error ""cannot set missing field for required type +PFloat64"". I was concerned that there may have been issues in the actual VCF file, and therefore I tested the script on older datasets that I managed to process without any problem previously; the error was recapitulated on all these old datasets. Therefore I do not think there is an intrinsic issue in the VCFs, but rather in the way the current version of Hail (inside the terra container) is parsing information from the lines. I was not capable of running older versions of the Terra container (1.0.x) because the versions of Hail implemented there are not compatible with the current version of Spark on Terra. . I hope you may have a solution to this irritating problem. I have added the scripts and logs below. . Thanks in advance,; Sean Jurgens. ### Version. 0.2.126-ee77707f4fab. ### Relevant log output. ```shell; ## PLEASE NOTE: to protect privacy as much as possible, I have removed almost all entries shown by the code, and for the few line entries that remain I have changed/randomized the numeric values. The order and structure is preserved for enrtries nonetheless. `#import libraries; import os; import hail as hl; from pprint import pprint. #### Start hail; hl.init(); hl.spark_context()`. /opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:44: UserWarning:. Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:257,error,error,257,https://hail.is,https://github.com/hail-is/hail/issues/14102,4,['error'],['error']
Availability,"### What happened?. Double quote `""""` is frequently used to mean just one `""` when it appears inside a quoted field a la:. ```; a	b; 1	""""""""; ```; This contains one row whose value for column a is `1` and whose value for column b is `""`. The outer quotes are redundant indicators of the bounds of that column for that row. A less trivial case involves having a tab inside the column:. ```; a	b; 1	""	""""	""; ```; In this file, the b column's value in the first row has length three and consists of a tab, a quote character and a tab: `	""	`. Another test case. The `test.txt` contains:; ```; a	b	c; ""hello"",""a""""b"",""goodbye""; ```; This code,; ```python3; hl.import_table(""test.txt"", quote='""').collect(); ```; should return:; ```python3; [hl.Struct(a=""hello"", b=""a\""b"", c=""goodbye"")]; ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13563:258,redundant,redundant,258,https://hail.is,https://github.com/hail-is/hail/issues/13563,1,['redundant'],['redundant']
Availability,"### What happened?. Due to limited GPU availability, it is common for GPU private jobs (esp. preemptible) to fail multiple times with exhausted resource errors before obtaining a VM. When this happens, Batch still changes for the attempt. An example is batch 8166586, job 1, attempt ZMkGaS, instance ID batch-worker-default-job-private-u4fxc which failed with ZONE_RESOURCE_POOL_EXHAUSTED. ### Version. SaaS. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14505:39,avail,availability,39,https://hail.is,https://github.com/hail-is/hail/issues/14505,2,"['avail', 'error']","['availability', 'errors']"
Availability,### What happened?. Example failure: https://batch.azure.hail.is/batches/3883658 (nb: driver job failed even though its marked success). User report: https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/problem.20writing.20output.3F. ### Version. 0.2.116. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13061:28,failure,failure,28,https://hail.is,https://github.com/hail-is/hail/issues/13061,1,['failure'],['failure']
Availability,"### What happened?. Figure out why the k8s cache fails. Is this due to asyncio task cancellation? Is it a known rare transient error?. If this is a rare transient error, we should retry this a limited number of times. Example: https://batch.hail.is/batches/8071211/jobs/186. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13909:127,error,error,127,https://hail.is,https://github.com/hail-is/hail/issues/13909,2,['error'],['error']
Availability,"### What happened?. Filtering a locus-keyed dataset will throw an assertion error if any comparison operator other than equality is used on the contig, e.g. `mt.locus.contig != 'X'`. The relevant assertion is [here](https://github.com/hail-is/hail/blob/728f43bab4a474442b61d746e1881fa450f7ade5/hail/src/main/scala/is/hail/expr/ir/ExtractIntervalFilters.scala#L644). We should add support for at least not-equals. Inequalities would technically also make sense to add, but are probably not likely to be used. If we don't add them, we should at least improve the error. ### Version. 0.2.127. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14288:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/14288,2,['error'],['error']
Availability,### What happened?. From Mike Wilson on hail zulip [here](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/hail.200.2E2.2E132.20streamconstraintsexception/near/453934859). > I'm running the new vds combiner on ~340 DRAGEN gVCFs and have hit this error; >; > ```; > Error summary: StreamConstraintsException: String length (20054016) exceeds the maximum length (20000000); > ```. ### Version. 0.2.132. ### Relevant log output. ```shell; Full java stack trace:. Java stack trace:; com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20054016) exceeds the maximum length (20000000); at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324); at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27); at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939); at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2584); at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishAndReturnString(UTF8StreamJsonParser.java:2560); at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.getText(UTF8StreamJsonParser.java:335); at is.hail.relocated.org.json4s.jackson.JValueDeserializer._deserialize$1(JValueDeserializer.scala:26); at is.hail.relocated.org.json4s.jackson.JValueDeserializer._deserialize$1(JValueDeserializer.scala:48); at is.hail.relocated.org.json4s.jackson.JValueDeserializer.deserialize(JValueDeserializer.scala:57); at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323); at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2105); at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1481); at is.hail.relocated.org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:35); at is.hail.relocated.org.json4s.jackson.JsonMethods.parse$(J,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14650:277,error,error,277,https://hail.is,https://github.com/hail-is/hail/issues/14650,2,"['Error', 'error']","['Error', 'error']"
Availability,### What happened?. GCS library throws a `StorageException: Unknown Error` on 503s resulting in the below stacktrace. Such a transient error should be gracefully retried. ### Version. 0.2.124. ### Relevant log output. ```shell; hail.utils.java.FatalError: NullPointerException: null. Java stack trace:; is.hail.relocated.com.google.cloud.storage.StorageException: Unknown Error; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_analysis/o?name=250k/data/utils/aou_variant_qc_250k.ht/index/part-57205-e0113aa0-c1e8-43fc-af14-ccb68d989bd5.idx/index&uploadType=resumable&upload_id=ABPtcPrw7n_weAuHvL4cEyCdL-JKVVX-HaG7fnwAjTgRn4Uxm0JdIcWYasCHyuvK36Fc1UgVJkDC8kvlFgWcDkBcEy-_jxjQZpEFxJb2W8gLRkOavA; 	|> content-range: bytes 0-50129/50130; 	|> x-goog-gcs-idempotency-token: 5e36e53c-5dce-4690-844b-2cfd6f553861; 	| ; 	|< HTTP/1.1 503 Service Unavailable; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ABPtcPrw7n_weAuHvL4cEyCdL-JKVVX-HaG7fnwAjTgRn4Uxm0JdIcWYasCHyuvK36Fc1UgVJkDC8kvlFgWcDkBcEy-_jxjQZpEFxJb2W8gLRkOavA; 	| ; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScenario.java:185); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScenario.java:117); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScenario.java:106); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:224); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(Retry,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:68,Error,Error,68,https://hail.is,https://github.com/hail-is/hail/issues/13937,3,"['Error', 'error']","['Error', 'error']"
Availability,"### What happened?. Hail propagates nicely explained error messages from java to python when an exception is thrown in the user's pipeline. However, the hail python front end does not handle a situation where the java backend disappears entirely, which can happen in the case of an OOM killer killing the JVM. The result is an error as seen below. In such a scenario, the python front end should add a useful message suggesting that the backend is not reachable and might have run out of memory. ### Version. 0.2.130. ### Relevant log output. ```shell; File ~/Library/Python/3.9/lib/python/site-packages/hail/table.py:2814, in Table.collect(self, _localize, _timed); 2812 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 2813 if _localize:; → 2814 return Env.backend().execute(e._ir, timed=_timed); 2815 else:; 2816 return e. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), ‘{“name”:“StreamBufferSpec”}’, timed); 187 try:; → 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/Library/Python/3.9/lib/python/site-packages/hail/backend/py4j_backend.py:218, in Py4JBackend._rpc(self, action, payload); 216 path = action_routes[action]; 217 port = self._backend_server_port; → 218 resp = self._requests_session.post(f’http://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""“Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (…); 634 :rtype: requests.Response; 635 “””; → 637 return self.request(“POST”, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/issues/14557,2,['error'],['error']
Availability,"### What happened?. Hello,. I installed hail into an empty, new Python 3.12.2 virtual environment, and was not able to import it. I see a failure like this:. ```; (venv) (py312) alex@rpi400:~/hail $ python; Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:38:53) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/alex/hail/venv/lib/python3.12/site-packages/hail/__init__.py"", line 2, in <module>; import pkg_resources; ModuleNotFoundError: No module named 'pkg_resources'; ```. It looks like in Python 3.12, the bundled setuptools was removed and new virtual environments will not have setuptools in them, it needs to be specifically installed through pip: https://github.com/python/cpython/issues/95299. This could be fixed either by adding `setuptools` to hail's requirements so that it will be installed when users install hail, or hail could remove usage of setuptools & its associated modules (`pkg_resources`) at runtime, as some other projects have done: https://github.com/TDAmeritrade/stumpy/issues/950. At a glance, the cleanest thing to do here may be to move off of the deprecated `pkg_resources` and to the recommended `importlib` if it has what you need: https://setuptools.pypa.io/en/latest/pkg_resources.html. I also have to admit that I discovered this while playing around with hail on a Raspberry Pi 4, so it is possible that something else broken caused this failure, but I believe I understand what's happening. Here's my full `pip freeze` for reference:. ```; (venv) (py312) alex@rpi400:~/hail $ pip freeze; aiodns==2.0.0; aiohttp==3.9.3; aiosignal==1.3.1; attrs==23.2.0; avro==1.11.3; azure-common==1.1.28; azure-core==1.30.1; azure-identity==1.15.0; azure-mgmt-core==1.4.0; azure-mgmt-storage==20.1.0; azure-storage-blob==12.19.1; bokeh==3.4.0; boto3==1.34.73; botocore==1.34.73; cachetools==5.3.3; certifi==2024.2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14428:138,failure,failure,138,https://hail.is,https://github.com/hail-is/hail/issues/14428,1,['failure'],['failure']
Availability,"### What happened?. Hello,; I have some gwas results that I have converted into a pandas dataframe. Typically with dataframes I pickle my outputs for speed and easily maintaining data types. Within All of Us we have separate analysis environments whether we're using hail or not. The environment that doesn't have hail is much cheaper for simple analyses and does not have pyspark installed. You can see in the error below when I try to reread the pickled dataframe, I get an error that it can't find the pyspark from within a hail module. If I write the dataframe as a csv, read it back in, and then pickle it then the error goes away. This suggests to me that the dataframe created by hail maintains reference to hail objects and pandas is attempting to recreate these objects when unpickling. I suspect this is not intentional. ```python; # Hail environment; vat_simplified_file = os.path.join(bucket, 'vat.ht'); gwas = hl.read_table(gwas_results_file_no_sex_chr); vat = hl.read_table(vat_simplified_file); gwas = gwas.filter(gwas.p_value <= 1e-4); combined = gwas.join(vat, how='left'); combined_pandas = combined.to_pandas(). gwas_pandas_file = os.path.join(bucket, 'gwas_results.pkl'); combined_pandas.to_pickle(gwas_pandas_file); ```. ```python; # Non hail environment without pyspark; combined_pandas = pd.read_pickle(gwas_pandas_file). ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 216 # expected ""IO[bytes]""; --> 217 return pickle.load(handles.handle) # type: ignore[arg-type]; 218 except excs_to_catch:. /opt/conda/lib/python3.7/site-packages/hail/__init__.py in <module>; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:411,error,error,411,https://hail.is,https://github.com/hail-is/hail/issues/14004,3,['error'],['error']
Availability,"### What happened?. Hi,; I am on a macOS Ventura and I have successfully installed hail (v 0.2.109) on a conda env. Everything seems to run properly, except that I don't get any plots. Bokeh was installed in the env, v1.4.0., pysark =3.13 and scala=2.11.8 are some relevant packages that may contribute to this issue. When starting Hail, this is the output I get:. 2023-02-20 11:07:38.798 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://amaru-2.local:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.109-b71b065e4bb6; LOGGING: writing to /Users/alanmejiamaza/hail-20230220-1107-0.2.109-b71b065e4bb6.log. It seems to be that the issue comes from the spark version? which is the correct spark version for a conda env on a mac? I have followed the tutorials and seemed to work fine except for the plots. I don't have any output when invoking commands for plots. Can anyone tell me the specific versions needed to run all Hail properties?. Thanks. ### Version. 0.2.109. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717:698,avail,available,698,https://hail.is,https://github.com/hail-is/hail/issues/12717,1,['avail'],['available']
Availability,"### What happened?. I added ""Huang"" to a billing project. This user name does not exist. It should have given me an error. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13858:116,error,error,116,https://hail.is,https://github.com/hail-is/hail/issues/13858,1,['error'],['error']
Availability,"### What happened?. I am trying to install Hail v0.2.120 on AWS EMR 6.9.0. Versions:; - Python 3.8.16; - Java 1.8.0; - Spark 3.3.0. After updating Python to 3.8 and cloning hail repo, I compile hail using the command below. ```sh; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; ```. Here I get an error. ```sh ; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.aWUFJ1BMnP; ../check_pip_requirements.sh: line 13: pip-compile: command not found; ```. While I do have pip-compile installed. ```sh ; pip-compile --help; Usage: pip-compile [OPTIONS] [SRC_FILES]... Compiles requirements.txt from requirements.in, pyproject.toml, setup.cfg,; or setup.py specs. Options:; ```. Note that `make clean` did not solve the issue. see logs attached. ### Version. 0.2.120. ### Relevant log output. ```shell; BUILD SUCCESSFUL in 2m 46s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; /usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type'; warnings.warn(msg); installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.120.dist-info/WHEEL; creating 'dist/hail-0.2.120-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:345,error,error,345,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['error'],['error']
Availability,"### What happened?. I executed; ```; vds = hl.vds.filter_intervals(vds, pca_snps, keep=True); ```; And got this error. `pca_snps` is a Table. ; ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1640>"", line 2, in filter_intervals; File ""/Users/juliasealock/opt/miniconda3/lib/python3.9/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/juliasealock/opt/miniconda3/lib/python3.9/site-packages/hail/vds/methods.py"", line 739, in filter_intervals; return _parameterized_filter_intervals(vds, intervals, keep=keep,; File ""<decorator-gen-1636>"", line 2, in _parameterized_filter_intervals; File ""/Users/juliasealock/opt/miniconda3/lib/python3.9/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/juliasealock/opt/miniconda3/lib/python3.9/site-packages/hail/vds/methods.py"", line 613, in _parameterized_filter_intervals; ref_intervals = intervals.map(; AttributeError: 'list' object has no attribute 'map'; ```. ### Version. 0.2.113. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12920:112,error,error,112,https://hail.is,https://github.com/hail-is/hail/issues/12920,1,['error'],['error']
Availability,"### What happened?. I expected this to not error but it did.; ```python3; import hail as hl. x = [hl.import_vcf(f, force_bgz=True, reference_genome='GRCh38', min_partitions=k).rows().select() for f, k in (; ('hail/src/test/resources/gvcfs/HG00096.g.vcf.gz', 100),; ('hail/src/test/resources/gvcfs/HG00268.g.vcf.gz', 1); )]; hl.Table.multi_way_zip_join(x, 'data', 'new_globals').write('/tmp/foo.ht', overwrite=True); ```; The error:; ```; 2024-02-02 15:39:35.977 Hail: INFO: scanning VCF for sortedness...; 2024-02-02 15:39:37.571 Hail: INFO: Coerced sorted VCF - no additional import work to do; 2024-02-02 15:39:38.925 Hail: INFO: wrote table with 234687 rows in 1 partition to /tmp/__iruid_1841-E0rqVWB0ysj7E0SIeJeumv; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[2], line 7; 1 import hail as hl; 3 x = [hl.import_vcf(f, force_bgz=True, reference_genome='GRCh38', min_partitions=k).rows().select() for f, k in (; 4 ('hail/src/test/resources/gvcfs/HG00096.g.vcf.gz', 100),; 5 ('hail/src/test/resources/gvcfs/HG00268.g.vcf.gz', 1); 6 )]; ----> 7 hl.Table.multi_way_zip_join(x, 'data', 'new_globals').write('/tmp/foo.ht', overwrite=True). File <decorator-gen-1242>:2, in write(self, output, overwrite, stage_locally, _codec_spec). File ~/projects/hail/hail/python/hail/typecheck/check.py:584, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 581 @decorator; 582 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 583 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 584 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/table.py:2002, in Table.write(self, output, overwrite, stage_locally, _codec_spec); 1976 """"""Write to disk.; 1977 ; 1978 Examples; (...); 1997 If ``True``, overwrite an existing file at the destination.; 1998 """"""; 2000 hl.current_backend().validate_file(output); -> 2002 Env.backend().",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/issues/14245,2,['error'],['error']
Availability,"### What happened?. I expected this to raise an error with a message like ""when indexing a matrix table, you must provide a row key and column key"".; ```; In [3]: import hail as hl; ...: mt = hl.balding_nichols_model(1, 1, 1); ...: mt2 = hl.balding_nichols_model(1,1,1); ...: ; ...: mt.annotate_rows(x=mt2[mt.locus, mt.alleles]); 2024-02-01 13:16:23.573 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1 samples, and 1 variants...; 2024-02-01 13:16:23.594 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1 samples, and 1 variants...; ---------------------------------------------------------------------------; ExpressionException Traceback (most recent call last); Cell In[3], line 5; 2 mt = hl.balding_nichols_model(1, 1, 1); 3 mt2 = hl.balding_nichols_model(1,1,1); ----> 5 mt.annotate_rows(x=mt2[mt.locus, mt.alleles]). File ~/projects/hail/hail/python/hail/matrixtable.py:818, in MatrixTable.__getitem__(self, item); 815 col_key = wrap_to_tuple(exprs[1]); 817 try:; --> 818 return self.index_entries(row_key, col_key); 819 except TypeError as e:; 820 raise invalid_usage from e. File ~/projects/hail/hail/python/hail/matrixtable.py:3193, in MatrixTable.index_entries(self, row_exprs, col_exprs); 3191 return self.index_entries(tuple(row_exprs[0].values()), col_exprs); 3192 elif len(row_exprs) != len(self.row_key):; -> 3193 raise ExpressionException(; 3194 f'Key mismatch: matrix table has {len(self.row_key)} row key fields, '; 3195 f'found {len(row_exprs)} index expressions'; 3196 ); 3197 else:; 3198 raise ExpressionException(; 3199 f""Key type mismatch: Cannot index matrix table with given expressions\n""; 3200 f"" MatrixTable row key: {', '.join(str(t) for t in self.row_key.dtype.values())}\n""; 3201 f"" Row index expressions: {', '.join(str(e.dtype) for e in row_exprs)}""; 3202 ). ExpressionException: Key mismatch: matrix table has 2 row key fields, found 1 index expressions; ```. ### Version. 0.2.127. ### Relevant log output. _No r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14237:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/14237,1,['error'],['error']
Availability,### What happened?. I expected to see no error messages in the logs for a JVM container. Instead I found errors relating to assertion errors when setting up log4j. https://console.cloud.google.com/logs/query;query=%2528%0Aresource.type%3D%22gce_instance%22%0AlogName:%22jvm-2%22%0Alabels.%22compute.googleapis.com%2Fresource_name%22:%22batch-worker-default-standard-np-xj3g%22%0A%2529;timeRange=PT1H;summaryFields=:false:32:beginning;cursorTimestamp=2023-07-13T18:49:03.668876534Z?project=hail-vdc. ### Version. 0.2.119. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/issues/13242,3,['error'],"['error', 'errors']"
Availability,"### What happened?. I expected when I submitted a Batch job with `attributes={'foo': 1}` that it would return an error telling me the values must be strings. Instead, I got a 500 error from the server. ### Version. 0.2.128. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/aiohttp/web_protocol.py"", line 452, in _handle_request; resp = await request_handler(request); File ""/usr/local/lib/python3.9/dist-packages/aiohttp/web_app.py"", line 543, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.9/dist-packages/aiohttp/web_middlewares.py"", line 114, in impl; return await handler(request); File ""/usr/local/lib/python3.9/dist-packages/aiohttp/web_middlewares.py"", line 114, in impl; return await handler(request); File ""/usr/local/lib/python3.9/dist-packages/gear/csrf.py"", line 27, in check_csrf_token; return await handler(request); File ""/usr/local/lib/python3.9/dist-packages/batch/utils.py"", line 19, in unavailable_if_frozen; return await handler(request); File ""/usr/local/lib/python3.9/dist-packages/gear/metrics.py"", line 28, in monitor_endpoints_middleware; response = await prom_async_time(REQUEST_TIME.labels(endpoint=endpoint, verb=verb), handler(request)) # type: ignore; File ""/usr/local/lib/python3.9/dist-packages/prometheus_async/aio/_decorators.py"", line 55, in measure; rv = await future; File ""/usr/local/lib/python3.9/dist-packages/aiohttp_session/__init__.py"", line 199, in factory; response = await handler(request); File ""/usr/local/lib/python3.9/dist-packages/gear/auth.py"", line 67, in wrapped; return await fun(request, userdata); File ""/usr/local/lib/python3.9/dist-packages/batch/utils.py"", line 45, in wrapped; return await fun(request, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/front_end/front_end.py"", line 1531, in create_batch_fast; await _create_job_groups(db, batch_id, update_id, user, job_groups); File ""/usr/local/lib/python3.9/dist-packages/batc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14350:113,error,error,113,https://hail.is,https://github.com/hail-is/hail/issues/14350,2,['error'],['error']
Availability,"### What happened?. I installed from PyPI and obtained hail 0.2.132. I made sure I used a completely clean environment with nothing in it (using pixi). . When I did . ```; import hail; ```. I got this error:. ```; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/__init__.py"", line 40, in <module>; from hail.utils import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/utils/__init__.py"", line 4, in <module>; from .hadoop_utils import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/utils/hadoop_utils.py"", line 7, in <module>; from hail.fs.hadoop_fs import HadoopFS; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hail/fs/hadoop_fs.py"", line 8, in <module>; from hailtop.fs.fs import FS; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/fs/__init__.py"", line 1, in <module>; from .fs_utils import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/fs/fs_utils.py"", line 4, in <module>; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiocloud/aiogoogle/__init__.py"", line 1, in <module>; from .client import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiocloud/aiogoogle/client/__init__.py"", line 8, in <module>; from .storage_client import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiocloud/aiogoogle/client/storage_client.py"", line 14, in <module>; from hailtop.aiotools import FeedableAsyncIterable, WriteBuffer; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3.12/site-packages/hailtop/aiotools/__init__.py"", line 1, in <module>; from .fs import (; File ""/Users/srinivas/test/.pixi/envs/default/lib/python3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14630:201,error,error,201,https://hail.is,https://github.com/hail-is/hail/issues/14630,1,['error'],['error']
Availability,"### What happened?. I've been trying to run VEP on ~3k variants using QoB but keep getting an output: 'ERROR: could not find log file' error (full error attached). I was able to run chr1 and chr2 separately but when I tried, I still got the same error message.; [vep_batch_error.txt](https://hail.zulipchat.com/user_uploads/4771/FwakSVmKTEQI7695UCsdF1fw/vep_batch_error.txt). I've also tried running it on dataproc but it takes hours and won't progress, which is weird for only 3k variants. trying again now on dataproc with high memory machines. The code I'm using is:; ```python3; import hail as hl; hl.init(gcs_requester_pays_configuration='daly-neale-sczmeta', driver_cores=8, driver_memory='highmem'); HT = 'gs://schema_jsealock/de_novo_analysis/schema1_de_novo_variants_grch38.ht'. ht = hl.read_table(HT).key_by(); ht = ht.key_by(ht.locus, ht.alleles).select(); ht_vep = hl.vep(ht, ""gs://hail-us-vep/vep95-GRCh38-loftee-gcloud.json""); ht_vep.write(""gs://schema_jsealock/de_novo_analysis/downsampled_vep95_annotated_de_novo_variants.ht"", overwrite=True); ```. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989:103,ERROR,ERROR,103,https://hail.is,https://github.com/hail-is/hail/issues/13989,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"### What happened?. In #14675 I replaced `END` with `LEN` in VDS. In doing so, I made sure that both fields were present so as to not break people's existing pipelines. I added a hidden `_drop_end` flag to `read_vds` in order to be able to (mostly in the combiner) not have the `END` field present. This lead to a strange code pattern:. https://github.com/chrisvittal/hail/blob/f39364c177e0b009589826b2c6b3cd36c3ec359d/hail/python/hail/vds/variant_dataset.py#L44-L46. When running the final VDS+VDS merge in [`test_combiner_run`](https://github.com/chrisvittal/hail/blob/f39364c177e0b009589826b2c6b3cd36c3ec359d/hail/python/test/hail/vds/test_combiner.py#L178-L222) on the local backend, this failed with a memory error (in debug mode):. ```; RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; ```. Applying this patch fixed `test_combiner_run`:; ```patch; diff --git a/hail/python/hail/vds/variant_dataset.py b/hail/python/hail/vds/variant_dataset.py; index 0f851e7364..01be83a982 100644; --- a/hail/python/hail/vds/variant_dataset.py; +++ b/hail/python/hail/vds/variant_dataset.py; @@ -41,9 +41,14 @@ def read_vds(; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). - reference_data = VariantDataset._add_len_end(reference_data); + # if LEN is missing, add it, _add_len is a no-op if LEN is already present; + reference_data = VariantDataset._add_len(reference_data); if _drop_end:; - reference_data = reference_data.drop('END'); + if 'END' in reference_data.entry:; + reference_data = reference_data.drop('END'); + else: # if END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:714,error,error,714,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['error'],['error']
Availability,"### What happened?. In older versions of hail (tested with 0.2.115), when starting a dataproc cluster with VEP, e.g.; ```{bash}; hailctl dataproc start hail-test --region australia-southeast1 --project my-project --vep GRCh38 --packages gnomad --num-workers 2; ```; the dataproc cluster command would be provided the following environment variable through the `--metadata` flag: `VEP_REPLICATE=aus-sydney`. This variable is used within the script `gs://hail-common/hailctl/dataproc/0.2.115/vep-GRCh38.sh` to determine which bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:886,error,error,886,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['error'],['error']
Availability,"### What happened?. It seems that the local filesystem can, infrequently, stall when executing `rmtree`. Note that the error about the directory being non-empty is because we have a bug in `rm_dir`: we try to remove the directory even if the children tasks failed. It oddly seems to have happened on both a deploy batch and a PR batch:; - PR: https://ci.hail.is/batches/7706444/jobs/170; - deploy: https://ci.hail.is/batches/7707793/jobs/172. ```; [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] PASSED; +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_1 (139802083059456) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:119,error,error,119,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['error'],['error']
Availability,"### What happened?. Job cancellation by users or the batch system is a normal operation, yet error logs are emitted when the operation occurs. This will raise false alarms as alerts are sent when error logs are observed. The batch driver, and the system moreover, should not emit ERROR logs for expected operations. ### Version. 0.2.124. ### Relevant log output. ```shell; Example log:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1013, in _run_until_done_or_deleted; return await run_until_done_or_deleted(self.deleted_event, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 681, in run_until_done_or_deleted; raise StepInterruptedError; StepInterruptedError. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1888, in run_container; await container.run(on_completion); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 918, in run; await self.wait(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 904, in wait; await self._run_fut; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1015, in _run_until_done_or_deleted; raise ContainerDeletedError from e; ContainerDeletedError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13803:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/issues/13803,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"### What happened?. Julia Sealock reported this https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/vep.20issue/near/352790173. We also saw it in test_dataproc. Cal also reported it.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collectio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:275,failure,failure,275,https://hail.is,https://github.com/hail-is/hail/issues/12936,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"### What happened?. Lindo tried to use JobResourceFiles a second time after updating the original batch, but got `FileNotFoundError`. This is because the default behavior is to delete temporary files with `b.run()`. We should add this use case to our documentation, but it might also be a good idea to eagerly catch these errors if possible and provide a better error message. I don't think we can change the default value at this point. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/File.20dependency.20error/near/416647170. ### Version. 0.2.127. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14177:322,error,errors,322,https://hail.is,https://github.com/hail-is/hail/issues/14177,2,['error'],"['error', 'errors']"
Availability,"### What happened?. No hail log file is available. > On 0.2.109: 5k samples and 8 interval lists -- WORKED; > 5k samples and 1 interval list -- WORKED; > On 0.2.120: 2k samples and 1 interval list -- WORKED; > On 0.2.120: 2k samples and 2 interval lists -- WORKED; > On 0.2.120: 2k samples and 4 interval list -- ERROR; > On 0.2.120: 2k samples and 8 interval list -- ERROR (edited); > ; > All of these runs were on driver: 96 CPU/684G RAM; > Workers 4 CPU and 8GB RAM; > Spark configuration allocated 512GB for driver; > ; > I have tried the above in various configurations... Maybe a specific interval list is problematic, but that does not seem to be the case; > ; > The interval lists are the same across runs.; > ; > And lastly, the error is the usual Py4J Error. Usually I address this w/ more driver RAM, but I can't go any higher and this used to work fine in Hail 0.2.109.; > ; > I tried downgrading from 120-->109, but I don't believe that I can in Terra, due to Spark incompatibilities. > filtered_mt is a MatrixTable that has already been split and filtered (to drop irrelevant variants). By the time the [following] code blocks are run, `filtered_mt = hl.read_matrix_table(filtered_mt_url)` has been executed.; > Some more information: The code after this (not shown [in the below code blocks]) does additional filtering. If I skip the step `variant_data.export(f""{variant_stat_file_path_stem}_FULL.tsv"")`, I can complete successfully. The issue is that we need the `*_FULL.tsv` output. So, I believe that this is likely a RAM issue on the driver, but this used to work. ```; variant_mt = generate_variant_stats(filtered_mt, interval_names, interval_table_dict). # Main loop to compute variant stats and save to files. # File path stem to use for saving variant stats over different interval lists; variant_stat_file_path_stem = f""{bucket}/batchE/{workflow_nickname}/variant_stats"". variant_data = variant_mt.cols(); variant_data.describe(); #variant_data.to_pandas().to_csv(f""{variant_st",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:40,avail,available,40,https://hail.is,https://github.com/hail-is/hail/issues/13960,6,"['ERROR', 'Error', 'avail', 'down', 'error']","['ERROR', 'Error', 'available', 'downgrading', 'error']"
Availability,"### What happened?. Note: this is an Azure-specific issue. When submitting a batch/job that requests more storage than is available on the temp disk of any standing worker, but doesn't request a specific number of cores or amount of memory, a NotImplementedError is raised in `batch/cloud/azure/worker/disk.py`. See this Batch record for an example of the issue in action: https://batch.azure.hail.is/batches/4563654/jobs/1. The corresponding base case to reproduce this is:. ```python; import hailtop.batch as hb; backend = hb.ServiceBackend(billing_project=""<YOUR BILLING PROJECT>""); b = hb.Batch(backend=backend, name=""storage_test""); j = b.new_job(); j.image(""ubuntu:20.04""); j.storage(""700GiB""); j.command(""df -h""); b.run(wait=False); ```. On the cluster azure.hail.is this job gets scheduled on a `Standard_D16ds_v4` instance which has a 600 GiB temp disk. On GCP, when requests exceed this amount a data disk is provisioned to service the request. While this is feasible on Azure and could be implemented, it may not be the recommended solution as temp disks are much better suited to ephemeral workloads than data disks. On clusters with a smaller standing worker (i.e. fewer cores) there is a workaround, which also possibly suggests a reasonable partial solution. This workaround is to specify a required number of cores that forces a larger VM of the same family to be provisioned. This makes a larger temp disk available for the job to leverage. The corresponding partial solution would be to take knowledge of the temp disk size for any VM into account when scheduling jobs and provision larger VMs when warranted by the storage requirement of a job. . Based on current limitations for VM core count (16) this suggests a ceiling on storage that can be allocated to any job in Azure of 600 GiB. At that point it would be necessary to allocate a data disk. This issue reproduces on both azure.hail.is and our own Azure cluster.; . ### Version. 0.2.126-cdd2c132bfa2. ### Relevant log output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14522:122,avail,available,122,https://hail.is,https://github.com/hail-is/hail/issues/14522,1,['avail'],['available']
Availability,"### What happened?. On GNU/Linux, when spawning a process, there are practical limits to the size of the arguments array *and* the environment. These limits are documented in [the `execve` man page](https://man7.org/linux/man-pages/man2/execve.2.html). Bioinformatics tools sometimes work around this limitation by accepting a single file which contains new-line separated arguments. For example: [`bcftools --file-list NAME ...`](https://samtools.github.io/bcftools/bcftools.html). The concrete proposal was to provide an operation like `hb.file_list`:; ```python3; j.command(f'bcftools --file-list {hb.file_list(resources_list)}'); ```. The question is how to construct this file list. We could generate code to create the file but this would still fail if there were so many resources that the environment was too large. A really resilient way to do this would be for the Batch worker to mount a list of filenames into the container as a file. That gives users a robust way to project a very large number of resource filenames into the container. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14356:833,resilien,resilient,833,https://hail.is,https://github.com/hail-is/hail/issues/14356,2,"['resilien', 'robust']","['resilient', 'robust']"
Availability,"### What happened?. On startup, a Batch Worker pre-allocates network namespaces equal to the maximum number of slots on the worker (64 for a 16 core worker). When a job finishes, its network namespace is deleted and a task is created to replenish the namespace. This way, jobs seldom need to wait on the creation of a network namespace and there *should* always be one available or creating. However, sometimes the creation of a new network namespace is disrupted, causing new jobs to hang indefinitely waiting on one. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13402:369,avail,available,369,https://hail.is,https://github.com/hail-is/hail/issues/13402,1,['avail'],['available']
Availability,"### What happened?. On the driver (but this could happen anywhere), a `read` call failed in GoogleStorageFS. In particular line 205:; ```; if (reader != null) {; reader.read(bb); } else {; ```; We don't retry transient errors here or below in the other call to `read`. We only retry on the initial creation of the stream. I think we are concerned that the stream is in a bad state, possible advanced a few bytes. If we were to read from it, we might drop some data. The safe thing to do is to `seek` to the correct position. This will likely initiate a new HTTP request to GCS, which is fine, because we almost certainly lost the old connection due to the transient error. I also think we need to remove `lazyPosition`. I think we can achieve the requester pays nonsense by just relying on the `pos` from the parent class (see FS.scala). ### Version. 0.2.115-71fc978b5c22. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 531, in <module>; main(subs=args.s, date=processed_date, variants=args.v, out=args.o); File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 505, in main; parse_into_table(json_path=temp_output, out_path=out); File ""/usr/local/lib/python3.10/site-packages/reanalysis/summarise_clinvar_entries.py"", line 439, in parse_into_table; ht.write(out_path, overwrite=True); File ""<decorator-gen-1106>"", line 2, in write; File ""/usr/local/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/lib/python3.10/site-packages/hail/table.py"", line 1392, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/usr/local/lib/python3.10/site-packages/hail/backend/service_backend.py"", line 490, in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=timed)); File",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:219,error,errors,219,https://hail.is,https://github.com/hail-is/hail/issues/12983,2,['error'],"['error', 'errors']"
Availability,"### What happened?. Removing the FIXME messages in `hailtop.batch_client.aioclient` in favor of a proper issue. This error needs to be fixed for `hailtop.batch_client.aioclient.JobGroup` as well once that code merges with #14282. ```python3; # FIXME Error if this is called while within a job of the same Batch; async def wait(; self,; *,; disable_progress_bar: bool = False,; description: str = '',; progress: Optional[BatchProgressBar] = None,; starting_job: int = 1,; ) -> Dict[str, Any]:; self._raise_if_not_created(); if description:; description += ': '; if progress is not None:; return await self._wait(description, progress, disable_progress_bar, starting_job); with BatchProgressBar(disable=disable_progress_bar) as progress2:; return await self._wait(description, progress2, disable_progress_bar, starting_job); ```. I'm not sure what the best fix is for this. An advanced user who wants to use Batch inside of Batch should really be making separate job groups from the job that job group is in and waiting for the job group that job is not a part of. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14338:117,error,error,117,https://hail.is,https://github.com/hail-is/hail/issues/14338,2,"['Error', 'error']","['Error', 'error']"
Availability,"### What happened?. Reported by Ben Weisburd: https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/batch.20OOM.20on.20input/near/351698314. <img width=""825"" alt=""image"" src=""https://user-images.githubusercontent.com/106194/234014727-78f535be-26bc-4229-9568-a4bc72bce173.png"">. BAM file is 62.4 GB. Files are available for 60 days from today for our debugging use. ```; ""resources"": {; ""req_cpu"": ""1"",; ""req_memory"": ""standard"",; ""req_storage"": ""75Gi"",; ""preemptible"": true,; ""actual_memory"": ""3.8 GiB"",; ""actual_storage"": ""75.0 GiB"",; ""actual_cpu"": 1.0; },; ""input_files"": [; {; ""from"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""to"": ""/io/batch/9fe5bc/inputs/Mscsi/Homo_sapiens_assembly38.fasta.fai""; },; {; ""from"": ""gs://bw2-delete-after-30-days/CHM1_CHM13_WGS2.downsampled_to_30x.bam"",; ""to"": ""/io/batch/9fe5bc/inputs/1VH03/CHM1_CHM13_WGS2.downsampled_to_30x.bam""; },; {; ""from"": ""gs://bw2-delete-after-30-days/CHM1_CHM13_WGS2.downsampled_to_30x.bam.bai"",; ""to"": ""/io/batch/9fe5bc/inputs/X7RRo/CHM1_CHM13_WGS2.downsampled_to_30x.bam.bai""; },; {; ""from"": ""gs://str-bucket/hg38/variant_catalogs/expansion_hunter/positive_loci.EHv5.006_of_293.json"",; ""to"": ""/io/batch/9fe5bc/inputs/gV89e/positive_loci.EHv5.006_of_293.json""; },; {; ""from"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""to"": ""/io/batch/9fe5bc/inputs/eQZbT/Homo_sapiens_assembly38.fasta""; }; ],; ```. ### Version. batch. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12922:329,avail,available,329,https://hail.is,https://github.com/hail-is/hail/issues/12922,1,['avail'],['available']
Availability,"### What happened?. Reproduction example, passed on spark, fails on batch and local; ```python; import hail as hl. def main():; hl.init(backend='batch'). args = type('Args', (object,), { 'radius': 1e7, 'overwrite': True }). with hl.TemporaryDirectory(ensure_exists=True) as tmp:; mt = hl.balding_nichols_model(3, 100, 100); bm_ldadj = hl.linalg.BlockMatrix.random(100, 100). starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius=args.radius, _localize=False); bm_ldadj = bm_ldadj._sparsify_row_intervals_expr(starts_and_stops, blocks_only=False). # sparcify to a triangle matrix; bm_ldadj = bm_ldadj.sparsify_triangle(); bm_ldadj = bm_ldadj.checkpoint(f'{tmp}/ldadj', overwrite=args.overwrite, force_row_major=True). # This is required, as the squaring/multiplication densifies, so this re-sparsifies.; ht = hl.utils.genomic_range_table(100); n = 100. r2 = bm_ldadj ** 2; r2_adj = ((n - 1.0) / (n - 2.0)) * r2 - (1.0 / (n - 2.0)); starts_and_stops = hl.linalg.utils.locus_windows(ht.locus, args.radius, _localize=False); r2_adj = r2_adj._sparsify_row_intervals_expr(starts_and_stops, blocks_only=False); r2_adj = r2_adj.sparsify_triangle(); r2_adj = r2_adj.checkpoint(f'{tmp}/adj', overwrite=args.overwrite). if __name__ == '__main__':; main(); ```. ### Version. 0.2.128. ### Relevant log output. ```shell; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.128-17247d8990c6; LOGGING: writing to /home/edmund/.local/src/hail/hail-20240508-1553-0.2.128-17247d8990c6.log; Traceback (most recent call last):; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:653,checkpoint,checkpoint,653,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['checkpoint'],['checkpoint']
Availability,"### What happened?. Searching for experimental.vcf_combiner or experimental.densify or experimental.lgt_to_gt returns links that lead to error 404. They used to work fine.; The pages don't seem to exist at all (eg https://hail.is/docs/0.2/_modules/hail/experimental/vcf_combiner/densify.html), as via a google search it also fails.; Were the pages removed for a reason? I don't see anything in the changelogs. ### Version. 0.2.117. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13377:137,error,error,137,https://hail.is,https://github.com/hail-is/hail/issues/13377,1,['error'],['error']
Availability,"### What happened?. See #13489 for context. We want to use terraform to keep track of artifact registry cleanup policies once it is available in Terraform. Relevant links:; https://github.com/hashicorp/terraform-provider-google-beta/commit/bc4aa512356891f78415d5f309bfe47b0697ac11; https://github.com/hashicorp/terraform-provider-google/issues/13824. It's not in 4.79.0 (see [what was added since then](https://github.com/hashicorp/terraform-provider-google-beta/compare/v4.79.0...main)). Releases appear to happen ~once a week, so we should be able to import into terraform in September. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13504:132,avail,available,132,https://hail.is,https://github.com/hail-is/hail/issues/13504,1,['avail'],['available']
Availability,"### What happened?. Since we guarantee a job will run at least once, there are two issues that can happen:. 1. A user can write a pipeline in which two jobs race to write the same file, e.g.; ```; j = b.new_job(); j.command('echo hello > {j.out}'); j.write_output(j.out, ""gs://bucket/final-output""); ```; 2. Or, a clever user can avoid this race with some randomness:; ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$RANDOM'); ```. The former is a really common pattern and a bit of a footgun! The latter is rare (I don't know anyone who does it) and hard to work with: how would you know the output file of the *successful* attempt?. Hail should provide some mechanism for a user to get the list of successful attempts and their outputs. One simple option is to include some kind of seeded randomness which the user can access and to return either the seed or all the draws of the successful attempt for each job in `/jobs` or for the one job in `/job/{job_id}`. For example, consider:. ```; j = b.new_job(); j.command('echo hello gsutil cp - gs://bucket/final-output-$(/hail-random-str)'); ```. Where `/hail-random-str` is a binary we mount into the container that randomly generates numbers seeded by `(batch id, job id, attempt id)`. Hail should use the same randomness to ensure that `write_output` is reliable. We might also want a way to automatically remove the output files of the non-successful (e.g. preempted) attempts. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13502:225,echo,echo,225,https://hail.is,https://github.com/hail-is/hail/issues/13502,4,"['echo', 'reliab']","['echo', 'reliable']"
Availability,"### What happened?. Sometimes when I push a branch to my fork using the `pre-push` hooks, I get errors that some scala files (which I did not touch) violate the pre-push hooks for `trailing-whitespace` and `end-of-file-fixer`. I suspect that this is because of the following:. 1. These hooks are not tested in CI and not everyone necessarily has them installed; 2. Therefore, changes that violate these hooks can make it into `main`; 3. (theory) When I run the `pre-push` hooks, many commits that I did not author are on the history that is being pushed to my fork. The pre-push hooks run on any changes on that history, and fail on those changes from 2.; 4. I am forced to use `--no-verify` or introduce trailing-whitespace fixes into an irrelevant PR. Note that there are two ways to satisfy the title. The easiest thing might to not run the untested hooks on `pre-push`, but preferably we should just test all the pre-commit hooks in CI. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13875:96,error,errors,96,https://hail.is,https://github.com/hail-is/hail/issues/13875,1,['error'],['errors']
Availability,"### What happened?. Still a lot of deadlock errors. Largely from MJC https://cloudlogging.app.goo.gl/N8hoXPWYYWLiDPPi9. Looks like workers are leaving tasks running when they shutdown https://cloudlogging.app.goo.gl/JFYoACF9qcDvCaqk8. Looks like we need to set the severity correctly in the worker logs. I'm also seeing a lot of this. ~~WARNING: Published ports are discarded when using host network mode; Also looks like we incorrectly log a ContainerTimeoutError as an error log even though that's a user error: https://cloudlogging.app.goo.gl/TUGWNxnFiBiEdsDo9~~ Moved to https://github.com/hail-is/hail/issues/14262. And we log ImageCannotBePulled as an error even though that's a user error: https://cloudlogging.app.goo.gl/TchqwUKNCrd6qqmh7. Also a few like this: Unknown child process pid 12331, will report returncode 255. ### Version. 0.2.127. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14240:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/issues/14240,5,['error'],"['error', 'errors']"
Availability,"### What happened?. Struct decoding currently uses `Region.loadBit` which:; 1. Calculates the address of the byte has this bit (e.g. the 65th bit is in the second byte).; 2. Loads the byte out of memory.; 3. Masks the bit out of the byte.; 4. Compares to zero. We don't have concrete data, but we suspect that the JVM can't avoid loading the byte out of memory 8 times. If we can instead load it once per 8 missing fields, there may be a speed up for structs that are frequently decoded (e.g. an entry struct). ### See also. - https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107 . ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811:208,Mask,Masks,208,https://hail.is,https://github.com/hail-is/hail/issues/13811,1,['Mask'],['Masks']
Availability,"### What happened?. TBD. ### Version. 0.2.172. ### Relevant log output. ```shell; 24/02/05 11:52:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.2; SparkUI available at http://192.168.1.140:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d82c34a83360; LOGGING: writing to /private/tmp/varo/avro/hail-20240205-1152-0.2.127-d82c34a83360.log; 2024-02-05 11:53:03.679 Hail: INFO: import_gvs: Importing and writing site filters to temporary storage; Traceback (most recent call last):; File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/hail_gvs_import.py"", line 180, in <module>; create_vds(arguments, vds_path, references_path, temp_path, use_classic_vqsr,; File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/hail_gvs_import.py"", line 35, in create_vds; import_gvs.import_gvs(; File ""<decorator-gen-1896>"", line 2, in import_gvs; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/import_gvs.py"", line 211, in import_gvs; site.write(site_path, overwrite=True); File ""<decorator-gen-1224>"", line 2, in write; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/table.py"", line 2002, in write; Env.backend().execute(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py"", l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:405,avail,available,405,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['avail'],['available']
Availability,"### What happened?. Tasks that depend on the DB and TaskManager, like the scheduler, need to be properly cancelled upon shutdown before the DB/TaskManager are shutdown. This causes noisy errors in the logs and alerts. ### Version. 0.2.120. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/gear/database.py"", line 190, in async_init; self.conn = await aenter(self.conn_context_manager); File ""/usr/local/lib/python3.8/dist-packages/gear/database.py"", line 79, in aenter; return await acontext_manager.__aenter__() # pylint: disable=unnecessary-dunder-call; File ""/usr/local/lib/python3.8/dist-packages/aiomysql/utils.py"", line 134, in __aenter__; self._conn = await self._coro; File ""/usr/local/lib/python3.8/dist-packages/aiomysql/pool.py"", line 139, in _acquire; raise RuntimeError(""Cannot acquire connection after closing pool""); RuntimeError: Cannot acquire connection after closing pool. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py"", line 895, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py"", line 917, in run_if_changed; should_wait = await f(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/batch/driver/instance_collection/pool.py"", line 591, in schedule_loop_body; user_resources = await self.compute_fair_share(); File ""/usr/local/lib/python3.8/dist-packages/batch/driver/instance_collection/pool.py"", line 499, in compute_fair_share; return await self._compute_fair_share(free_cores_mcpu); File ""/usr/local/lib/python3.8/dist-packages/batch/driver/instance_collection/pool.py"", line 525, in _compute_fair_share; async for record in records:; File ""/usr/local/lib/python3.8/dist-packages/gear/database.py"", line 320, in execute_and_fetchall; async with self.start() as tx:; File ""/usr/local/lib/python3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13324:187,error,errors,187,https://hail.is,https://github.com/hail-is/hail/issues/13324,1,['error'],['errors']
Availability,### What happened?. The GSA keys in Azure can expire without any notification to us developer or automated system to rotate them. This ticket is complete when:; - [ ] Azure auth-driver periodically checks for any expired keys and logs WARN for soon-to-expire (within 1 month) and ERROR for expired.; - [ ] Azure auth-driver automatically rotates keys when they are 1 month from expiration. ### Version. 0.2.126. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14104:280,ERROR,ERROR,280,https://hail.is,https://github.com/hail-is/hail/issues/14104,1,['ERROR'],['ERROR']
Availability,"### What happened?. The NDArrayNumericExpression currently lacks a max method that allows users to compute the maximum value along a specified axis of the NDArray, akin to numpy's max method. Implementing this feature would enhance the functionality and user-friendliness of the Hail library. # Expected Behavior with Examples:. ## Basic Usage:. The max method can be called on an NDArrayNumericExpression object to return the maximum value along a specified axis.; If no axis is specified, it should return the maximum value in the entire NDArray.; Example:. ```python; import hail as hl. # Creating an NDArrayNumericExpression object; nd = hl.nd.array([[1, 2, 3], [4, 5, 6]]). # Getting the maximum value along axis 0; max_along_axis0 = nd.max(axis=0). # Getting the maximum value along axis 1; max_along_axis1 = nd.max(axis=1). # Getting the maximum value in the entire array; overall_max = nd.max(). # Expected outputs; # max_along_axis0: [4, 5, 6]; # max_along_axis1: [3, 6]; # overall_max: 6; ```. ## Handling of NaN Values:; The method should be able to handle NaN values, similar to numpy, where an optional parameter can be provided to ignore NaN values.; Example:. ```python; import hail as hl; import numpy as np. # Creating an NDArrayNumericExpression object with NaN values; nd = hl.nd.array([[1, np.nan, 3], [4, 5, np.nan]]). # Getting the maximum value while ignoring NaN values; max_ignore_nan = nd.max(axis=1, ignore_nan=True). # Expected output: [3, 5] instead of [nan, nan]; ```. ## Errors and Exceptions:; Appropriate errors and exceptions should be raised for invalid inputs, such as non-integer or out-of-bound axis values. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13781:1502,Error,Errors,1502,https://hail.is,https://github.com/hail-is/hail/issues/13781,2,"['Error', 'error']","['Errors', 'errors']"
Availability,"### What happened?. The `hailtop.batch` client uploads a script file when the command becomes too large. This functionality frees users from thinking about the size of their commands. It's a great abstraction!. The client *does not* provide a mechanism to automatically upload local files or local directories which means that users must messily combine all their code and supporting data into one big file. For example,. ```python; local_in = hb.read_input('/path/to/local/script.sh'); local_data = hb.read_input('/path/to/small/local/reference.dat'). j = b.new_job(); j.command(f'bash {local_in} {local_data} {other_job.out_file} > {j.out_file}'); ```. It need not necessarily be `hb.read_input`, but it does seem like a good way to re-use an extant interface. `hailtop.batch` should upload those files when it uploads large script files and then download them to the appropriate jobs. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14354:849,down,download,849,https://hail.is,https://github.com/hail-is/hail/issues/14354,1,['down'],['download']
Availability,"### What happened?. The error below shows an assertion error in the client code for the GCP activity log. Some aspect of the logging API client code has faulty invariants/assumptions about the logging API. ### Version. 0.2.120. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py"", line 895, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py"", line 941, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/batch/cloud/gcp/driver/driver.py"", line 170, in process_activity_logs; await process_outstanding_events(self.db, _process_activity_log_events_since); File ""/usr/local/lib/python3.8/dist-packages/batch/driver/driver.py"", line 19, in process_outstanding_events; mark = await process_events_since(mark); File ""/usr/local/lib/python3.8/dist-packages/batch/cloud/gcp/driver/driver.py"", line 160, in _process_activity_log_events_since; return await process_activity_log_events_since(; File ""/usr/local/lib/python3.8/dist-packages/batch/cloud/gcp/driver/activity_logs.py"", line 114, in process_activity_log_events_since; async for event in await activity_logs_client.list_entries(body=body):; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/aiogoogle/client/logging_client.py"", line 25, in __anext__; assert self._page; AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13332:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/13332,3,"['error', 'fault']","['error', 'faulty']"
Availability,"### What happened?. The following fails:; ```; import hail as hl; hl.init(); ````; with the error:; ```; ImportError: cannot import name 'getargspec' from 'inspect' (/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py); ```; when running Python 3.11. The code importing `getargspec` is the Parsimonious library (see stacktrace below). ### Version. 0.2.109. ### Relevant log output. ```shell; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); Cell In[1], line 1; ----> 1 import hail as hl; 2 hl.init(). File /usr/local/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages/hail/__init__.py:33; 14 __doc__ = r""""""; 15 __ __ <>__; 16 / /_/ /__ __/ /; (...); 27 To report a bug, please open an issue: https://github.com/hail-is/hail/issues; 28 """"""; 30 # F403 'from .expr import *' used; unable to detect undefined names; 31 # F401 '.expr.*' imported but unused; 32 # E402 module level import not at top of file; ---> 33 from .table import Table, GroupedTable, asc, desc # noqa: E402; 34 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 35 from .expr import * # noqa: F401,F403,E402. File /usr/local/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages/hail/table.py:8; 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence; ----> 8 from hail.expr.expressions import Expression, StructExpression, \; 9 BooleanExpression, expr_struct, expr_any, expr_bool, analyze, Indices, \; 10 construct_reference, to_expr, construct_expr, extract_refs_by_indices, \; 11 ExpressionException, TupleExpression, unify_all, NumericExpression, \; 12 StringExpression, CallExpression, CollectionExpression, DictExpression, \; 13 IntervalExpression, LocusExpression, NDArrayExpression, expr_stream; 14 from hail.expr.types import hail_type, tstruct, types_match, tarray, tset, dtypes_from_pandas; 15 from hail.expr.table_type import ttable. Fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12759:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/12759,1,['error'],['error']
Availability,"### What happened?. The following is a valid `Dockerfile` that does not result in a user entry in `/etc/passwd` in the image. ```; FROM ubuntu:22.04; USER ""1001""; ```. This fails in Batch with the error `ValueError: Container user not found in image's /etc/passwd` because it only supports specifying user and group names instead of also uid/gids that may or may not actually exist in the image. See [here](https://docs.docker.com/engine/reference/builder/#user) for more information on the `USER` directive. This can be be fixed by either properly implementing the `USER` directive in Hail Batch or switching from `crun` to using a higher-level runtime like `podman` that will handle this for us. The latter might be better long-term (podman now supports specifying a user-provided `upperdir` which we must handle ourselves for XFS quota setting as well as user-provided network namespaces). EDIT: See comment, this also requires `hailtop.batch` not to create the tmpdir if there are no input or output files for the job. ### Version. 0.2.123. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13679:197,error,error,197,https://hail.is,https://github.com/hail-is/hail/issues/13679,1,['error'],['error']
Availability,"### What happened?. This is a known issue in our underlying serialization library: https://github.com/uqfoundation/dill/issues/609. I've discovered that if you have dill 0.3.7 (latest) and try to use our image (which has dill 0.3.5.1) you get this error:. Traceback (most recent call last):; File ""<string>"", line 27, in <module>; File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 373, in load; return Unpickler(file, ignore=ignore, **kwds).load(); File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 646, in load; obj = StockUnpickler.load(self); File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 805, in _create_code; return CodeType(args[0], 0, 0, *args[1:]); TypeError: code expected at most 16 arguments, got 19; Switching local dill to the remote version eliminates the error. Fix is to ensure the *de*-serializer is >0.3.5.1. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13535:248,error,error,248,https://hail.is,https://github.com/hail-is/hail/issues/13535,2,['error'],['error']
Availability,"### What happened?. This is really confusing for users. The error is about incompatible number of fields (because the ""locus"" is used for the row_key but the mt has a two-field row key, so Hail tells us we're trying to index the row_key with one field and that does not work. ### Version. 0.2.119. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13280:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/issues/13280,1,['error'],['error']
Availability,"### What happened?. This probably means rethinking our use of rich. We want a little header of things like total cluster size, cores available for us, and cores starting up. Then we want a table of jobs that we've run with the URLs and whatnot. ### Version. 0.2.116. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13063:133,avail,available,133,https://hail.is,https://github.com/hail-is/hail/issues/13063,1,['avail'],['available']
Availability,"### What happened?. This works:. ```; foo = foo.annotate(csq=foo.gene.map(lambda _: 1)); foo.show(); ```. This fails:. ```; foo = foo.annotate(csq=foo.gene.map(lambda _: hl.rand_cat([0.3, 0.2, 0.5], seed=0))); foo.show(); ```. ```; FatalError: ClassCastException: null. Java stack trace:; java.lang.RuntimeException: typ: inference failure:; 	at is.hail.expr.ir.IR.typ(IR.scala:38); 	at is.hail.expr.ir.IR.typ$(IR.scala:33); 	at is.hail.expr.ir.ToStream.typ(IR.scala:300); 	at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr_1$81(Parser.scala:1111); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2157); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:345); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$1(SparkBackend.scala:690); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.parse_value_ir(SparkBackend.scala:689); 	at sun.reflect.GeneratedMethodAccessor193.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13693:332,failure,failure,332,https://hail.is,https://github.com/hail-is/hail/issues/13693,1,['failure'],['failure']
Availability,"### What happened?. Try running a job with `_machine_type: 'n1-highmem-64'`. This is necessary to get enough memory for some larger jobs (> ~200GB). Startup on the batch worker fails because the job is calculating how many theoretical network namespaces it could support (4 per CPU, 64 CPUS, plus some for JVMs), but not considering that the IPv4 schema puts a hard limit of 255 on namespaces if only one subnet value is changing each time. ### Version. Live 7/30/24. ### Relevant log output. _No response_. ### Security considerations:. Low risk of impacting security. High CPU machine types are not materially different from others with respect to security considerations, and the bug is a simple logic error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14644:705,error,error,705,https://hail.is,https://github.com/hail-is/hail/issues/14644,1,['error'],['error']
Availability,"### What happened?. Try writing to a bucket to which your service account has read-only access:; ```; hl.utils.range_table(5,n_partitions=5).write('gs://neale-bge/foo.ht'); ```. https://batch.hail.is/batches/8042383. The client gets an error like this:; ```; Java stack trace:; is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$20(StorageImpl.java:610); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:236,error,error,236,https://hail.is,https://github.com/hail-is/hail/issues/13697,2,"['down', 'error']","['download', 'error']"
Availability,"### What happened?. Upon worker shutdown, we warn errors if there are any unclosed asyncio Tasks before the process exits. Lately, the worker logs have been noisy with such warnings. See [here](https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%0AlogName:%22workerlog%22%0Aseverity!%3D%22INFO%22%0Alabels.%22compute.googleapis.com%2Fresource_name%22:%22batch-worker-default%22;pinnedLogId=2024-03-27T15:37:03.406989417Z%2F1ye73wf703wad;cursorTimestamp=2024-03-27T15:37:03.458721962Z;duration=P1D?project=hail-vdc) for an example. We should investigate and make sure all tasks are properly closed so that workers can shutdown gracefully. This may also be relevant to #14261, as the running theory is that the nonsensical assertion error is a byproduct of trying to log after the python process is partially torn down. Ideally, we shouldn't have anything left to log once the process tries to exit but this might happen if there are unjoined tasks. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14424:50,error,errors,50,https://hail.is,https://github.com/hail-is/hail/issues/14424,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,### What happened?. Users need a way to control their risk tolerance for preemptions. ### Version. 0.2.120. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13395:59,toler,tolerance,59,https://hail.is,https://github.com/hail-is/hail/issues/13395,1,['toler'],['tolerance']
Availability,### What happened?. We are leaving tasks alive when workers shut down and we do not know which tasks they are. This issue has two parts:. 1. Fix `dump_all_stacktraces` to actually show all the outstanding tasks. Perhaps `log.debug` isn't generating output b/c of our logging configuration.; 2. Figure out why these tasks are running and prevent them from staying running. Several examples [here](https://cloudlogging.app.goo.gl/aMfqzLB4FBa864WJ7). ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13908:41,alive,alive,41,https://hail.is,https://github.com/hail-is/hail/issues/13908,2,"['alive', 'down']","['alive', 'down']"
Availability,"### What happened?. We expect the matrix table abstraction to break down at 2M+ samples. Consider that each row now has 2M entries. Even if all the RAM of a highmem machine was made available to row processing, we only have 6.5kB per sample. This is a fair bit of memory! But many reasonable Hail pipelines will generate enough garbage to overflow this. We must develop a block-partitioned matrix table. Instead of each partition containing the entries for every column and a subset of rows, each partition will contain the entries for a subset of rows and columns. While there are many ways to do this partitioning, a simple and workable first try is a ""grid""-like partitioning. Every matrix table has a set of row-key intervals and column-key intervals which define the partitioning. The bounds of the partitions are given by the cross product of these sets of intervals. The visual conception of the partitioning of this matrix table (with its globals, column margin data, row margin data, and entry data) might look like:. ```; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; +--+ +-----+--+--++---+------+------+. ck1 ck2 ...; +--+ rk1 +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ rk2 +-----+--+--++---+------+------+; | | | | | || | | |; +--+ ... +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; +--+ +-----+--+--++---+------+------+; | | | | | || | | |; | | | | | || | | |; | | | | | || | | |; +--+ +-----+--+--++---+------+------+; ```. The first row-key interval is `[rk1, rk2)`. The first col-key interval is `[ck1, ck2)`. These intervals are define a ""rectangle"" corresponding to the first partition. . All the partitions in the fourth partition column are empty (perhaps these column keys are absent in this dataset). Likewise, all the partitions in the fifth partition row are emp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13800:68,down,down,68,https://hail.is,https://github.com/hail-is/hail/issues/13800,2,"['avail', 'down']","['available', 'down']"
Availability,"### What happened?. We have a dataset with joint call multi-sample VCF files (not from imputation). We converted those multi-sample VCFs to hailmatrix tables with following WDL code in Terra:. ```python; import hail as hl. hl.init(spark_conf={""spark.driver.memory"": ""~{memory_gb}g""}). callset = hl.import_vcf(""~{source_vcf}"",; array_elements_required=False,; force_bgz=True,; reference_genome='~{reference_genome}'). callset.write(""~{target_prefix}"", overwrite=True); ```. After sample filtering, we want to export it to VCF. ```python; import hail as hl. hl.init(spark_conf={; ""spark.driver.memory"": ""~{memory_gb}g"",; ""spark.local.dir"": ""./tmp""; },; tmp_dir=""./tmp"",; local_tmpdir=""./tmp"",; idempotent=True); hl.default_reference(""~{reference_genome}""). mt = hl.read_matrix_table(""~{input_hail_mt_path}""); hl.export_vcf(mt, ""~{hail_vcf}"", tabix = False); ```. It worked on chr1 to chr22, but failed at chrX and chrY with error: VCF spec does not support phased haploid calls. What should we do to export chrX and chrY?. ### Version. 0.2.127-py3.11. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""<stdin>"", line 14, in <module>; File ""<decorator-gen-1448>"", line 2, in export_vcf; File ""/usr/local/lib/python3.10/dist-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/lib/python3.10/dist-packages/hail/methods/impex.py"", line 634, in export_vcf; Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)); File ""/usr/local/lib/python3.10/dist-packages/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/usr/local/lib/python3.10/dist-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/usr/local/lib/python3.10/dist-packages/hail/backend/py4j_backend.py"", line 220, in _rpc; raise fatal_error_from_java_error_triplet(; hail.utils.java.FatalError: HailException: VCF spec does not support pha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14330:922,error,error,922,https://hail.is,https://github.com/hail-is/hail/issues/14330,1,['error'],['error']
Availability,"### What happened?. We have been burned by rare transient errors in Google Cloud Storage three times now. 1. https://github.com/hail-is/hail/pull/14080; 2. https://github.com/hail-is/hail/issues/13721; 3. https://github.com/hail-is/hail/issues/13937. > Fool me once, shame on you, fool me twice shame on me. Before releases, Hail *must* run tests that read on the order of 10 TiB of data so as to ensure that any changes since the last release do not introduce rare transient bugs or at-scale-only memory issues that our users will discover. There are at least four tests in my mind:; 1. Large-scale linear algebra (e.g. PC-Relate & LD-Prune).; 2. gnomAD style frequency calculations on 1M samples grouped 500 ways.; 3. The VDS Combiner.; 4. `show` on 1M samples. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14082:58,error,errors,58,https://hail.is,https://github.com/hail-is/hail/issues/14082,1,['error'],['errors']
Availability,"### What happened?. We have reported to their GitHub, but we don't have a simple enough repro for them to make progress. https://github.com/Azure/azure-sdk-for-java/issues/35125. Personal correspondence with some MSFT researchers suggested there could be an issue with threading:; > It sort of reminds me of an issue we saw with Cromwell where their old akka pool code caused a bunch of unexpected network behavior that broke their API in certain cases. I've asked if BlobServiceClient is thread-safe or not. We share an object of that class, but none of the things it produces (e.g. blobs). We know that the java.io libraries can improperly drop an HTTP response if it is followed by a TCP RST. In particular, we've seen this happen when a server is load shedding and sends an HTTP ""429 Too Many Requests"" rapidly followed by a TCP RST. This might explain the ""Connection reset"" errors that we sometimes see. We have fewer intuitions about the ""Stream is already closed"". That specific error was reported to Azure in the aforementioned GitHub issue. We treat both stream is closed and connection reset as ""limited retry"" errors. We might retry too quickly. Our initial delay is `100ms * x` where `x` is drawn uniformly from `[0, 1]`. Perhaps we should try an initial delay of at least 1s? . For example, [Azure gives as an example retrying after 2s, 4s, 10s, and 30s](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist#timeout-and-server-busy-errors). Google's [code examples](https://cloud.google.com/storage/docs/retry-strategy#client-libraries_1) suggest an initial delay of 1s with a multiplier of 2. AWS seems to use 500ms as the [default base backoff for ""throttled"" exceptions](https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/retry/PredefinedBackoffStrategies.java#L39). ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13351:880,error,errors,880,https://hail.is,https://github.com/hail-is/hail/issues/13351,4,['error'],"['error', 'errors']"
Availability,"### What happened?. We observed this error log message . ```; deleting disk batch-disk-3d106c666a364d82bec3 from instance that no longer exists; ```. followed by what appears to be an endless stream of the following assertion error in the background task loop that tries to delete orphaned disks:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 915, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 959, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/cloud/gcp/driver/driver.py"", line 180, in delete_orphaned_disks; await delete_orphaned_disks(; File ""/usr/local/lib/python3.9/dist-packages/batch/cloud/gcp/driver/disks.py"", line 30, in delete_orphaned_disks; async for disk in await compute_client.list(f'/zones/{zone}/disks', params=params):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/aiogoogle/client/compute_client.py"", line 59, in __anext__; assert 'pageToken' not in self._request_params; AssertionError; ```. This is an invalid state for the `PagedIterator` to be in, and could imply that this garbage disk collection loop just doesn't work. We should track down the broken invariant here and fix it, if possible testing that the async iterator works correctly. ### Version. 0.2.132. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14613:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/issues/14613,3,"['down', 'error']","['down', 'error']"
Availability,### What happened?. We should use a more obvious name like `downsample=False`. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13696:60,down,downsample,60,https://hail.is,https://github.com/hail-is/hail/issues/13696,1,['down'],['downsample']
Availability,"### What happened?. We're dividing by X in two places when it should be Y for computing E10. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/P.28I.3D1.7CZ.3D0.29.20computation.20in.20IBD/near/403886796. This is a trivial fix, but I want to make sure the value for E11 is also correct. We're using T/2 where as in the paper it's 1. Regardless, we need better tests for IBD that would have caught this error. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14052:432,error,error,432,https://hail.is,https://github.com/hail-is/hail/issues/14052,1,['error'],['error']
Availability,"### What happened?. When I try select some rows (10) of a large matrixtable and convert it into a pandas dataframe the execution fails with `ClassTooLargeException`. The problem arises after I invoke `make_table()` and try to take some rows. I expected hail to be able to handle data with dimensions 10x3202, which is not too large. Data was downloaded from the 1000 Genomes ftp site: [link](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20201028_3202_phased/). ```; # load data; vcf_path = "".../1000G_b38_20201028_3202_phased/CCDG_14151_B01_GRM_WGS_2020-08-05_chr*.filtered.shapeit2-duohmm-phased.vcf.gz""; mt = hl.import_vcf(vcf_path, force_bgz=True). # select a few random variants; n_selected_variants = 10; selected_variants = np.random.choice(mt.rsid.collect(), n_selected_variants); selected_variants = hl.array(list(selected_variants)). (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).count(); ```; ```; [Stage 18:=====================================================>(255 + 1) / 256]; (10, 3202); ```. Trying to convert to pandas dataframe `.make_table().to_pandas()`, or even just taking 1 row `.make_table().take(1)` results in the following error:; ```; (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).make_table().take(1); ```; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[10], [line 5](vscode-notebook-cell:?execution_count=10&line=5); [1](vscode-notebook-cell:?execution_count=10&line=1) (; [2](vscode-notebook-cell:?execution_count=10&line=2) mt.filter_rows(selected_variants.contains(mt.rsid)); [3](vscode-notebook-cell:?execution_count=10&line=3) .select_rows('rsid'); [4](vscode-notebook-cell:?execution_count=10&line=4) .select_entries('GT'); ----> [5](vscode-notebook-cell:?execution_count=10&line=5) ).make_table().take(1). File ...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:342,down,downloaded,342,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['down'],['downloaded']
Availability,"### What happened?. When a job creates a log file in excess of about half a GB, loading the job page can cause the batch front-end pod to crash as it loads the log file into memory and interpolates it directly into the job page. The front-end should instead:. - Fully stream job logs in the log endpoint; - Show a truncated view of the log in the job page, with a pointer to download the full log if it's truncated. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13328:375,down,download,375,https://hail.is,https://github.com/hail-is/hail/issues/13328,1,['down'],['download']
Availability,"### What happened?. With the merger of https://github.com/hail-is/hail/pull/13672 and https://github.com/hail-is/hail/pull/13670, Hail Batch now has an auth and database model that is compatible with Azure-Terra. We also made changes to support SAS Token ABS URLs. Daniel has a diff which makes the final changes here: https://github.com/hail-is/hail/commit/94e5b468b0dcbbe6a1de5a296a2103c193b3c61b#diff-a06605397cc32c70a9d43b97fd2ab400374b5172092c5f6a0e26bf2bf1bd559b. The outstanding issues are:; 1. There is no single API in Terra to delete a VM and all its disks. We need to convince ourselves that we won't leak disks.; 2. Ensure we can mount and format disks on the fly as expected.; 3. A reliable & automated testing system.; 4. Merging the diff mentioned above. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13941:695,reliab,reliable,695,https://hail.is,https://github.com/hail-is/hail/issues/13941,1,['reliab'],['reliable']
Availability,"### What happened?. [SAIGE](https://github.com/weizhouUMICH/SAIGE) and its competitor [REGENIE](https://rgcgithub.github.io/regenie/) are the standard bearers for modern GWAS. Hail should expose SAIGE within the Hail Query language. The interface should roughly match `hl.linear_regression_rows`. A Batch pipeline would serve the needs of Broadies (and, indeed, such a pipeline already exists) but has two downsides:; 1. There is substantial I/O involved in exporting the data from Hail-native formats to SAIGE-compatible formats.; 2. Non-Broadies cannot use this pipeline. Query language support for SAIGE would transform the accessibility of SAIGE by making it usable at scale by anyone with access to Hail, which is basically anyone with a large dataset (e.g. [DNANexus](https://med.stanford.edu/gbsc/projects/vapahcs.html), [AoU RWB](https://support.researchallofus.org/hc/en-us/articles/6090679838100-How-to-Work-with-All-of-Us-Genomic-Data-Hail-Plink-), [MVP](https://med.stanford.edu/gbsc/projects/vapahcs.html), [FinnGen](https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1.full)). There are two options:; 1. Determine and implement the linear algebraic primitives necessary for SAIGE.; 2. Compile and link directly against SAIGE. Expose these functions, via JNI, to the Hail Query language. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13442:406,down,downsides,406,https://hail.is,https://github.com/hail-is/hail/issues/13442,1,['down'],['downsides']
Availability,"### What happened?. [reporter's note: IIRC, exit code 137 indicates that the ""container"" in which the worker JVM was executing exceeded memory limits. It seems likely that whole stage codegen has either (1) changed memory management in a way that uses more memory or (2) is newly lowering code that exposes a latent issue in memory management that uses too much (or leaks) memory.]. Reported by Ben Weisburd and Julia Goodrich. [Ben is] running the first step of readviz for gnomAD v4 and we are hitting a 137 error on a partition that includes a site that has 27374 alleles. His code is [here](https://github.com/broadinstitute/gnomad-readviz/blob/step1_optimizations/step1__select_samples.py). I was testing his code out on just that failing partition (just added mt = vds.variant_data._filter_partitions([41229])) and I was able to recreate the error using Hail 0.2.119 (this is what Ben was using when he hit the error on the full dataset). However, the first time I tried to recreate the error I was accidentally using a different version of Hail and it ran with no memory error. It seems that 0.2.117 runs without error, but 0.2.118 and 0.2.119 both hit the 137 error. I am currently rerunning these tests so I can get logs:. Test with Hail 0.2.118:. Cluster:; ```; hailctl dataproc start readviz-118 \; --requester-pays-allow-all \; --packages=""git+https://github.com/broadinstitute/gnomad_methods.git@main"",""git+https://github.com/broadinstitute/gnomad_qc.git@main"" \; --autoscaling-policy=max-20 \; --master-machine-type n1-highmem-16 \; --no-off-heap-memory \; --worker-machine-type n1-highmem-8 \; --max-idle 560m \; --labels gnomad_release=gnomad_v4,gnomad_v4_testing=readviz_test_118; ```; Command:; ```; hailctl dataproc submit readviz-118 /Users/jgoodric/PycharmProjects/gnomad-readviz/step1__select_samples.py --sample-metadata-tsv gs://gnomad-readviz/v4.0/gnomad.exomes.v4.0.metadata.tsv.gz --output-ht-path gs://gnomad-tmp/julia/readviz/gnomad.exomes.v4.0.readviz_crams.part_41229.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248:510,error,error,510,https://hail.is,https://github.com/hail-is/hail/issues/13248,3,['error'],['error']
Availability,### What happened?. ```; + gcloud artifacts repositories set-cleanup-policies hail --project=hail-vdc --location=us --policy=/io/repo/infra/gcp-broad/gcp-ar-cleanup-policy.txt --no-dry-run; ERROR: (gcloud.artifacts.repositories) Invalid choice: 'set-cleanup-policies'.; ```. I think the release needs an updated version of cloud and then this should work fine. ### Version. 0.2.127. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14154:190,ERROR,ERROR,190,https://hail.is,https://github.com/hail-is/hail/issues/14154,1,['ERROR'],['ERROR']
Availability,"### What happened?. ```; =================================== FAILURES ===================================; _______________ ServiceTests.test_python_job_incorrect_signature _______________. self = <test.hailtop.batch.test_batch.ServiceTests testMethod=test_python_job_incorrect_signature>. def setUp(self):; # https://stackoverflow.com/questions/42332030/pytest-monkeypatch-setattr-inside-of-test-class-method; self.monkeypatch = MonkeyPatch(); ; self.backend = ServiceBackend(); ; remote_tmpdir = get_remote_tmpdir('hailtop_test_batch_service_tests'); if not remote_tmpdir.endswith('/'):; remote_tmpdir += '/'; self.remote_tmpdir = remote_tmpdir + str(uuid.uuid4()) + '/'; ; if remote_tmpdir.startswith('gs://'):; match = re.fullmatch('gs://(?P<bucket_name>[^/]+).*', remote_tmpdir); assert match; self.bucket = match.groupdict()['bucket_name']; else:; assert remote_tmpdir.startswith('hail-az://'); if remote_tmpdir.startswith('hail-az://'):; match = re.fullmatch('hail-az://(?P<storage_account>[^/]+)/(?P<container_name>[^/]+).*', remote_tmpdir); assert match; storage_account, container_name = match.groups(); else:; assert remote_tmpdir.startswith('https://'); match = re.fullmatch('https://(?P<storage_account>[^/]+).blob.core.windows.net/(?P<container_name>[^/]+).*', remote_tmpdir); assert match; storage_account, container_name = match.groups(); self.bucket = f'{storage_account}/{container_name}'; ; self.cloud_input_dir = f'{self.remote_tmpdir}batch-tests/resources'; ; token = uuid.uuid4(); self.cloud_output_path = f'/batch-tests/{token}'; self.cloud_output_dir = f'{self.remote_tmpdir}{self.cloud_output_path}'; ; self.router_fs = RouterAsyncFS(); ; > if not self.sync_exists(f'{self.remote_tmpdir}batch-tests/resources/hello.txt'):. ../test/hailtop/batch/test_batch.py:533: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; ../test/hailtop/batch/test_batch.py:544: in sync_exists; return async_to_blocking(self.router_fs.exists(url)); utils/utils.py:160",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13997:61,FAILURE,FAILURES,61,https://hail.is,https://github.com/hail-is/hail/issues/13997,1,['FAILURE'],['FAILURES']
Availability,"### What happened?. ```; In [9]: import hail as hl; ...: mt = hl.utils.range_matrix_table(2,2); ...: mt = mt.annotate_entries(prod = mt.row_idx * mt.col_idx); ...: hl.logistic_regression_rows(y=mt.row_idx, x=mt.prod, test='wald', covariates=[1.0]).describe(); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[9], line 4; 2 mt = hl.utils.range_matrix_table(2,2); 3 mt = mt.annotate_entries(prod = mt.row_idx * mt.col_idx); ----> 4 hl.logistic_regression_rows(y=mt.row_idx, x=mt.prod, test='wald', covariates=[1.0]).describe(). File <decorator-gen-1708>:2, in logistic_regression_rows(test, y, x, covariates, pass_through, max_iterations, tolerance). File ~/projects/hail/hail/python/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/methods/statgen.py:921, in logistic_regression_rows(test, y, x, covariates, pass_through, max_iterations, tolerance); 918 if not y_is_list:; 919 result = result.transmute(**result.logistic_regression[0]); --> 921 return result.persist(). File <decorator-gen-1242>:2, in persist(self, storage_level). File ~/projects/hail/hail/python/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/table.py:2112, in Table.persist(self, storage_level); 2076 @typecheck_method(storage_level=storage_level); 2077 def persist(self, storage_level='MEMORY_AND_DISK') -> 'Tab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13788:717,toler,tolerance,717,https://hail.is,https://github.com/hail-is/hail/issues/13788,1,['toler'],['tolerance']
Availability,"### What happened?. ```python3; --- Logging error ---; Traceback (most recent call last):; File ""/usr/lib/python3.9/logging/__init__.py"", line 1083, in emit; msg = self.format(record); File ""/usr/lib/python3.9/logging/__init__.py"", line 927, in format; return fmt.format(record); File ""/usr/local/lib/python3.9/dist-packages/pythonjsonlogger/jsonlogger.py"", line 246, in format; return self.serialize_log_record(log_record); File ""/usr/local/lib/python3.9/dist-packages/pythonjsonlogger/jsonlogger.py"", line 215, in serialize_log_record; return ""%s%s"" % (self.prefix, self.jsonify_log_record(log_record)); File ""/usr/local/lib/python3.9/dist-packages/pythonjsonlogger/jsonlogger.py"", line 207, in jsonify_log_record; return self.json_serializer(log_record,; File ""/usr/local/lib/python3.9/dist-packages/hailtop/hail_logging.py"", line 18, in logger_json_serializer; assert default is None and cls is OrJsonEncoder and indent is None and ensure_ascii is False, (; AssertionError: (None, <class 'hailtop.hail_logging.OrJsonEncoder'>, None, False); Call stack:; File ""/usr/local/lib/python3.9/dist-packages/aiohttp/client.py"", line 367, in __del__; self._loop.call_exception_handler(context); File ""/usr/lib/python3.9/asyncio/base_events.py"", line 1779, in call_exception_handler; self.default_exception_handler(context); File ""/usr/lib/python3.9/asyncio/base_events.py"", line 1753, in default_exception_handler; logger.error('\n'.join(log_lines), exc_info=exc_info); Message: 'Unclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7ff9c8559490>'. ```. https://cloudlogging.app.goo.gl/PafWAh6xZEuQFhr78. ### Version. 0.2.127. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14261:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/issues/14261,2,['error'],['error']
Availability,"### What happened?. `add_liftover` fails when provided a local chain file on Dataproc. The error says the file does not exist, even though it does. . ## To reproduce; This is run in my GCP virtual machine following a simple example from [your documentation](https://github.com/hail-is/hail/blob/67801dfc66b504a7d49daa53f7ec6d22c1194585/hail/python/hail/genetics/reference_genome.py#L467):; ```sh; gsutil cp gs://hail-common/references/grch37_to_grch38.over.chain.gz .; ```; ```py; import hail as hl; from pathlib import Path. local_chain_file = 'grch37_to_grch38.over.chain.gz'. if Path(local_chain_file).is_file():; rg37 = hl.get_reference('GRCh37'); rg38 = hl.get_reference('GRCh38'); rg37.add_liftover(local_chain_file, rg38); ```; I've reproduced the bug trying with relative/absolute paths, and also referring to HDFS and the local filesystem. None of them worked on the GCP VM, however reading local chain files works on my computer. ### Version. 0.2.126-ee77707f4fab. ### Relevant log output. ```shell; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-17-ef14f8017832> in <cell line: 3>(); 4 rg37 = hl.get_reference('GRCh37'); 5 rg38 = hl.get_reference('GRCh38'); ----> 6 rg37.add_liftover(local_chain_file, rg38); 7. <decorator-gen-150> in add_liftover(self, chain_file, dest_reference_genome). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/genetics/reference_genome.py in add_liftover(self, chain_file, dest_reference_genome); 507 """"""; 508; --> 509 Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:91,error,error,91,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['error'],['error']
Availability,"### What happened?. `hailctl dataproc start` fails with an error message like the one below because [in Dataproc 2.2](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#:~:text=Internal%20addresses%20only%20(no%2Daddress)%20is%20set%20by%20default%20when%20creating%20a%20Dataproc%202.2%20image%20version%20cluster.%20You%20can%20use%20the%20gcloud%20dataproc%20clusters%20create%20%2D%2Dpublic%2Dip%2Daddress%20flag%20to%20enable%20public%20IP%20addresses.), clusters are created without public internet access by default. A workaround is to pass the `--public-ip-address` flag to the command. Error message:. ```python; pip packages are ['setuptools', 'mkl<2020', 'lxml<5', 'https://github.com/hail-is/jgscm/archive/v0.1.13+hail.zip', 'ipykernel==6.22.0', 'ipywidgets==8.0.6', 'jupyter-console==6.6.3', 'nbconvert==7.3.1', 'notebook==6.5.6', 'qtconsole==5.4.2', 'aiodns==2.0.0', 'aiohttp==3.9.5', 'aiosignal==1.3.1', 'async-timeout==4.0.3', 'attrs==23.2.0', 'avro==1.11.3', 'azure-common==1.1.28', 'azure-core==1.30.2', 'azure-identity==1.17.1', 'azure-mgmt-core==1.4.0', 'azure-mgmt-storage==20.1.0', 'azure-storage-blob==12.20.0', 'bokeh==3.3.4', 'boto3==1.34.138', 'botocore==1.34.138', 'cachetools==5.3.3', 'certifi==2024.6.2', 'cffi==1.16.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'commonmark==0.9.1', 'contourpy==1.2.1', 'cryptography==42.0.8', 'decorator==4.4.2', 'deprecated==1.2.14', 'dill==0.3.8', 'frozenlist==1.4.1', 'google-auth==2.31.0', 'google-auth-oauthlib==0.8.0', 'humanize==1.1.0', 'idna==3.7', 'isodate==0.6.1', 'janus==1.0.0', 'jinja2==3.1.4', 'jmespath==1.0.1', 'jproperties==2.1.1', 'markupsafe==2.1.5', 'msal==1.29.0', 'msal-extensions==1.2.0', 'msrest==0.7.1', 'multidict==6.0.5', 'nest-asyncio==1.6.0', 'numpy==1.26.4', 'oauthlib==3.2.2', 'orjson==3.10.6', 'packaging==24.1', 'pandas==2.2.2', 'parsimonious==0.10.0', 'pillow==10.4.0', 'plotly==5.22.0', 'portalocker==2.10.0', 'protobuf==3.20.2', 'py4j==0.10.9.7', 'pyasn1==0.6.0', 'pyasn1-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14652:59,error,error,59,https://hail.is,https://github.com/hail-is/hail/issues/14652,2,"['Error', 'error']","['Error', 'error']"
Availability,"### What happened?. `query_billing_projects_with_cost` runs a `select_and_fetchall` query against the database to load information about certain billing projects. It currently locks the affected rows in share mode, but I don't believe there's any reason to do this and it can lead to deadlock errors in `monitor_billing_limits`. It is also worth noting that in `monitor_billing_limits`, we might not want to reuse this method at all, as it only needs to load rows from the database that have exceeded their billing limit. In practice currently this doesn't much matter as the number of billing projects is fairly small, but it is still not ideal. ### Version. 0.2.128. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 900, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 944, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1288, in monitor_billing_limits; records = await query_billing_projects_with_cost(db); File ""/usr/local/lib/python3.9/dist-packages/batch/utils.py"", line 165, in query_billing_projects_with_cost; async for record in db.select_and_fetchall(sql, tuple(args)):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 339, in select_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 254, in execute_and_fetchall; await cursor.execute(sql, args); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/loc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14423:293,error,errors,293,https://hail.is,https://github.com/hail-is/hail/issues/14423,1,['error'],['errors']
Availability,"### What happened?. dev namespace scaling is doing a good job of keeping costs down over the weekend and over night, but if devs don't `k delete deployments -n NAMESPACE` and `k delete statefulsets -n NAMESPACE`, these deployments stick around all day every day. It seems, in practice, our db's get an entire 2 core node to themselves. Let's eliminate the scale-up job. We will keep the cronjob that scales down namespaces at the end of each workday. We will provide a scale-up command to either the make targets or `devbin/functions.sh`. `hailctl dev deploy`, of course, would also force a scale up (while also blowing away whatever was there before). ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14020:79,down,down,79,https://hail.is,https://github.com/hail-is/hail/issues/14020,2,['down'],['down']
Availability,### What happened?. e.g. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/Error.20in.20QoB.3A.20unknown.20opcode/near/364895723. We should provide tags like `hailgenetics/hail:0.2.117-py3.8` or something. Maybe look a Tensorflow or PyTorch or pyspark for naming inspiration? I think we should still provide `hailgenetics/hail:0.2.117` with our most well-tested version of Python. ### Version. 0.2.117. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13162:99,Error,Error,99,https://hail.is,https://github.com/hail-is/hail/issues/13162,1,['Error'],['Error']
Availability,"### What happened?. e.g. see an attempt to use build_python_image to execute some Hail code in the cloud: https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/Error.20in.20QoB.3A.20unknown.20opcode. We should provide users with a simple and straightforward project structure and mechanism for working with a local Python project, its Python dependencies, including, possibly, Hail. It seems to me that a relatively straightforward way to do this would be to recommend the user create a normal, installable python package (and provide instructions on doing so), and then to provide some Hail Batch client functionality that builds an image based on `hailgenetics/hail` (if Hail is required) or `hailgenetics/python-dill` or a user-provided base image (which must have Python, but we'll ensure dill gets installed). The Dockerfile should look something like:. ```; FROM {base_image}; COPY {users_project_dir} /users_project; RUN pip install /users_project; ```. And then that image can be used as the python_default_image (maybe also the default_image?). ### Version. 0.2.117. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13161:180,Error,Error,180,https://hail.is,https://github.com/hail-is/hail/issues/13161,1,['Error'],['Error']
Availability,"### What happened?. gnomAD team asks:. > We would like to get these same sample stats broken down by different variant stratifications, so essentially, this is like what we do for the frequencies, but this is for samples. With Frequencies we can use the hl.agg.filter with hl.agg.call_stats to get what we want, but there isn’t really an equivalent for samples since hl.sample_qc and hl.vds.sample_qc take MTs and return HTs rather than taking expressions and returning aggregation expressions. Would the Hail team have the bandwidth in the next couple weeks to put in a modification to the hl.vds.sample_qc to make it function similar to the hl.agg.call_stats ?. My thoughts on it:. It's a reasonable ask. You'll have four parts (first three are aggregators): rmt_sq, vmt_sq, ac_and_atype, and combine. The user must ensure the same filters, groups, etc. are applied to each aggregator. If you group, you'll need to ensure the right grouped AC is passed around. It might look like this on the variant matrix table. The reference stuff looks similar.; ```python3; vmt = vmt.annotate_entries(GT=hl.vds.lgt_to_gt(vmt.LGT, vmt.LA)); vmt = vmt.annotate_rows(ac_atype=hl.agg.group_by(foo, ac_and_atype(vmt.GT, vmt.alleles))); vmt = vmt.annotate_cols(; qc=hl.agg.group_by(; foo,; vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; ac=ac_atype[foo].ac,; atype=ac_atype[foo].atype,; dp=vmt.DP); ); ); ```; The atype needn't really be grouped. That should maybe be its own aggregator and then you can use stats directly to get a valid AC. I sketched this here: https://github.com/hail-is/hail/compare/main...danking:hail:agg-sample-qc but there are probably bugs in that. This issue is complete when we merge a PR that:; 1. Exposes one or more aggregators which compute all the statistics from `hl.vds.sample_qc` on the component matrix tables of a VDS: the variant data and the reference data.; 2. Exposes a combination function which combines reference and variant stats to produce `bases_over_dp_threshold` and ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14264:93,down,down,93,https://hail.is,https://github.com/hail-is/hail/issues/14264,1,['down'],['down']
Availability,"### What happened?. gnomAD went with a custom CUDA-implementation of KING instead of PC-Relate because `hl.pc_relate` was very slow on their gnomAD v4 1M sample dataset. I'm fairly certain the bottleneck is writing out a 1M by 1M dense matrix of 64-bit floating point numbers (aka the relatedness matrix). This matrix is too large. Our users only care about the small subset of entires indicating close relatedness between the samples [1]. Instead of writing a dense BlockMatrix, we should write a Hail Table with the columns `sample1`, `sample2`, and `kinship`. I have some (very old) skeleton code for this [here](https://github.com/hail-is/hail/compare/main...danking:hail:sparse-pc-relate). If I recall correctly, this only calculates the kinship coefficient, not the full set of coefficients. I'm not sure it even works currently, but it demonstrates how we can generate a BlockMatrixIR directly rather than trying to construct it using the Python-level API. ---. #### Footnotes. [1] Consider Figure 2 from [the PC-Relate paper (Conomos, et al. 2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/). Beyond 3rd degree relatedness (i.e. avuncular pair), SNP-based relatedness (as opposed to haplotype-based) isn't reliable (see Figures 6 and 7). 3rd degree pair have have kinship 0.0625, in expectation, so only keeping entries >=0.03 is very reasonable. gnomAD is even more aggressive [considering only 2nd degree or higher pairs](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/), which presumably corresponds to keeping only entries >=0.0625. ---. #### Further reading. - https://en.wikipedia.org/wiki/Coefficient_of_relationship. ### Version. 0.2.124",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13798:1226,reliab,reliable,1226,https://hail.is,https://github.com/hail-is/hail/issues/13798,1,['reliab'],['reliable']
Availability,"### What happened?. https://batch.azure.hail.is/batches/4375769/jobs/126. ```; io/test/test_batch.py::test_always_run_job_private_instance_cancel ; -------------------------------- live log setup --------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; -------------------------------- live log call ---------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:974,error,error,974,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['error'],['error']
Availability,"### What happened?. https://ci.azure.hail.is/batches/3778899/jobs/47; ```; E hail.utils.java.FatalError: NativeIoException: readAddress(..) failed: Connection reset by peer; E ; E Java stack trace:; E io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer; E 	at ; E ; E ; E ; E Hail version: 0.2.115-330031a5d973; E Error summary: NativeIoException: readAddress(..) failed: Connection reset by peer; ```. I'm not sure why we lost the stack trace. ### Version. 330031a5d9734fd33a50e5651e7a2505f352b239. ### Relevant log output. ```shell; ________________________ test_pc_relate_against_R_truth ________________________; [gw2] linux -- Python 3.8.10 /usr/bin/python3. def test_pc_relate_against_R_truth():; mt = hl.import_vcf(resource('pc_relate_bn_input.vcf.bgz')); > hail_kin = hl.pc_relate(mt.GT, 0.00, k=2).checkpoint(utils.new_temp_file(extension='ht')). test/hail/methods/relatedness/test_pc_relate.py:9: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; <decorator-gen-1104>:2: in checkpoint; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/hail/table.py:1347: in checkpoint; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); <decorator-gen-1106>:2: in write; ???; /usr/local/lib/python3.8/dist-packages/hail/typecheck/check.py:584: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.8/dist-packages/hail/table.py:1393: in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:490: in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=timed)); /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:481: in _cancel_on_ctrl_c; return async_to_bl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:223,Error,Errors,223,https://hail.is,https://github.com/hail-is/hail/issues/12980,3,"['Error', 'checkpoint']","['Error', 'Errors', 'checkpoint']"
Availability,### What happened?. https://discuss.hail.is/t/error-summary-classtoolargeexception-class-too-large/3491. Likely due to the large number of fields. We presumably we were able to parse this at some point. It would be good to at least understand why there's been a regression. ### Version. 0.2.108. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13249:46,error,error-summary-classtoolargeexception-class-too-large,46,https://hail.is,https://github.com/hail-is/hail/issues/13249,1,['error'],['error-summary-classtoolargeexception-class-too-large']
Availability,### What happened?. https://discuss.hail.is/t/matrixtable-filter-rows-produces-error-for-data-on-secure-lustre/3344/2. Seems like we drop the file:// scheme at some point when generating code that uses PartitionNativeIntervalReader. ### Version. ????. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13998:79,error,error-for-data-on-secure-lustre,79,https://hail.is,https://github.com/hail-is/hail/issues/13998,1,['error'],['error-for-data-on-secure-lustre']
Availability,### What happened?. https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/setting.20the.20gcloud.20project. An example of a user problem is linked above. It is difficult to discover how to set the GCS requester pays project id for Hail Query-on-Batch and hailtop.fs. We should error or warn on invalid names (error seems best?). We should have a list of valid names and make that documentation available publicly. ### Version. 0.2.118. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13195:293,error,error,293,https://hail.is,https://github.com/hail-is/hail/issues/13195,3,"['avail', 'error']","['available', 'error']"
Availability,"### What happened?. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/QoB.20Error.3A.20GoogleJsonResponseException.3A.20404.20Not.20Found/near/398355473. > I was running hl.pca on the wheel you created for me -> 0.2.124-fcaafc533ec1. and there seems to be a transient error going on https://batch.hail.is/batches/8069235?q=state+%3D+failed, not sure whether this is the same as the previous ones. I just cancelled the job before error summary appears. and here is the code I am running:. ```python3; vat_ht = hl.read_table(get_aou_util_path(name=""vat"")); vat_ht = vat_ht.collect_by_key(); meta_ht = hl.read_table(get_sample_meta_path(annotation=True)); meta_ht = meta_ht.filter(~meta_ht.related); pops = args.pops.split("","") if (args.pops is not None) else POPS; for pop in pops:; mt = get_filtered_mt(analysis_type='variant', filter_variants=True, filter_samples=False,; adj_filter=True, pop=pop); variants_to_keep = vat_ht.filter(; (vat_ht.locus.in_autosome()) &; (hl.is_snp(vat_ht.alleles[0], vat_ht.alleles[1])) &; (vat_ht['values'][f'gvs_{pop}_af'][0] >= 0.0001) &; ((vat_ht.values[f""gvs_{pop}_an""][0] >= (N_SAMPLES[pop] * 2 * MIN_CALL_RATE[pop]))); ); print('Filtering Variants...'); mt = mt.filter_rows(hl.is_defined(variants_to_keep[mt.row_key])) # filter to high quality variants; print('Filtering Samples...'); mt = mt.filter_cols(hl.is_defined(meta_ht[mt.col_key])) # filter to unrelated samples -> later to project; print('Running PCA...'); eigenvalues, scores, loadings = hl.pca(; hl.int(hl.is_defined(mt.GT)),; compute_loadings=True,; k=50,; ); print('Writing tables...'); eigenvalues.write(; get_pca_ht_path(pop=pop, name='evals'),; overwrite=args.overwrite,; ); scores.write(; get_pca_ht_path(pop=pop, name='scores'),; overwrite=args.overwrite,; ); loadings.write(; get_pca_ht_path(pop=pop, name='loadings'),; overwrite=args.overwrite,; ); ```. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979:292,error,error,292,https://hail.is,https://github.com/hail-is/hail/issues/13979,2,['error'],['error']
Availability,"### What happened?. while trying to run the following code I get the error mention in the title (Invalid maximum heap size: -Xmx0m). import hail as hl; hl.init(default_reference=""GRCh38""). However I tried to resolve the issue with overloading the default setting with new values for spark configuration (command below), unfortunately the error still exists; hl.init(driver_memory='1024m’). ### Version. latest version used in allOfUs research workbench platform. ### Relevant log output. ```shell; Invalid maximum heap size: -Xmx0m; Error: Could not create the Java Virtual Machine.; Error: A fatal exception has occurred. Program will exit.; ---------------------------------------------------------------------------; RuntimeError Traceback (most recent call last); Cell In[14], line 2; 1 #hl.init(default_reference=""GRCh38""); ----> 2 hl.init(driver_memory='1024m'). File <decorator-gen-1756>:2, in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions, gcs_bucket_allow_list). File /opt/conda/lib/python3.10/site-packages/hail/typecheck/check.py:587, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 584 @decorator; 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_). File /opt/conda/lib/python3.10/site-packages/hail/context.py:364, in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14489:69,error,error,69,https://hail.is,https://github.com/hail-is/hail/issues/14489,4,"['Error', 'error']","['Error', 'error']"
Availability,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269:177,error,error,177,https://hail.is,https://github.com/hail-is/hail/issues/5269,1,['error'],['error']
Availability,"#10676 correctly points out a documentation error, which I am addressing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10781:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/10781,1,['error'],['error']
Availability,"#12447 Added some assertions to appease mypy checking use of optional types, and these two were too aggressive but aren't necessary to pass the test suite, just noticed the additional error log entries.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12493:184,error,error,184,https://hail.is,https://github.com/hail-is/hail/pull/12493,1,['error'],['error']
Availability,"#133</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/133"">brettcannon/gidgethub#133</a>).</li>; <li>Make the minimum version of PyJWT be v2.0.0.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/brettcannon/gidgethub/blob/main/docs/changelog.rst"">gidgethub's changelog</a>.</em></p>; <blockquote>; <h2>5.2.1</h2>; <ul>; <li>; <p>Fix cgi and importlib_resources deprecations.; (<code>PR [#185](https://github.com/brettcannon/gidgethub/issues/185) &lt;https://github.com/brettcannon/gidgethub/pull/185&gt;_</code>)</p>; </li>; <li>; <p>Add support for Python 3.11 and drop EOL Python 3.6; (<code>PR [#184](https://github.com/brettcannon/gidgethub/issues/184) &lt;https://github.com/brettcannon/gidgethub/pull/184&gt;_</code>)</p>; </li>; </ul>; <h2>5.2.0</h2>; <ul>; <li>Make the minimum version of PyJWT be v2.4.0.</li>; </ul>; <h2>5.1.0</h2>; <ul>; <li>; <p>Use <code>X-Hub-Signature-256</code> header for webhook validation when available.; (<code>PR [#160](https://github.com/brettcannon/gidgethub/issues/160) &lt;https://github.com/brettcannon/gidgethub/pull/160&gt;</code>_).</p>; </li>; <li>; <p>The documentation is now built using Sphinx v&gt;= 4.0.0.; (<code>Issue [#143](https://github.com/brettcannon/gidgethub/issues/143) &lt;https://github.com/brettcannon/gidgethub/issues/143&gt;</code>_)</p>; </li>; <li>; <p>:meth:<code>gidgethub.abc.GitHubAPI.getiter</code> now accepts <code>iterable_key</code> parameter; in order to support the Checks API.; (<code>Issue [#164](https://github.com/brettcannon/gidgethub/issues/164) &lt;https://github.com/brettcannon/gidgethub/issues/164&gt;</code>_)</p>; </li>; <li>; <p>Accept HTTP 202 ACCEPTED as successful.; (<code>PR [#174](https://github.com/brettcannon/gidgethub/issues/174) &lt;https://github.com/brettcannon/gidgethub/pull/174&gt;</code>_)</p>; </li>; </ul>; <h2>5.0.1</h2>; <ul>; <li>Drop the <code>machine-man-preview</code> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:4953,avail,available,4953,https://hail.is,https://github.com/hail-is/hail/pull/12328,1,['avail'],['available']
Availability,"#26</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/ipython/comm/graphs/contributors?from=2024-01-02&amp;to=2024-03-12&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Ablink1073+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Apre-commit-ci+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ipython/comm/blob/main/CHANGELOG.md"">comm's changelog</a>.</em></p>; <blockquote>; <h2>0.2.2</h2>; <p>(<a href=""https://github.com/ipython/comm/compare/v0.2.1...76149e7ee0f331772c964ae86cdb8bafebe6dfa2"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update Release Scripts <a href=""https://redirect.github.com/ipython/comm/pull/27"">#27</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Other merged PRs</h3>; <ul>; <li>chore: update pre-commit hooks <a href=""https://redirect.github.com/ipython/comm/pull/26"">#26</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/ipython/comm/graphs/contributors?from=2024-01-02&amp;to=2024-03-12&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Ablink1073+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Apre-commit-ci+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; <!-- raw HTML omitted -->;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14492:1732,Mainten,Maintenance,1732,https://hail.is,https://github.com/hail-is/hail/pull/14492,1,['Mainten'],['Maintenance']
Availability,#290 - nocompress in vcf wasn't working; #283 - fam export now tab separated; #262 - fatalIf is dead; #246 - add g.fractionReadsAlt; #237 - give tsv annotators a delimiter option. Fixed fatal error in case of old VDS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/294:192,error,error,192,https://hail.is,https://github.com/hail-is/hail/pull/294,1,['error'],['error']
Availability,"#4659 teaches CI to recover from a build job gone missing, but; I neglected to teach CI how to recover from a deploy job gone; missing. This follows the same strategy but for deploy jobs. If a deploy job is not found in the list of refreshed jobs; it is simply removed from the deploy_jobs map. The next heal; stage of CI will kick off a new batch job for whatever the; latest undeployed SHA is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4683:20,recover,recover,20,https://hail.is,https://github.com/hail-is/hail/pull/4683,2,['recover'],['recover']
Availability,"#5228](https://github.com/aio-libs/aiohttp/issues/5228) &lt;https://github.com/aio-libs/aiohttp/issues/5228&gt;</code>_</li>; </ul>; <h2>Misc</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.7.4 (2021-02-25)</h1>; <h2>Bugfixes</h2>; <ul>; <li>; <p><strong>(SECURITY BUG)</strong> Started preventing open redirects in the; <code>aiohttp.web.normalize_path_middleware</code> middleware. For; more details, see; <a href=""https://github.com/aio-libs/aiohttp/security/advisories/GHSA-v6wp-4m6f-gcjg"">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-v6wp-4m6f-gcjg</a>.</p>; <p>Thanks to <code>Beast Glatisant &lt;https://github.com/g147&gt;</code>__ for; finding the first instance of this issue and <code>Jelmer Vernooĳ &lt;https://jelmer.uk/&gt;</code>__ for reporting and tracking it down; in aiohttp.; <code>[#5497](https://github.com/aio-libs/aiohttp/issues/5497) &lt;https://github.com/aio-libs/aiohttp/issues/5497&gt;</code>_</p>; </li>; <li>; <p>Fix interpretation difference of the pure-Python and the Cython-based; HTTP parsers construct a <code>yarl.URL</code> object for HTTP request-target.</p>; <p>Before this fix, the Python parser would turn the URI's absolute-path; for <code>//some-path</code> into <code>/</code> while the Cython code preserved it as; <code>//some-path</code>. Now, both do the latter.; <code>[#5498](https://github.com/aio-libs/aiohttp/issues/5498) &lt;https://github.com/aio-libs/aiohttp/issues/5498&gt;</code>_</p>; </li>; </ul>; <hr />; <h1>3.7.3 (2020-11-18)</h1>; <h2>Features</h2>; <ul>; <li>Use Brotli instead of brotlipy; <code>[#3803](https://github.com/aio-libs/aiohttp/issues/3803) &lt;https://github.com/aio-libs/aiohttp/issues/3803&gt;</code>_</li>; <li>Made exceptions pickleable. Also changed the repr of some exceptions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10115:4507,down,down,4507,https://hail.is,https://github.com/hail-is/hail/pull/10115,1,['down'],['down']
Availability,"#9425 fixes the bug that caused us to need to checkpoint twice. This PR removes the second checkpoint. For the `hl.balding_nichols_model(20, 6000, 50000)`, 2 iterations test I've been doing, this gets us down to more like 35 seconds, as opposed to ~40. Current hail PCA takes more like 16 seconds, so we are getting closer (though again, it's not clear that 2 is going to be the right number of iterations in the end).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9434:46,checkpoint,checkpoint,46,https://hail.is,https://github.com/hail-is/hail/pull/9434,3,"['checkpoint', 'down']","['checkpoint', 'down']"
Availability,$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&uploadType=resumable&upload_id=ADPycduMEzX6d_uX4CiP6_XItJKmP8UnUnYBfyPoselMbyLUkxs1wDLPnxWl5gXr5LnBaVntYR_i7jchyxgVsRb_5PknvcCIcfDJ; chunkOffset: 16777216; chunkLength: 0; localOffset: 0; remoteOffset: 16777216; lastChunk: false. at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); at java.util.concurrent.Executo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:2296,recover,recover,2296,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['recover'],['recover']
Availability,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:2182,error,error,2182,https://hail.is,https://github.com/hail-is/hail/issues/825,3,"['echo', 'error']","['echo', 'error']"
Availability,"$11$adapted(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:637) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	... 12 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 		at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpSt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:17539,error,error,17539,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['error'],['error']
Availability,$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:345); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$1(SparkBackend.scala:690); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.parse_value_ir(SparkBackend.scala:689); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.124-b115f6a6ec23; Error summary: ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:8531,Error,Error,8531,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['Error'],['Error']
Availability,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:14031,Error,Error,14031,https://hail.is,https://github.com/hail-is/hail/issues/3790,1,['Error'],['Error']
Availability,$class.isEmpty(Iterator.scala:330); 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336); 	at is.hail.utils.TextTableReader$$anonfun$14.apply(TextTableReader.scala:218); 	at is.hail.utils.TextTableReader$$anonfun$14.apply(TextTableReader.scala:215); 	at is.hail.utils.richUtils.RichHadoopConfiguration$$anonfun$readLines$extension$1.apply(RichHadoopConfiguration.scala:301); 	at is.hail.utils.richUtils.RichHadoopConfiguration$$anonfun$readLines$extension$1.apply(RichHadoopConfiguration.scala:292); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:285); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readLines$extension(RichHadoopConfiguration.scala:292); 	at is.hail.utils.TextTableReader$.read(TextTableReader.scala:215); 	at is.hail.HailContext$$anonfun$importTables$3.apply(HailContext.scala:516); 	at is.hail.HailContext$$anonfun$importTables$3.apply(HailContext.scala:518); 	at is.hail.HailContext.maybeGZipAsBGZip(HailContext.scala:586); 	at is.hail.HailContext.importTables(HailContext.scala:515); 	at is.hail.HailContext.importTable(HailContext.scala:477); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.8-70304a52d33d; Error summary: MalformedInputException: Input length = 1; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5221:4964,Error,Error,4964,https://hail.is,https://github.com/hail-is/hail/issues/5221,1,['Error'],['Error']
Availability,$mcJ$sp(CompileAndEvaluate.scala:26); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1$$anonfun$1.apply(CompileAndEvaluate.scala:26); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1$$anonfun$1.apply(CompileAndEvaluate.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Hail version: 0.2.32-e973d7f3c15c; Error summary: ArrayIndexOutOfBoundsException: 3366; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8114:2505,Error,Error,2505,https://hail.is,https://github.com/hail-is/hail/issues/8114,1,['Error'],['Error']
Availability,"' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ArrayIndexOutOfBoundsException: 0. Java stack trace:; java.lang.ArrayIndexOutOfBoundsException: 0; 	at org.apache.spark.sql.catalyst.expressions.GenericRow.get(rows.scala:173); 	at org.apache.spark.sql.Row$class.apply(Row.scala:163); 	at org.apache.spark.sql.catalyst.expressions.GenericRow.apply(rows.scala:165); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply$mcZI$sp(Annotation.scala:70); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply(Annotation.scala:70); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply(Annotation.scala:70); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.ArrayOps$ofInt.forall(ArrayOps.scala:234); 	at is.hail.annotations.Annotation$.isSafe(Annotation.scala:70); 	at is.hail.annotations.BroadcastRow.<init>(BroadcastValue.scala:27); 	at is.hail.variant.MatrixTable$.fromRowsTable(MatrixTable.scala:462); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-1ea0ab823b1f; Error summary: ArrayIndexOutOfBoundsException: 0; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3668:3730,Error,Error,3730,https://hail.is,https://github.com/hail-is/hail/issues/3668,1,['Error'],['Error']
Availability,"'data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$findField(ScalaSigReader.scala:100); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$read$1(ScalaSigReader.scala:45); 	at org.json4s.reflect.ScalaSigReader$.readField(ScalaSigReader.scala:49); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:69); 	a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299:1606,Error,Error,1606,https://hail.is,https://github.com/hail-is/hail/issues/6299,1,['Error'],['Error']
Availability,'may or may not' is redundant phrasing. The word 'may' is sufficient to indicate the optional nature of glob expressions in the `path` argument to `import_vcf`. ## Security Assessment; - This change has no security impact. ### Impact Description; Docs only,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14746:20,redundant,redundant,20,https://hail.is,https://github.com/hail-is/hail/pull/14746,1,['redundant'],['redundant']
Availability,"'s the only event loop that will exist forever. Pytest (and newer version of IPython, afaict) violate this pretty liberally. ~~pytest_asyncio has [explicit instructions on how to run every test in the same event loop](https://pytest-asyncio.readthedocs.io/en/latest/how-to-guides/run_session_tests_in_same_loop.html). I've implemented those here.~~ [These instructions don't work](https://github.com/pytest-dev/pytest-asyncio/issues/744). It seems that the reliable way to ensure we're using one event loop everywhere is to use pytest-asyncio < 0.23 and to define an event_loop fixture with scope `'session'`. I also switched test_batch.py into pytest-only style. This allows me to use session-scoped fixtures so that they exist exactly once for the entire test suite execution. Also:; - `RouterAsyncFS` methods must either be a static method or an async method. We must not create an FS in a sync method. Both `parse_url` and `copy_part_size` now both do not allocate an FS.; - `httpx.py` now eagerly errors if the running event loop in `request` differs from that at allocation time. Annoying but much better error message than this nonsense about timeout context managers.; - `hail_event_loop` either gets the current thread's event loop (running or not, doesn't matter to us) or creates a fresh event loop and sets it as the current thread's event loop. The previous code didn't guarantee we'd get an event loop b/c `get_event_loop` fails if `set_event_loop` was previously called.; - `conftest.py` is inherited downward, so I lifted fixtures out of test_copy.py and friends and into a common `hailtop/conftest.py`; - I added `make -C hail pytest-inter-cloud` for testing the inter cloud directory. You still need appropriate permissions and authn.; - I removed extraneous pytest.mark.asyncio since we use auto mode everywhere.; - `FailureInjectingClientSession` creates an `aiohttp.ClientSession` and therefore must be used while an event loop is running. Easiest fix was to make the test async.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14097:1186,error,errors,1186,https://hail.is,https://github.com/hail-is/hail/pull/14097,4,"['Failure', 'down', 'error']","['FailureInjectingClientSession', 'downward', 'error', 'errors']"
Availability,"'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight1(OrderedJoinDistinctIterator.scala:36); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight(OrderedJoinDistinctIterator.scala:42); 	at is.hail.spa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:1654,Error,ErrorHandling,1654,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['Error'],['ErrorHandling']
Availability,"'tsv']}; bgzip {j.counts['tsv']}; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; """"""). b.write_output(j.counts['tsv.gz'], output_path); b.write_output(j.counts['tsv.gz.tbi'], output_index_path); ```. (Trying to use `b.write_output(j.counts, output_base)` to write out the whole ResourceGroup fails because the `…/counts.counts.tsv` file no longer exists because it was removed by `bgzip`. In any case, we don't want to write that one to the final bucket anyway. Hence the two separate `write_output` invocations for the two desired output files.). This fails in the second `write_output` with a fairly mysterious exception:. ```; File ""…"", line 92, in test; b.write_output(j.counts['tsv.gz.tbi'], output_index_path); File ""…/lib/python/site-packages/hailtop/batch/batch.py"", line 595, in write_output; name = resource._source._resources_inverse[resource]; KeyError: __RESOURCE_FILE__11; ```. Checking that line of _batch.py_, it is failing while trying to print an error message because `_resources_inverse` is not set up for the JobResourceFiles within ResourceGroups. PR #13192 is a suggested fix for this. With that PR applied, this results in a more useful hail error message exception:. ```; hailtop.batch.exceptions.BatchException: undefined resource 'counts[""tsv.gz.tbi""]'; Hint: resources must be defined within the job methods 'command' or 'declare_resource_group'; ```. This can be worked around by mentioning the filename in the commands to be run — in a comment, because none of the commands actually need to specify the `.tbi` output filename:. ```python; …; j.command(f""""""; gatk SubCommand … --output {j.counts['tsv']}; bgzip {j.counts['tsv']}; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; : {j.counts['tsv.gz.tbi']}; """"""); …; ```. This produces the desired two files — compressed data and the associated index — written to the final bucket. Is it kosher to use `write_output` on the individual items within a ResourceGroup like this?. However this resource **was** defined ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191:1646,error,error,1646,https://hail.is,https://github.com/hail-is/hail/issues/13191,1,['error'],['error']
Availability,"()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:30217,failure,failure,30217,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['failure'],['failure']
Availability,"().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:1627,failure,failure,1627,https://hail.is,https://github.com/hail-is/hail/issues/3465,1,['failure'],['failure']
Availability,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:14989,Error,Error,14989,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['Error'],['Error']
Availability,"(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowSto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:11550,Error,ErrorHandling,11550,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Error'],['ErrorHandling']
Availability,"(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMet",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:2869,Error,ErrorHandling,2869,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['Error'],['ErrorHandling']
Availability,(Table.scala:1003); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:7658,Error,ErrorHandling,7658,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Error'],['ErrorHandling']
Availability,"(dup_ht[mt.s]) | (mt.s == 'NA12878') | (mt.s == 'syndip'))); File ""<decorator-gen-510>"", line 2, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:1503,failure,failure,1503,https://hail.is,https://github.com/hail-is/hail/issues/3235,1,['failure'],['failure']
Availability,"(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:30159,failure,failure,30159,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['failure'],['failure']
Availability,") [""{\""path\"":\""gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht\""}""] ; !191 = MakeStruct(); !192 = WriteMetadata(!191) [""{\""path\"":\""gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht/globals\""}""] ; !193 = MakeStruct(); !194 = WriteMetadata(!193) [""{\""path\"":\""gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht/cols\""}""] ; !195 = MakeStruct(); !196 = WriteMetadata(!195) [""{\""path\"":\""gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht/rows\""}""] ; !197 = MakeStruct(); !198 = WriteMetadata(!197) [""{\""path\"":\""gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht/entries\""}""]; !199 = Begin(!154, !160, !166, !169, !172, !176, !188, !190, !192, !194, !196, !198); Begin(!11, !199); ```. </details>. Notice that, after lowering to CDAIR, the `WriteMetadata` for; `gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht`, `....ht/globals`, `....ht/cols`,; `...ht/rows`, and `...ht/entries` all appear thrice. The error message is clearly coming from; `WriteMetadata` writing into the root of the Hail Table. ```; !1 = MakeStruct(); !2 = WriteMetadata(!1) [""{\""path\"":\""gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht\"",\""overwrite\"":false,\""refs\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],filters:Set[String],a_index:Int32,was_split:Boolean,variant_qc:Struct{gq_stats:Struct{mean:Float64,stdev:Float64,min:Float64,max:Float64},call_rate:Float64,n_called:Int64,n_not_called:Int64,n_filtered:Int64,n_het:Int64,n_non_ref:Int64,het_freq_hwe:Float64,p_value_hwe:Float64,p_value_excess_het:Float64},info:Struct{AC:Array[Int32],AF:Array[Float64],AN:Int32,homozygote_count:Array[Int32]},`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[Struct{GT:Call,GQ:Int32,RGQ:Int32,FT:String,AD:Array[Int32]}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{__cols:Array[Struct{s:String,mt_sample_qc:Struct{gq_stats:Struct{mean:Float64,stdev:Float64,min:Float64,max:Float64},call_rate:Float64,n_called:Int64,n_not_called:Int64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:42429,error,error,42429,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['error'],['error']
Availability,") and getting the following error with jinja2 (see below).; From the error it seems like this is due to Hail's dependency of bokeh using the latest version of jinja2. Downgrading jinja2 to 3.0.0 solves the problem, and it seems like other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1044,error,error,1044,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['error'],['error']
Availability,") from 1.17.54 to 1.21.13.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.21.13</h1>; <ul>; <li>api-change:<code>synthetics</code>: [<code>botocore</code>] Allow custom handler function.</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Add waiters for server online and offline.</li>; <li>api-change:<code>devops-guru</code>: [<code>botocore</code>] Amazon DevOps Guru now integrates with Amazon CodeGuru Profiler. You can view CodeGuru Profiler recommendations for your AWS Lambda function in DevOps Guru. This feature is enabled by default for new customers as of 3/4/2022. Existing customers can enable this feature with UpdateEventSourcesConfig.</li>; <li>api-change:<code>macie</code>: [<code>botocore</code>] Amazon Macie Classic (macie) has been discontinued and is no longer available. A new Amazon Macie (macie2) is now available with significant design improvements and additional features.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] Documentation updates for Amazon EC2.</li>; <li>api-change:<code>sts</code>: [<code>botocore</code>] Documentation updates for AWS Security Token Service.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] This release updates the *InstanceStorageConfig APIs so they support a new ResourceType: REAL_TIME_CONTACT_ANALYSIS_SEGMENTS. Use this resource type to enable streaming for real-time contact analysis and to associate the Kinesis stream where real-time contact analysis segments will be published.</li>; </ul>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the file system path</li>; <li>api-change:<code>ke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:1026,avail,available,1026,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['avail'],['available']
Availability,") from None; 181 if ir.typ == tvoid:; 182 value = None. File /opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File /opt/conda/lib/python3.10/site-packages/hail/backend/py4j_backend.py:213, in Py4JBackend._rpc(self, action, payload); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content); --> 213 raise fatal_error_from_java_error_triplet(error_json['short'], error_json['expanded'], error_json['error_id']); 214 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: HailException: cannot set missing field for required type +PFloat64. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 6.0 failed 4 times, most recent failure: Lost task 5.3 in stage 6.0 (TID 67) (saturn-machinenumber.c.terra-code.internal executor 4): is.hail.utils.HailException: gs://path/to/bucket/chrY.0002.hard_filtered_with_genotypes.vcf.gz:offset 23933331019603: error while parsing line; chrY	113	.	GG	G,*,AG,CG	596	PASS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,90",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:6075,failure,failure,6075,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['failure'],['failure']
Availability,")._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(IndexedSeq.scala:34); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:39); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:21); at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); at com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:6973,failure,failure,6973,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['failure'],['failure']
Availability,"); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; ```. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:12727,Error,Error,12727,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['Error'],['Error']
Availability,); 	at is.hail.table.Table.take(Table.scala:637); 	at is.hail.table.Table.showString(Table.scala:673); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:21); 	at is.hail.utils.StagingIterator.isValid(Fli,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:8600,Error,ErrorHandling,8600,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['Error'],['ErrorHandling']
Availability,"); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.105-3f053140ad00; Error summary: IllegalArgumentException: requirement failed: Invalid method, methods may have at most 255 arguments, found 1163; Return Type Info: V; Parameter Type Info: JLjava/lang/String;ZJJIZJIJJIJJIJJIJJIJJIJJIJJIJJIJZJIJZJIJZJIJZJIJZJIJZJIJZJIJZJIJZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZJIJZJJIJZZJJIJZZJZJZJZJZJZJZJZJZJIJJIJJZJZJZJZLis/hail/io/OutputBuffer;; ```. Presumably this used to work fine in earlier Hail versions. However, it seems impossible to revert to such a version at the moment, as 0.2.81 is the oldest version that one can still start a Dataproc cluster with -- earlier versions use a Debian image without a fix to the `log4j` vulnerability. 0.2.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:12397,Error,Error,12397,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['Error'],['Error']
Availability,"); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.utils.package$.using(package.scala:596); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); at is.hail.expr.ir.ExecuteCont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8161:1218,error,error,1218,https://hail.is,https://github.com/hail-is/hail/issues/8161,1,['error'],['error']
Availability,"* Fix error in TableKeyByAndAggregate caused by field name clobbering; * Fix error in TableLeftJoinRightDistinct with non-strict left tables; * Fix erroneous key preservation in TableStage.mapPartition; * Add Consume to TypeCheck; * Fix error where globals were not exposed in TableAggregate; * Fix error where globals were not exposed in TableAggregate. New local backend success rate:. ```; 271 failed, 496 passed, 75 skipped, 15 warnings in 368.17 seconds; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9202:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/9202,4,['error'],['error']
Availability,* Hailtop wasn't a module.; * The import paths were fucked.; * There was a syntax error in setup.py; * No dirty trees after builds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6442:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/6442,1,['error'],['error']
Availability,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2109:290,error,errors,290,https://hail.is,https://github.com/hail-is/hail/pull/2109,1,['error'],['errors']
Availability,"* add simulated BGEN file based on distributions in real data; * add import, info score, filter benchmarks; * fix bad error message in `export_bgen`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6976:118,error,error,118,https://hail.is,https://github.com/hail-is/hail/pull/6976,1,['error'],['error']
Availability,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2329:364,toler,tolerate,364,https://hail.is,https://github.com/hail-is/hail/pull/2329,3,"['error', 'toler']","['error', 'tolerate']"
Availability,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2039:319,error,error,319,https://hail.is,https://github.com/hail-is/hail/pull/2039,1,['error'],['error']
Availability,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2124:108,error,error,108,https://hail.is,https://github.com/hail-is/hail/pull/2124,1,['error'],['error']
Availability,"** Actually used python stdlib package `difflib`. This PR improves error messages for things like:. ```text; In [3]: ds.INFO; AttributeError: MatrixTable instance has no field, method, or property 'INFO'; Did you mean:; Data field: 'info' [row]; ```; ```text; In [4]: ds['INFO']; LookupError: MatrixTable instance has no field 'INFO'; Did you mean:; 'info' [row field]; Hint: use 'describe()' to show the names of all data fields.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2735:67,error,error,67,https://hail.is,https://github.com/hail-is/hail/pull/2735,1,['error'],['error']
Availability,"**Things still to do:**; 1. Docs for importbgen, importgen, dosage representation, info score in variant qc; 2. Make sure info score is computed properly -- either implement correctly for non-autosomal variants or return None; 3. Add tests for info score (once we finalized how we're computing); 4. Remove null variant in GenotypeBuilder (from import plink block reader code); 5. Decide how to handle fake ref for multiallelics when original genotype call was null (could be because of rounding errors we get same integer value for close doubles such as 0.4035 and 0.4021); 6. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 7. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 8. Update the readFam function in import plink to utilize functionality Jon wrote already. **Questions:**; 1. I set the default value of --no-compress to true for `importplink`, `importgen`, and `importbgen`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/372:495,error,errors,495,https://hail.is,https://github.com/hail-is/hail/pull/372,1,['error'],['errors']
Availability,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9222:894,error,error,894,https://hail.is,https://github.com/hail-is/hail/pull/9222,1,['error'],['error']
Availability,"*kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192); 	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186); 	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69); 	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22); 	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:105); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.58-3f304aae6ce2; Error summary: NullPointerException: null; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9600:2995,Error,Error,2995,https://hail.is,https://github.com/hail-is/hail/issues/9600,1,['Error'],['Error']
Availability,"*kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/bin/anaconda3/lib/python3.6/site-packages/hail/methods/impex.py in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2110,Error,Error,2110,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Error'],['Error']
Availability,", 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). t1.join(t2, how='outer').show(). # or. t1.join(t2, how='right').show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. FatalError: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 24, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:21); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:1116,Error,ErrorHandling,1116,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['Error'],['ErrorHandling']
Availability,", **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 times, most recent failure: Lost task 15.3 in stage 16.0 (TID 178, ip-172-31-1-20.ec2.internal, executor 4): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:4822,Error,Error,4822,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['Error'],['Error']
Availability,", **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:29807,Error,Error,29807,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['Error'],['Error']
Availability,", full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:5740,failure,failure,5740,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['failure'],['failure']
Availability,", min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:4585,error,error,4585,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['error'],['error']
Availability,", not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2019-10-22T09:25:42.477556224Z"",; ""finished_at"": ""2019-10-22T09:25:42.476019599Z"",; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.031185626983642578,; ""creating"": 0.09947538375854492,; ""starting"": 4.786264657974243,; ""running"": 0.44185924530029297,; ""runtime"": 5.228753566741943,; ""uploading_log"": 0.22835826873779297,; ""deleting"":",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7354:1873,Error,Error,1873,https://hail.is,https://github.com/hail-is/hail/pull/7354,1,['Error'],['Error']
Availability,", the 8.0.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-0</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/9?closed=1"">https://github.com/pallets/click/milestone/9?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/blob/main/CHANGES.rst"">click's changelog</a>.</em></p>; <blockquote>; <h2>Version 8.1.3</h2>; <p>Released 2022-04-28</p>; <ul>; <li>Use verbose form of <code>typing.Callable</code> for <code>@command</code> and; <code>@group</code>. :issue:<code>2255</code></li>; <li>Show error when attempting to create an option with; <code>multiple=True, is_flag=True</code>. Use <code>count</code> instead.; :issue:<code>2246</code></li>; </ul>; <h2>Version 8.1.2</h2>; <p>Released 2022-03-31</p>; <ul>; <li>Fix error message for readable path check that was mixed up with the; executable check. :pr:<code>2236</code></li>; <li>Restore parameter order for <code>Path</code>, placing the <code>executable</code>; parameter at the end. It is recommended to use keyword arguments; instead of positional arguments. :issue:<code>2235</code></li>; </ul>; <h2>Version 8.1.1</h2>; <p>Released 2022-03-30</p>; <ul>; <li>Fix an issue with decorator typing that caused type checking to; report that a command was not callable. :issue:<code>2227</code></li>; </ul>; <h2>Version 8.1.0</h2>; <p>Released 2022-03-28</p>; <ul>; <li>; <p>Drop support for Python 3.6. :pr:<code>2129</code></p>; </li>; <li>; <p>Remove previously deprecated code. :pr:<code>2130</code></p>; <ul>; <li><code>Group.resultcallback</code> is renamed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11808:2776,error,error,2776,https://hail.is,https://github.com/hail-is/hail/pull/11808,1,['error'],['error']
Availability,", timed); 77 (result, timings) = (result_tuple._1(), result_tuple._2()); 78 value = ir.typ._from_encoding(result). File ~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args); 1315 command = proto.CALL_COMMAND_NAME +\; 1316 self.command_header +\; 1317 args_command +\; 1318 proto.END_COMMAND_PART; 1320 answer = self.gateway_client.send_command(command); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1324 for temp_arg in temp_args:; 1325 temp_arg._detach(). File ~/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py:35, in handle_java_exception.<locals>.deco(*args, **kwargs); 33 tpl = Env.jutils().handleForPython(e.java_exception); 34 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'). Java stack trace:; java.lang.ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); 	at is.hail.expr.ir.functions.RegistryFunctions.unwrapReturn(Functions.scala:364); 	at is.hail.expr.ir.Emit.$anonfun$emitI$85(Emit.scala:1173); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:35",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:3720,Error,Error,3720,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Error'],['Error']
Availability,", tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel...; use 'python setup.py download_pandoc' to download pandoc.; usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]; or: setup.py --help [cmd1 cmd2 ...]; or: setup.py --help-commands; or: setup.py cmd --help; ; error: invalid command 'bdist_wheel'; ----------------------------------------; ERROR: Failed building wheel for pypandoc; ERROR: Failed to build one or more wheels; Traceback (most recent call last):; File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/site-packages/setuptools/installer.py"", line 128, in fetch_build_egg; subprocess.check_call(cmd); File ""/Users/spascal/.pyenv/versions/3.7.9/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/Users/spascal/.pyenv/versions/3.7.9/bin/python3.7', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/tmpspi2awj3', '--quiet', 'pypandoc']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py"", line 224, in <module>; 'Programming Language :: Python :: Implementati",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:2229,error,error,2229,https://hail.is,https://github.com/hail-is/hail/issues/9742,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,", two new hypothesis tests, three; new sample statistics, a class for greater control over calculations; involving covariance matrices, and many other enhancements.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.datasets</code> introduction</h1>; <ul>; <li>A new dedicated <code>datasets</code> submodule has been added. The submodules; is meant for datasets that are relevant to other SciPy submodules ands; content (tutorials, examples, tests), as well as contain a curated; set of datasets that are of wider interest. As of this release, all; the datasets from <code>scipy.misc</code> have been added to <code>scipy.datasets</code>; (and deprecated in <code>scipy.misc</code>).</li>; <li>The submodule is based on <a href=""https://www.fatiando.org/pooch/latest/"">Pooch</a>; (a new optional dependency for SciPy), a Python package to simplify fetching; data files. This move will, in a subsequent release, facilitate SciPy; to trim down the sdist/wheel sizes, by decoupling the data files and; moving them out of the SciPy repository, hosting them externally and</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/dde50595862a4f9cede24b5d1c86935c30f1f88a""><code>dde5059</code></a> REL: 1.10.0 final [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/7856f281b016c585b82d03723c4494bcdbdcd4a5""><code>7856f28</code></a> Merge pull request <a href=""https://redirect.github.com/scipy/scipy/issues/17696"">#17696</a> from tylerjereddy/treddy_110_final_prep</li>; <li><a href=""https://github.com/scipy/scipy/commit/205b6243c6d075d05695e7ac6d007e0f03bfbf42""><code>205b624</code></a> DOC: add missing author</li>; <li><a href=""https://github.com/scipy/scipy/commit/1ab9f1b10145f0a974d5531700e72d1fb4229b76""><code>1ab9f1b</code></a> DOC: update 1.10.0 relnotes</li>; <li><a href=""https://github.com/scipy/scipy/commit/ac2f45fbe1e39a8f52c1ea2e687640",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:2688,down,down,2688,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['down'],['down']
Availability,",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14296:11784,avail,available,11784,https://hail.is,https://github.com/hail-is/hail/pull/14296,1,['avail'],['available']
Availability,",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JINJA2-6150717"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PROMPTTOOLKIT-6141120"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,624,531,556,604,589,726,434,589,449,399,696,589,479,519,509,711,701,586,586,384,494,539,589],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Cross-site Scripting (XSS)](https://learn.snyk.io/lesson/xss/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:14354,avail,available,14354,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,",; 489 url=url,; 490 body=request.body,; 491 headers=request.headers,; 492 redirect=False,; 493 assert_same_host=False,; 494 preload_content=False,; 495 decode_content=False,; 496 retries=self.max_retries,; 497 timeout=timeout,; 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 785 e = ProtocolError(""Connection aborted."", e); --> 787 retries = retries.increment(; 788 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]; 789 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, method, url, response, error, _pool, _stacktrace); 549 if read is False or not self._is_method_retryable(method):; --> 550 raise six.reraise(type(error), error, _stacktrace); 551 elif read is not None:. File /opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py:769, in reraise(tp, value, tb); 768 if value.__traceback__ is not tb:; --> 769 raise value.with_traceback(tb); 770 raise value. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 702 # Make the request on the httplib connection object.; --> 703 httplib_response = self._make_request(; 704 conn,; 705 method,; 706 url,; 707 timeout=timeout_obj,; 708 body=body,; 709 headers=headers,; 710 chunked=chunked,; 711 ); 713 # If we're going to release the connection in ``finally:``, then; 714 # the response doesn't need to know about the connection. Otherwise; 715 # it will also try to release it and we'll have a double-release; 716 # mess. File /opt/conda/lib/python3.10/site-packages/urllib3/connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:7410,error,error,7410,https://hail.is,https://github.com/hail-is/hail/issues/13960,2,['error'],['error']
Availability,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1725:2888,error,error,2888,https://hail.is,https://github.com/hail-is/hail/issues/1725,3,"['error', 'failure']","['error', 'failure']"
Availability,"- **Requires the python modules ** `nbsphinx`, `matplotlib`, `pandas`, `numpy`, and `seaborn`.; - Use property `-Dtutorial.home=/path/to/tutorial/files` with `gradle` to avoid downloading tutorial files with `wget`.; - Added new tgz file with tutorial files (reduced number of samples to 248 from 2535) https://storage.googleapis.com/hail-tutorial/Hail_Tutorial_Data-v2.tgz; - Edited tutorial to reflect smaller input file.; - Added iPython notebook to repository (this should be edited from now on); - Added tutorial to Sphinx docs.; - Changed tutorial location on website.; - Removed old tutorial infrastructure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1374:176,down,downloading,176,https://hail.is,https://github.com/hail-is/hail/pull/1374,1,['down'],['downloading']
Availability,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5921:234,error,error,234,https://hail.is,https://github.com/hail-is/hail/pull/5921,1,['error'],['error']
Availability,- BPE fix plus ignoring exceeded allocation scheduler errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9467:54,error,errors,54,https://hail.is,https://github.com/hail-is/hail/pull/9467,1,['error'],['errors']
Availability,"- Basic JSON format for NGINX access logs; - `message`, `remote_address`, `request_duration`, `response_status` and `x_real_ip` should match the Python Access logger so we should be able to query these fields all at once; - Unfortunately the NGINX `error_log` does not allow the same custom formatter, but we can create multiple, [custom access logs](https://www.nginx.com/blog/diagnostic-logging-nginx-javascript-module/#proxy_next_upstream) based on arbitrary failure criteria if there's a particular class of error logs we want to get in the future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9928:462,failure,failure,462,https://hail.is,https://github.com/hail-is/hail/pull/9928,2,"['error', 'failure']","['error', 'failure']"
Availability,"- Botocore (AWS) produces its own ConnectionClosedError (:sad:).; - TIMEDOUT is yet another transient socket issue; - I have seen EAI_NONAME as a transient error, even though I would hope to receive EAI_AGAIN.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10924:156,error,error,156,https://hail.is,https://github.com/hail-is/hail/pull/10924,1,['error'],['error']
Availability,"- Expose init_local.; - Fix formatting of some error messages (stray }).; - Fix index paths, they don't have a ""parts"" component, have "".idx"" suffix. This showed up as an issue interopreating between Spark and local modes. FYI @tpoterba rather than just testing them independently, it might be worthwhile to have write/read interop tests between the various backends. Spark to local is partially tested by the pre-existing (matrix)tables tests, but not the other way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9596:47,error,error,47,https://hail.is,https://github.com/hail-is/hail/pull/9596,1,['error'],['error']
Availability,- I don't know why I was recomputing the whole output index instead of just moving forward one. The loops will always just iterate down the new data buffer. Also means I can use the normal array builder abstraction instead.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6029:131,down,down,131,https://hail.is,https://github.com/hail-is/hail/pull/6029,1,['down'],['down']
Availability,"- In the `combOp`, the `TakeByAggregator` was calling the `seqOp` which recomputes the sort-key. Since the evaluation context was not set correctly for the keys, the expression that evaluates the sort-key obviously produced the wrong value for the given key. - `TakeByAggregator.result` used `PriorityQueue.toArray`, which is not guaranteed to produce the elements in sorted order, according to [the PriorityQueue docs](http://www.scala-lang.org/api/current/scala/collection/mutable/PriorityQueue.html). Instead, we must `clone` the `PriorityQueue` and then `dequeueAll` the elements. I'm not certain the `clone` is necessary. @cseed, does the Aggregator interface permit multiple calls to `result`?. > Only the dequeue and dequeueAll methods will return elements in priority order (while removing elements from the heap). Standard collection methods including drop, iterator, and toString will remove or traverse the heap in whichever order seems most convenient. I also added some fairly robust tests that compare `takeBy(f, 10)` to `collect().sortBy(f)`. In particular: ; - the `takeBy` should be a prefix of `sortBy` when lensing by `f`,; - for every sort-key except last one, the elements should be the same as in `sortBy`, and; - for the last sort-key, the elements should be a subset of those in `sortBy`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1761:990,robust,robust,990,https://hail.is,https://github.com/hail-is/hail/pull/1761,1,['robust'],['robust']
Availability,"- Nested arrays appear to be supported in the current code, and I don't think this is intended.; - Why do exportFormat and exportInfo differ?; - Number is accessed from the field attrs without being checked. There could be something silly in there.; - Bad error messages (don't say which field was the problem)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1820:256,error,error,256,https://hail.is,https://github.com/hail-is/hail/issues/1820,1,['error'],['error']
Availability,- Only batch for now -- will add for pipeline and ci later; - This should fail until the kubernetes secret is added; - Requires a password `CLOUD_SQL_PASSWORD` to run the tests locally; (not sure what the best way to distribute this is); - Requires downloading the `cloud_sql_proxy` binary to run the tests locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5615:249,down,downloading,249,https://hail.is,https://github.com/hail-is/hail/pull/5615,1,['down'],['downloading']
Availability,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2063:228,error,errors,228,https://hail.is,https://github.com/hail-is/hail/pull/2063,2,['error'],['errors']
Availability,"- Using `-x` prints every line of the script to an error log entry. The main thing we're concerned about here is if it can fetch from notebook, so capture that output in `image-fetch-output.log` and cat that out to `stderr` only if it failed so it shows up as errors in the logs. - Removed old templating from before the `jinja2` times. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9930:51,error,error,51,https://hail.is,https://github.com/hail-is/hail/pull/9930,2,['error'],"['error', 'errors']"
Availability,"- Using the full domain name instead of the shorthand `<service>.<namespace>` isn't strictly necessary but just made me nervous and I opted for the full unambiguous domain for in-cluster services (what if we had a namespace named `com`?? I feel like that would break some things); - I got the configuration wrong on how to tell envoy *not* to worry about certs of internal namespaces (I'd recommend using the `split` view for the diff because otherwise it's pretty hard to read); - For internal namespaces, using the `prefix` parameter for matching a route allowed `/foo/batch` to match a route like `/foo/batch-driver` which is obviously not great. the `path_separated_prefix` parameter actually does what we want; - Fixed the batch tests not to look at the HTTP 1.1 reason phrase and instead look at the response body to determine the error",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12392:837,error,error,837,https://hail.is,https://github.com/hail-is/hail/pull/12392,1,['error'],['error']
Availability,"- Values of types other than `Array` and `Boolean` get output in VCF format (e.g. `.` instead of `NA` for missing values); - `NaN` values are converted to missing (`.`) when exporting VCF since VCF doesn't handle `NaN`; - Changes to handling of filters:; - `.` <=> `NA:Set[String]`; - `PASS` <=> `{}:Set[String]`; - `other` <=> `{""other""}:Set[String]""`; - Removed `va.pass` entirely (redundant with `va.filters` and needs constant synchronization)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1517:384,redundant,redundant,384,https://hail.is,https://github.com/hail-is/hail/pull/1517,1,['redundant'],['redundant']
Availability,"- [ ] (@tpoterba) caf1e1e673 add fails_service_backend; - [ ] (@tpoterba, @cseed) a979dfba58 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) dcf026b01c [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) 807f38c20e [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) 12df8eb456 [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) 03357ee83d [query-service] make user cache thread-safe; - [ ] (@tpoterba) 6c6734bc71 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) a3d2572ce7 [shuffler] log ShuffleCodecSpec anytime it is created; - [ ] (@daniel-goldstein) 8949dfec3c [scala-lsm] bugfix: least key may equal greatest key; - [ ] (@daniel-goldstein) 6067bd8e51 [services] discovered new transient error; - [ ] (@daniel-goldstein) c8356d30bb [shuffler] more assertions in ShuffleClient; - [ ] (@daniel-goldstein) 9991da90f0 [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [ ] (@daniel-goldstein) bc0140ab6f [query-service] move hail.jar earlier in Dockerfile; - [ ] (@daniel-goldstein) f96c28174d [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 6ae26339fe [query-service] simplify socket handling; - [ ] (@jigold) f3db30e23f [batch] teach JVMJob where to find the hail configuration files; - [ ] (@daniel-goldstein) b5c6d85554 [query-service] switch to services team approved logging; - [ ] (@tpoterba) 35a306c066 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 051c89b8e7 [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) ad9ea73d7a [query-service] run tests against query service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072:814,error,error,814,https://hail.is,https://github.com/hail-is/hail/pull/10072,1,['error'],['error']
Availability,"- [ ] Signature is wrong for load_references_from_dataset; * abstractMethod returns []. This doesn't do anything (`load_references_from_dataset` still needs to be implemented), and is also not what the concrete implementation of SparkBackend returns (which is a string).; * In general I think we should have type annotations on these methods, to document the expected inputs, outputs. - [ ] ServiceBackend needs to implement load_references_from_dataset. - [ ] ServiceBackend constructor is wrong. states it takes `deploy_config`, but backend.py passes a string (apiserver_url). Error generated is `str has no method base_url`. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7068:579,Error,Error,579,https://hail.is,https://github.com/hail-is/hail/issues/7068,1,['Error'],['Error']
Availability,"- [ ] annotate with mypy; - [ ] add pylint to build; - [x] add basic unit tests; - [x] docs in python; - [x] tutorial in python; - [ ] run tests on dataproc; - [x] delete commands, state, no args4j; - [ ] accumulators/error reporting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1072:218,error,error,218,https://hail.is,https://github.com/hail-is/hail/issues/1072,1,['error'],['error']
Availability,"- [ ] input pods: exit if the PVC is not empty; - [ ] main pods: use initContainers to delete all content other than /io/inputs; - [ ] exit pods: use pod_name in path and add a _SUCCESS file on completion; - [ ] input pods: use any input directory containing a _SUCCESS file; - [x] if a failure occurs, check the database before mark_unscheduled. e.g. if `read_log` fails because the pod doesn't exist, check if the log is already uploaded. if it is, do nothing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6494:287,failure,failure,287,https://hail.is,https://github.com/hail-is/hail/issues/6494,1,['failure'],['failure']
Availability,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5199:297,down,download,297,https://hail.is,https://github.com/hail-is/hail/issues/5199,1,['down'],['download']
Availability,"- [x] DANN, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4341060/; - [x] CADD, http://europepmc.org/abstract/med/11384848; - [x] reference genomes 95, GRCh37 & 38., https://useast.ensembl.org/index.html; - [x] low complexity regions 95, GRCh37 & 38, https://useast.ensembl.org/index.html; - [x] Gencode (names and gene metadata) v19 GRCh37, v38 GRCh38, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3431492/, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3431492/; - [x] gnomAD LOF by gene, https://gnomad.broadinstitute.org/; - [x] gnomAD sites exomes & genomes (includes VEP, AF, etc.), https://gnomad.broadinstitute.org/; - [ ] MAF for 1KG, should be in the VCFs which are available via FTP here: http://www.internationalgenome.org/data; - [ ] VEP (?? need more information about what is requested); - [ ] which allele is ancestral (?? Elizabeth has more info?); - [X] clinvar for GRCh37 & 38; - [ ] LCR (?? is gnomAD sufficient?); - [ ] GERP or other sequence conservation score; - [ ] major exome platform capture areas (?? need more information); - [ ] All the missense annotation scores (from dbNSFP, ftp://dbnsfp.softgenetics.com/dbNSFP4.0a.zip)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6715:674,avail,available,674,https://hail.is,https://github.com/hail-is/hail/issues/6715,1,['avail'],['available']
Availability,"- [x] There should be a test in the tests that verify that the account is operational, so if it disabled, we get an informative error message. (see issue #4533). - [x] There should be documentation/a playbook about how to get Github unstuck when this happens. (see [here](; https://github.com/hail-is/hail/issues/4517#issuecomment-429135514)). - [ ] We should mock Github so we don't rely on it during the tests, which makes me sad because yay integration tests, and what do we do when the Github API changes/breaks/doesn't behave the way we expect?. Assigning to @danking since he's been through the first two, but maybe someone else can handle the third.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517:128,error,error,128,https://hail.is,https://github.com/hail-is/hail/issues/4517,1,['error'],['error']
Availability,"- add PIndexableValue. This represents canonical array, set and dict values.; - use in ArrayRef; - newP{Local, FIeld} now has a `PV <: PValue` type parameter to eliminate some casting. Where I'm going:. There will be a abstract PValue with the interface for each virtual type (container/indexable, base struct, etc.) Each concrete PType will have a corresponding PValue implementation (in this case, PCanonicalArray is implemented by PCanonicalIndexableValue.) I think this will allow us to get rid of PArrayBackedContainer. Only primitive PValues will have a code method (since other types might be compound). The code generator should then dispatch through downcasts of PValues get access to the relevant methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8213:659,down,downcasts,659,https://hail.is,https://github.com/hail-is/hail/pull/8213,1,['down'],['downcasts']
Availability,- add priority class infrastructure throughout; - all pod specs have resource requests and limits; - make es tolerate preemptibles; - make monitoring router tolerate preemptibles. Deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7425:109,toler,tolerate,109,https://hail.is,https://github.com/hail-is/hail/pull/7425,2,['toler'],['tolerate']
Availability,"- added `keep_higher_maf` option (`true` by default) to `ld_prune` to prefer to keep higher MAF variants in the global (MIS) stage.; - improved the `ld_prune` flow to reduce duplicated work; - set BlockMatrix.entries to set `i` and `j` as key fields and improved its doc; - corrected references to standard deviation that are actually n times standard deviation, i.e. centered length; - switched `computeCoverByUpperTriangularBlocks` to use the newer`rowIntervalsBlocks` rather than `rectanglesBlocks` directly. I've tested `keep_higher_maf` in notebooks, will add a test of MAF soon and then assign. @danking rather than:; ```; def tie_breaker(l, r):; return hl.cond(l.twice_maf > r.twice_maf,; -1,; hl.cond(l.twice_maf < r.twice_maf,; 1,; 0)); ```; I'd prefer:; ```; def tie_breaker(l, r):; return hl.signum(r.twice_maf - l.twice_maf); ```; I'm having trouble figuring out why the latter throws the error below. Your `tie_breaker` code looks like it should work with Int32Expressions. Any idea what's going on?; ```; FatalError: ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row. Java stack trace:; java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row; 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3704:901,error,error,901,https://hail.is,https://github.com/hail-is/hail/pull/3704,1,['error'],['error']
Availability,- automatically download catch.hpp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5329:16,down,download,16,https://hail.is,https://github.com/hail-is/hail/pull/5329,1,['down'],['download']
Availability,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331:16,down,download,16,https://hail.is,https://github.com/hail-is/hail/pull/5331,2,"['down', 'error']","['download', 'error']"
Availability,- better error message for invalid export type; - don't support nested collections (bugfix),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1821:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/1821,1,['error'],['error']
Availability,"- conda environments are always up to date and enabled (important for developers); - flake8 and pylint are test (and, ergo, CI) dependencies; - use pytest with `--first-failure` so that re-runs run the failing tests first (important for developers)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4924:169,failure,failure,169,https://hail.is,https://github.com/hail-is/hail/pull/4924,1,['failure'],['failure']
Availability,- fix double-counting error in openNoCompression method ; - add tests to read/write more bytes than can be held in the ByteBuffer; - abstract out seek method into FS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11861:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/11861,1,['error'],['error']
Availability,- fixed serialization error with `oneHotGenotype` and `oneHotAlleles`; - Call type is a `java.lang.Integer`; - Missing value is `null`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1532:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/1532,1,['error'],['error']
Availability,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875:103,failure,failures,103,https://hail.is,https://github.com/hail-is/hail/pull/7875,4,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,"- gateway does all the reverse proxy; - site does the polling and serves the website up with nginx; - stop using crontab, just poll in the background; - set up liveness and readiness checks for 0 downtime changes; - ripped out letsencrypt stuff, which I will PR separately; - made certs into a secret. And something that got glommed on:. - put projects into a single file (projects.txt), still a lot to do here but I guess it is an improvement. FYI this is deployed now",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4602:196,downtime,downtime,196,https://hail.is,https://github.com/hail-is/hail/pull/4602,1,['downtime'],['downtime']
Availability,"- gateway does all the reverse proxy; - site does the polling and serves the website up with nginx; - stop using crontab, just poll in the background; - set up liveness and readiness checks for 0 downtime changes; - ripped out letsencrypt stuff, which I will PR separately; - made certs into a secret. And something that got glommed on:. - put projects into a single file (projects.txt), still a lot to do here but I guess it is an improvement. FYI this is deployed now. fixes #4464; fixes #4463",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4601:196,downtime,downtime,196,https://hail.is,https://github.com/hail-is/hail/pull/4601,1,['downtime'],['downtime']
Availability,- generate appropriate error message; - don't test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8637:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/8637,1,['error'],['error']
Availability,- made sure the batch client is closed after a pipeline finishes; - fixed cpu and memory to take ints and floats; - Split AsyncWorkerPool into AsyncWorkerPool and AsyncThrottledGather; - Raise errors in AsyncThrottledGather (errors ignored in AsyncWorkerPool),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7333:193,error,errors,193,https://hail.is,https://github.com/hail-is/hail/pull/7333,2,['error'],['errors']
Availability,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194:158,avail,available,158,https://hail.is,https://github.com/hail-is/hail/pull/5194,1,['avail'],['available']
Availability,"- moved k8s accounts, roles and bindings to vdc/k8s-config.yaml (applied to current cluster); - created deploy service account with privileges on default (should lock down to minimal set); - added batch service account option, use to launch deploy job with deploy-svc; - give batch-svc default-deploy binding so it can launch the deploy pod with suitable privileges",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4596:167,down,down,167,https://hail.is,https://github.com/hail-is/hail/pull/4596,1,['down'],['down']
Availability,- provide a default session; - made cancel not wait for all jobs to be cancelled before returning; - some log statements; - fix error catching in retry function; - add retry to batch client create jobs; - add cancel if batch is not submitted successfully,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6557:128,error,error,128,https://hail.is,https://github.com/hail-is/hail/pull/6557,1,['error'],['error']
Availability,- remove some redundant project git ignores; - add a few more file types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8192:14,redundant,redundant,14,https://hail.is,https://github.com/hail-is/hail/pull/8192,1,['redundant'],['redundant']
Availability,"- removed repeated imports in dataset; - fixed typos and clarified docs; - collapsed RegUtils to provide one ingest function each for hardCalls, dosages, and keyedRows. These optionally impute missing and produce sparse, dense, and dense vectors respectively. The first two also optionally take a mask. hardCalls no longer checks for constant vectors.; - killed the two RegUtils mutate matrix functions entirely; - simplified code in regression methods taking Vector[Double] input; - temp added constantHardCalls to RegUtils and in regressions to retrofit changes to preserve 0.1 behavior of missing for constant vectors. In 0.2 constant vectors will be treated the same as others, resulting in lack of convergence, Double.NaN, etc.; - temp added hardCallsAndAC to RegUtils and in linreg to retrofit changes to preserve 0.1 behavior of calculating AC pre-imputation for filtering. In 0.2, AC will be calculated post-imputation for hardCalls and dosages using sum(x); - combined empirical and HWE normalized arrays into one function; - removed various default parameters in tests. About 75 lines are due to retrofit, so for 0.2 it's about 200 added and 420 deleted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1863:297,mask,mask,297,https://hail.is,https://github.com/hail-is/hail/pull/1863,1,['mask'],['mask']
Availability,"- retry every deadlock in two deadlock prone SQL operations; - add prometheus metrics for cores; - fix prometheus when you're not in the default namespace; - retry every docker 500 error, it's 500, not our fault, just retry, right?; - create a billing account for the dev deploying user; - rewrite a couple queries to harmonize table locking a bit; - add globals to delete tables script; - fix list_batches, which was broken by the query language changes; - include primary services developers' namespaces in prometheus monitoring. Fixes #7756",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7783:181,error,error,181,https://hail.is,https://github.com/hail-is/hail/pull/7783,2,"['error', 'fault']","['error', 'fault']"
Availability,- run cancels in parallel with scheduling; - handle errors asap rather than serially after all jobs have finished,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7852:52,error,errors,52,https://hail.is,https://github.com/hail-is/hail/pull/7852,1,['error'],['errors']
Availability,"- version `devel-6ee2919`; - source: git@github.com/hail-is/hail.git ; compiled with `gradle shadowJar`. Desired behavior:. When reading an old version of a VDS, hail should print a message such as:. ```; The vds ""/users/dking/projects/hail-data/profile225.vds"" cannot be read by this version of hail. It must be regenerated from the VCF source using this version of hail.; ```. Actual behavior:. ```; dking@wmb16-359 # python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2159:713,avail,available,713,https://hail.is,https://github.com/hail-is/hail/issues/2159,1,['avail'],['available']
Availability,"-- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520:1284,error,error,1284,https://hail.is,https://github.com/hail-is/hail/issues/1520,1,['error'],['error']
Availability,"---- start of error ------------------; 2021-01-25 12:36:11 Hail: INFO: linear_regression_rows: running on 250 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:44859); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-40-d6d936b012db>"", line 3, in <module>; covariates=[1.0]); File ""<decorator-gen-1697>"", line 2, in linear_regression_rows; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/methods/statgen.py"", line 370, in linear_regression_rows; return ht_result.persist(); File ""<decorator-gen-1111>"", line 2, in persist; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:1833,Error,Error,1833,https://hail.is,https://github.com/hail-is/hail/issues/9939,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"--------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:1156,error,error,1156,https://hail.is,https://github.com/hail-is/hail/issues/3063,1,['error'],['error']
Availability,"--------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ca1bc15ebb3c> in <module>; ----> 1 hl.hadoop_ls(""gs://bw2/bla""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192); 	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186); 	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69); 	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22); 	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.fs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9600:1138,Error,Error,1138,https://hail.is,https://github.com/hail-is/hail/issues/9600,1,['Error'],['Error']
Availability,"----------------------------; FatalError Traceback (most recent call last); <ipython-input-4-3a6cee08b392> in <module>; ----> 1 tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:1228,Error,Error,1228,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['Error'],['Error']
Availability,"---------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **444/1000** <br/> **Why?** Has a fix available, CVSS 4.6 | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **429/1000** <br/> **Why?** Has a fix available, CVSS 4.3 | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:2000,avail,available,2000,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"--------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **551/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI4MmVlNzU5Ny0wZmFhLTQ1NmUtOTA3Ny0zOTM4ODRjNz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13370:1843,avail,available,1843,https://hail.is,https://github.com/hail-is/hail/pull/13370,1,['avail'],['available']
Availability,"--------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **551/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5Nzc0NDQwMi1iNzEyLTQ5NjMtYWQ0Zi01YjFhZWZmOT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13365:1642,avail,available,1642,https://hail.is,https://github.com/hail-is/hail/pull/13365,1,['avail'],['available']
Availability,"--------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **551/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxOGJjNGZiYS05ZTMwLTRmNWItYTE4Yy0wOGNmNDVmZD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13366:1851,avail,available,1851,https://hail.is,https://github.com/hail-is/hail/pull/13366,1,['avail'],['available']
Availability,"----|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:1818,avail,available,1818,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"----|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `2.11.3 -> 3.1.3` <br> | No | No Known Exploit ; ![high severity](htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:2136,avail,available,2136,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643:1686,error,error,1686,https://hail.is,https://github.com/hail-is/hail/issues/5643,4,"['error', 'failure']","['error', 'failure']"
Availability,"---|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyN2MzNWY4NC0yNDIyLTRmNzUtYWMxYy1mODQxOGJmN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14234:1940,avail,available,1940,https://hail.is,https://github.com/hail-is/hail/pull/14234,1,['avail'],['available']
Availability,"---|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJjMjFkYTE5Ny1lMDgzLTRiNzEtODc1Yi0xZmY0MjNhZ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14236:1674,avail,available,1674,https://hail.is,https://github.com/hail-is/hail/pull/14236,1,['avail'],['available']
Availability,"-->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tornadoweb/tornado/commit/34f5c1cf2696afec5532ca9e870ba32cbc7fee27""><code>34f5c1c</code></a> Version 6.3.2</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/32ad07c54e607839273b4e1819c347f5c8976b2f""><code>32ad07c</code></a> web: Fix an open redirect in StaticFileHandler</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/e0fa53ee96db720dc7800d0248c39a4ffb8911e9""><code>e0fa53e</code></a> Merge pull request <a href=""https://redirect.github.com/tornadoweb/tornado/issues/3257"">#3257</a> from bdarnell/build-workflow-wstest-warning</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/f5a1d5c7e235ad8860a4c2c5f259a43692bcbaab""><code>f5a1d5c</code></a> ci: Only run pypi actions from the main repo</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/1849ef6c48415ef8f5fecbd47d9f68225588507c""><code>1849ef6</code></a> test: Close a websocket client that causes occasional test failures</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/fcb09eba4bd45c2ebfb6356a38acdb3b4450c0d8""><code>fcb09eb</code></a> Merge pull request <a href=""https://redirect.github.com/tornadoweb/tornado/issues/3256"">#3256</a> from bdarnell/build-workflow-qemu</li>; <li><a href=""https://github.com/tornadoweb/tornado/commit/c3d50f41a29cda5f76031c60cf7902b175b79479""><code>c3d50f4</code></a> ci: Update setup-qemu-action version</li>; <li>See full diff in <a href=""https://github.com/tornadoweb/tornado/compare/v6.3.1...v6.3.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tornado&package-manager=pip&previous-version=6.3.1&new-version=6.3.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13120:2158,failure,failures,2158,https://hail.is,https://github.com/hail-is/hail/pull/13120,2,['failure'],['failures']
Availability,"--impute on the Chromosome from a table imputes it as an Int, not a String, which leads to errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/520:91,error,errors,91,https://hail.is,https://github.com/hail-is/hail/issues/520,1,['error'],['errors']
Availability,"-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::int32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:129:36: required from ‘simdpp::arch_avx2::uint64<2>& simdpp::arch_avx2::uint64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int32<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:272:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<2>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::int32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: ‘class simdpp::arch_avx2::uint64<2>’ declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::int64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:74896,error,error,74896,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6625:4318,toler,tolerations,4318,https://hail.is,https://github.com/hail-is/hail/issues/6625,1,['toler'],['tolerations']
Availability,"-5663682) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:6783,avail,available,6783,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"-5663682) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:6775,avail,available,6775,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:3788,avail,available,3788,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:3780,avail,available,3780,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:2257,avail,available,2257,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:2249,avail,available,2249,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium seve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:4765,avail,available,4765,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240:1264,down,downsample,1264,https://hail.is,https://github.com/hail-is/hail/issues/8240,1,['down'],['downsample']
Availability,"-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:133:36: required from ‘simdpp::arch_avx2::uint16<8>& simdpp::arch_avx2::uint16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:453:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: ‘class simdpp::arch_avx2::uint16<8>’ declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:52991,error,error,52991,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:133:36: required from ‘simdpp::arch_avx2::uint32<4>& simdpp::arch_avx2::uint32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:114:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:36412,error,error,36412,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint64<2>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:133:36: required from ‘simdpp::arch_avx2::uint32<4>& simdpp::arch_avx2::uint32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:159:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<2>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: ‘class simdpp::arch_avx2::uint32<4>’ declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:41924,error,error,41924,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint64<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:114:36: required from ‘simdpp::arch_avx2::uint32<8>& simdpp::arch_avx2::uint32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint64<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:181:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: ‘class simdpp::arch_avx2::uint32<8>’ declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<4>; T = simdpp::arch_avx2::float32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:45595,error,error,45595,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:129:36: required from ‘simdpp::arch_avx2::uint64<2>& simdpp::arch_avx2::uint64<2>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:157:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<2>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: ‘class simdpp::arch_avx2::uint64<2>’ declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:40088,error,error,40088,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:114:36: required from ‘simdpp::arch_avx2::uint64<4>& simdpp::arch_avx2::uint64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:179:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: ‘class simdpp::arch_avx2::uint64<4>’ declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint64<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:43760,error,error,43760,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22887"">#22887</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/5b17097d555d32df51e58b75ee12a48a5b60df88""><code>5b17097</code></a> add validate_authority support (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22786"">#22786</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/9acb1882379dab5e910e2a5e3ef6c4a6ac08aadf""><code>9acb188</code></a> fix cspell issues (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22774"">#22774</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/a497e5aba391075ebbbd42c2df4937c2d11185a4""><code>a497e5a</code></a> rename troubleshooting (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22771"">#22771</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/9791fb5bc4cb6001768e6e1fb986b8d8f8326c43""><code>9791fb5</code></a> [core] add error body to HttpResponseError str (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22302"">#22302</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/772054c9cf24e860cf08563ac33caab50e904dd5""><code>772054c</code></a> drop py27 support (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22531"">#22531</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/Azure/azure-sdk-for-python/compare/azure-identity_1.6.0...azure-identity_1.8.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=azure-identity&package-manager=pip&previous-version=1.6.0&new-version=1.8.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11493:4303,error,error,4303,https://hail.is,https://github.com/hail-is/hail/pull/11493,2,['error'],['error']
Availability,"-storage/commit/803a90b7747b8972f51d1407616c51084d97c589"">803a90b</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1758"">#1758</a>) (<a href=""https://github.com/googleapis/java-storage/commit/140e90911229c876de7b674dd1e61b278e8b07fd"">140e909</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.17 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1759"">#1759</a>) (<a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf"">7e3175a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.13.1...v2.14.0"">2.14.0</a> (2022-10-26)</h2>; <h3>Google Cloud Storage gRPC API Preview</h3>; <p>The first release of <code>google-cloud-storage</code> with support for a subset of the Google Cloud Storage gRPC API which is in private preview. The most common operations have all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annotations will be updated.</p>; <p>All new gRPC related APIs are annotated with <code>@BetaApi</code> to denote they are in preview and the possibility of breaking change is present. At this time, opting to use any of the gRPC transport mode means you are okay with the possibility of a breaking change happening. When the APIs are out of preview, we will remove the <code>@BetaApi</code> annotation to signal they are now considered stable and will not break outside a major version.</p>; <p><strong><em>NOTICE</em></strong>: Using the gRPC transport is exclusive. Any operations which have not yet been implemented for gRPC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:7739,avail,available,7739,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['avail'],['available']
Availability,-stream 2023-06-09T12:44:22+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/main/resource_usage BlockBlob Hot 680 application/octet-stream 2023-06-09T12:44:22+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json BlockBlob Hot 4453 application/octet-stream 2023-06-09T12:44:22+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:34+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:3496,down,download,3496,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['down'],['download']
Availability,"-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016:9756,toler,tolerationSeconds,9756,https://hail.is,https://github.com/hail-is/hail/issues/7016,1,['toler'],['tolerationSeconds']
Availability,"-|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:1888,avail,available,1888,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"-|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![high",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:1880,avail,available,1880,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,". /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.Reflec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2653:2347,error,error,2347,https://hail.is,https://github.com/hail-is/hail/issues/2653,1,['error'],['error']
Availability,". /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2655:2195,error,error,2195,https://hail.is,https://github.com/hail-is/hail/issues/2655,1,['error'],['error']
Availability,". /opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:44: UserWarning:. Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`. SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.0; SparkUI available at http://saturn-3f2d119c-05e5-496d-97b9-8f40efff98a3-m.c.terra-db12d060.internal:36235/; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.126-ee77707f4fab; LOGGING: writing to /home/jupyter/Ellinor_Lubitz_PHB_Joint_Analyses/edit/hail-20231216-1801-0.2.126-ee77707f4fab.log. SparkContext. [Spark UI](http://saturn-3f2d119c-05e5-496d-97b9-8f40efff98a3-m.c.terra-db12d060.internal:36235/). Version; v3.3.0; Master; yarn; AppName; pyspark-shell. `#### Read vcf; vcfs = [""gs://path/to/bucket/chrY.*.hard_filtered_with_genotypes.vcf.gz""]. #####; ##### Read vcf file; mt = hl.import_vcf(vcfs , force_bgz= True, reference_genome='GRCh38', find_replace=('null', '.')). mt.count(); `. 2023-12-16 18:02:00.897 Hail: INFO: scanning VCF for sortedness... (4 + 3) / 7]; 2023-12-16 18:02:16.278 Hail: INFO: Coerced sorted VCF - no additional import work to do; [Stage 3:===================================================> (10 + 1) / 11]; (15472, 13279). `##### Split the multi-alleleic variant",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:2672,avail,available,2672,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['avail'],['available']
Availability,". ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:1078,Error,ErrorHandling,1078,https://hail.is,https://github.com/hail-is/hail/issues/2070,1,['Error'],['ErrorHandling']
Availability,". ```shell; ----> 1 gwas_weights = hl._linear_regression_rows_nd(y=mt.y,; 2 x=mt.GT.n_alt_alleles(),; 3 covariates=[1.0],; 4 weights=mt.weights). File <decorator-gen-1734>:2, in _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). File ~/hail/hail/python/hail/typecheck/check.py:585, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 582 @decorator; 583 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_). File ~/hail/hail/python/hail/methods/statgen.py:717, in _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through); 714 res = res.select_globals(); 716 temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); --> 717 res = res.checkpoint(temp_file_name); 719 return res. File <decorator-gen-1234>:2, in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). File ~/hail/hail/python/hail/typecheck/check.py:585, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 582 @decorator; 583 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_). File ~/hail/hail/python/hail/table.py:1963, in Table.checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1960 hl.current_backend().validate_file(output); 1962 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1963 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1964 _assert_type = self._type; 1965 _load_refs = False. File <decorator-gen-1236>:2, in write(self, output, overwrite, stage_locally, _codec_spec). File ~/hail/hail/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14594:1626,checkpoint,checkpoint,1626,https://hail.is,https://github.com/hail-is/hail/issues/14594,1,['checkpoint'],['checkpoint']
Availability,"...once KinshipMatrix is in, with RRM going there. The current export to file formats on GRM should be moved to KinshipMatrix too. And then doc on lmmreg should be updated to reflect there are more options than RRM available.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1633:215,avail,available,215,https://hail.is,https://github.com/hail-is/hail/issues/1633,1,['avail'],['available']
Availability,".0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1075,down,downloads,1075,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['down'],['downloads']
Availability,".1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1075,down,downloads,1075,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['down'],['downloads']
Availability,".16"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,624,531,604,589,726,434,589,449,696,589,479,519,509,711,701,586,586,384,494,539,589],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:13182,avail,available,13182,https://hail.is,https://github.com/hail-is/hail/pull/14108,1,['avail'],['available']
Availability,".17. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/103692"">kubernetes/kubernetes#103692</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>)</li>; <li>Performs strict server side schema validation requests via the <code>fieldValidation=[Strict,Warn,Ignore]</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105916"">kubernetes/kubernetes#105916</a>, <a href=""https://github.com/kevindelgado""><code>@​kevindelgado</code></a>)</li>; <li>Promote <code>IPv6DualStack</code> feature to stable.; Controller Manager flags for the node IPAM controller have slightly changed:; <ol>; <li>When configuring a dual-stack cluster, the user must specify both <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code> to set the per-node IP mask sizes, instead of the previous <code>--node-cidr-mask-size</code> flag.</li>; <li>The <code>--node-cidr-mask-size</code> flag is mutually exclusive with <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code>.</li>; <li>Single-stack clusters do not need to change, but may choose to use the more specific flags. Users can use either the older <code>--node-cidr-mask-size</code> flag or one of the newer <code>--node-cidr-mask-size-ipv4</code> or <code>--node-cidr-mask-size-ipv6</code> flags to configure the per-node IP mask size, provided that the flag's IP family matches the cluster's IP family (--cluster-cidr). (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104691"">kubernetes/kubernetes#104691</a>, <a href=""https://github.com/khenidak""><code>@​khenidak</code></a>)</li>; </ol>; </li>; <li>Remove <code>NodeLease</code> feature gate that was graduated and locked to stable in 1.17 release. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105222"">kubernetes/kubernetes#105222</a>, <a href=""https://github.com/cyclinder""><code>@​cyclinder</code></a>)</li>; <li>Rem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:10810,mask,mask-size,10810,https://hail.is,https://github.com/hail-is/hail/pull/11957,3,['mask'],"['mask-size', 'mask-size-']"
Availability,".3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12358:1076,down,downloads,1076,https://hail.is,https://github.com/hail-is/hail/pull/12358,1,['down'],['downloads']
Availability,".3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-30_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1075,down,downloads,1075,https://hail.is,https://github.com/hail-is/hail/pull/12319,1,['down'],['downloads']
Availability,".6/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/table.py"", line 1870, in persist; return Env.backend().persist_table(self, storage_level); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/backend/spark_backend.py"", line 285, in persist_table; return Table._from_java(self._jbackend.pyPersistTable(storage_level, self._to_java_table_ir(t._tir))); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/backend/py4j_backend.py"", line 16, in deco; return f(*args, **kwargs); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/protocol.py"", line 336, in get_return_value; format(target_id, ""."", name)); py4j.protocol.Py4JError: An error occurred while calling o1.pyPersistTable. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback; stb = value._render_traceback_(); AttributeError: 'Py4JError' object has no attribute '_render_traceback_'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 929, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1067, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:3723,error,error,3723,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['error'],['error']
Availability,".ClientResponseError'> 500, message='Internal Server Error', url=URL('http://batch.hail/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'. ; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/hailtop/utils/utils.py"", line 809, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/aiocloud/common/session.py"", line 117, in _request_with_valid_authn; return await self._http_session.request(method, url, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/httpx.py"", line 148, in request_and_raise_for_status; raise ClientResponseError(; hailtop.httpx.ClientResponseError: 500, message='Internal Server Error', url=URL('http://batch.hail/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'; 2024-09-25 01:54:55,288 - hailtop.utils 835 - WARNING - A transient error occured. We will automatically retry. We have thus far seen 50 transient errors (next delay: 60.0s).; ```. The corresponding server-side error was. ```; pymysql.err.DataError: (1406, \""Data too long for column 'value' at row 106\""); ```. coming from the `INSERT INTO job_attributes …` query in `insert_jobs_into_db()`. We write a list of the samples being processed as a job attribute, and it turned out that for at least some of the jobs of this batch this list had grown to longer than 64K of text. The `job_attributes.value` database field is of type TEXT, which limits each individual attribute to 64KiB bytes. While writing a long list of sample ids as an attribute may or may not be a great idea :smile: it is fair to say that 64K is not a large maximum for user-supplied data here in the 21st century!. It may be worth adding a database migration to change the `job_attributes.value` column type (and perhaps also that of `job_group_attributes.value`) from TEXT to MEDIUMTEXT, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14702:1589,error,error,1589,https://hail.is,https://github.com/hail-is/hail/issues/14702,1,['error'],['error']
Availability,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755:7437,Error,Error,7437,https://hail.is,https://github.com/hail-is/hail/issues/4755,2,"['Error', 'down']","['Error', 'down']"
Availability,".RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but ¯\_(ツ)_/¯). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:5099,failure,failure,5099,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['failure'],['failure']
Availability,.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:8439,failure,failure,8439,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['failure'],['failure']
Availability,.SparkBackend.withExecuteContext(SparkBackend.scala:393); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:631); at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:89); at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83); at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82); at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:822); at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:794); at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:199); at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.127-bb535cd096c5; Error summary: NegativeArraySizeException: null; tar: chr12: Cannot stat: No such file or directory; tar: Exiting with failure status due to previous errors; 2024/01/17 21:10:34 Starting delocalization.; 2024/01/17 21:10:34 Delocalization script execution started...; 2024/01/17 21:10:34 Delocalizing output /cromwell_root/memory_retry_rc -> gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/memory_retry_rc; 2024/01/17 21:10:37 Delocalizing output /cromwell_root/rc -> gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/rc; 2024/01/17 21:10:39 Delocalizing output /cromwell_root/stdout -> gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/stdout; 2024/01/17 21:10:41 Delocalizing output /cromwell_root/stderr -> gs://fc-5a8938eb-129,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:26206,Error,Error,26206,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,"['Error', 'error', 'failure']","['Error', 'errors', 'failure']"
Availability,.Table.take(Table.scala:637); 	at is.hail.table.Table.showString(Table.scala:673); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1012); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$4$$anon$1.hasNext(RVD.scala:226); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:21); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:8626,Error,ErrorHandling,8626,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['Error'],['ErrorHandling']
Availability,".__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:2803,failure,failure,2803,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['failure'],['failure']
Availability,.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:390); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:8695,Error,Error,8695,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['Error'],['Error']
Availability,".com/tox-dev/sphinx-autodoc-typehints/commit/f75d19be9275b25d2f6caa23392a6072f49c3d56""><code>f75d19b</code></a> Add handling of tuples in type subscriptions (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/212"">#212</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/7b8c357be15d7b7b4ec4ad272f554f7ab3e0e197""><code>7b8c357</code></a> ADMIN: Update pepy.tech link in badge (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/213"">#213</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/3492d491fe09701a035715bb1dbaba13734ad69c""><code>3492d49</code></a> Prevents reaching inner blocks that contains <code>if TYPE_CHECKING</code> (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/211"">#211</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/01b3a1abeac063a5530b96b78252aa17a93f040c""><code>01b3a1a</code></a> More robust handling of type guard imports (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/208"">#208</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/d770c69c1e6c7e4a809a145140cfc1033eac57dd""><code>d770c69</code></a> Fix fully_qualified should be typehints_fully_qualified (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/204"">#204</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.11.1...1.17.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx-autodoc-typehints&package-manager=pip&previous-version=1.11.1&new-version=1.17.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11503:5174,robust,robust,5174,https://hail.is,https://github.com/hail-is/hail/pull/11503,2,['robust'],['robust']
Availability,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:13021,Error,Error,13021,https://hail.is,https://github.com/hail-is/hail/issues/3235,1,['Error'],['Error']
Availability,.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.stats.RegressionUtils$.getPhenoCovCompleteSamples(RegressionUtils.scala:36); at is.hail.methods.LinearRegression$.apply(LinearRegression.scala:21); at is.hail.variant.VariantDatasetFunctions$.linreg$extension(VariantDataset.scala:848); at is.hail.variant.VariantDatasetFunctions.linreg(VariantDataset.scala:846); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-07f60b2; Error summary: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1654:3043,Error,Error,3043,https://hail.is,https://github.com/hail-is/hail/pull/1654,2,"['Avail', 'Error']","['Available', 'Error']"
Availability,.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:2186,Heartbeat,HeartbeatReceiver,2186,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['Heartbeat'],['HeartbeatReceiver']
Availability,.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18755,Error,ErrorHandling,18755,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Error'],['ErrorHandling']
Availability,".hail/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'. ; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/hailtop/utils/utils.py"", line 809, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/aiocloud/common/session.py"", line 117, in _request_with_valid_authn; return await self._http_session.request(method, url, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/httpx.py"", line 148, in request_and_raise_for_status; raise ClientResponseError(; hailtop.httpx.ClientResponseError: 500, message='Internal Server Error', url=URL('http://batch.hail/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'; 2024-09-25 01:54:55,288 - hailtop.utils 835 - WARNING - A transient error occured. We will automatically retry. We have thus far seen 50 transient errors (next delay: 60.0s).; ```. The corresponding server-side error was. ```; pymysql.err.DataError: (1406, \""Data too long for column 'value' at row 106\""); ```. coming from the `INSERT INTO job_attributes …` query in `insert_jobs_into_db()`. We write a list of the samples being processed as a job attribute, and it turned out that for at least some of the jobs of this batch this list had grown to longer than 64K of text. The `job_attributes.value` database field is of type TEXT, which limits each individual attribute to 64KiB bytes. While writing a long list of sample ids as an attribute may or may not be a great idea :smile: it is fair to say that 64K is not a large maximum for user-supplied data here in the 21st century!. It may be worth adding a database migration to change the `job_attributes.value` column type (and perhaps also that of `job_group_attributes.value`) from TEXT to MEDIUMTEXT, which would raise the limit to 16 MiB bytes (at, it appears, a cost of 1 byte per r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14702:1668,error,errors,1668,https://hail.is,https://github.com/hail-is/hail/issues/14702,1,['error'],['errors']
Availability,".io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:2656,avail,available,2656,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,".io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **454/1000** <br/> **Why?** Has a fix available, CVSS 4.8 | Expected Behavior Violation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3314966](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3314966) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Use After Free <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315324](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315324) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **584/1000** <br/> **Why?** Has a fix available, CVSS 7.4 | Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:2648,avail,available,2648,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,".iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in _repr_html_(self); 1256 ; 1257 def _repr_html_(self):; -> 1258 return self._html_str(); 1259 ; 1260 def _ascii_str(self):. /usr/local/lib/python3.6/site-packages/hail/table.py in _html_str(self); 1342 types = self.types; 1343 ; -> 1344 rows, has_more, dtype = self.data(); 1345 fields = list(dtype); 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:26250,Error,Error,26250,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['Error'],['Error']
Availability,".java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.561 : ERROR: error while applying lowering 'LowerAndExecuteShuffles'; 2023-05-04 01:04:37.600 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.603 : ERROR: SocketException: Connection reset; From javax.net.ssl.SSLException: Connection reset; 	at sun.security.ssl.Alert.createSSLException(Alert.java:127); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:324); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:267); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:262); 	at sun.security.ssl.S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:22900,ERROR,ERROR,22900,https://hail.is,https://github.com/hail-is/hail/issues/12983,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,".json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:4491,Error,Error,4491,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['Error'],['Error']
Availability,.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:512); 	at org.json4s.Extraction$.org$json4s$Extraction$$customOrElse(Extraction.scala:524); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:512); 	at org.json4s.Extraction$.extract(Extraction.scala:351); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$mkWithTypeHint(Extraction.scala:507); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:514); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:512); 	at org.json4s.Extraction$.org$json4s$Extraction$$customOrElse(Extraction.scala:524); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:512); 	at org.json4s.Extraction$.extract(Extraction.scala:351); 	at org.json4s.Extraction$.extract(Extraction.scala:42); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.variant.RelationalSpec$.read(MatrixTable.scala:69); 	at is.hail.expr.ir.TableIR$.read(TableIR.scala:23); 	at is.hail.table.Table$.read(Table.scala:56); 	at is.hail.HailContext.readTable(HailContext.scala:572); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-51961fa0ef80; Error summary: AssertionError: assertion failed```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4325:7956,Error,Error,7956,https://hail.is,https://github.com/hail-is/hail/issues/4325,1,['Error'],['Error']
Availability,".log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@14d32b0c{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 15:04:34 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 15:04:34 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:34 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 15:04:36 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 15:04:36 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 15:04:36 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 15:04:36 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:36 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:36 DAGSched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:14386,AVAIL,AVAILABLE,14386,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,".py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/usr/local/lib/python3.10/dist-packages/hail/backend/py4j_backend.py"", line 220, in _rpc; raise fatal_error_from_java_error_triplet(; hail.utils.java.FatalError: HailException: VCF spec does not support phased haploid calls. Java stack trace:; is.hail.utils.HailException: VCF spec does not support phased haploid calls.; at __C83collect_distributed_array_matrix_vcf_writer.apply_region154_245(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply_region133_246(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply_region1_250(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply(Unknown Source); at is.hail.backend.BackendUtils.$anonfun$collectDArray$19(BackendUtils.scala:142); at is.hail.utils.package$.using(package.scala:665); at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:170); at is.hail.backend.BackendUtils.$anonfun$collectDArray$18(BackendUtils.scala:141); at is.hail.backend.spark.SparkBackend$$anon$5.compute(SparkBackend.scala:474); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:136); at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.127-bb535cd096c5; Error summary: HailException: VCF spec does not support phased haploid calls.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14330:3622,Error,Error,3622,https://hail.is,https://github.com/hail-is/hail/issues/14330,1,['Error'],['Error']
Availability,".py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:1186,error,error,1186,https://hail.is,https://github.com/hail-is/hail/issues/8029,3,['error'],['error']
Availability,".rdd.RDD.treeReduce(RDD.scala:1037); at is.hail.methods.SampleQC$.results(SampleQC.scala:206); at is.hail.methods.SampleQC$.apply(SampleQC.scala:221); at is.hail.methods.SampleQC.apply(SampleQC.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:7473,Error,ErrorHandling,7473,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['Error'],['ErrorHandling']
Availability,.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:726); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$24(StorageImpl.java:574); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.120-f00f916faf78; Error summary: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:10009,Error,Error,10009,https://hail.is,https://github.com/hail-is/hail/issues/13409,2,"['Error', 'down']","['Error', 'download']"
Availability,.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-bfea6715901c; Error summary: HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:11336,Error,Error,11336,https://hail.is,https://github.com/hail-is/hail/issues/4096,2,"['Error', 'error']","['Error', 'error']"
Availability,.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:8553,failure,failure,8553,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['failure'],['failure']
Availability,".scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.105-3f053140ad00; Error summary: ClassFormatError: Too many arguments in method signature in class file __C2866stream; ```. This used to work fine in earlier Hail versions, e.g. 0.2.85.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:13805,Error,Error,13805,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['Error'],['Error']
Availability,".scala:618); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:50); 	at is.hail.table.Table.aggregate(Table.scala:373); 	at is.hail.table.Table.aggregate(Table.scala:369); 	at is.hail.table.Table.aggregateJSON(Table.scala:364); 	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-414f3f183bd5; Error summary: RuntimeException: Class file too large!; ```; Code was:; ```; cutoff = 10. agg_expr = {; 'downsampling': hl.agg.collect(ht.downsamplings)[0]; }; locations = list(zip(('syn', 'mis', 'lof'), ('', '', '_classic_hc'))); agg_expr.update({; f'median_expected_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'exp_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'median_observed_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'obs_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_expected_{var}_{pop}': [hl.agg.mean(ht[f'exp_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_observed_{var}_{pop}': [hl.agg.mean(ht[f'obs_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'fraction_expected_{var}_{pop}': [hl.agg.fraction(ht[f'exp_{var",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516:1931,down,downsampling,1931,https://hail.is,https://github.com/hail-is/hail/issues/4516,1,['down'],['downsampling']
Availability,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:15075,Error,Error,15075,https://hail.is,https://github.com/hail-is/hail/issues/4780,4,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:460); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:460); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.113-0b5bc2eb0c95; Error summary: SocketException: Connection reset; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:25994,Error,Error,25994,https://hail.is,https://github.com/hail-is/hail/issues/12982,1,['Error'],['Error']
Availability,.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:15808,failure,failure,15808,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['failure'],['failure']
Availability,"//]: # (snyk:metadata:{""prId"":""648a0aea-922c-46c9-b851-66c7e282d2e5"",""prPublicId"":""648a0aea-922c-46c9-b851-66c7e282d2e5"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.6""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14196:10739,avail,available,10739,https://hail.is,https://github.com/hail-is/hail/pull/14196,1,['avail'],['available']
Availability,"//github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739d0400c333a72b32f5d514""><code>0fdebf3</code></a> Improve verify example</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/02b8e1a79d9e00acd61f9ac42e5555619fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/717877121299cea8f216d3a595eaa56731a6acd3""><code>7178771</code></a> Support changing a target file's relative path in an eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e5af1bd7f9daa8a9222aee0dd1b703727cb5e94e""><code>e5af1bd</code></a> Bump version number to 5.3.0-SNAPSHOT</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.3.0"">compare view</a></li>; </ul>; </d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:4111,down,download-task,4111,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,"//snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **444/1000** <br/> **Why?** Has a fix available, CVSS 4.6 | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **429/1000** <br/> **Why?** Has a fix available, CVSS 4.3 | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **501/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.3 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:2731,avail,available,2731,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"//snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![medium severit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:5496,avail,available,5496,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatical errors (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2642"">#2642</a>)</li>; <li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with J",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:3430,error,error,3430,https://hail.is,https://github.com/hail-is/hail/pull/11080,4,['error'],['error']
Availability,"/>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/v3.8.5/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.8.5 (2023-07-19)</h1>; <h2>Security bugfixes</h2>; <ul>; <li>; <p>Upgraded the vendored copy of llhttp_ to v8.1.1 -- by :user:<code>webknjaz</code>; and :user:<code>Dreamsorcerer</code>.</p>; <p>Thanks to :user:<code>sethmlarson</code> for reporting this and providing us with; comprehensive reproducer, workarounds and fixing details! For more; information, see; <a href=""https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w"">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w</a>.</p>; <p>.. _llhttp: <a href=""https://llhttp.org"">https://llhttp.org</a></p>; <p><code>[#7346](https://github.com/aio-libs/aiohttp/issues/7346) &lt;https://github.com/aio-libs/aiohttp/issues/7346&gt;</code>_</p>; </li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p>Added information to C parser exceptions to show which character caused the error. -- by :user:<code>Dreamsorcerer</code></p>; <p><code>[#7366](https://github.com/aio-libs/aiohttp/issues/7366) &lt;https://github.com/aio-libs/aiohttp/issues/7366&gt;</code>_</p>; </li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Fixed a transport is :data:<code>None</code> error -- by :user:<code>Dreamsorcerer</code>.</p>; <p><code>[#3355](https://github.com/aio-libs/aiohttp/issues/3355) &lt;https://github.com/aio-libs/aiohttp/issues/3355&gt;</code>_</p>; </li>; </ul>; <hr />; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/9c13a52c21c23dfdb49ed89418d28a5b116d0681""><code>9c13a52</code></a> Bump aiohttp to v3.8.5 a security release</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/7c02129567bc4ec59be467b70fc937c82920948c""><code>7c02129</code></a>  Bump pypa/cibuildwheel to v2.14.1</li>; <li><a href=""https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13270:2526,error,error,2526,https://hail.is,https://github.com/hail-is/hail/pull/13270,5,['error'],['error']
Availability,"/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3430,failure,failure,3430,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['failure'],['failure']
Availability,"/Users/mkanai/.anyenv/envs/pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/hailtop/aiocloud/common/base_client.py"", line 21, in request; async with await self._session.request(method, url, **kwargs) as resp:; File ""/Users/mkanai/.anyenv/envs/pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/hailtop/aiocloud/common/session.py"", line 103, in request; return await retry_transient_errors(self._request_with_valid_authn, method, url, **kwargs); File ""/Users/mkanai/.anyenv/envs/pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 769, in retry_transient_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); File ""/Users/mkanai/.anyenv/envs/pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 785, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/Users/mkanai/.anyenv/envs/pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/hailtop/aiocloud/common/session.py"", line 115, in _request_with_valid_authn; return await self._http_session.request(method, url, **kwargs); File ""/Users/mkanai/.anyenv/envs/pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/hailtop/httpx.py"", line 138, in request_and_raise_for_status; raise ClientResponseError(; hailtop.httpx.ClientResponseError: 403, message='Forbidden', url=URL('https://storage.googleapis.com/storage/v1/b/hail-common?userProject=finngen-xavier') body='{\n ""error"": {\n ""code"": 403,\n ""message"": ""mkanai@broadinstitute.org does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission \'storage.buckets.get\' denied on resource (or it may not exist)."",\n ""errors"": [\n {\n ""message"": ""mkanai@broadinstitute.org does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission \'storage.buckets.get\' denied on resource (or it may not exist)."",\n ""domain"": ""global"",\n ""reason"": ""forbidden""\n }\n ]\n }\n}\n'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14291:5146,error,error,5146,https://hail.is,https://github.com/hail-is/hail/issues/14291,2,['error'],"['error', 'errors']"
Availability,"/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'. ; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/hailtop/utils/utils.py"", line 809, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/aiocloud/common/session.py"", line 117, in _request_with_valid_authn; return await self._http_session.request(method, url, **kwargs); File ""/usr/local/lib/python3.10/site-packages/hailtop/httpx.py"", line 148, in request_and_raise_for_status; raise ClientResponseError(; hailtop.httpx.ClientResponseError: 500, message='Internal Server Error', url=URL('http://batch.hail/api/v1alpha/batches/485962/updates/1/jobs/create') body='500 Internal Server Error\n\nServer got itself in trouble'; 2024-09-25 01:54:55,288 - hailtop.utils 835 - WARNING - A transient error occured. We will automatically retry. We have thus far seen 50 transient errors (next delay: 60.0s).; ```. The corresponding server-side error was. ```; pymysql.err.DataError: (1406, \""Data too long for column 'value' at row 106\""); ```. coming from the `INSERT INTO job_attributes …` query in `insert_jobs_into_db()`. We write a list of the samples being processed as a job attribute, and it turned out that for at least some of the jobs of this batch this list had grown to longer than 64K of text. The `job_attributes.value` database field is of type TEXT, which limits each individual attribute to 64KiB bytes. While writing a long list of sample ids as an attribute may or may not be a great idea :smile: it is fair to say that 64K is not a large maximum for user-supplied data here in the 21st century!. It may be worth adding a database migration to change the `job_attributes.value` column type (and perhaps also that of `job_group_attributes.value`) from TEXT to MEDIUMTEXT, which would raise the limit to 16 MiB bytes (at, it appears, a cost of 1 byte per row).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14702:1732,error,error,1732,https://hail.is,https://github.com/hail-is/hail/issues/14702,1,['error'],['error']
Availability,"/astroid/issues/1282"">#1282</a>; Ref <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1103"">#1103</a></p>; </li>; <li>; <p>Fixed crash with recursion error for inference of class attributes that referenced; the class itself.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5408"">PyCQA/pylint#5408</a></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/astroid/commit/07c0f60ffc1017d0a9a2bb605a5c645781a8c088""><code>07c0f60</code></a> Bump astroid to 2.10.0, update changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/e6dc5ef0f8c2d28bc9d2ffa226fbb5e4e58d88f3""><code>e6dc5ef</code></a> Fix some typoes in the Changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/b6d17107f2e02df4ce5080536bb783a25273b33f""><code>b6d1710</code></a> Changed NodeNG.tolineno to use end_lineno when it is available (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1351"">#1351</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/0acb961d7375131c3d1e7a3580f974b6e8c5ef94""><code>0acb961</code></a> Refactor: Stop adding arbitrary attributes to module obj when building (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1215"">#1215</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/62aa3bb63c3ca0cda19a1bb294a6b052c2346189""><code>62aa3bb</code></a> Restore custom distutils handling for resolving paths to submodules. (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1386"">#1386</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/8f7f07898720b875cfbf447a7106875db4a904b3""><code>8f7f078</code></a> Limit expensive decorator function (<a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1407"">#1407</a>)</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/98280b57b5ed3db8a4d431cb60e21f136",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:3724,avail,available,3724,https://hail.is,https://github.com/hail-is/hail/pull/11463,2,['avail'],['available']
Availability,"/code> and <code>category_orders</code> now available in <code>px.pie()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3775"">#3775</a></li>; </ul>; <h3>Performance</h3>; <ul>; <li><code>px</code> methods no longer call <code>groupby</code> on the input dataframe when the result would be a single group, and no longer groups by a lambda, for significant speedups <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3765"">#3765</a> with thanks to <a href=""https://github.com/jvdd""><code>@​jvdd</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Allow non-string extras in <code>flaglist</code> attributes, to support upcoming changes to <code>ax.automargin</code> in plotly.js <a href=""https://github-redirect.dependabot.com/plotly/plotly.js/pull/6193"">plotly.js#6193</a>, <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3749"">#3749</a></li>; </ul>; <h2>[5.8.2] - 2022-06-10</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed a syntax error that caused rendering issues in Databricks notebooks and likely elsewhere. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3763"">#3763</a> with thanks to <a href=""https://github.com/fwetdb""><code>@​fwetdb</code></a></li>; </ul>; <h2>[5.8.1] - 2022-06-08</h2>; <p>(no changes, due to a mixup with the build process!)</p>; <h2>[5.8.0] - 2022-05-09</h2>; <h3>Fixed</h3>; <ul>; <li>Improve support for type checking and IDE auto-completion by bypassing lazy-loading when type checking. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3425"">#3425</a> with thanks to <a href=""https://github.com/JP-Ellis""><code>@​JP-Ellis</code></a></li>; <li>line dash-style validators are now correctly used everywhere so that values like <code>10px 2px</code> are accepted <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3722"">#3722</a></li>; <li>Resolved various deprecation warning messages and compatibility issues with upstream dependencies and Python 3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12113:3193,error,error,3193,https://hail.is,https://github.com/hail-is/hail/pull/12113,1,['error'],['error']
Availability,"/code></a> Release v3.9.4 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8201"">#8201</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/a7e240a9f625a0b9559bdf5f0049c71565352400""><code>a7e240a</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8320"">#8320</a>/9ba9a4e5 backport][3.9] Fix Python parser to mark responses without...</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/28335525d1eac015a7e7584137678cbb6ff19397""><code>2833552</code></a> Escape filenames and paths in HTML when generating index pages (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8317"">#8317</a>) (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8319"">#8319</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/ed43040613988fc4666109aca82a5180ff165df5""><code>ed43040</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8309"">#8309</a>/c29945a1 backport][3.9] Improve reliability of run_app test (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8315"">#8315</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/ec2be0500e2674eea019c0966a7a905e9b3d6608""><code>ec2be05</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8299"">#8299</a>/28d026eb backport][3.9] Create marker for internal tests (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8307"">#8307</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/292d961f4ee2829a1b13fad92444a4fd693fbc87""><code>292d961</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8304"">#8304</a>/88c80c14 backport][3.9] Check for backports in CI (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8305"">#8305</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/cebe526b9c34dc3a3da9140409db63014bc4cf19""><code>cebe526</code></a> Fix handling of multipart/form-data (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/828",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477:4899,reliab,reliability,4899,https://hail.is,https://github.com/hail-is/hail/pull/14477,6,['reliab'],['reliability']
Availability,"/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/02b8e1a79d9e00acd61f9ac42e5555619fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/717877121299cea8f216d3a595eaa56731a6acd3""><code>7178771</code></a> Support changing a target file's relative path in an eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e5af1bd7f9daa8a9222aee0dd1b703727cb5e94e""><code>e5af1bd</code></a> Bump version number to 5.3.0-SNAPSHOT</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:4895,down,download-task,4895,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,"/compare/4.0.0...4.0.1"">https://github.com/samtools/htsjdk/compare/4.0.0...4.0.1</a></p>; <h2>4.0.0</h2>; <h2>Moving forward</h2>; <p>This is the first release to be built exclusively for java 17. Java 17 features are now allowed in our source code and we will no longer support older versions of java. We've also updated dependencies to fix security issues. There are several small bug fixes as well.</p>; <h3>JSON dependency:</h3>; <p>We've dropped the MJSON library which was no longer being updated and replaced it with a similarly small json library from org.json</p>; <h2>What's Changed</h2>; <ul>; <li>Migrate to Java 17 by <a href=""https://github.com/lbergelson""><code>@​lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1649"">samtools/htsjdk#1649</a></li>; <li>Remove low-value progress logging message by <a href=""https://github.com/nh13""><code>@​nh13</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1659"">samtools/htsjdk#1659</a></li>; <li>removed redundant code by <a href=""https://github.com/KleinSamuel""><code>@​KleinSamuel</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1664"">samtools/htsjdk#1664</a></li>; <li>Update snappy-java and migrate mjson to org.json to address CVEs by <a href=""https://github.com/bbimber""><code>@​bbimber</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1670"">samtools/htsjdk#1670</a></li>; <li>Remove incorrect zero-length-B-array checks <a href=""https://github.com/gileshall""><code>@​gileshall</code></a> and <a href=""https://github.com/jmarshall""><code>@​jmarshall</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1674"">samtools/htsjdk#1674</a></li>; <li>add SINGULAR platform to read group by <a href=""https://github.com/omicsorama""><code>@​omicsorama</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1635"">samtools/htsjdk#1635</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576:2222,redundant,redundant,2222,https://hail.is,https://github.com/hail-is/hail/pull/13576,1,['redundant'],['redundant']
Availability,"/dataset""},; ""version"": ""one_version""},; {""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""another_version""}]}; ```. The `annotation_db.json` file is now used by the `load_dataset()` function in `datasets.py` as well, any dataset in the JSON file should now be able to be loaded this way. Made changes to the following:; - `DB` class now requires a `region` parameter.; - `Dataset.from_name_and_json()` has had a `custom_config` parameter added that indicates whether or not the user has supplied their own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progress. Changes to datasets and datasets API site:; - Added the `ldsc_baselineLD_annotations`, `ldsc_baselineLD_ldscores`, and `ldsc_baseline_ldscores` datasets to the `annotation_db.json` configuration file. Now accessible via `load_dataset()` and `db.annotate_rows_db()` (for the annotations at least).; - New `.rst` files in `hail/python/hail/docs/datasets` have been generated to reflect the available datasets in the config file, and `hail/python/hail/docs/datasets.rst` has been updated with ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:1619,avail,available,1619,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['avail'],['available']
Availability,"/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.3; SparkUI available at http://192.168.1.142:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.125-c4e2880b3279; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20231026-0957-0.2.125-c4e2880b3279.log; --------------------------------------------------------------------------- / 1]; FatalError Traceback (most recent call last); Cell In[1], line 2; 1 import hail as hl; ----> 2 hl.import_vcf('gs://danking/chr*.vcf').count(). File ~/miniconda3/lib/python3.10/site-packages/hail/matrixtable.py:2631, in MatrixTable.count(self); 2618 """"""Count the number of rows and columns in the matrix.; 2619 ; 2620 Examples; (...); 2628 Number of rows, number of cols.; 2629 """"""; 2630 count_ir = ir.MatrixCount(self._mir); -> 2631 return Env.backend().execute(count_ir). File ~/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py:180, in Backend.execute(self, ir, timed); 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; --> 180",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:2345,avail,available,2345,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['avail'],['available']
Availability,"/driver/main.py"", line 1288, in monitor_billing_limits; records = await query_billing_projects_with_cost(db); File ""/usr/local/lib/python3.9/dist-packages/batch/utils.py"", line 165, in query_billing_projects_with_cost; async for record in db.select_and_fetchall(sql, tuple(args)):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 339, in select_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 254, in execute_and_fetchall; await cursor.execute(sql, args); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1172, in read; await self._read_result_packet(first_packet); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1232, in _read_result_packet; await self._read_rowdata_packet(); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1282, in _read_rowdata_packet; packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; packet.raise_for_error(); File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 221, in raise_for_error; err.raise_mysql_exception(self._data); File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 143, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14423:2903,error,errorclass,2903,https://hail.is,https://github.com/hail-is/hail/issues/14423,1,['error'],['errorclass']
Availability,"/elastic/elasticsearch-hadoop) from 7.17.1 to 8.4.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12358:1039,down,downloads,1039,https://hail.is,https://github.com/hail-is/hail/pull/12358,1,['down'],['downloads']
Availability,"/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2653:2657,Error,ErrorHandling,2657,https://hail.is,https://github.com/hail-is/hail/issues/2653,1,['Error'],['ErrorHandling']
Availability,"/fonttools/issues/3020"">#3020</a>).</li>; <li>[cython] Prevent <code>cython.compiled</code> raise AttributeError if cython not installed properly (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3017"">#3017</a>).</li>; <li>[OS/2] Guard against ZeroDivisionError when calculating xAvgCharWidth in the unlikely scenario no glyph has non-zero advance (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3015"">#3015</a>).</li>; <li>[subset] Recompute xAvgCharWidth independently of --no-prune-unicode-ranges, previously the two options were involuntarily bundled together (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3012"">#3012</a>).</li>; <li>[fontBuilder] Add <code>debug</code> parameter to addOpenTypeFeatures method to add source debugging information to the font in the <code>Debg</code> private table (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3008"">#3008</a>).</li>; <li>[name] Make NameRecord <code>__lt__</code> comparison not fail on Unicode encoding errors (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3006"">#3006</a>).</li>; <li>[featureVars] Fixed bug in <code>overlayBox</code> (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3003"">#3003</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/3005"">#3005</a>).</li>; <li>[glyf] Added experimental support for cubic bezier curves in TrueType glyf table, as outlined in glyf v1 proposal (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2988"">#2988</a>):<br />; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-cubicOutlines.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-cubicOutlines.md</a></li>; <li>Added new qu2cu module and related qu2cuPen, the reverse of cu2qu for converting TrueType quadratic splines to cubic bezier curves (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2993"">#2993</a>).</li>; <li>[glyf] Added experimental s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:5174,error,errors,5174,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['error'],['errors']
Availability,"/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 479 def _typecheck(__orig_func__, *args, **kwargs):; 480 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 481 return __orig_func__(*args_, **kwargs_); 482; 483 return decorator(_typecheck). ~/tools/hail/python/hail/table.py in _select(self, caller, s); 377 base, cleanup = self._process_joins(s); 378 analyze(caller, s, self._row_indices); --> 379 return cleanup(Table(base._jt.select(s._ast.to_hql()))); 380; 381 @typecheck_method(caller=str, s=expr_struct()). ~/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. ~/tools/hail/python/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ClassCastException: java.lang.Integer cannot be cast to java.lang.Double. Java stack trace:; java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double; 	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114); 	at is.hail.expr.ir.Literal$.apply(IR.scala:31); 	at is.hail.expr.ir.functions.UtilFunctions$$anonfun$registerAll$2.apply(UtilFunctions.scala:22); 	at is.hail.expr.ir.functions.UtilFunctions$$anonfun$registerAll$2.apply(UtilFunctions.scala:18); 	at is.hail.expr.ir.functions.RegistryFunctions$$anonfun$registerIR$2.apply(Functions.scala:165); 	at is.hail.expr.ir.functions.RegistryFunctions$$anonfun$registerIR$2.apply(Functions.scala:165); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12$$anonfun$apply$58.apply(AST.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3436:4006,Error,Error,4006,https://hail.is,https://github.com/hail-is/hail/issues/3436,1,['Error'],['Error']
Availability,"/issues/458"">#458</a>)</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Changed DockerNetwork.delete() to return True if successful (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/464"">#464</a>)</li>; </ul>; <h1>0.18.9 (2020-07-07)</h1>; <p>Bugfixes</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiodocker/commit/234522e191d208bab11175e2684b02ed9caedc43""><code>234522e</code></a> Bump to 0.21.0</li>; <li><a href=""https://github.com/aio-libs/aiodocker/commit/9b459934c3b1fde2fb9f0fa4e692be2403994cda""><code>9b45993</code></a> Fix <a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/536"">#536</a>: ssl_context not used (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/607"">#607</a>)</li>; <li><a href=""https://github.com/aio-libs/aiodocker/commit/72c157378490cfd4ef463759169c8be2bdfbfd19""><code>72c1573</code></a> Fix error when Stream is closed after container stopped (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/608"">#608</a>)</li>; <li><a href=""https://github.com/aio-libs/aiodocker/commit/e35e9698c93d5e9df59e81267a65ff355109af5c""><code>e35e969</code></a> Bump to 0.20.0</li>; <li><a href=""https://github.com/aio-libs/aiodocker/commit/d85e607f2c3d100b6415273665e75bc1fd75657c""><code>d85e607</code></a> Fix <a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/295"">#295</a>: allow passing credentials into run() call to pulling absent image f...</li>; <li><a href=""https://github.com/aio-libs/aiodocker/commit/2ad735b17d4ee7c1b1617373d2858ae776672ef3""><code>2ad735b</code></a> Fix <a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/295"">#295</a>: allow passing credentials into run() call to pulling absent image f...</li>; <li><a href=""https://github.com/aio-libs/aiodocker/commit/50421c40815af68e066cad81eaf57e899ec42415""><code>504",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11537:3705,error,error,3705,https://hail.is,https://github.com/hail-is/hail/pull/11537,1,['error'],['error']
Availability,"/li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>grpc:</strong> Return error if credentials are detected to be null (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2142"">#2142</a>) (<a href=""https://github.com/googleapis/java-storage/commit/b61a9764a9d953d2b214edb2b543b8df42fbfa06"">b61a976</a>)</li>; <li>Possible NPE when HttpStorageOptions deserialized (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2153"">#2153</a>) (<a href=""https://github.com/googleapis/java-storage/commit/68ad8e7357097e3dd161c2ab5f7a42a060a3702c"">68ad8e7</a>)</li>; <li>Update grpc default metadata projection to include acl same as json (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2150"">#2150</a>) (<a href=""https://github.com/googleapis/java-storage/commit/330e795040592e5df22d44fb5216ad7cf2448e81"">330e795</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.14.0 (<a href=""https://redirect.github.com/google",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:6544,error,error,6544,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['error'],['error']
Availability,"/li>; <li>Fix TXT CHAOS test</li>; <li>Add support for CAA queries</li>; <li>Support Python &gt;= 3.6</li>; <li>Bump pycares dependency</li>; <li>Drop tasks.py</li>; <li>Allow specifying dnsclass for queries</li>; <li>Set URL to https</li>; <li>Add license args in setup.py</li>; <li>Converted Type Annotations to Py3 syntax Closes</li>; <li>Only run mypy on cpython versions</li>; <li>Also fix all type errors with latest mypy - pycares seems to have no typing / stubs so lets ignore it via <code>mypy.ini</code></li>; <li>setup: typing exists since Python 3.5</li>; <li>Fix type annotation of gethostbyname()</li>; <li>Updated README</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/saghul/aiodns/blob/master/ChangeLog"">aiodns's changelog</a>.</em></p>; <blockquote>; <h1>3.0.0</h1>; <ul>; <li>Release wheels and source to PyPI with GH actions</li>; <li>Try to make tests more resilient</li>; <li>Don't build universal wheels</li>; <li>Migrate CI to GH Actions</li>; <li>Fix TXT CHAOS test</li>; <li>Add support for CAA queries</li>; <li>Support Python &gt;= 3.6</li>; <li>Bump pycares dependency</li>; <li>Drop tasks.py</li>; <li>Allow specifying dnsclass for queries</li>; <li>Set URL to https</li>; <li>Add license args in setup.py</li>; <li>Converted Type Annotations to Py3 syntax Closes</li>; <li>Only run mypy on cpython versions</li>; <li>Also fix all type errors with latest mypy - pycares seems to have no typing / stubs so lets ignore it via <code>mypy.ini</code></li>; <li>setup: typing exists since Python 3.5</li>; <li>Fix type annotation of gethostbyname()</li>; <li>Updated README</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/saghul/aiodns/commit/cdb33385f46be1e18bc525ccb153c8abc8ac92d4""><code>cdb3338</code></a> Updated changelog</li>; <li><a href=""https://github.com/saghul/aiodns/commit/a57968007a0e6f826e1a8a2160eade23c254bc42",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11570:1387,resilien,resilient,1387,https://hail.is,https://github.com/hail-is/hail/pull/11570,1,['resilien'],['resilient']
Availability,"/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 914, in create_jobs; success = await jobs_builder.commit(); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 161, in commit; await cursor.executemany(self._jobs_sql, self._jobs); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 283, in executemany; self._get_db().encoding)); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 622, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 1105, in read; first_packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 593, in _read_packet; packet.check_error(); File ""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py"", line 220, in check_error; err.raise_mysql_exception(self._data); File ""/usr/local/lib/python3.6/dist-packages/pymysql/err.py"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6543:4795,error,errorclass,4795,https://hail.is,https://github.com/hail-is/hail/issues/6543,1,['error'],['errorclass']
Availability,"/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:3007,failure,failure,3007,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['failure'],['failure']
Availability,"/snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `2.11.3 -> 3.1.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:3247,avail,available,3247,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:244); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:77); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at __C1310collect_distributed_array_table_native_writer.apply_region1_87(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C1310collect_distr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:6009,Error,ErrorHandling,6009,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Error'],['ErrorHandling']
Availability,"/summary>. ```; pytest-html 1.22.1 requires pytest-metadata, which is not installed.; jupyter 1.0.0 requires qtconsole, which is not installed.; jupyter 1.0.0 requires notebook, which is not installed.; curlylint 0.13.1 requires pathspec, which is not installed.; beautifulsoup4 4.12.3 requires soupsieve, which is not installed.; astroid 2.15.8 requires lazy-object-proxy, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:1453,avail,available,1453,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['avail'],['available']
Availability,"/ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/80f04c6a46fe7df053ac55bcfc6f90ff74c4b873""><code>80f04c6</code></a> Bump up version number to 5.1.3</li>; <li>Additional commits viewable in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:3756,down,download-task,3756,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability,"/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.; The command '/bin/sh -c python3 -m pip install --no-cache-dir -r requirements.txt -r dev-requirements.txt' returned a non-zero code: 2; [0m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8390:2283,down,download,2283,https://hail.is,https://github.com/hail-is/hail/issues/8390,1,['down'],['download']
Availability,"/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,435	job.py	schedule_job:473	error while scheduling job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:15920,error,error,15920,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,"/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:37,447	job.py	schedule_job:473	error while scheduling job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:17926,error,error,17926,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,"/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 535, in connect\n proto = await self._create_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 892, in _create_connection\n _, proto = await self._create_direct_connection(req, traces, timeout)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1051, in _create_direct_connection\n raise last_exc\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 1032, in _create_direct_connection\n client_error=client_error,\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 975, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.128.0.11:5000 ssl:default [Connection refused]; ERROR	2022-03-02 19:06:39,204	job.py	schedule_job:473	error while scheduling job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n timeout=aiohttp.ClientTimeout(total=2),\n File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 113, in request_and_raise_for_status\n resp = await self.client_session._request(method, url, **kwargs)\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 521, in _request\n req, traces=traces, timeout=real_timeout\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:24080,error,error,24080,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['error'],['error']
Availability,"/v1561977819/icon/m.png ""medium severity"") | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNA",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14074:2640,Error,Error,2640,https://hail.is,https://github.com/hail-is/hail/pull/14074,1,['Error'],['Error']
Availability,"/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **444/1000** <br/> **Why?** Has a fix available, CVSS 4.6 | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **429/1000** <br/> **Why?** Has a fix available, CVSS 4.3 | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **501/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.3 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:2359,avail,available,2359,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v15",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:3684,avail,available,3684,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v156",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:4376,avail,available,4376,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.0&new-version=5.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:3402,down,download,3402,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download']
Availability,"0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/start.py"", line 195, in main; sp.check_call(cmd); File ""/usr/local/Cellar/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634:2146,ERROR,ERROR,2146,https://hail.is,https://github.com/hail-is/hail/issues/6634,1,['ERROR'],['ERROR']
Availability,"011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.Hado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10530:2956,Error,Error,2956,https://hail.is,https://github.com/hail-is/hail/issues/10530,2,['Error'],['Error']
Availability,"019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request; response = self.make_response(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1740, in make_response; rv = self.response_class.force_type(rv, request.environ); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/wrappers.py"", line 885, in force_type; response = BaseResponse(*_run_wsgi_app(response, environ)); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/test.py"", line 884, in run_wsgi_app; app_rv = app(environ, start_response); TypeError: 'int' object is not callable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5418:1530,ERROR,ERROR,1530,https://hail.is,https://github.com/hail-is/hail/pull/5418,1,['ERROR'],['ERROR']
Availability,"05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3404:2024,Error,Error,2024,https://hail.is,https://github.com/hail-is/hail/issues/3404,1,['Error'],['Error']
Availability,"1 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:5585,Error,Error,5585,https://hail.is,https://github.com/hail-is/hail/issues/10682,2,['Error'],['Error']
Availability,"1 if ir.typ == tvoid:; 192 value = None. File ~/projects/hail/hail/python/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); 186 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 187 try:; --> 188 result, timings = self._rpc(ActionTag.EXECUTE, payload); 189 except FatalError as e:; 190 raise e.maybe_user_error(ir) from None. File ~/projects/hail/hail/python/hail/backend/py4j_backend.py:223, in Py4JBackend._rpc(self, action, payload); 221 if resp.status_code >= 400:; 222 error_json = orjson.loads(resp.content); --> 223 raise fatal_error_from_java_error_triplet(; 224 error_json['short'], error_json['expanded'], error_json['error_id']; 225 ); 226 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()). Java stack trace:; is.hail.utils.HailException: error after applying LowerToDistributedArray; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:32); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:3339,error,error,3339,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['error'],['error']
Availability,1(LocalBackend.scala:272); E 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:271); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); E 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$2(LocalBackend.scala:120); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); E 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:105); E 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:271); E 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); E 	at java.base/java.lang.Thread.run(Thread.java:829); E; E; E; E Hail version: 0.2.132-f39364c177e0; E Error summary: RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:8571,Error,Error,8571,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['Error'],['Error']
Availability,"1-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:17:20,179 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:17:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:19:31,732 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /jobs/create HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:19:31,745 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:19:31,764 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; ERROR	| 2018-11-13 18:19:31,779 	| app.py 	| log_exception:1761 | Exception on /pod_changed [POST]; Traceback (most recent call last):; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app; response = self.full_dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request; rv = self.handle_user_exception(e); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception; reraise(exc_type, exc_value, tb); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise; raise value; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request; rv = self.dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4773:2600,ERROR,ERROR,2600,https://hail.is,https://github.com/hail-is/hail/issues/4773,1,['ERROR'],['ERROR']
Availability,1. Don't ignore parse errors by default. These shouldn't be happening; with modern VEP versions anyway (they were caused by genes named NaN; or something in older versions); 2. Don't silently drop variants that don't have a match in VEP. All keys; coming in will have a row coming out with a VEP annotation (maybe NA); and a partition/block index.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9973:22,error,errors,22,https://hail.is,https://github.com/hail-is/hail/pull/9973,1,['error'],['errors']
Availability,"1. HailException now causes job failure; 2. ServiceBackend doesn't short-circuit on batch failure, but instead finds the first bad job (either success with exception or failed with no output) and throws a contextual exception from that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12318:32,failure,failure,32,https://hail.is,https://github.com/hail-is/hail/pull/12318,2,['failure'],['failure']
Availability,"1. I ended up removing the test with the VCF from 1000 genomes; 2. I couldn't figure out a good way to get the number of variants used in the computation. It's either in the annotations as ""sa.imputegender.T"" or we'd have to do RDD.count() ; 3. I'm open to naming suggestions if you think something else is better. I didn't want to use ""sex check"" because that implies comparing to the reported gender and reporting errors -- not what I have implemented here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/386:416,error,errors,416,https://hail.is,https://github.com/hail-is/hail/pull/386,1,['error'],['errors']
Availability,"1. If a job errors rather than fails, we still want to see its logs in the debug info. 2. The backend from before `hl_stop_for_test` is broken. In particular, it does not have an open ClientSession, so it cannot make HTTP requests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13872:12,error,errors,12,https://hail.is,https://github.com/hail-is/hail/pull/13872,1,['error'],['errors']
Availability,"1. If we receive an error other than 404 from Google when asking about an instance, we should raise. This is unexpected. (The later lines will fail anyway because spec is `None`); 2. (the main issue) if the instance is not active, do not bother contacting it and, crucially, continue `check_on_instance` eventually learning the instance does not exist.; 3. Drop timeout to 5s to talk to a batch agent. Fixes the zombie instance issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8023:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/8023,1,['error'],['error']
Availability,"1. Lazily load the tokens on the end user machine because the first time someone logs in to the default namespace, they will have no tokens. 2. Teach `get_tokens_file` to default to the default end-user location. 3. Add a bunch of types which would have caught this error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11054:266,error,error,266,https://hail.is,https://github.com/hail-is/hail/pull/11054,1,['error'],['error']
Availability,"1. Makefile is a bit more resilient to changes in the `dk-test` instance that is used to route traffic from GitHub to a local laptop test. It now looks up the ip. The zone is still hardcoded and it's moved to zone `us-central1-a`. The name is also hardcoded to `dk-test`.; 2. I renamed `is_running` to `is_building`; 3. When a job refresh happens, it is now `PRS` responsibility to determine what to do. It starts the same as it always does, updating existing PRs with new job information. The difference is that it tracks which (believed to be) currently building jobs are not seen in the job list. All such jobs are re-built, under the assumption that the job must have failed. cc: @cseed . This should allow CI to recover from the loss of batch. Fixes #4654.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4659:26,resilien,resilient,26,https://hail.is,https://github.com/hail-is/hail/pull/4659,2,"['recover', 'resilien']","['recover', 'resilient']"
Availability,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923:503,down,down,503,https://hail.is,https://github.com/hail-is/hail/pull/8923,1,['down'],['down']
Availability,"1. NativeModule manages the lifecycle of Scala-generated C++ functions. Generate the C++; source code as a Scala String, then construct ; NativeModule(compileOptions, sourceString, forceBuild); From that object you can do getKey(): String and getBinary(): Array[Byte] to get the compiled ; code in a serializable form which can be passed to other nodes in a cluster, and then used to; construct a local NativeModule(key, binary). 2. NativeCode.java now does a two-stage bootstrap on Linux to make sure that libhail.so is; loaded with the RTLD_GLOBAL flag so that dynamic-generated C++ can use functions; defined in libhail.so. On Mac, this works ok without the bootstrap. [We needed this for the; RowStore with generated-C++ decoders]. 3. Added a NativeStatus* parameter to all NativeLongFunc's, to encourage consistent handling; of errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973:833,error,errors,833,https://hail.is,https://github.com/hail-is/hail/pull/3973,1,['error'],['errors']
Availability,"1. The default log path includes the version and a; timestamp. This will help people avoid overwriting; log files, which will help us.; 2. Echo the full path to the log after the hail logo; 3. Add a function `hl.copy_log` which can be used to; copy the session log to a hadoop-api-compliant; location.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4421:139,Echo,Echo,139,https://hail.is,https://github.com/hail-is/hail/pull/4421,1,['Echo'],['Echo']
Availability,"1. Treat any 500 from Docker as a retryable error.; 2. Move DockerError transiency to is_transient_errors and use retry_transient_errors instead of a hand rolled transient wrapper. The first change also makes us robust to changes in error messages on the GCR side. In particular, we started seeing this error message:. ```; Head https://gcr.io/v2/hail-vdc/ubuntu/manifests/18.04: Get https://gcr.io/v2/token?account=_json_key&scope=repository%3Ahail-vdc%2Fubuntu%3Apull&service=gcr.io: net/http: request canceled (Client.Timeout exceeded while awaiting headers); ```. which is slightly different from the extant messages we check for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11943:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/11943,4,"['error', 'robust']","['error', 'robust']"
Availability,"1. `hl.plot.cdf`, `hl.plot.histogram`: `values` is a method on `hl.Struct`s (because they are `Mapping`), use `[""values""]` to get the `values` field of a struct.; 2. `hl.plot.pdf`: Do not error if there are no compactions, just set error to zero.; 3. *: Add return type annotations to improve IDE hints.; 4. `hl.plot.joint_plot`: I think a bokeh upgrade completely broke this; the first argument is the grid of figures.; 5. `hl.plot.qq`: Kill unnecessary persists with fire.; 6. `hl.plot.visualize_missingness`: Informative error message when row key is not ""windowable"".; 7. `hl.plot.visualize_missingness`: When row key is a one-field struct, use the one field instead of the struct as the windowable value.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12739:188,error,error,188,https://hail.is,https://github.com/hail-is/hail/pull/12739,3,['error'],['error']
Availability,"1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$findField(ScalaSigReader.scala:100); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$read$1(ScalaSigReader.scala:45); 	at org.json4s.reflect.ScalaSigReader$.readField(ScalaSigReader.scala:49); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:69); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:68); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299:1820,failure,failure,1820,https://hail.is,https://github.com/hail-is/hail/issues/6299,1,['failure'],['failure']
Availability,"1095>"", line 2, in take; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 2087, in take; return self.head(n).collect(_localize); File ""<decorator-gen-1089>"", line 2, in collect; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1886, in collect; return Env.backend().execute(e._ir); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/Users/konradk/programs/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchElementException: key not found: 1; [...]; java.util.NoSuchElementException: key not found: 1; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.MapLike$class.apply(MapLike.scala:141); at scala.collection.AbstractMap.apply(Map.scala:59); at is.hail.types.encoded.EBaseStruct.fieldType(EBaseStruct.scala:34); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:84); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:83); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.Traver",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9016:4530,Error,Error,4530,https://hail.is,https://github.com/hail-is/hail/issues/9016,1,['Error'],['Error']
Availability,"1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/cseed/hail/python/hail/utils/java.py"", line 200, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 2.0 failed 1 times, most recent failure: Lost task 7.0 in stage 2.0 (TID 23, localhost, executor driver): is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1011); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RowStore.scala:1071); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:42); 	at is.hail.utils.richUtils.RichContextRDD$$anonfun$1.apply(RichContextRDD.scala:27); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:1644,Error,ErrorHandling,1644,https://hail.is,https://github.com/hail-is/hail/issues/4096,1,['Error'],['ErrorHandling']
Availability,"131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5b299ddae758; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3728:6143,Error,Error,6143,https://hail.is,https://github.com/hail-is/hail/issues/3728,1,['Error'],['Error']
Availability,"149 return self._jbackend.parse_value_ir(; 150 code,; 151 {k: t._parsable_string() for k, t in ref_map.items()},; 152 ir_map). File ~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args); 1315 command = proto.CALL_COMMAND_NAME +\; 1316 self.command_header +\; 1317 args_command +\; 1318 proto.END_COMMAND_PART; 1320 answer = self.gateway_client.send_command(command); -> 1321 return_value = get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1324 for temp_arg in temp_args:; 1325 temp_arg._detach(). File /private/tmp/hail/hail/python/hail/backend/py4j_backend.py:35, in handle_java_exception.<locals>.deco(*args, **kwargs); 33 tpl = Env.jutils().handleForPython(e.java_exception); 34 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'). Java stack trace:; java.lang.RuntimeException: typ: inference failure:; 	at is.hail.expr.ir.IR.typ(IR.scala:38); 	at is.hail.expr.ir.IR.typ$(IR.scala:33); 	at is.hail.expr.ir.ToStream.typ(IR.scala:300); 	at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr_1$81(Parser.scala:1111); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:3118,Error,Error,3118,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['Error'],['Error']
Availability,"151"">#8151</a>) (<a href=""https://github.com/vitejs/vite/commit/9fdd0a3"">9fdd0a3</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8151"">#8151</a></li>; <li>feat: new hook <code>configurePreviewServer</code> (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7658"">#7658</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8437"">#8437</a>) (<a href=""https://github.com/vitejs/vite/commit/7b972bc"">7b972bc</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7658"">#7658</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8437"">#8437</a></li>; <li>fix: remove empty chunk css imports when using esnext (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8345"">#8345</a>) (<a href=""https://github.com/vitejs/vite/commit/9fbc1a9"">9fbc1a9</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8345"">#8345</a></li>; <li>fix: EPERM error on Windows when processing dependencies (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8235"">#8235</a>) (<a href=""https://github.com/vitejs/vite/commit/dfe4307"">dfe4307</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8235"">#8235</a></li>; <li>fix(css): remove <code>?used</code> hack (fixes <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/6421"">#6421</a>, <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8245"">#8245</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8278"">#8278</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8471"">#8471</a>) (<a href=""https://github.com/vitejs/vite/commit/8d7bac4"">8d7bac4</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/6421"">#6421</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8245"">#8245</a> <a href=""https://github-redirect.dependabot.com/vitejs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:4742,error,error,4742,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['error'],['error']
Availability,"19603: error while parsing line; chrY	113	.	GG	G,*,AG,CG	596	PASS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7284,Error,ErrorHandling,7284,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Error'],['ErrorHandling']
Availability,"1977819/icon/m.png ""medium severity"") | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TOR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14259:3296,Error,Error,3296,https://hail.is,https://github.com/hail-is/hail/pull/14259,1,['Error'],['Error']
Availability,"19fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/717877121299cea8f216d3a595eaa56731a6acd3""><code>7178771</code></a> Support changing a target file's relative path in an eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e5af1bd7f9daa8a9222aee0dd1b703727cb5e94e""><code>e5af1bd</code></a> Bump version number to 5.3.0-SNAPSHOT</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:5331,down,download,5331,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download']
Availability,"1_70_65,GAAGTCTCATCCCATC-1_45;AACGTCAGTGTTTCAG-1_74:1:19:255	0/1:2,4:6:60:101,0,60:.:.:.:.; chr1	12198	rs62635282	G	C	69.6	.	BaseQRankSum=0;ClippingRankSum=0;DB;ExcessHet=3.0103;FS=4.771;MQ=42;MQRankSum=-0.967;QD=23.2;ReadPosRankSum=0.967;SOR=2.225;CSQ=C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000450305|transcribed_unprocessed_pseudogene|2/6||||68|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000456328|processed_transcript|1/3||||330|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|downstream_gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinva",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:36022,error,error,36022,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['error'],['error']
Availability,"1bc1497"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""42.0.2""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""5ecb4152-94d0-44ff-86c6-21e542bb123d"",""projectUrl"":""https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-CRYPTOGRAPHY-6050294"",""SNYK-PYTHON-CRYPTOGRAPHY-6092044"",""SNYK-PYTHON-CRYPTOGRAPHY-6126975"",""SNYK-PYTHON-CRYPTOGRAPHY-6210214"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[554,704,509,454,616,584,479,509,509,509,509,589,509,691,399,479,399,539,479,479,616,616,561,519],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:13458,avail,available,13458,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['avail'],['available']
Availability,"2 19:11:13.126 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.126 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.126 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.127 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:13.138 : ERROR: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; From is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:3947,ERROR,ERROR,3947,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['ERROR'],['ERROR']
Availability,"2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int8<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int8<16>]’; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:133:36: required from ‘simdpp::arch_avx2::uint8<16>& simdpp::arch_avx2::uint8<16>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int8<16, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_avg_trunc.h:64:25: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<16>::d_’ from an array of ‘const class simdpp::arch_avx2::int8<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: ‘class simdpp::arch_avx2::uint8<16>’ declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::int16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:84016,error,error,84016,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:114:36: required from ‘simdpp::arch_avx2::uint32<8>& simdpp::arch_avx2::uint32<8>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:136:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint32<8>’ with ‘private’ member ‘simdpp::arch_avx2::uint32<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: ‘class simdpp::arch_avx2::uint32<8>’ declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:38252,error,error,38252,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-14287a4; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3119:3366,Error,Error,3366,https://hail.is,https://github.com/hail-is/hail/issues/3119,1,['Error'],['Error']
Availability,"24-04-08&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3AAshokChoudhary11+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​AshokChoudhary11</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3Aholzman+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​holzman</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3Amanics+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​manics</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab_server+involves%3Awelcome+updated%3A2024-03-11..2024-04-08&amp;type=Issues""><code>@​welcome</code></a></p>; <!-- raw HTML omitted -->; <h2>2.25.4</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab_server/compare/v2.25.3...15e796699f04e06db9ed23a689d454feae36ffbd"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Use updated releaser workflows <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/442"">#442</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Use json5 typings <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/441"">#441</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Enforce pytest 7 <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/439"">#439</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Fix test util typings <a href=""https://redirect.github.com/jupyterlab/jupyterlab_server/pull/437"">#437</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab_server/graphs/contributors?from=2024-02-14&amp;to=2024-03-11&amp;type=c"">GitHub contributors page for this release</a>)</p>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14483:5312,Mainten,Maintenance,5312,https://hail.is,https://github.com/hail-is/hail/pull/14483,1,['Mainten'],['Maintenance']
Availability,"245227,; ""duration"": 10461; },; ""starting"": {; ""start_time"": 1586188234767,; ""finish_time"": 1586188236190,; ""duration"": 1423; },; ""running"": {; ""start_time"": 1586188236190,; ""finish_time"": 1586188245227,; ""duration"": 9037; },; ""uploading_log"": {; ""start_time"": 1586188245231,; ""finish_time"": 1586188245276,; ""duration"": 45; },; ""deleting"": {; ""start_time"": 1586188245276,; ""finish_time"": 1586188245305,; ""duration"": 29; }; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2020-04-06T15:50:36.182009912Z"",; ""finished_at"": ""2020-04-06T15:50:44.884808909Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188245305,; ""finish_time"": 1586188245404,; ""duration"": 99; },; ""creating"": {; ""start_time"": 1586188245404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8473:1381,error,error,1381,https://hail.is,https://github.com/hail-is/hail/issues/8473,2,['error'],['error']
Availability,"2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.0&new-version=5.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:2982,down,download-task,2982,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"2814; 2815 return cleanup(MatrixTable(base._jvds.selectRows(row._ast.to_hql(),; -> 2816 new_key))); 2817; 2818 @typecheck_method(caller=str,. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 9.0 failed 20 times, most recent failure: Lost task 24.19 in stage 9.0 (TID 2874, berylc-sw-68wx.c.broad-mpg-gnomad.internal, executor 39): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:915); 	at is.hail.rvd.Ordered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3583:3706,failure,failure,3706,https://hail.is,https://github.com/hail-is/hail/issues/3583,1,['failure'],['failure']
Availability,"2::uint32<16>]’; libsimdpp-2.0-rc2/simdpp/types/int32.h:50:35: required from ‘simdpp::arch_avx2::int32<N>& simdpp::arch_avx2::int32<N>::operator=(const simdpp::arch_avx2::any_vec<(N * 4), V>&) [with V = simdpp::arch_avx2::uint32<16>; unsigned int N = 16]’; libsimdpp-2.0-rc2/simdpp/types/int32.h:43:73: required from ‘simdpp::arch_avx2::int32<N>::int32(const simdpp::arch_avx2::uint32<N, E>&) [with E = void; unsigned int N = 16]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:73:69: required from ‘simdpp::arch_avx2::int32<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int32<N, E>&, const simdpp::arch_avx2::int32<N, E2>&) [with unsigned int N = 8; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int32.h:74:26: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<16>’ with ‘private’ member ‘simdpp::arch_avx2::int32<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:37,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32.h:31:7: note: ‘class simdpp::arch_avx2::int32<16>’ declared here; class int32<N, void> : public any_int32<N, int32<N,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int32<8>; T = simdpp::arch_avx2::uint32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:129600,error,error,129600,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"2](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **444/1000** <br/> **Why?** Has a fix available, CVSS 4.6 | Access Control Bypass <br/>[SNYK-PYTHON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **429/1000** <br/> **Why?** Has a fix available, CVSS 4.3 | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **501/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.3 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:3115,avail,available,3115,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"2a9-4d26-ac71-5e6676ff3392/pyscripts_F47nn5.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/generate_qc_annotations.py"", line 203, in main; vds.write(annotations_vds_path(data_type, 'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3040:1425,failure,failure,1425,https://hail.is,https://github.com/hail-is/hail/issues/3040,1,['failure'],['failure']
Availability,"3 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:6044,failure,failure,6044,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['failure'],['failure']
Availability,"3""><code>@​blink1073</code></a>)</li>; <li>Clean up 7.x workflows <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/865"">#865</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-10-25&amp;to=2022-11-10&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-10-25..2022-11-10&amp;type=Issues""><code>@​blink1073</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/v7.4.5/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>7.4.5</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.4...d27c8a497c6cbb1a232fbbe75cb1fd0f53faa9b0"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[7.x] Handle Jupyter Core Warning <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/875"">#875</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up 7.x workflows <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/865"">#865</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-10-25&amp;to=2022-11-10&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-10-25..2022-11-10&amp;type=Issues""><code>@​blink1073</code></a></p>; <!-- raw HTML omitted -->; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyter/jupyter_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12459:1666,Mainten,Maintenance,1666,https://hail.is,https://github.com/hail-is/hail/pull/12459,1,['Mainten'],['Maintenance']
Availability,"3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:16); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:42); 	at is.hail.table.Table.x$3$lzycompute(Table.scala:211); 	at i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5212:3253,Error,ErrorHandling,3253,https://hail.is,https://github.com/hail-is/hail/issues/5212,1,['Error'],['ErrorHandling']
Availability,"30/hail-1e2e8c7e8.zip/hail/typecheck/check.py"", line 547, in wrapper; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/matrixtable.py"", line 2893, in _select_rows; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:340); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:80); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:60); 	at is.hail.expr.Parser$.eval(Parser.scala:73); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:88); 	at is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1232); 	at is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1205); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-1e2e8c7e86f3; Error summary: RuntimeException: Method code too large!; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3746:2993,Error,Error,2993,https://hail.is,https://github.com/hail-is/hail/issues/3746,1,['Error'],['Error']
Availability,"32""><code>ab633b6</code></a> Build x86_64 musllinux wheels (<a href=""https://github-redirect.dependabot.com/ijl/orjson/issues/242"">#242</a>)</li>; <li><a href=""https://github.com/ijl/orjson/commit/8bf078b27e7479f2cfbea1bac7155d4449ce7e30""><code>8bf078b</code></a> Cross compile wheels for armv7l on GitHub Actions (<a href=""https://github-redirect.dependabot.com/ijl/orjson/issues/241"">#241</a>)</li>; <li><a href=""https://github.com/ijl/orjson/commit/c196f0e55bd51d3693d381ccc06f2fd4b5443d86""><code>c196f0e</code></a> 3.6.6</li>; <li><a href=""https://github.com/ijl/orjson/commit/81890b097f7a479d1c1e697d21467952e0be24a9""><code>81890b0</code></a> Fix 53-bit error on value between isize and usize</li>; <li><a href=""https://github.com/ijl/orjson/commit/8fc1e8989d6a72581aa71533384cb1ef9a260ebc""><code>8fc1e89</code></a> Fast conditional for zoneinfo.ZoneInfo</li>; <li><a href=""https://github.com/ijl/orjson/commit/853ffbdf8dc5f34792765c22aa835e1b67d90a76""><code>853ffbd</code></a> fix(errors): adjust column offset if not at char boundary</li>; <li>Additional commits viewable in <a href=""https://github.com/ijl/orjson/compare/3.6.4...3.6.7"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=orjson&package-manager=pip&previous-version=3.6.4&new-version=3.6.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:4114,error,errors,4114,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['error'],['errors']
Availability,"3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `2.11.3 -> 3.1.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:3616,avail,available,3616,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:2605,recover,recover-dangling-heads,2605,https://hail.is,https://github.com/hail-is/hail/issues/8469,6,"['error', 'failure', 'recover']","['error-correct-kmers', 'error-correct-reads', 'error-correction', 'failure-bam', 'recover-dangling-branches', 'recover-dangling-heads']"
Availability,"37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.561 : ERROR: error while applying lowering 'LowerAndExecuteShuffles'; 2023-05-04 01:04:37.600 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=0, peakBytesReadable=0.00 B, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 0 allocated (0 blocks / 0 chunks), regions.size = 0, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.601 : INFO: RegionPool: FREE: 128.0K allocated (128.0K blocks / 0 chunks), regions.size = 2, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.603 : ERROR: SocketException: Connection reset; From javax.net.ssl.SSLException: Connection reset; 	at sun.security.ssl.Alert.createSSLException(Alert.java:127); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:324); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:267); 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:262); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:138); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73); 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:962); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:23545,ERROR,ERROR,23545,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['ERROR'],['ERROR']
Availability,"37</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; <li>Prevent DeprecationWarning about internal use of <code>asyncio.get_event_loop()</code> from affecting test cases <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/757"">#757</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.5</h2>; <h1>0.23.5 (2024-02-09)</h1>; <ul>; <li>Declare compatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; <li>Prevent DeprecationWarning about internal use of <code>asyncio.get_event_loop()</code> from affecting test cases <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/757"">#757</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:2547,down,down,2547,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['down'],['down']
Availability,"37</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; <li>Prevent DeprecationWarning about internal use of <code>asyncio.get_event_loop()</code> from affecting test cases <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/757"">#757</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.5a0</h2>; <h1>0.23.5 (UNRELEASED)</h1>; <ul>; <li>Declare compatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:3917,down,down,3917,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['down'],['down']
Availability,"3:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:2203,Error,Error,2203,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['Error']
Availability,"3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **496/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.2 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-6002459](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-6002459) | `urllib3:` <br> `1.26.17 -> 1.26.18` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3OWQ3MTdiYy05MThjLTRlMjctOGQ2OC0xNTNhNWI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13850:1127,avail,available,1127,https://hail.is,https://github.com/hail-is/hail/pull/13850,1,['avail'],['available']
Availability,"3a90b7747b8972f51d1407616c51084d97c589"">803a90b</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1758"">#1758</a>) (<a href=""https://github.com/googleapis/java-storage/commit/140e90911229c876de7b674dd1e61b278e8b07fd"">140e909</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.17 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1759"">#1759</a>) (<a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf"">7e3175a</a>)</li>; </ul>; <h2>v2.14.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.13.1...v2.14.0"">2.14.0</a> (2022-10-26)</h2>; <h3>Google Cloud Storage gRPC API Preview</h3>; <p>The first release of <code>google-cloud-storage</code> with support for a subset of the Google Cloud Storage gRPC API which is in private preview. The most common operations have all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annotations will be updated.</p>; <p>All new gRPC related APIs are annotated with <code>@BetaApi</code> to denote they are in preview and the possibility of breaking change is present. At this time, opting to use any of the gRPC transport mode means you are okay with the possibility of a breaking change happening. When the APIs are out of preview, we will remove the <code>@BetaApi</code> annotation to signal they are now considered stable and will not break outside a major version.</p>; <p><strong><em>NOTICE</em></strong>: Using the gRPC transport is exclusive. Any operations which have not yet been implemented for gRPC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:2245,avail,available,2245,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['avail'],['available']
Availability,"4 (Krishnan Mahadevan); 7.6.0; Fixed: GITHUB-2741: Show fully qualified name of the test instead of just the function name for better readability of test output.(Krishnan Mahadevan); Fixed: GITHUB-2725: Honour custom attribute values in TestNG default reports (Krishnan Mahadevan); Fixed: GITHUB-2726: <a href=""https://github.com/AfterClass""><code>@​AfterClass</code></a> config method is executed for EACH <a href=""https://github.com/Test""><code>@​Test</code></a> method when parallel == methods (Krishnan Mahadevan); Fixed: GITHUB-2752: TestListener is being lost when implenting both IClassListener and ITestListener (Krishnan Mahadevan); New: GITHUB-2724: DataProvider: possibility to unload dataprovider class, when done with it (Dzmitry Sankouski); Fixed: GITHUB-217: Configure TestNG to fail when there's a failure in data provider (Krishnan Mahadevan); Fixed: GITHUB-2743: SuiteRunner could not be initial by default Configuration (Nan Liang); Fixed: GITHUB-2729: beforeConfiguration() listener method should be invoked for skipped configurations as well(Nan Liang); Fixed: assertEqualsNoOrder for Collection and Iterators size check was missing (Adam Kaczmarek); Fixed: GITHUB-2709: Testnames not working together with suites in suite (Martin Aldrin); Fixed: GITHUB-2704: IHookable and IConfigurable callback discrepancy (Krishnan Mahadevan); Fixed: GITHUB-2637: Upgrade to JDK11 as the minimum JDK requirements (Krishnan Mahadevan); Fixed: GITHUB-2734: Keep the initial order of listeners (Andrei Solntsev); Fixed: GITHUB-2359: Testng <a href=""https://github.com/BeforeGroups""><code>@​BeforeGroups</code></a> is running in parallel with testcases in the group (Anton Velma); Fixed: Possible StringIndexOutOfBoundsException in XmlReporter (Anton Velma); Fixed: GITHUB-2754: <a href=""https://github.com/AfterGroups""><code>@​AfterGroups</code></a> is executed for each &quot;finished&quot; group when it has multiple groups defined (Anton Velma)</p>; <p>7.5; Fixed: GITHUB-2701: Bump gradle ve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:13257,failure,failure,13257,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['failure'],['failure']
Availability,"4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739d0400c333a72b32f5d514""><code>0fdebf3</code></a> Improve verify example</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/02b8e1a79d9e00acd61f9ac42e5555619fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/717877121299cea8f216d3a595eaa56731a6acd3""><code>7178771</code></a> Support changing a target file's relative path in an eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e5af1bd7f9daa8a9222aee0dd1b703727cb5e94e""><code>e5af1bd</code></a> Bump vers",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:3925,down,download-task,3925,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,"404s occur in `hail-ci-build.sh` because batch is already bound to port 5000 when CI tries to start. https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/39a94649482a2512a7a514e6084c5b84f48b8205/index.html. The error from `ci.py`:; ```; Traceback (most recent call last):; File ""ci/ci.py"", line 372, in <module>; app.run(host='0.0.0.0', threaded=False); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/flask/app.py"", line 943, in run; run_simple(host, port, self, **options); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 814, in run_simple; inner(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 774, in inner; fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 666, in make_server; passthrough_errors, ssl_context, fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 577, in __init__; self.address_family), handler); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 449, in __init__; self.server_bind(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/http/server.py"", line 137, in server_bind; socketserver.TCPServer.server_bind(self); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 463, in server_bind; self.socket.bind(self.server_address); OSError: [Errno 98] Address already in use; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4531:245,error,error,245,https://hail.is,https://github.com/hail-is/hail/issues/4531,1,['error'],['error']
Availability,"416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2074,failure,failure,2074,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['failure'],['failure']
Availability,"458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadable=129.00 KiB, chunks requested=0, cache hits=0; 2023-05-04 01:04:37.560 : INFO: RegionPool: FREE: 129.0K allocated (129.0K blocks / 0 chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1; 2023-05-04 01:04:37.561 : ERROR: error while applying lowering 'LowerAndExecuteShuffles'; 2023-05-04 01:04:37.600 : INFO: RegionPool: initialized for thread 8: pool-1-threa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:22030,Error,Error,22030,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['Error'],['Error']
Availability,460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18671,Error,ErrorHandling,18671,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Error'],['ErrorHandling']
Availability,"46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:19650,error,error,19650,https://hail.is,https://github.com/hail-is/hail/issues/8944,3,['error'],['error']
Availability,"49f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.0&new-version=5.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:2586,down,download-task,2586,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 199 'Hail version: %s\n'; --> 200 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 201 except pyspark.sql.utils.CapturedException as e:; 202 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Cannot find row in Map(). Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (titv; (ApplyBinaryPrimOp FloatingPointDivide; (GetField n_ti; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)); (GetField n_tv; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)))); (min_score; (GetField `0`; (In Struct{`0`:Float64,`1`:Float64} 0))); (max_score; (GetField `1`; (In Struct{`0`:Float64,`1`:Float64} 0)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:11); 	at is.hail.expr.ir.Emit$.emit(Emit.scala:42); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:28); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:51); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:74); 	at is.hail.expr.ir.TableKeyByAndAggregate.execute(TableIR.scala:843); 	at is.hail.table.Table.value$lzycompute(Table.scala:215); 	at is.hail.table.Table.value(Table.scala:213); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:218); 	at is.hail.table.Table.x$5(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110:2657,Error,Error,2657,https://hail.is,https://github.com/hail-is/hail/issues/4110,1,['Error'],['Error']
Availability,"4p.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; func(*args); File ""/tmp/d30041623ee542dca820faecd29538a9/generate_frequency_data.py"", line 155, in main; write_temp_gcs(ht, annotations_ht_path(data_type, location), args.overwrite); File ""/tmp/d30041623ee542dca820faecd29538a9/pyscripts_tyqA4p.zip/gnomad_hail/utils/generic.py"", line 36, in write_temp_gcs; t.write(temp_path, overwrite=True); File ""/tmp/d30041623ee542dca820faecd29538a9/hail-devel-cb98819b64ad.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/tmp/d30041623ee542dca820faecd29538a9/hail-devel-cb98819b64ad.zip/hail/table.py"", line 1183, in write; self._jt.write(output, overwrite, stage_locally, _codec_spec); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/d30041623ee542dca820faecd29538a9/hail-devel-cb98819b64ad.zip/hail/utils/java.py"", line 200, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: +Struct{}; expect: Struct{}. Java stack trace:; java.lang.AssertionError: assertion failed: type mismatch:; name: global; actual: +Struct{}; expect: Struct{}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:55); at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:17); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:186); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1636); at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1634); at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1147); at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1634); at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1634); at is.hail.expr.ir.MatrixMapCols.execute(Matri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4127:1196,Error,Error,1196,https://hail.is,https://github.com/hail-is/hail/issues/4127,1,['Error'],['Error']
Availability,"50294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **489/1000** <br/> **Why?** Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:9413,avail,available,9413,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['avail'],['available']
Availability,"51cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/table.py"", line 1133, in aggregate; File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-436>"", line 2, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 94, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 237, in get_refs; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 212, in _get_refs; AttributeError: 'NoneType' object has no attribute '_indices_from_ref'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [251cbf7beb5f4503ba74e4d69bd09ec3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 132, in <module>; main(args, pass_through_args); File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 113, in main; subprocess.check_output(job); File ""/anaconda3/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/anaconda3/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); ```. Note that first filtering the table, then running agg.group_by it works, if I just run agg_filter without agg.group_by (just agg.count()), it also works. For ref, this is `pops_ht` schema:; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 's': str ; 'known_pop': str ; 'known_subpop': str ; '_kgp': bool ; 'pop': str ; 'prob_afr': float64 ; 'prob_amr': float64 ; 'prob_eas': float64 ; 'prob_eur': float64 ; 'prob_sas': float64 ; -------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5296:1936,ERROR,ERROR,1936,https://hail.is,https://github.com/hail-is/hail/issues/5296,1,['ERROR'],['ERROR']
Availability,"5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:2555,avail,available,2555,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"59"">#1559</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/1449dec45b4e95293db14595ec0d11a3839bac23""><code>1449dec</code></a> Support loading of CSI from URLs/streams. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1507"">#1507</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1595"">#1595</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/22aec6782b33f8d169a5d1cf63e952126a3f09e0""><code>22aec67</code></a> Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/70e42597ee8e2db6241f7b147f1356a1f8a846bc""><code>70e4259</code></a> Remove unnecessary println in test (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1602"">#1602</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/6507249a4422d021b984e710e8f031816f6d8da2""><code>6507249</code></a> Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1607"">#1607</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/samtools/htsjdk/compare/2.24.1...3.0.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.github.samtools:htsjdk&package-manager=gradle&previous-version=2.24.1&new-version=3.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:8643,failure,failure,8643,https://hail.is,https://github.com/hail-is/hail/pull/12229,1,['failure'],['failure']
Availability,"5975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316211](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No |",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:5629,avail,available,5629,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"5975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316211](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No |",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:5621,avail,available,5621,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"5:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:4256,repair,repair,4256,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"6 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:2183,avail,available,2183,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['avail'],['available']
Availability,"6"",""to"":""6.4.12""},{""name"":""pygments"",""from"":""2.5.2"",""to"":""2.15.0""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""sphinx"",""from"":""1.8.6"",""to"":""3.3.0""},{""name"":""tornado"",""from"":""5.1.1"",""to"":""6.3.3""},{""name"":""wheel"",""from"":""0.30.0"",""to"":""0.38.0""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-IPYTHON-2348630"",""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERCORE-3063766"",""SNYK-PYTHON-MISTUNE-2940625"",""SNYK-PYTHON-NBCONVERT-2979829"",""SNYK-PYTHON-NOTEBOOK-1041707"",""SNYK-PYTHON-NOTEBOOK-2441824"",""SNYK-PYTHON-NOTEBOOK-2928995"",""SNYK-PYTHON-PYGMENTS-1086606"",""SNYK-PYTHON-PYGMENTS-1088505"",""SNYK-PYTHON-PYGMENTS-5750273"",""SNYK-PYTHON-REQUESTS-5595532"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-SPHINX-570772"",""SNYK-PYTHON-SPHINX-570773"",""SNYK-PYTHON-SPHINX-5811865"",""SNYK-PYTHON-SPHINX-5812109"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803"",""SNYK-PYTHON-TORNADO-6041512"",""SNYK-PYTHON-WHEEL-3180413""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,531,null,null,null,null,null,null,null,null,null,null,509,null,null,null,null,384,494,539,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Improper Privilege Management](https://learn.snyk.io/lesson/insecure-design/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14024:11524,avail,available,11524,https://hail.is,https://github.com/hail-is/hail/pull/14024,1,['avail'],['available']
Availability,"64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:3914,repair,repair,3914,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:19863,error,error,19863,https://hail.is,https://github.com/hail-is/hail/issues/8944,3,['error'],['error']
Availability,"7-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:1962,Heartbeat,HeartbeatReceiver,1962,https://hail.is,https://github.com/hail-is/hail/issues/4780,2,['Heartbeat'],['HeartbeatReceiver']
Availability,"7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:4880,error,error,4880,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['error'],['error']
Availability,"71, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:2574,failure,failure,2574,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['failure'],['failure']
Availability,72); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:590); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:38); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); 	at is.hail.table.Table.write(Table.scala:618); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1031); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.next(OrderedRVD.scala:1011); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RowStore.scala:1071); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.io.RichContextRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:1096); 	at is.hail.utils.richUtils.RichCon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:8030,error,error,8030,https://hail.is,https://github.com/hail-is/hail/issues/4096,1,['error'],['error']
Availability,"7366"">#7366</a>) (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7380"">#7380</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/9337fb3f2ab2b5f38d7e98a194bde6f7e3d16c40""><code>9337fb3</code></a> Fix bump llhttp to v8.1.1 (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7367"">#7367</a>) (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7377"">#7377</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/f07e9b44b5cb909054a697c8dd447b30dbf8073e""><code>f07e9b4</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7373"">#7373</a>/66e261a5 backport][3.8] Drop azure mention (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7374"">#7374</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/01d9b70e5477cd746561b52225992d8a2ebde953""><code>01d9b70</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7370"">#7370</a>/22c264ce backport][3.8] fix: Spelling error fixed (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7371"">#7371</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/3577b1e3719d4648fa973dbdec927f78f9df34dd""><code>3577b1e</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7359"">#7359</a>/7911f1e9 backport][3.8]  Set up secretless publishing to PyPI (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7360"">#7360</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/8d45f9c99511cd80140d6658bd9c11002c697f1c""><code>8d45f9c</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7333"">#7333</a>/3a54d378 backport][3.8] Fix TLS transport is <code>None</code> error (<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7357"">#7357</a>)</li>; <li><a href=""https://github.com/aio-libs/aiohttp/commit/dd8e24e77351df9c0f029be49d3c6d7862706e79""><code>dd8e24e</code></a> [PR <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7343"">#7343</a>/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13270:4632,error,error,4632,https://hail.is,https://github.com/hail-is/hail/pull/13270,5,['error'],['error']
Availability,"7</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/gradle-download-task/issues/295"">#295</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/c1e212c0fb41b3ea9185a9ea463fb1ea7142f748""><code>c1e212c</code></a> Add integration tests for Gradle 8.0 and 8.0.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/304f68e25f53633a92a4d2d6ce003a4986929503""><code>304f68e</code></a> Fix type inference issue</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.1...5.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=5.3.1&new-version=5.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:2709,down,download-task,2709,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download-task']
Availability,"7](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `2.11.3 -> 3.1.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:4012,avail,available,4012,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"7e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va))))))); ```; ### What went wrong (all error messages here, including the full java stack trace):; This wasn't optimized to a read of the metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5100:2470,error,error,2470,https://hail.is,https://github.com/hail-is/hail/issues/5100,1,['error'],['error']
Availability,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240:1779,down,downsampled,1779,https://hail.is,https://github.com/hail-is/hail/issues/8240,11,['down'],['downsampled']
Availability,"8*hl.median(hl.abs(hl.agg.collect(ht[metric])-hl.median(hl.agg.collect(ht[metric]))))); medmad = ht.aggregate(hl.struct(**medmad_dict)); print(medmad); print(hl.eval_expr(hl.json(medmad))); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 0:==================================================>(9853 + 93) / 10000]#; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fbeaec3ca22, pid=6662, tid=0x00007fbe3dd81700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-1~deb9u1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 14270 C1 is.hail.annotations.Region.storeInt(JI)V (6 bytes) @ 0x00007fbeaec3ca22 [0x00007fbeaec3c980+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/828e66d5a71741d7ab2c8d6580997da3/hs_err_pid6662.log; Compiled method (c1) 88328 14270 3 is.hail.annotations.Region::storeInt (6 bytes); total in heap [0x00007fbeaec3c810,0x00007fbeaec3cbc0] = 944; relocation [0x00007fbeaec3c938,0x00007fbeaec3c968] = 48; main code [0x00007fbeaec3c980,0x00007fbeaec3caa0] = 288; stub code [0x00007fbeaec3caa0,0x00007fbeaec3cb30] = 144; oops [0x00007fbeaec3cb30,0x00007fbeaec3cb38] = 8; metadata [0x00007fbeaec3cb38,0x00007fbeaec3cb48] = 16; scopes data [0x00007fbeaec3cb48,0x00007fbeaec3cb78] = 48; scopes pcs [0x00007fbeaec3cb78,0x00007fbeaec3cbb8] = 64; dependencies [0x00007fbeaec3cbb8,0x00007fbeaec3cbc0] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; FATAL: caught signal 6 SIGABRT; /tmp/libhail7224206977949339430.so(+0x1788c)[0x7fbdea5db88c]; /lib/x86_64-linux-gnu/libc.so.6(+0x33060)[0x7fbec2eae060]; /lib/x86_64-linux-gnu/libc.so.6(gsign",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4418:2402,error,error,2402,https://hail.is,https://github.com/hail-is/hail/issues/4418,1,['error'],['error']
Availability,"8/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:1098,failure,failure,1098,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['failure'],['failure']
Availability,"813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:7515,avail,available,7515,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:7507,avail,available,7507,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:7884,avail,available,7884,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:7876,avail,available,7876,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"869: in _run_once; event_list = self._selector.select(timeout); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <selectors.EpollSelector object at 0x7fae890f2d30>; timeout = 15.402000000000001. def select(self, timeout=None):; if timeout is None:; timeout = -1; elif timeout <= 0:; timeout = 0; else:; # epoll_wait() has a resolution of 1 millisecond, round away; # from zero to wait *at least* timeout seconds.; timeout = math.ceil(timeout * 1e3) * 1e-3; ; # epoll_wait() expects `maxevents` to be greater than zero;; # we want to make sure that `select()` can be called when no; # FD is registered.; max_ev = max(len(self._fd_to_key), 1); ; ready = []; try:; > fd_event_list = self._selector.poll(timeout, max_ev); E Failed: Timeout >360.0s. usr/lib/python3.9/selectors.py:469: Failed; ------------------------------ Captured log setup ------------------------------; 2023-09-06T21:45:24 INFO test.conftest conftest.py:14:log_before_after starting test; 2023-09-06T21:45:24 INFO hailtop.aiocloud.aioazure.credentials credentials.py:99:default_credentials using credentials file /test-gsa-key/key.json; ------------------------------ Captured log call -------------------------------; 2023-09-06T21:45:25 INFO azure.identity.aio._internal.get_token_mixin get_token_mixin.py:93:get_token ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . ------------------------------ live log teardown -------------------------------; 2023-09-06T21:51:25 INFO test.conftest conftest.py:16:log_before_after ending test. ```. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:4412,error,error,4412,https://hail.is,https://github.com/hail-is/hail/issues/13582,3,['error'],"['error', 'errors']"
Availability,"87bf0c19bL101-R101; ):; ```diff; -orjson==3.9.10; +orjson==3.9.12; ```. orjson [reduced the frequency of this segfault in 3.9.13](https://github.com/ijl/orjson/commit/58a8bd3e31aa3b5fd3d962fb5b03479fa0014ee9) by eliding some of the code that caused buffer overheads; however, [the problem persists](https://github.com/ijl/orjson/issues/452#issuecomment-1943053799). I complete fix is currently awaiting [pull request review](https://github.com/ijl/orjson/pull/457). Reports:; - https://hail.zulipchat.com/#narrow/stream/127527-team/topic/seg.20faults.20in.20tests; - https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/segfault.20in.20ci.20tests. Batches:; - https://batch.hail.is/batches/8123269/jobs/86; - https://batch.hail.is/batches/8127894/jobs/53. ### Version. 0.2.127. ### Relevant log output. ```shell; [2024-02-08 22:36:47] test/hail/matrixtable/test_file_formats.py::test_backward_compatability_ht[/io/resources/backward_compatability/1.6.0/table/6.ht/] Fatal Python error: Segmentation fault. Thread 0x00007fa51d817640 (most recent call first):; File ""/usr/lib/python3.9/selectors.py"", line 416 in select; File ""/usr/lib/python3.9/socketserver.py"", line 232 in serve_forever; File ""/usr/lib/python3.9/threading.py"", line 917 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Thread 0x00007fa5273ff640 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py"", line 58 in run; File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner; File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap. Current thread 0x00007fa52bd6b000 (most recent call first):; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py"", line 217 in _rpc; File ""/usr/local/lib/python3.9/dist-packages/hail/backend/backend.py"", line 212 in table_type; File ""/usr/local/lib/python3.9/dist-packages/hail/ir/table_ir.py"", line 438 in _co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14299:3188,error,error,3188,https://hail.is,https://github.com/hail-is/hail/issues/14299,2,"['error', 'fault']","['error', 'fault']"
Availability,"88> in repartition(self, n_partitions, shuffle); /home/hail/hail.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 484 def _typecheck(__orig_func__, *args, **kwargs):; 485 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 486 return __orig_func__(*args_, **kwargs_); 487 ; 488 return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:1889,Error,Error,1889,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['Error'],['Error']
Availability,"8928e9468335d8efab963f""><code>a9825c2</code></a> Bump py-actions/py-dependency-install from 2.1.0 to 3 (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1239"">#1239</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/7f65c4ccb0e954c17f2a3e1ecc665c62e4a1aaeb""><code>7f65c4c</code></a> Remove <strong>del</strong> from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/5062740974e493c390fb8db33982f97d6e08df2d""><code>5062740</code></a> Fix typing on blpop (etc) timeout argument (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1224"">#1224</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/dbdd0add63f986f2ed2d56c9736303d133add23c""><code>dbdd0ad</code></a> fix socket.error raises (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/2ba15fb6947fa2347d401ba436e362ad62ed38ff""><code>2ba15fb</code></a> Fix buffer is closed error when using PythonParser class (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/0aa06df10b9531f4ba734ec7567f8621c00e65e9""><code>0aa06df</code></a> Fix typing on evalsha keys_and_args argument (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1215"">#1215</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/33b2dbd0a40ac148e6a36ba2fc7ab5d438a9a71d""><code>33b2dbd</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1201"">#1201</a>)</li>; <li><a href=""https://github.com/aio-libs/aioredis-py/commit/a708bd14b1a8bec0a1f3d469bf5384",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:5001,error,error,5001,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['error'],['error']
Availability,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T03:09:04Z""; ```; PVC in question; ```; # k describe pvc batch-2554-job-4-8vvgl -n batch-pods; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Ann",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466:6590,toler,tolerationSeconds,6590,https://hail.is,https://github.com/hail-is/hail/issues/6466,1,['toler'],['tolerationSeconds']
Availability,"9 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png). directly provides 1kg.mt so the conversion step ; ```python; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True); ```; is unnecessary:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:3519,Error,Error,3519,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Error'],['Error']
Availability,"9% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=201",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:2062,error,error,2062,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['error'],['error']
Availability,"972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316211](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:5260,avail,available,5260,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316211](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:5252,avail,available,5252,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"9</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix search coming back in notebook and editor <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15443"">#15443</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix <code>jupyter labextension watch --help</code> <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15542"">#15542</a> (<a href=""https://github.com/akx""><code>@​akx</code></a>)</li>; <li>Fix <code>FormComponent</code> showing error indicators in all fields when using a <code>customValidate</code> function <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15464"">#15464</a> (<a href=""https://github.com/mmichilot""><code>@​mmichilot</code></a>)</li>; <li>Fix Shift + L not working in stdin <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15440"">#15440</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15499"">#15499</a>: Adopt ruff format <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15564"">#15564</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Pin <code>actions/labeler</code> to v4 to fix failing CI action <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15496"">#15496</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix URLs in debugger-extension <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15462"">#15462</a> (<a href=""https://github.com/fcollonval""><code>@​fcollonval</code></a>)</li>; <li>More robust galata/UI tests <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15355"">#15355</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Backport PR <a hr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:4720,Mainten,Maintenance,4720,https://hail.is,https://github.com/hail-is/hail/pull/14184,2,['Mainten'],['Maintenance']
Availability,": 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it would be nice if there was a way to bring it back up either automatically or manually. Hail version: 0.2.30-2ae07d872f43; Spark version: 2.4.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076:2142,down,down,2142,https://hail.is,https://github.com/hail-is/hail/issues/8076,2,['down'],['down']
Availability,: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.table.Table.rvd$lzycompute(Table.scala:246); at is.hail.table.Table.rvd(Table.scala:246); at is.hail.table.Table.take(Table.scala:961); at is.hail.table.Table.showString(Table.scala:1002); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); Hail version: devel-10a75bb57a6f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3744:4559,Error,Error,4559,https://hail.is,https://github.com/hail-is/hail/issues/3744,1,['Error'],['Error']
Availability,": INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:2016,failure,failure,2016,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['failure'],['failure']
Availability,"://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105916"">kubernetes/kubernetes#105916</a>, <a href=""https://github.com/kevindelgado""><code>@​kevindelgado</code></a>)</li>; <li>Promote <code>IPv6DualStack</code> feature to stable.; Controller Manager flags for the node IPAM controller have slightly changed:; <ol>; <li>When configuring a dual-stack cluster, the user must specify both <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code> to set the per-node IP mask sizes, instead of the previous <code>--node-cidr-mask-size</code> flag.</li>; <li>The <code>--node-cidr-mask-size</code> flag is mutually exclusive with <code>--node-cidr-mask-size-ipv4</code> and <code>--node-cidr-mask-size-ipv6</code>.</li>; <li>Single-stack clusters do not need to change, but may choose to use the more specific flags. Users can use either the older <code>--node-cidr-mask-size</code> flag or one of the newer <code>--node-cidr-mask-size-ipv4</code> or <code>--node-cidr-mask-size-ipv6</code> flags to configure the per-node IP mask size, provided that the flag's IP family matches the cluster's IP family (--cluster-cidr). (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104691"">kubernetes/kubernetes#104691</a>, <a href=""https://github.com/khenidak""><code>@​khenidak</code></a>)</li>; </ol>; </li>; <li>Remove <code>NodeLease</code> feature gate that was graduated and locked to stable in 1.17 release. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105222"">kubernetes/kubernetes#105222</a>, <a href=""https://github.com/cyclinder""><code>@​cyclinder</code></a>)</li>; <li>Removed deprecated <code>--seccomp-profile-root</code>/<code>seccompProfileRoot</code> config. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/103941"">kubernetes/kubernetes#103941</a>, <a href=""https://github.com/saschagrunert""><code>@​saschagrunert</code></a>)</li>; <li>Since golang 1.17 both net.ParseIP and ne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:11095,mask,mask-size,11095,https://hail.is,https://github.com/hail-is/hail/pull/11957,4,['mask'],"['mask', 'mask-size', 'mask-size-']"
Availability,"://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add integration tests for Gradle 6.9.3 and 7.6</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a998a544908a8b39f713f4526f717fcb328c06eb""><code>a998a54</code></a> Upgrade Gradle to 7.6</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/5.3.0...5.3.1"">compare view</a></li>; </u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:2177,down,download-task,2177,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739d0400c333a72b32f5d514""><code>0fdebf3</code></a> Improve verify example</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/02b8e1a79d9e00acd61f9ac42e5555619fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/717877121299cea8f216d3a595eaa56731a6acd3""><code>7178771</code></a> Support changing a target file's relative path in an eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e5af1bd7f9daa8a9222aee0dd1b703727cb5e94e""><code>e5af1bd</code></a> Bump version number to 5.3.0-SNAPSHOT</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:4304,down,download-task,4304,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,":0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 	... 12 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 		at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpStorageRpc.java:1029) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.ResumableMedia.lambda$startUploadForBlobInfo$0(ResumableMedia.java:40) ~[gs:__hail-query-ger",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:17783,error,errors,17783,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['error'],['errors']
Availability,:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12849,Error,ErrorHandling,12849,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Error'],['ErrorHandling']
Availability,"::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:255:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float32<4>’ with ‘private’ member ‘simdpp::arch_avx2::float32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float32x4.h:32:7: note: ‘class simdpp::arch_avx2::float32<4>’ declared here; class float32<4, void> : public any_float32<4, float32<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:3800,error,error,3800,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]’; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::float64<2>’ with ‘private’ member ‘simdpp::arch_avx2::float64<2>::d_’ from an array of ‘const class simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:32:7: note: ‘class simdpp::arch_avx2::float64<2>’ declared here; class float64<2, void> : public any_float64<2, float64<2,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:2073,error,error,2073,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,":; Name: hail; Version: 0.2.93; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages; Requires: dill, bokeh, scipy, azure-storage-blob, janus, parsimonious, botocore, google-cloud-storage, tabulate, Jinja2, python-json-logger, plotly, avro, azure-identity, PyJWT, orjson, tqdm, aiohttp-session, google-auth, nest-asyncio, uvloop, humanize, hurry.filesize, decorator, requests, Deprecated, aiohttp, asyncinit, numpy, pyspark, sortedcontainers, boto3, pandas. -----------------------------------------------------------------------------. Importing hail via the IPython console in Spyder causes the following error:. Python 3.8.12 (default, Oct 12 2021, 06:23:56) ; IPython 8.2.0 -- An enhanced Interactive Python. In [1]: `import hail`. > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py"", line 36, in run; > task = asyncio.ensure_future(main); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/asyncio/tasks.py"", line 684, in ensure_future; > raise TypeError('An asyncio.Future, a coroutine or an awaitable is '; > TypeError: An asyncio.Future, a coroutine or an awaitable is required; > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py"", line 36, in run; > task = asyncio.ensure_future(mai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758:1251,ERROR,ERROR,1251,https://hail.is,https://github.com/hail-is/hail/issues/11758,1,['ERROR'],['ERROR']
Availability,":; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc87544; Error summary: AssertionError: assertion failed. We also check the key of input file:; ; >>> vcfVds.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5657:2008,Error,Error,2008,https://hail.is,https://github.com/hail-is/hail/issues/5657,1,['Error'],['Error']
Availability,":</p>; <ul>; <li>Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li>Allow <code>download</code> and <code>verify</code> extensions to be created on demand in custom tasks, so these tasks can be made compatible with Gradle's configuration cache (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:1282,down,download-task,1282,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,":[\""A\"",\""<NON_REF>\""]},\""includeStart\"":true,\""includeEnd\"":true},{\""start\"":{\""locus\"":{\""contig\"":\""chr20\"",\""position\"":17994753},\""alleles\"":[\""A\"",\""<NON_REF>\""]},\""end\"":{\""locus\"":{\""cont...""] ; !s20 = ToStream(%29) [False]; !s21 = StreamMap(!s20) { (%elt10) =>; SelectFields(%elt10) [; (partitionCounts distinctlyKeyed firstKey; lastKey)]; }; !37 = ToArray(!s21); !38 = WriteMetadata(!37) [""{\""name\"":\""TableSpecWriter\"",\""path\"":\""/tmp/foo.ht\"",\""typ\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],data:Array[Struct{}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{new_globals:Array[Struct{}]}\""},\""rowRelPath\"":\""rows\"",\""globalRelPath\"":\""globals\"",\""refRelPath\"":\""references\"",\""log\"":true}""]; !39 = Begin(!34, !36, !38); WriteMetadata(!39) [""{\""name\"":\""RelationalWriter\"",\""path\"":\""/tmp/foo.ht\"",\""overwrite\"":true,\""maybeRefs\"":{\""references\"":[\""GRCh38\""]}}""]. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:17); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:15644,Error,ErrorHandling,15644,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,":arch_avx2::uint64<4>; T = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:114:36: required from ‘simdpp::arch_avx2::uint64<4>& simdpp::arch_avx2::uint64<4>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int64<4>]’; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:107:73: required from ‘simdpp::arch_avx2::uint64<4>::uint64(const simdpp::arch_avx2::int64<4, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:79:49: required from ‘simdpp::arch_avx2::int64<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int64<N, E>&, const simdpp::arch_avx2::int64<N, E2>&) [with unsigned int N = 4; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int64.h:70:26: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint64<4>’ with ‘private’ member ‘simdpp::arch_avx2::uint64<4>::d_’ from an array of ‘const class simdpp::arch_avx2::int64<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: ‘class simdpp::arch_avx2::uint64<4>’ declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<8>; T = simdpp::arch_avx2::uint64<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:134003,error,error,134003,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,":offset 23933331019603: error while parsing line; chrY	113	.	GG	G,*,AG,CG	596	PASS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7264,Error,ErrorHandling,7264,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Error'],['ErrorHandling']
Availability,":run(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<16>; T = simdpp::arch_avx2::uint32<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:115:37: required from ‘simdpp::arch_avx2::uint16<16>& simdpp::arch_avx2::uint16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::uint16<16> >, simdpp::arch_avx2::uint32<8, simdpp::arch_avx2::uint32<8> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/unzip_lo.h:107:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint16<16>’ with ‘private’ member ‘simdpp::arch_avx2::uint16<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:92:7: note: ‘class simdpp::arch_avx2::uint16<16>’ declared here; class uint16<16, void> : public any_int16<16, uint16<16,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:34569,error,error,34569,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,":uint16<32>]’; libsimdpp-2.0-rc2/simdpp/types/int16.h:50:35: required from ‘simdpp::arch_avx2::int16<N>& simdpp::arch_avx2::int16<N>::operator=(const simdpp::arch_avx2::any_vec<(N * 2), V>&) [with V = simdpp::arch_avx2::uint16<32>; unsigned int N = 32]’; libsimdpp-2.0-rc2/simdpp/types/int16.h:43:73: required from ‘simdpp::arch_avx2::int16<N>::int16(const simdpp::arch_avx2::uint16<N, E>&) [with E = void; unsigned int N = 32]’; libsimdpp-2.0-rc2/simdpp/core/combine.h:66:69: required from ‘simdpp::arch_avx2::int16<(N * 2)> simdpp::arch_avx2::combine(const simdpp::arch_avx2::int16<N, E>&, const simdpp::arch_avx2::int16<N, E2>&) [with unsigned int N = 16; E1 = void; E2 = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int16.h:125:26: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int16<32>’ with ‘private’ member ‘simdpp::arch_avx2::int16<32>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<32>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:36,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16.h:31:7: note: ‘class simdpp::arch_avx2::int16<32>’ declared here; class int16<N, void> : public any_int16<N, int16<N,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint16<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:138495,error,error,138495,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,; 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2$adapted(SparkBackend.scala:612); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:347); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1(SparkBackend.scala:612); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1$adapted(SparkBackend.scala:611); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.pyAddLiftover(SparkBackend.scala:611); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.126-ee77707f4fab; Error summary: HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:5648,Error,Error,5648,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['Error'],['Error']
Availability,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:14543,Error,Error,14543,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['Error'],['Error']
Availability,"; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; jupyter 1.0.0 requires qtconsole, which is not installed.; curlylint 0.13.1 requires pathspec, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; astroid 2.15.8 requires lazy-object-proxy, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:1357,avail,available,1357,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"; 1258 ; 1259 for temp_arg in temp_args:. /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 14 import pyspark; 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:; 18 s = e.java_exception.toString(). /fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o1.pyPersistTable; --------------- end of error -------------------. The above is for running the GWAS tutorial. While running my own GWAS, I have encountered a similar error when running GWAS with the line :. gwas = hl.agg.linreg(hl.float(mt.phenos.height),; [1.0, mt.hapcounts0.x, mt.anc1dos.x, mt.anc2dos.x]). The error message is as follows:. -------------- start of error ------------------. ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1115, in start; self.socket.connect((self.address, self.port)); ConnectionRefusedError: [Errno 111] Connection refused; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38197); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 977, in _get_connection; connection = self.deque.pop(); IndexError: pop from an empty deque",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:8038,error,error,8038,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['error'],['error']
Availability,"; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 199 'Hail version: %s\n'; --> 200 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 201 except pyspark.sql.utils.CapturedException as e:; 202 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Cannot find row in Map(). Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (titv; (ApplyBinaryPrimOp FloatingPointDivide; (GetField n_ti; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)); (GetField n_tv; (Ref Struct{rank_id:String,snv:Boolean,bi_allelic:Boolean,singleton:Boolean,bin:Int32,min_score:Float64,max_score:Float64,n_ti:Int64,n_tv:Int64,model:String} row)))); (min_score; (GetField `0`; (In Struct{`0`:Float64,`1`:Float64} 0))); (max_score; (GetField `1`; (In Struct{`0`:Float64,`1`:Float64} 0)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:11); 	at is.hail.expr.ir.Emit$.emit(Emit.scala:42); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:28); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:51); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:74); 	at is.hail.expr.ir.TableKeyByAndAggregate.execute(TableIR.scala:843); 	at is.hail.table.Table.value$lzycompute(Table.scala:215); 	at is.hail.table.Table.value(Table.scala:213); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:218); 	at is.hail.table.Table.x$5(Table.scala:218); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:218); 	at is.hail.table.Table.rvd(Table.scala:218); 	at is.hail.table.Table.take(Table.scala:649); 	at is.hail.table.Table.showString(Table.scala:685); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110:3268,Error,ErrorHandling,3268,https://hail.is,https://github.com/hail-is/hail/issues/4110,1,['Error'],['ErrorHandling']
Availability,"; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/blob/master/CHANGES.rst"">async-timeout's changelog</a>.</em></p>; <blockquote>; <h1>4.0.2 (2021-12-20)</h1>; <h2>Misc</h2>; <ul>; <li><code>[#259](https://github.com/aio-libs/async-timeout/issues/259) &lt;https://github.com/aio-libs/async-timeout/issues/259&gt;</code><em>, <code>[#274](https://github.com/aio-libs/async-timeout/issues/274) &lt;https://github.com/aio-libs/async-timeout/issues/274&gt;</code></em></li>; </ul>; <h1>4.0.1 (2121-11-10)</h1>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h1>4.0.0 (2021-11-01)</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Dropped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:2959,avail,available,2959,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['avail'],['available']
Availability,"; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:3396,down,download-task,3396,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability,"; <ipython-input-17-671d2e9c22c8> in <module>(); 1 #ht = hl.import_table('gs://gnomad/annotations/hail-0.2/ht/genomes/score_rankings/gnomad.sites.RF.newStats24.txt.bgz', types={'chrom': hl.tstr}, impute=True, min_partitions=100).cache(); ----> 2 ht.export('gs://gnomad-tmp/genomes_rf.txt.bgz', parallel=True). /home/hail/hail.zip/hail/table.py in export(self, output, types_file, header, parallel); 994 """"""; 995 ; --> 996 self._jt.export(output, types_file, header, Env.hail().utils.ExportType.getExportType(parallel)); 997 ; 998 def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 186 import pyspark; 187 try:; --> 188 return f(*args, **kwargs); 189 except py4j.protocol.Py4JJavaError as e:; 190 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 321 raise Py4JError(; 322 ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; --> 323 format(target_id, ""."", name, value)); 324 else:; 325 raise Py4JError(. Py4JError: An error occurred while calling z:is.hail.utils.ExportType.getExportType. Trace:; py4j.Py4JException: Method getExportType([class java.lang.Boolean]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); 	at py4j.Gateway.invoke(Gateway.java:274); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033:2016,error,error,2016,https://hail.is,https://github.com/hail-is/hail/issues/4033,2,['error'],['error']
Availability,"; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/15cf7eecfbc17d2466143828b9b69494c6cb6f2b""><code>15cf7ee</code></a> Bump up version number to 5.3.1-SNAPSHOT</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/e3c65ffcb49b9c5a33fde5f31fb63043dbf21134""><code>e3c65ff</code></a> Allow extensions to be created from tasks</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/34e2dd41477f18b1ae3d6d5a71dca5449d6cd1e0""><code>34e2dd4</code></a> Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b3fa29f9ffb4d4544e13ef84601e371fb2778ddf""><code>b3fa29f</code></a> Revert &quot;Update Apache HttpClient to 5.2.1&quot;</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/01f05e046be0dca18f506723c79e88f208336e71""><code>01f05e0</code></a> Add",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:1820,down,download-task,1820,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download-task']
Availability,"; <li>winbuild: Refactor dependency versions into constants <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7843"">#7843</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Build macOS arm64 wheels natively <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7852"">#7852</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed typo <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7855"">#7855</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Open 16-bit grayscale PNGs as I;16 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7849"">#7849</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Handle truncated chunks at the end of PNG images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7709"">#7709</a> [<a href=""https://github.com/lajiyuan""><code>@​lajiyuan</code></a>]</li>; <li>Match mask size to pasted image size in GifImagePlugin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7779"">#7779</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Changed SupportsGetMesh protocol to be public <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7841"">#7841</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Release GIL while calling <code>WebPAnimDecoderGetNext</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7782"">#7782</a> [<a href=""https://github.com/evanmiller""><code>@​evanmiller</code></a>]</li>; <li>Fixed reading FLI/FLC images with a prefix chunk <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7804"">#7804</a> [<a href=""https://github.com/twolife""><code>@​twolife</code></a>]</li>; <li>Updated package name for Tidelift <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7810"">#7810</a> [<a href=""https://github.com/radarhere""><code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:8626,mask,mask,8626,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['mask'],['mask']
Availability,"; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted --",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:2076,down,download,2076,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download']
Availability,"; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maint",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:1357,down,download,1357,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download']
Availability,"; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:1718,Error,ErrorHandling,1718,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['Error'],['ErrorHandling']
Availability,"; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/nbconvert/exporters/__init__.py"", line 3, in <module>; May 16 14:17:07 mw116-m python[8309]: from .html import HTMLExporter; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/nbconvert/exporters/html.py"", line 12, in <module>; May 16 14:17:07 mw116-m python[8309]: from jinja2 import contextfilter; May 16 14:17:07 mw116-m python[8309]: ImportError: cannot import name 'contextfilter' from 'jinja2' (/opt/conda/miniconda3/lib/python3.10/site-packages/jinja2/__init__.py); May 16 14:17:07 mw116-m python[8309]: [D 14:17:07.045 NotebookApp] Using contents: services/contents; May 16 14:17:07 mw116-m python[8309]: [D 14:17:07.045 NotebookApp] Using contents: services/contents; May 16 14:17:07 mw116-m python[8309]: [E 14:17:07.046 NotebookApp] {; May 16 14:17:07 mw116-m python[8309]: ""Host"": ""localhost:8123"",; May 16 14:17:07 mw116-m python[8309]: ""Connection"": ""keep-alive"",; May 16 14:17:07 mw116-m python[8309]: ""Sec-Ch-Ua"": ""\""Google Chrome\"";v=\""113\"", \""Chromium\"";v=\""113\"", \""Not-A.Brand\"";v=\""24\"""",; May 16 14:17:07 mw116-m python[8309]: ""Sec-Ch-Ua-Mobile"": ""?0"",; May 16 14:17:07 mw116-m python[8309]: ""Sec-Ch-Ua-Platform"": ""\""macOS\"""",; May 16 14:17:07 mw116-m python[8309]: ""Upgrade-Insecure-Requests"": ""1"",; May 16 14:17:07 mw116-m python[8309]: ""User-Agent"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"",; May 16 14:17:07 mw116-m python[8309]: ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"",; May 16 14:17:07 mw116-m python[8309]: ""Sec-Fetch-Site"": ""same-origin"",; May 16 14:17:07 mw116-m python[8309]: ""Sec-Fetch-Mode"": ""navigate"",; May 16 14:17:07 mw116-m python[8309]: ""Sec-Fetch-User"": ""?1"",; May 16 14:17:07 mw116-m python[8309]: ""Sec-Fetch-Dest"": ""document"",; M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13059:4456,alive,alive,4456,https://hail.is,https://github.com/hail-is/hail/issues/13059,1,['alive'],['alive']
Availability,"; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:3196,Error,Error,3196,https://hail.is,https://github.com/hail-is/hail/issues/4609,2,"['Error', 'error']","['Error', 'error']"
Availability,"; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not havi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5746:2409,ERROR,ERROR,2409,https://hail.is,https://github.com/hail-is/hail/pull/5746,1,['ERROR'],['ERROR']
Availability,"; at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum alloreamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_15657888296Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:16207,failure,failure,16207,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['failure'],['failure']
Availability,"; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9-8dfb-da8aa20807b0.png"">. * The event loop call stack: https://www.youtube.com/watch?v=8aGhZQkoFbQ. At a high level, a function that defines a callback will return immediately. The callback is pushed on to the event-loop stack, and on each tick, is checked to determine whether it has returned or not. Blocking operations within the callbacks will block the event loop. This is how CPU viruses, like blockchain manage to slow down web pages that are hijacked to include some mining script: hashing something 30 million times, takes a long time, and JS cannot do anything besides waiting for those operations to finish in a synchronous fashion. Luckily, asynchronous functions are the norm in the JS ecosystem, such that both in the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed from kernel threads that Node maintains in the background, effectively making these operations non-blocking (until the thread pool is exhausted). Browsers and NodeJS use different event loops:. NodeJS: libuv event loop; * Node maintains a hidden worker thread pool (kernel threads) through which it issues sys calls, to avoid blocking the event loop. Web: depends on the underlying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/Jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:3814,down,down,3814,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['down'],['down']
Availability,"</a> Webgl problem in stream app with multiple glyphs</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/6669"">#6669</a> [component: bokehjs] BoxAnnotation does not appear to handle formal NumberSpec</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8168"">#8168</a> [component: bokehjs] Strange behavior with BoxSelectTool when click+dragging on toolbar</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8332"">#8332</a> [component: bokehjs] Autohide toolbar quirks</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8346"">#8346</a> [component: bokehjs] update datasource cause error with webgl backend</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8469"">#8469</a> Modifying a child element in a tab causes the whole tab to rerender</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8531"">#8531</a> [component: bokehjs] Save tool in gridplot initiates multiple downloads</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8684"">#8684</a> Allow at least partial alignment of fixed sized frames</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9113"">#9113</a> [component: bokehjs] Empty group widgets don't size properly once populated</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9133"">#9133</a> [BUG] Tabs ignore explicitly set dimensions</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9208"">#9208</a> [component: bokehjs] [BUG] sizing_mode='stretch_width' makes plot too wide if scrollbar is showing</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9320"">#9320</a> [BUG] Bokeh rendering performance</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9448"">#9448</a> [component: bokehjs] [BUG] Google Fonts not loading on Glyph on standalone HTML until interacting wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:2641,down,downloads,2641,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['down'],['downloads']
Availability,"</b></summary>. ```; aiodocker 0.21.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;) <br/>[SNYK-PYTHON-AIOHTTP-6209406](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209406) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **718/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-6209407](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209407) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlMzQ0ZjYzNy00MjQwLTQxNmEtYjE2Yi1kODhmYjc2YTUwZm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14226:1441,avail,available,1441,https://hail.is,https://github.com/hail-is/hail/pull/14226,1,['avail'],['available']
Availability,"</details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/plotly/plotly.py/blob/master/CHANGELOG.md"">plotly's changelog</a>.</em></p>; <blockquote>; <h2>[5.10.0] - 2022-08-11</h2>; <h3>Updated</h3>; <ul>; <li>Updated Plotly.js to from version 2.12.1 to version 2.14.0. See the <a href=""https://github.com/plotly/plotly.js/blob/master/CHANGELOG.md#2140----2022-08-10"">plotly.js CHANGELOG</a> for more information. Notable changes include:; <ul>; <li>Add support for <code>sankey</code> links with arrows</li>; <li>Add <code>selections</code>, <code>newselection</code> and <code>activeselection</code> layout attributes to have persistent and editable selections over cartesian subplots</li>; <li>Add <code>unselected.line.color</code> and <code>unselected.line.opacity</code> options to <code>parcoords</code> trace</li>; <li>Display Plotly's new logo in the modebar</li>; </ul>; </li>; </ul>; <h2>[5.9.0] - 2022-06-23</h2>; <h3>Added</h3>; <ul>; <li><code>pattern_shape</code> options now available in <code>px.timeline()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3774"">#3774</a></li>; <li><code>facet_*</code> and <code>category_orders</code> now available in <code>px.pie()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3775"">#3775</a></li>; </ul>; <h3>Performance</h3>; <ul>; <li><code>px</code> methods no longer call <code>groupby</code> on the input dataframe when the result would be a single group, and no longer groups by a lambda, for significant speedups <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3765"">#3765</a> with thanks to <a href=""https://github.com/jvdd""><code>@​jvdd</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Allow non-string extras in <code>flaglist</code> attributes, to support upcoming changes to <code>ax.automargin</code> in plotly.js <a href=""https://github-redirect.dependabot.com/plotly/plotly.js/pull/6193"">plotly.js#6193</a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12113:2051,avail,available,2051,https://hail.is,https://github.com/hail-is/hail/pull/12113,1,['avail'],['available']
Availability,"</em></p>; <blockquote>; <h2>7.4.6</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.5...3394591f161be4a19f9e61c66ba510d7e29afd59"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Reconcile connection information <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/879"">#879</a> (<a href=""https://github.com/kevin-bates""><code>@​kevin-bates</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-11-10&amp;to=2022-11-15&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ameeseeksmachine+updated%3A2022-11-10..2022-11-15&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; <!-- raw HTML omitted -->; <h2>7.4.5</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.4...d27c8a497c6cbb1a232fbbe75cb1fd0f53faa9b0"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[7.x] Handle Jupyter Core Warning <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/875"">#875</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up 7.x workflows <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/865"">#865</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-10-25&amp;to=2022-11-10&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-10-25..2022-11-10&amp;type=Issues""><code>@​blink1073</code></a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/f71daff259071f307",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12467:3304,Mainten,Maintenance,3304,https://hail.is,https://github.com/hail-is/hail/pull/12467,1,['Mainten'],['Maintenance']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2002"">#2002</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/9f44fe66d0ff82f18a13a38cae6abf3f72183a94""><code>9f44fe6</code></a> Bump to version 8.4.3</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/c9e3b114b98bb0e340555311c82e2d9f32c880b6""><code>c9e3b11</code></a> [DOCS] Add 8.4.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/1998"">#1998",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:4259,Down,Downloads,4259,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> Update index.adoc (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2000"">#2000</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2002"">#2002</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/9f44fe66d0ff82f18a13a38cae6abf3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3933,Down,Downloads,3933,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/a7873eb46985d849286ab89fa0a51f8b5374e02e""><code>a7873eb</code></a> U",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3607,Down,Downloads,3607,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.0.html</a></p>; <h2>Elasticsearch Hadoop 8.1.3</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:3281,Down,Downloads,3281,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.1.html</a></p>; <h2>Elasticsearch Hadoop 8.2.0</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2955,Down,Downloads,2955,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.2.html</a></p>; <h2>Elasticsearch Hadoop 8.2.1</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2629,Down,Downloads,2629,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.2/eshadoop-8.2.3.html</a></p>; <h2>Elasticsearch Hadoop 8.2.2</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:2303,Down,Downloads,2303,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.0.html</a></p>; <h2>Elasticsearch Hadoop 8.2.3</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1977,Down,Downloads,1977,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.1.html</a></p>; <h2>Elasticsearch Hadoop 8.3.0</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1651,Down,Downloads,1651,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.2.html</a></p>; <h2>Elasticsearch Hadoop 8.3.1</h2>; <p>Downloads: <a href=""https://elastic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:1325,Down,Downloads,1325,https://hail.is,https://github.com/hail-is/hail/pull/12319,2,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2050"">#2050</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/79d592abdce1cd90845d153afab8b66069e2a172""><code>79d592a</code></a> [DOCS] Add 8.5.2 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2042"">#2042</a>) (<a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1651,Down,Downloads,1651,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16881edb96180d48""><code>e665cd9</code></a> Update Spark to 3.2.3 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2056"">#2056</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2057"">#2057</a>)</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/07380b0e17c7d908d50d59fc69ac2953adfa5a0d""><code>07380b0</code></a> Use DRA repository for build-tools dependencies</li>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/77bce30bfefb39c39bd34a6f147b17f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1977,Down,Downloads,1977,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/da4f3c3f209aea47d69c4faf90029a6933bd3a60""><code>da4f3c3</code></a> [DOCS] Add 8.5.3 release notes (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2045"">#2045</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:1325,Down,Downloads,1325,https://hail.is,https://github.com/hail-is/hail/pull/12601,1,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/elastic/elasticsearch-hadoop/commit/e665cd918aa8d0ba7806b92e16881edb96180d48""><code>e665cd9</code></a> Update Spark to 3.2.3 (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch-hadoop/issues/2056"">#2056</a>) (<a href=""https://github-redirect.dependabot.com/elastic/elasticsearch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1651,Down,Downloads,1651,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['Down'],['Downloads']
Availability,"</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:1325,Down,Downloads,1325,https://hail.is,https://github.com/hail-is/hail/pull/12623,1,['Down'],['Downloads']
Availability,"</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; sphinx 5.3.0 has requirement docutils<0.20,>=0.14, but you have docutils 0.20.1.; sphinx-rtd-theme 1.3.0 has requirement docutils<0.19, but you have docutils 0.20.1.; notebook 6.5.6 has requirement pyzmq<25,>=17, but you have pyzmq 25.1.2.; aiohttp-devtools 1.1 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `7.34.0 -> 8.10.0` <br> | No | Proof of Concept ; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:1238,avail,available,1238,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"</li>; <li>1996_, [BSD]: add support for MidnightBSD. (patch by Saeed Rasooli)</li>; <li>1999_, [Linux]: <code>disk_partitions()</code>_: convert <code>/dev/root</code> device (an alias; used on some Linux distros) to real root device path.</li>; <li>2005_: <code>PSUTIL_DEBUG</code> mode now prints file name and line number of the debug; messages coming from C extension modules.</li>; <li>2042_: rewrite HISTORY.rst to use hyperlinks pointing to psutil API doc.</li>; </ul>; <p><strong>Bug fixes</strong></p>; <ul>; <li>1456_, [macOS], <strong>[critical]</strong>: <code>cpu_freq()</code>_ <code>min</code> and <code>max</code> are set to; 0 if can't be determined (instead of crashing).</li>; <li>1512_, [macOS]: sometimes <code>Process.connections()</code>_ will crash with; <code>EOPNOTSUPP</code> for one connection; this is now ignored.</li>; <li>1598_, [Windows]: <code>disk_partitions()</code>_ only returns mountpoints on drives; where it first finds one.</li>; <li>1874_, [SunOS]: swap output error due to incorrect range.</li>; <li>1892_, [macOS]: <code>cpu_freq()</code>_ broken on Apple M1.</li>; <li>1901_, [macOS]: different functions, especially <code>Process.open_files()</code>_ and; <code>Process.connections()</code><em>, could randomly raise <code>AccessDenied</code></em> because the; internal buffer of <code>proc_pidinfo(PROC_PIDLISTFDS)</code> syscall was not big enough.; We now dynamically increase the buffer size until it's big enough instead of; giving up and raising <code>AccessDenied</code>_, which was a fallback to avoid crashing.</li>; <li>1904_, [Windows]: <code>OpenProcess</code> fails with <code>ERROR_SUCCESS</code> due to; <code>GetLastError()</code> called after <code>sprintf()</code>. (patch by alxchk)</li>; <li>1913_, [Linux]: <code>wait_procs()</code>_ should catch <code>subprocess.TimeoutExpired</code>; exception.</li>; <li>1919_, [Linux]: <code>sensors_battery()</code>_ can raise <code>TypeError</code> on PureOS.</li>; <li>1921_, [Windows]: <co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11459:1870,error,error,1870,https://hail.is,https://github.com/hail-is/hail/pull/11459,1,['error'],['error']
Availability,"</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/ca4cb2d6a4b95a6925de85a47b323d2235032c74""><code>ca4cb2d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/804"">#804</a> from blink1073/fix-docs-build</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/2c545599e1da419c096abffcd81f922fb709e239""><code>2c54559</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/803"">#803</a> from ccordoba12/fix-threaded-client</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/30ce7539778e2a25ff5e6eba4ccb6c08b8a0fe20""><code>30ce753</code></a> fix sphinx 5.0 support</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/a2e90574645052320de861bb84ba1752e25ef2dd""><code>a2e9057</code></a> ignore type error</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/3c6fc38e8dda754aba4a1217733eb1a0146b4c57""><code>3c6fc38</code></a> Run qtconsole test suite as a another downstream project</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/dcb45960b337fb089e04b0c3dde880e8f0f10ae5""><code>dcb4596</code></a> Revert changes related to _handle_recv in ThreadedZMQSocketChannel</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/01bfdd18c2eb8ea34cbb9915cb2bc7d9806f81a4""><code>01bfdd1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/799"">#799</a> from jupyter/pre-commit-ci-update-config</li>; <li>Additional commits viewable in <a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...v7.3.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyter-client&package-manager=pip&previous-version=7.3.1&new-version=7.3.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:9188,down,downstream,9188,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['down'],['downstream']
Availability,"</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/09d1eca91afbf21ace3672be24c68d9028ee1e33""><code>09d1eca</code></a> Document runAsync method</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/800e3df1647c5ce65bffdd25c3240dfa5244e6c5""><code>800e3df</code></a> Add runAsync method to download extension</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/80f04c6a46fe7df053ac55bcfc6f90ff74c4b873""><code>80f04c6</code></a> Bump up version number to 5.1.3</li>; <li>Additional commits viewable in <a href=""https://github.com/michel-kraemer/gradle-download-task/compare/3.2.0...5.2.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=de.undercouch.download&package-manager=gradle&previous-version=3.2.0&new-version=5.2.1)](https://docs.github.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:4109,down,download-task,4109,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability,"</li>; <li>Added support for blob names containing invalid XML characters.; Previously \uFFFE and \uFFFF would fail if present in blob name.</li>; <li>Added support for listing system containers with get_blob_containers().</li>; <li>Added support for <code>find_blobs_by_tags()</code> on a container.</li>; <li>Added support for <code>Find (f)</code> container SAS permission.</li>; </ul>; <h3>Bugs Fixed</h3>; <ul>; <li>Added all missing Service SAS permissions.</li>; <li>Fixed a bug that prevented <code>upload_blob()</code> from working with an OS pipe; reader stream on Linux. (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23131"">#23131</a>)</li>; </ul>; <h2>azure-storage-blob_12.10.0b4</h2>; <h2>12.10.0b4 (2022-02-24)</h2>; <h3>Features Added</h3>; <ul>; <li>Updated clients to support both SAS and OAuth together.</li>; <li>Updated OAuth implementation to use the AAD scope returned in a Bearer challenge.</li>; </ul>; <h3>Bugs Fixed</h3>; <ul>; <li>Addressed a few <code>mypy</code> typing hint errors.</li>; </ul>; <h2>azure-storage-blob_12.10.0b3</h2>; <h2>12.10.0b3 (2022-02-08)</h2>; <p>This version and all future versions will require Python 3.6+. Python 2.7 is no longer supported.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/4c24f4f2d09d3e00668cb273f5257dd2b19c115a""><code>4c24f4f</code></a> [Storage][Hotfix] Fix <code>BlobSasPermission</code> default value for Tag (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23651"">#23651</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/699acfe143cc0ca570de2d040c8ffcf7cb2a3c55""><code>699acfe</code></a> [Storage] Fix <code>detination_lease</code> type hint (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23417"">#23417</a>)</li>; <li><a href=""https://gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11703:2742,error,errors,2742,https://hail.is,https://github.com/hail-is/hail/pull/11703,1,['error'],['errors']
Availability,"</li>; <li>Added support for blob names containing invalid XML characters.; Previously \uFFFE and \uFFFF would fail if present in blob name.</li>; <li>Added support for listing system containers with get_blob_containers().</li>; <li>Added support for <code>find_blobs_by_tags()</code> on a container.</li>; <li>Added support for <code>Find (f)</code> container SAS permission.</li>; </ul>; <h3>Bugs Fixed</h3>; <ul>; <li>Added all missing Service SAS permissions.</li>; <li>Fixed a bug that prevented <code>upload_blob()</code> from working with an OS pipe; reader stream on Linux. (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23131"">#23131</a>)</li>; </ul>; <h2>azure-storage-blob_12.10.0b4</h2>; <h2>12.10.0b4 (2022-02-24)</h2>; <h3>Features Added</h3>; <ul>; <li>Updated clients to support both SAS and OAuth together.</li>; <li>Updated OAuth implementation to use the AAD scope returned in a Bearer challenge.</li>; </ul>; <h3>Bugs Fixed</h3>; <ul>; <li>Addressed a few <code>mypy</code> typing hint errors.</li>; </ul>; <h2>azure-storage-blob_12.10.0b3</h2>; <h2>12.10.0b3 (2022-02-08)</h2>; <p>This version and all future versions will require Python 3.6+. Python 2.7 is no longer supported.</p>; <h3>Features Added</h3>; <ul>; <li>Added support for service version 2021-04-10.</li>; <li>Added support for <code>find_blobs_by_tags()</code> on a container.</li>; <li>Added support for <code>Find (f)</code> container SAS permission.</li>; </ul>; <h3>Bugs Fixed</h3>; <ul>; <li>Update <code>azure-core</code> dependency to avoid inconsistent dependencies from being installed.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/699acfe143cc0ca570de2d040c8ffcf7cb2a3c55""><code>699acfe</code></a> [Storage] Fix <code>detination_lease</code> type hint (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23417"">#23417</a>)</li>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11610:1947,error,errors,1947,https://hail.is,https://github.com/hail-is/hail/pull/11610,1,['error'],['errors']
Availability,"</p>; <ul>; <li>Binary wheels provided on PyPi for <code>aarch64</code> Linux systems and macOS; native silicon where supported by Python when using <code>pypa/cibuildwheel</code>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/f2f1a680113d500f525de78da91ae19235efef16""><code>f2f1a68</code></a> Merge branch 'release/1.14.1'</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/97b72d49a8cda771c6006571486530ca84f3a834""><code>97b72d4</code></a> Update version of cibuildwheel for recent Python versions.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/337072730beddd653f19c8b1a1157ecbb9d62790""><code>3370727</code></a> Only test Python 3.10 on aarch64 linux due to unreliability of GitHub runners...</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/982ddecf52013ce9bbdf8b48b76ae054844ba31b""><code>982ddec</code></a> Python 3.6 no longer available on aarch64 linux for testing.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/240fea86df0357f3642db040f912031e4ecdfcb1""><code>240fea8</code></a> Update copyright notice year.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/9668bbd7c7314d81b7cf8ce4293d04212ae1edee""><code>9668bbd</code></a> Update version in preparation for 1.14.1 release.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/c86a4d37fa61494957153f76b1d6bbdacfd83205""><code>c86a4d3</code></a> Add classifier for Python 3.11.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/07239ac21a68ced86860cf3bb52ee0c60faf0915""><code>07239ac</code></a> Document fix for module importers using deprecated APIs.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/df0e62c2740143cceb6cafea4c306dae1c559ef8""><code>df0e62c</code></a> Deal with module importers that don't implement newer API.</li>; <li><a href=""https://github.com/GrahamDumpleton/wrapt/commit/726275923",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12102:2909,avail,available,2909,https://hail.is,https://github.com/hail-is/hail/pull/12102,1,['avail'],['available']
Availability,"</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14203:1105,avail,available,1105,https://hail.is,https://github.com/hail-is/hail/pull/14203,1,['avail'],['available']
Availability,"</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | N",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:1105,avail,available,1105,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.1.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **554/1000** <br/> **Why?** Has a fix available, CVSS 6.8 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CERTIFI-3164749](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3172287](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3172287) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | N",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:1105,avail,available,1105,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['avail'],['available']
Availability,"</summary>. ```; aiohttp-jinja2 1.5.1 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;) <br/>[SNYK-PYTHON-AIOHTTP-6209406](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209406) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **718/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-6209407](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209407) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5ZWNjYjQ0YS1jYWZiLTQ0OTgtYjU1NS02NDdmZjUwY2ExOT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14225:1450,avail,available,1450,https://hail.is,https://github.com/hail-is/hail/pull/14225,1,['avail'],['available']
Availability,"<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49806"">#49806</a> on branch 1.5.x (DOC: Update what's new notes for 1.5.2 re...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/98c6139ff12107b9aa34441d25ef1593b6a0adca""><code>98c6139</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49579"">#49579</a> on Branch 1.5.x (BUG: Behaviour change in 1.5.0 when using...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/9196f8d545d1118f1233c1b45e7b740cb95c370c""><code>9196f8d</code></a> Backport PR STYLE enable pylint: method-cache-max-size-none (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49784"">#49784</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8c4b559c87561ca68ccdc3e81ff3c5218c7b4db7""><code>8c4b559</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49776"">#49776</a> on branch 1.5.x (REGR: arithmetic ops recursion error with...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/1616fb3d2c00905a5f3af510db893206ae00ea09""><code>1616fb3</code></a> Backport PR Revert &quot;Add color and size to arguments (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/44856"">#44856</a>)&quot; (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49752"">#49752</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/6f8e1745472c9d107367da1e38494425c3938234""><code>6f8e174</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49720"">#49720</a> on branch 1.5.x (Suppress spurious warning) (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49726"">#49726</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/63a91d0992b5f2837dc028d1bab34e659535b6b4""><code>63a91d0</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49676"">#496",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:3884,error,error,3884,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['error'],['error']
Availability,"<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/psf/requests/blob/v2.27.1/HISTORY.md#2271-2022-01-05"">https://github.com/psf/requests/blob/v2.27.1/HISTORY.md#2271-2022-01-05</a></p>; <h2>v2.27.0</h2>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <code>requests.exceptions.JSONDecodeError</code> to unify JSON exceptions between; Python 2 and 3. This gets raised in the <code>response.json()</code> method, and is; backwards compatible as it inherits from previously thrown exceptions.; Can be caught from <code>requests.exceptions.RequestException</code> as well. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5856"">#5856</a>)</p>; </li>; <li>; <p>Improved error text for misnamed <code>InvalidSchema</code> and <code>MissingSchema</code>; exceptions. This is a temporary fix until exceptions can be renamed; (Schema-&gt;Scheme). (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6017"">#6017</a>)</p>; </li>; <li>; <p>Improved proxy parsing for proxy URLs missing a scheme. This will address; recent changes to <code>urlparse</code> in Python 3.9+. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5917"">#5917</a>)</p>; </li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>; <p>Fixed defect in <code>extract_zipped_paths</code> which could result in an infinite loop; for some paths. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a>)</p>; </li>; <li>; <p>Fixed handling for <code>AttributeError</code> when calculating length of files obtained; by <code>Tarfile.extractfile()</code>. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5239"">#5239</a>)</p>; </li>; <li>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:1422,error,error,1422,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['error'],['error']
Availability,"<a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1144"">PyMySQL/PyMySQL#1144</a></li>; <li>chore(deps): update dependency sphinx-rtd-theme to v2 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1147"">PyMySQL/PyMySQL#1147</a></li>; <li>chore(deps): update actions/setup-python action to v5 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1152"">PyMySQL/PyMySQL#1152</a></li>; <li>chore(deps): update github/codeql-action action to v3 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1154"">PyMySQL/PyMySQL#1154</a></li>; <li>chore(deps): update codecov/codecov-action action to v4 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1158"">PyMySQL/PyMySQL#1158</a></li>; <li>Support error packet without sqlstate by <a href=""https://github.com/methane""><code>@​methane</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1160"">PyMySQL/PyMySQL#1160</a></li>; <li>test json - mariadb without JSON type by <a href=""https://github.com/grooverdan""><code>@​grooverdan</code></a> in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1165"">PyMySQL/PyMySQL#1165</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://github.com/hugovk""><code>@​hugovk</code></a> made their first contribution in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1134"">PyMySQL/PyMySQL#1134</a></li>; <li><a href=""https://github.com/svaskov""><code>@​svaskov</code></a> made their first contribution in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1145"">PyMySQL/PyMySQL#1145</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/PyMySQL/PyMySQL/compare/v1.1.0...v1.1.1"">https://github.com/PyMySQL/PyMySQL/compare",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14556:3325,error,error,3325,https://hail.is,https://github.com/hail-is/hail/pull/14556,1,['error'],['error']
Availability,"<b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed.; jupyter 1.0.0 requires qtconsole, which is not installed.; curlylint 0.13.1 requires pathspec, which is not installed.; black 22.12.0 requires pathspec, which is not installed.; beautifulsoup4 4.12.2 requires soupsieve, which is not installed.; astroid 2.15.8 requires lazy-object-proxy, which is not installed.; argon2-cffi-bindings 21.2.0 requires cffi, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `3.1.2 -> 3.1.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlOTc1OTMyYy1kNmNhLTQ0NTUtYmU4ZC04NzY1ZGY0MTZjMWMiLCJldmVudC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14141:1435,avail,available,1435,https://hail.is,https://github.com/hail-is/hail/pull/14141,1,['avail'],['available']
Availability,"<blockquote>; <h2>v1.22.0</h2>; <h1>NumPy 1.22.0 Release Notes</h1>; <p>NumPy 1.22.0 is a big release featuring the work of 153 contributors; spread over 609 pull requests. There have been many improvements,; highlights are:</p>; <ul>; <li>Annotations of the main namespace are essentially complete. Upstream; is a moving target, so there will likely be further improvements,; but the major work is done. This is probably the most user visible; enhancement in this release.</li>; <li>A preliminary version of the proposed Array-API is provided. This is; a step in creating a standard collection of functions that can be; used across application such as CuPy and JAX.</li>; <li>NumPy now has a DLPack backend. DLPack provides a common interchange; format for array (tensor) data.</li>; <li>New methods for <code>quantile</code>, <code>percentile</code>, and related functions. The; new methods provide a complete set of the methods commonly found in; the literature.</li>; <li>A new configurable allocator for use by downstream projects.</li>; </ul>; <p>These are in addition to the ongoing work to provide SIMD support for; commonly used functions, improvements to F2PY, and better documentation.</p>; <p>The Python versions supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:1233,down,downstream,1233,https://hail.is,https://github.com/hail-is/hail/pull/11939,4,['down'],['downstream']
Availability,"<br/>[SNYK-PYTHON-NUMPY-2321970](https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321970) | `numpy:` <br> `1.21.3 -> 1.22.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI4ZjJmN2FlNC0wY2VjLTQ3ZTYtODIyZi1lODFiMTA2N2RhMjIiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13871:3961,avail,available,3961,https://hail.is,https://github.com/hail-is/hail/pull/13871,1,['avail'],['available']
Availability,"<br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **501/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.3 | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:3905,avail,available,3905,https://hail.is,https://github.com/hail-is/hail/pull/14109,1,['avail'],['available']
Availability,"<code>[#3008](https://github.com/urllib3/urllib3/issues/3008) &lt;https://github.com/urllib3/urllib3/issues/3008&gt;</code>__)</li>; <li>Fixed <code>assert_hostname=False</code> to correctly skip hostname check. (<code>[#3051](https://github.com/urllib3/urllib3/issues/3051) &lt;https://github.com/urllib3/urllib3/issues/3051&gt;</code>__)</li>; </ul>; <h1>2.0.2 (2023-05-03)</h1>; <ul>; <li>Fixed <code>HTTPResponse.stream()</code> to continue yielding bytes if buffered decompressed data; was still available to be read even if the underlying socket is closed. This prevents; a compressed response from being truncated. (<code>[#3009](https://github.com/urllib3/urllib3/issues/3009) &lt;https://github.com/urllib3/urllib3/issues/3009&gt;</code>__)</li>; </ul>; <h1>2.0.1 (2023-04-30)</h1>; <ul>; <li>Fixed a socket leak when fingerprint or hostname verifications fail. (<code>[#2991](https://github.com/urllib3/urllib3/issues/2991) &lt;https://github.com/urllib3/urllib3/issues/2991&gt;</code>__)</li>; <li>Fixed an error when <code>HTTPResponse.read(0)</code> was the first <code>read</code> call or when the internal response body buffer was otherwise empty. (<code>[#2998](https://github.com/urllib3/urllib3/issues/2998) &lt;https://github.com/urllib3/urllib3/issues/2998&gt;</code>__)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/56f01e088dc006c03d4ee6ea9da4ab810f1ed700""><code>56f01e0</code></a> Release 2.0.7</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/4e50fbc5db74e32cabd5ccc1ab81fc103adfe0b3""><code>4e50fbc</code></a> Merge pull request from GHSA-g4mx-q9vg-27p4</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/80808b04bfa68fbd099828848c96ee25df185f1d""><code>80808b0</code></a> Fix docs build on Python 3.12 (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3144"">#3144</a>)</li>; <li><a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13852:11560,error,error,11560,https://hail.is,https://github.com/hail-is/hail/pull/13852,1,['error'],['error']
Availability,"<code>[#3008](https://github.com/urllib3/urllib3/issues/3008) &lt;https://github.com/urllib3/urllib3/issues/3008&gt;</code>__)</li>; <li>Fixed <code>assert_hostname=False</code> to correctly skip hostname check. (<code>[#3051](https://github.com/urllib3/urllib3/issues/3051) &lt;https://github.com/urllib3/urllib3/issues/3051&gt;</code>__)</li>; </ul>; <h1>2.0.2 (2023-05-03)</h1>; <ul>; <li>Fixed <code>HTTPResponse.stream()</code> to continue yielding bytes if buffered decompressed data; was still available to be read even if the underlying socket is closed. This prevents; a compressed response from being truncated. (<code>[#3009](https://github.com/urllib3/urllib3/issues/3009) &lt;https://github.com/urllib3/urllib3/issues/3009&gt;</code>__)</li>; </ul>; <h1>2.0.1 (2023-04-30)</h1>; <ul>; <li>Fixed a socket leak when fingerprint or hostname verifications fail. (<code>[#2991](https://github.com/urllib3/urllib3/issues/2991) &lt;https://github.com/urllib3/urllib3/issues/2991&gt;</code>__)</li>; <li>Fixed an error when <code>HTTPResponse.read(0)</code> was the first <code>read</code> call or when the internal response body buffer was otherwise empty. (<code>[#2998](https://github.com/urllib3/urllib3/issues/2998) &lt;https://github.com/urllib3/urllib3/issues/2998&gt;</code>__)</li>; </ul>; <h1>2.0.0 (2023-04-26)</h1>; <p>Read the <code>v2.0 migration guide &lt;https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html&gt;</code>__ for help upgrading to the latest version of urllib3.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/262e3e332209ee93ff70e2b13502c8f20c105ac8""><code>262e3e3</code></a> Release 2.0.6</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/644124ecd0b6e417c527191f866daa05a5a2056d""><code>644124e</code></a> Merge pull request from GHSA-v845-jxx5-vc9f</li>; <li><a href=""https://github.com/urllib3/urllib3/comm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13768:11208,error,error,11208,https://hail.is,https://github.com/hail-is/hail/pull/13768,2,['error'],['error']
Availability,"<code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:1340,failure,failure,1340,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['failure'],['failure']
Availability,"<details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiohttp-jinja2 1.5.1 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlNDEzMjhlZS0zNDg5LTQ3NDItYTc3YS01ZDZhNTQ1ZWE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14037:1413,avail,available,1413,https://hail.is,https://github.com/hail-is/hail/pull/14037,1,['avail'],['available']
Availability,"<h2>[5.10.0] - 2022-08-11</h2>; <h3>Updated</h3>; <ul>; <li>Updated Plotly.js to from version 2.12.1 to version 2.14.0. See the <a href=""https://github.com/plotly/plotly.js/blob/master/CHANGELOG.md#2140----2022-08-10"">plotly.js CHANGELOG</a> for more information. Notable changes include:; <ul>; <li>Add support for <code>sankey</code> links with arrows</li>; <li>Add <code>selections</code>, <code>newselection</code> and <code>activeselection</code> layout attributes to have persistent and editable selections over cartesian subplots</li>; <li>Add <code>unselected.line.color</code> and <code>unselected.line.opacity</code> options to <code>parcoords</code> trace</li>; <li>Display Plotly's new logo in the modebar</li>; </ul>; </li>; </ul>; <h2>[5.9.0] - 2022-06-23</h2>; <h3>Added</h3>; <ul>; <li><code>pattern_shape</code> options now available in <code>px.timeline()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3774"">#3774</a></li>; <li><code>facet_*</code> and <code>category_orders</code> now available in <code>px.pie()</code> <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3775"">#3775</a></li>; </ul>; <h3>Performance</h3>; <ul>; <li><code>px</code> methods no longer call <code>groupby</code> on the input dataframe when the result would be a single group, and no longer groups by a lambda, for significant speedups <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3765"">#3765</a> with thanks to <a href=""https://github.com/jvdd""><code>@​jvdd</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Allow non-string extras in <code>flaglist</code> attributes, to support upcoming changes to <code>ax.automargin</code> in plotly.js <a href=""https://github-redirect.dependabot.com/plotly/plotly.js/pull/6193"">plotly.js#6193</a>, <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3749"">#3749</a></li>; </ul>; <h2>[5.8.2] - 2022-06-10</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed a syntax error that caused",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12113:2245,avail,available,2245,https://hail.is,https://github.com/hail-is/hail/pull/12113,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - gear/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzM2VkMzM4Ny0zZTVmLTRkZDgtYjIxYy1iYzIyNzk4ODViZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjMzZWQzMzg3LTNlNWYtNGRkOC1iMjFjLWJjMjI3OTg4NWJmMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_med",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13107:761,avail,available,761,https://hail.is,https://github.com/hail-is/hail/pull/13107,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlNTZiOGU3Ni1mMTk3LTQ0MmMtOGVlMC04MjFhMDk5YzM3YTAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImU1NmI4ZTc2LWYxOTctNDQyYy04ZWUwLTgyMWEwOTljMzdhMCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13072:772,avail,available,772,https://hail.is,https://github.com/hail-is/hail/pull/13072,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; sphinx 5.3.0 has requirement docutils<0.20,>=0.14, but you have docutils 0.20.; sphinx-rtd-theme 1.2.0 has requirement docutils<0.19, but you have docutils 0.20. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyOTUzNDFmZi1lMjQ4LTRiOTItYTY1Yy1kYjJiZWQ3ZDQxMGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13066:998,avail,available,998,https://hail.is,https://github.com/hail-is/hail/pull/13066,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `40.0.2 -> 41.0.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmZWM3ZmQ2Ny0xZmE0LTRlNzEtODQ4Ni1hMDk5YThmYWM3NzgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImZlYzdmZDY3LTFmYTQtNGU3MS04NDg2LWEwOTlhOGZhYzc3OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13138:776,avail,available,776,https://hail.is,https://github.com/hail-is/hail/pull/13138,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2YzU3NmY1Yi1lNGM5LTQ4ZjctYmYxNy04YjEzOTIxODlmZDQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjZjNTc2ZjViLWU0YzktNDhmNy1iZjE3LThiMTM5MjE4OWZkNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source&#x3D;git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13097:776,avail,available,776,https://hail.is,https://github.com/hail-is/hail/pull/13097,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affecte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14230:1125,avail,available,1125,https://hail.is,https://github.com/hail-is/hail/pull/14230,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.1.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiodns 2.0.0 requires pycares, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6261585](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6261585) | `cryptography:` <br> `42.0.2 -> 42.0.4` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1MzAxOWZkZC04YjQwLTQ5NmUtYjRmYS0wMzA5MTAx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14344:1125,avail,available,1125,https://hail.is,https://github.com/hail-is/hail/pull/14344,1,['avail'],['available']
Availability,"<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiNzM2NzI0Yi1hY2RiLTRiOTUtYWQwMy1hYWI3MjkyZGNlYzQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3MzY3MjRiLWFjZGItNGI5NS1hZDAzLWFhYjcyOTJkY2VjNCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/5ecb4152-94d0-44ff-86c6-21e542bb123d?utm_source&#x3D;github&amp;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13116:768,avail,available,768,https://hail.is,https://github.com/hail-is/hail/pull/13116,1,['avail'],['available']
Availability,"<li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with JSON documentation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3936"">#3936</a>)</li>; <li>Fixing README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3825"">#3825</a>)</li>; <li>Adding axios-multi-api to the ecosystem file (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3817"">#3817</a>)</li>; <li>Adding SECURITY.md to properly disclose security vulnerabilities (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3981"">#3981</a>)</li>; </ul>; <p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>; <ul>; <li><a href=""https://github.com/SashaKoro"">Sasha Korotkov</a></li>; <li><a href=""https://github.com/timemachine3030"">Daniel Lopretto</a></li>; <li><a href=""https://github.com/MikeBishop"">Mike Bishop</a></li>; <li><a href=""https://github.com/DigitalBrainJS"">Dmitriy Mozgovoy</a></li>; <li><a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:4405,error,errors,4405,https://hail.is,https://github.com/hail-is/hail/pull/11080,2,['error'],['errors']
Availability,"<li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with JSON documentation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3936"">#3936</a>)</li>; <li>Fixing README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3825"">#3825</a>)</li>; <li>Adding axios-multi-api to the ecosystem file (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3817"">#3817</a>)</li>; <li>Adding SECURITY.md to properly disclose security vulnerabilities (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3981"">#3981</a>)</li>; </ul>; <p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>; <ul>; <li><a href=""https://github.com/axios/axios/blob/master/mailto:jasonsaayman@gmail.com"">Jay</a></li>; <li><a href=""https://github.com/SashaKoro"">Sasha Korotkov</a></li>; <li><a href=""https://github.com/timemachine3030"">Daniel Lopretto</a></li>; <li><a href=""https://github.com/MikeBishop"">Mike Bishop</a></li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:9930,error,errors,9930,https://hail.is,https://github.com/hail-is/hail/pull/11080,2,['error'],['errors']
Availability,"<module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1605,error,error,1605,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['error'],['error']
Availability,"<p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.4.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to set request <code>method</code> and <code>body</code></li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 8.0.1</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4c983ed5cd229fa64912294737c858c2ba8486d6""><code>4c983ed</code></a> Bump up version number to 5.4.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/cc20442ab67bf37687c08e67af7e7de3a21c8fbe""><code>cc20442</code></a> Add integration tests for Gradle 8.0.2</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/472920e572e4cf45d321868874ced50ad8d1e2d5""><code>472920e</code></a> Add possibility to set request method and body</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/82e70cae2a8d48b4f5165a9b543d4e65bb793d88""><code>82e70ca</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86a15f1c16eb729dc71b6caf30237d07b8e0bb01""><code>86a15f1</code></a> Fix compiler warnings and deprecations</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86363072c8239330b28976109a622bdd073507b6""><code>8636307</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a1858b494b5f3a51ccef7580c243c6dfdf520731""><code>a1858b4</code></a> Merge pull request <a href=""https://redirect.github.com/michel-kraemer/grad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:1083,down,download-task,1083,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download-task']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - auth/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2023.5.7 -> 2023.7.22` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJjMDQ5NzlhMC1iYWM3LTRiMjEtYmE0ZS02OWU5YjAzMTE5ZjAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImMwNDk3OWEwLWJhYzctNGIyMS1iYTRlLTY5ZTliMDMxMTlmMCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13309:847,avail,available,847,https://hail.is,https://github.com/hail-is/hail/pull/13309,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - auth/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1ZDhmZDhmZC1mZGUxLTRiYmMtYWMzMi0xOTE1NmY0ZDFjZjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjVkOGZkOGZkLWZkZTEtNGJiYy1hYzMyLTE5MTU2ZjRkMWNmMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13112:853,avail,available,853,https://hail.is,https://github.com/hail-is/hail/pull/13112,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - batch/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **658/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.3 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-5798483](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-5798483) | `aiohttp:` <br> `3.8.4 -> 3.8.5` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmZmEwNjUyZi1hMzc2LTQ0NmQtYWJjNC04NmJhMzUwNmY3MzMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImZmYTA2NTJmLWEzNzYtNDQ2ZC1hYmM0LTg2YmEzNTA2ZjczMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project rep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13285:880,avail,available,880,https://hail.is,https://github.com/hail-is/hail/pull/13285,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - batch/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiodocker 0.21.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;) <br/>[SNYK-PYTHON-AIOHTTP-6209406](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209406) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **718/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-6209407](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209407) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ens",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14226:977,avail,available,977,https://hail.is,https://github.com/hail-is/hail/pull/14226,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - batch/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiodocker 0.21.0 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with y",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14044:1003,avail,available,1003,https://hail.is,https://github.com/hail-is/hail/pull/14044,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2023.5.7 -> 2023.7.22` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3Mzc3ZjFlZS1kMjJjLTQ0MDAtYmE1Yy04NGNkYWZmZWJmYzgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjczNzdmMWVlLWQyMmMtNDQwMC1iYTVjLTg0Y2RhZmZlYmZjOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest proj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13305:845,avail,available,845,https://hail.is,https://github.com/hail-is/hail/pull/13305,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **551/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `41.0.2 -> 41.0.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13365:845,avail,available,845,https://hail.is,https://github.com/hail-is/hail/pull/13365,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `41.0.7 -> 42.0.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14200:831,avail,available,831,https://hail.is,https://github.com/hail-is/hail/pull/14200,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **496/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.2 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-6002459](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-6002459) | `urllib3:` <br> `1.26.17 -> 1.26.18` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyNWE2ZGYzMi1kYmEzLTQzOTctYmIyNC0zNjdlMzhmZWQ3ZmUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjI1YTZkZjMyLWRiYTMtNDM5Ny1iYjI0LTM2N2UzOGZlZDdmZSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13848:851,avail,available,851,https://hail.is,https://github.com/hail-is/hail/pull/13848,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Use of a Broken or Risky Cryptographic Algorithm <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6149518](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6149518) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Uncontrolled Resource Consumption (&#x27;Resource Exhaustion&#x27;) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6157248](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6157248) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **451/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.3 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `41.0.7 -> 42.0.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14236:831,avail,available,831,https://hail.is,https://github.com/hail-is/hail/pull/14236,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `40.0.2 -> 41.0.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJjOTM3MTIxYy1lZTM3LTQ2ZmMtYTcxMC04MWY4YzdhZmUyN2IiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImM5MzcxMjFjLWVlMzctNDZmYy1hNzEwLTgxZjhjN2FmZTI3YiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13136:851,avail,available,851,https://hail.is,https://github.com/hail-is/hail/pull/13136,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-5926907](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-5926907) | `urllib3:` <br> `1.26.16 -> 1.26.17` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3ZGVlZGFlMy1mZmE3LTQxYmUtOGY4MS1lNmYwZTA5YTczOTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdkZWVkYWUzLWZmYTctNDFiZS04ZjgxLWU2ZjBlMDlhNzM5MyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13770:851,avail,available,851,https://hail.is,https://github.com/hail-is/hail/pull/13770,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6261585](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6261585) | `cryptography:` <br> `42.0.2 -> 42.0.4` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiYWUwMDM5My05NGUzLTRhNjYtYTE5Ni0xMjUwZDg0ZGZiZDgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImJhZTAwMzkzLTk0ZTMtNGE2Ni1hMTk2LTEyNTBkODRkZmJkOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14343:851,avail,available,851,https://hail.is,https://github.com/hail-is/hail/pull/14343,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlZjQxMWYxOC1hM2JiLTQ1YzgtODFjOS1hNmNhNjI4MWI1ZjMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImVmNDExZjE4LWEzYmItNDVjOC04MWM5LWE2Y2E2MjgxYjVmMyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13108:851,avail,available,851,https://hail.is,https://github.com/hail-is/hail/pull/13108,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - ci/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **611/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5914629](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `41.0.3 -> 41.0.4` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJiN2QwMTZlZS0zODA0LTQwMjItOWE0Yi01MzExNjZhNjBjMWQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImI3ZDAxNmVlLTM4MDQtNDAyMi05YTRiLTUzMTE2NmE2MGMxZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13698:851,avail,available,851,https://hail.is,https://github.com/hail-is/hail/pull/13698,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - gear/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2023.5.7 -> 2023.7.22` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmYWJiOGYzZi1mMDFjLTQxMjktODJjNC1kZjQzMjRmZTU4YTIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImZhYmI4ZjNmLWYwMWMtNDEyOS04MmM0LWRmNDMyNGZlNThhMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13296:847,avail,available,847,https://hail.is,https://github.com/hail-is/hail/pull/13296,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - gear/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **658/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.3 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-5798483](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-5798483) | `aiohttp:` <br> `3.8.4 -> 3.8.5` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2ZWQ3MzlmOS1mZjc4LTQzYzgtYWQwOC05MThjNmRhMWNlOTYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjZlZDczOWY5LWZmNzgtNDNjOC1hZDA4LTkxOGM2ZGExY2U5NiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project repo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13284:879,avail,available,879,https://hail.is,https://github.com/hail-is/hail/pull/13284,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - gear/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; prometheus-async 19.2.0 requires prometheus-client, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;) <br/>[SNYK-PYTHON-AIOHTTP-6209406](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209406) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **718/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-6209407](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209407) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the ch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14228:1000,avail,available,1000,https://hail.is,https://github.com/hail-is/hail/pull/14228,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - gear/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; prometheus-async 19.2.0 requires prometheus-client, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they wo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14034:1026,avail,available,1026,https://hail.is,https://github.com/hail-is/hail/pull/14034,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2023.5.7 -> 2023.7.22` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlMjNiYjk0OC04YjdmLTQ5MzUtYTRkMi05ZWJmNjg4NjZlMmUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImUyM2JiOTQ4LThiN2YtNDkzNS1hNGQyLTllYmY2ODg2NmUyZSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [Vie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13316:858,avail,available,858,https://hail.is,https://github.com/hail-is/hail/pull/13316,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzYWEwNDk2OC02NDIxLTRmODktYTBjYy03MjE4MzExNDNiZGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjNhYTA0OTY4LTY0MjEtNGY4OS1hMGNjLTcyMTgzMTE0M2JkZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13100:864,avail,available,864,https://hail.is,https://github.com/hail-is/hail/pull/13100,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.2` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1ZTRkMTU3Zi04YTdjLTRhNzctYTZlNC00YTdmNGU4Y2I0YzkiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjVlNGQxNTdmLThhN2MtNGE3Ny1hNmU0L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13323:966,avail,available,966,https://hail.is,https://github.com/hail-is/hail/pull/13323,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; jupyter 1.0.0 requires notebook, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **566/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIzYWE2MzZiYi00NmJmLTQ3MjgtOGVjMC0yMDg0OWE4NzgyZGMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13444:992,avail,available,992,https://hail.is,https://github.com/hail-is/hail/pull/13444,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/dev/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; matplotlib 3.5.3 requires numpy, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5NmE4NGVhMS1hYzgxLTQxYmEtOGYzNC02MGU1ZTdhYzNjZTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12896:972,avail,available,972,https://hail.is,https://github.com/hail-is/hail/pull/12896,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2023.5.7 -> 2023.7.22` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1NDMwZTFmMi0wNDZjLTQwNDctYmI3Mi1hZmJkZmM1MDViNGEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjU0MzBlMWYyLTA0NmMtNDA0Ny1iYjcyLWFmYmRmYzUwNWI0YSJ9fQ=="" width=""0"" height=""0""/>; 🧐 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13304:862,avail,available,862,https://hail.is,https://github.com/hail-is/hail/pull/13304,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIwMzdiOGRmZS1hZDA4LTRmZjUtYTFkOC1hNGM4Nzg2N2NkYjAiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjAzN2I4ZGZlLWFkMDgtNGZmNS1hMWQ4LWE0Yzg3ODY3Y2RiMCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13159:868,avail,available,868,https://hail.is,https://github.com/hail-is/hail/pull/13159,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **658/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.3 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-5798483](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-5798483) | `aiohttp:` <br> `3.8.4 -> 3.8.5` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI4YTljZTU5Zi0yOTY3LTQ2MTQtOGE5YS1iY2M5YjU1ZWZkZGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjhhOWNlNTlmLTI5NjctNDYxNC04YTlhLWJjYzliNTVlZmRkZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View late",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13282:894,avail,available,894,https://hail.is,https://github.com/hail-is/hail/pull/13282,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **661/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 7.5 | Relative Path Traversal <br/>[SNYK-PYTHON-ORJSON-6276643](https://snyk.io/vuln/SNYK-PYTHON-ORJSON-6276643) | `orjson:` <br> `3.9.7 -> 3.9.15` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3YTljMjVmNy0wMTBmLTQxNmItYjc0OS1jNzFkY2I4YjY5YjgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdhOWMyNWY3LTAxMGYtNDE2Yi1iNzQ5LWM3MWRjYjhiNjliOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14361:857,avail,available,857,https://hail.is,https://github.com/hail-is/hail/pull/14361,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14045:887,avail,available,887,https://hail.is,https://github.com/hail-is/hail/pull/14045,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **763/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `41.0.1 -> 41.0.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIwMjJhZDMzNS1kYzBkLTQxZWYtYmRjYi03ZTFkODQwNWJhYTYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjAyMmFkMzM1LWRjMGQtNDFlZi1iZGNiLTdlMWQ4NDA1YmFhNiJ9fQ=="" width=""0"" height=""0""/>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13247:882,avail,available,882,https://hail.is,https://github.com/hail-is/hail/pull/13247,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **471/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 3.7 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2023.5.7 -> 2023.7.22` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJkY2E2ZDI1ZC1hZGM3LTRiNTctYWU3Zi0yNjExOTYzNTY5MmUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImRjYTZkMjVkLWFkYzctNGI1Ny1hZTdmLTI2MTE5NjM1NjkyZSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View la",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13294:854,avail,available,854,https://hail.is,https://github.com/hail-is/hail/pull/13294,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `40.0.2 -> 41.0.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhYmU2OWI5ZC1kMzViLTQ1Y2ItYWY2NS04ZDEwN2YxZWMzZmMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImFiZTY5YjlkLWQzNWItNDVjYi1hZjY1LThkMTA3ZjFlYzNmYyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project repor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13139:860,avail,available,860,https://hail.is,https://github.com/hail-is/hail/pull/13139,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **591/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.28.2 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlNDQxZTBmNS1jZDQ4LTQzZDUtYTdkMy1kMTM4YzQ2ZTc2NTgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImU0NDFlMGY1LWNkNDgtNDNkNS1hN2QzLWQxMzhjNDZlNzY1OCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13158:860,avail,available,860,https://hail.is,https://github.com/hail-is/hail/pull/13158,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **658/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.3 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-5798483](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-5798483) | `aiohttp:` <br> `3.8.4 -> 3.8.5` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlMjk5ZmU1Ni0wNGI1LTQ3MzEtYmUzYS03M2ZmYzgxZTZjYjgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImUyOTlmZTU2LTA0YjUtNDczMS1iZTNhLTczZmZjODFlNmNiOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest proje",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13283:886,avail,available,886,https://hail.is,https://github.com/hail-is/hail/pull/13283,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - web_common/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **658/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.3 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-5798483](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-5798483) | `aiohttp:` <br> `3.8.4 -> 3.8.5` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJjZmU2NDEwYi1jYjQ3LTQ2YzgtOTYwYy1kOWRlY2UxMjI5ZTIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImNmZTY0MTBiLWNiNDctNDZjOC05NjBjLWQ5ZGVjZTEyMjllMiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest projec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13286:885,avail,available,885,https://hail.is,https://github.com/hail-is/hail/pull/13286,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - web_common/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14036:885,avail,available,885,https://hail.is,https://github.com/hail-is/hail/pull/14036,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - web_common/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiosignal 1.3.1 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `3.1.2 -> 3.1.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlYTQ5ODFkZC02M2FmLTQ4YzYtYTIwMC05NjkyZjg2ZTlhNjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14140:991,avail,available,991,https://hail.is,https://github.com/hail-is/hail/pull/14140,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - web_common/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiohttp-jinja2 1.5.1 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;) <br/>[SNYK-PYTHON-AIOHTTP-6209406](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209406) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **718/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-AIOHTTP-6209407](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6209407) | `aiohttp:` <br> `3.8.6 -> 3.9.2` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14225:986,avail,available,986,https://hail.is,https://github.com/hail-is/hail/pull/14225,1,['avail'],['available']
Availability,"<p>This PR was automatically created by Snyk using the credentials of a real user.</p><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - web_common/requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; aiohttp-jinja2 1.5.1 requires aiohttp, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14037:1012,avail,available,1012,https://hail.is,https://github.com/hail-is/hail/pull/14037,1,['avail'],['available']
Availability,"= self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . /databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw); 115 def deco(*a, **kw):; 116 try:; --> 117 return f(*a, **kw); 118 except py4j.protocol.Py4JJavaError as e:; 119 converted = convert_exception(e.java_exception). /databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); 325 if answer[1] == REFERENCE_TYPE:; --> 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value). Py4JJavaError: An error occurred while calling o504.pyPersistTable.; : is.hail.utils.HailException: 1 samples and 12 covariates (including x) implies -11 degrees of freedom.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:11); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.LinearRegressionRowsSingle.execute(LinearRegression.scala:51); 	at is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:51); 	at is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:2936); 	at is.hail.expr.ir.TableIR.analyzeAndExecute(TableIR.scala:57); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:27); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$2(SparkBackend.scala:502); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413:3851,Error,ErrorHandling,3851,https://hail.is,https://github.com/hail-is/hail/issues/11413,1,['Error'],['ErrorHandling']
Availability,"=""https://redirect.github.com/python-pillow/Pillow/issues/7497"">#7497</a> [<a href=""https://github.com/ZachNagengast""><code>@​ZachNagengast</code></a>]</li>; <li>Add .git-blame-ignore-revs file <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7528"">#7528</a> [<a href=""https://github.com/akx""><code>@​akx</code></a>]</li>; <li>Attempt memory mapping when tile args is a string <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7565"">#7565</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fill identical pixels with transparency in subsequent frames when saving GIF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7568"">#7568</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed unnecessary string length check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7560"">#7560</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Determine mask mode in Python instead of C <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7548"">#7548</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Corrected duration when combining multiple GIF frames into single frame <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7521"">#7521</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Handle disposing GIF background from outside palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7515"">#7515</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Seek past the data when skipping a PSD layer <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7483"">#7483</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>ImageMath: Inline <code>isinstance</code> check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7623"">#7623</a> [<a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:5093,mask,mask,5093,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['mask'],['mask']
Availability,"=======================> (222 + 80) / 353]; 	[PASS] with 353 partitions: (50000, 973); 	2020-06-10 10:30:15 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 9:> (0 + 18) / 18]; 	[FAIL] with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:3628,Error,Error,3628,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['Error'],['Error']
Availability,"=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:4085,repair,repair,4085,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['repair'],['repair']
Availability,"> **Why?** | Buffer Overflow <br/>[SNYK-PYTHON-NUMPY-2321966](https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321966) | `numpy:` <br> `1.21.3 -> 1.22.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **/1000** <br/> **Why?** | Denial of Service (DoS) <br/>[SNYK-PYTHON-NUMPY-2321970](https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321970) | `numpy:` <br> `1.21.3 -> 1.22.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13871:3620,avail,available,3620,https://hail.is,https://github.com/hail-is/hail/pull/13871,1,['avail'],['available']
Availability,"> +1,738 -2,452. Sorry :-(. High level summary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirror",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7420:489,failure,failure,489,https://hail.is,https://github.com/hail-is/hail/pull/7420,2,"['down', 'failure']","['down', 'failure']"
Availability,> TMPDIR; > This variable shall represent a pathname of a directory made; > available for programs that need a place to create temporary; > files. http://pubs.opengroup.org/onlinepubs/9699919799/. Requested by the discuss user rca:. http://discuss.hail.is/t/hailcontext-tmp-dir/323,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2327:76,avail,available,76,https://hail.is,https://github.com/hail-is/hail/pull/2327,1,['avail'],['available']
Availability,"> Tim Poterba: I think we need to kick up the local disk size requested when running VEP. > Tim Poterba: we were talking about this here https://discuss.hail.is/t/dataproc-workers-lost-after-intensive-task/1014/34. > Kyle Satterstrom: Interesting, thanks for pointing that out. Yeah, I had been using workers with 40GB boot disks (and no attached SSDs). I had looked in the log and seen the errors that said ""no space left on device"", but I guess I didn't really believe it would fill up all that space at once. I can try increasing the 40 to 100 and see if it helps. > Tim Poterba: that unblocked Kevin on VEP!. > Tim Poterba: and don't try to repartition, I think. > Kyle Satterstrom: Just an update -- I tried the same thing again with 100GB worker boot disks intead of 40GB, and it made more progress but ultimately failed again hail-20190726-1716-0.2.16-6da0d3571629.log. > Kyle Satterstrom: One more update -- I tried the same thing again but with a local SSD attached to each machine instead of increasing the boot disk size (so worker boot disks were 40 GB), and that worked!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6756:391,error,errors,391,https://hail.is,https://github.com/hail-is/hail/issues/6756,1,['error'],['errors']
Availability,> pytest-instafail is a plugin for py.test that shows failures and errors instantly instead of waiting until the end of test session. https://github.com/pytest-dev/pytest-instafail/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6854:54,failure,failures,54,https://hail.is,https://github.com/hail-is/hail/pull/6854,2,"['error', 'failure']","['errors', 'failures']"
Availability,">#15690</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Add scroll margin to headings for better alignment <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15703"">#15703</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix shortcut UI failing on filtering when empty command is given <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15695"">#15695</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix connection loop issue with standalone foreign document in LSP <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15262"">#15262</a> (<a href=""https://github.com/trungleduc""><code>@​trungleduc</code></a>)</li>; <li>Fix outputarea package from not detecting updates <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15642"">#15642</a> (<a href=""https://github.com/MFA-X-AI""><code>@​MFA-X-AI</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15524"">#15524</a>: Fix visual tests <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15578"">#15578</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Remove Python 3.0, Notebook 5 mentions from contributor docs <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15710"">#15710</a> (<a href=""https://github.com/JasonWeill""><code>@​JasonWeill</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/graphs/contributors?from=2024-01-19&amp;to=2024-01-30&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AFoSuCloud+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​FoSuCloud</code></a> | <a href=""https://github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14218:1791,Mainten,Maintenance,1791,https://hail.is,https://github.com/hail-is/hail/pull/14218,2,['Mainten'],['Maintenance']
Availability,">::run(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:56:36: required from ‘simdpp::arch_avx2::int16<16>& simdpp::arch_avx2::int16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:49:75: required from ‘simdpp::arch_avx2::int16<16>::int16(const simdpp::arch_avx2::uint16<16, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:64:29: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int16<16>’ with ‘private’ member ‘simdpp::arch_avx2::int16<16>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:33:7: note: ‘class simdpp::arch_avx2::int16<16>’ declared here; class int16<16, void> : public any_int16<16, int16<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int64<2>; T = simdpp::arch_avx2::uint64<2>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:106947,error,error,106947,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,">::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]’; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:114:36: required from ‘simdpp::arch_avx2::uint8<32>& simdpp::arch_avx2::uint8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint8<32> >, simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint16<16> > > >]’; libsimdpp-2.0-rc2/simdpp/detail/insn/unzip_lo.h:56:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::uint8<32>’ with ‘private’ member ‘simdpp::arch_avx2::uint8<32>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<16>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: ‘class simdpp::arch_avx2::uint8<32>’ declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint32<4>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:30641,error,error,30641,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,">; <h3>Features</h3>; <ul>; <li>Add support for Python 3.10 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/143"">#143</a>) (<a href=""https://github.com/googleapis/python-api-common-protos/commit/63ca888512be84508fcf95e4d5d40df036a85e18"">63ca888</a>)</li>; <li>Add support for Python 3.11 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/145"">#145</a>) (<a href=""https://github.com/googleapis/python-api-common-protos/commit/b9dbb219ea46abd9851af1fc41ea37f9d5631c0b"">b9dbb21</a>)</li>; <li>added google.api.JwtLocation.cookie (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; <li>added google.api.Service.publishing and client libraries settings (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; <li>new fields in enum google.api.ErrorReason (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>deprecate google.api.BackendRule.min_deadline (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; <li><strong>deps:</strong> Require protobuf &gt;=3.19.5 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/141"">#141</a>) (<a href=""https://github.com/googleapis/python-api-common-protos/commit/9ea3530b459269e964fcc98db1c5025e05d6495f"">9ea3530</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>minor updates to comments (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/python-api-common-protos/blo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12514:1439,Error,ErrorReason,1439,https://hail.is,https://github.com/hail-is/hail/pull/12514,1,['Error'],['ErrorReason']
Availability,">; <h3>Features</h3>; <ul>; <li>Add support for Python 3.10 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/143"">#143</a>) (<a href=""https://github.com/googleapis/python-api-common-protos/commit/63ca888512be84508fcf95e4d5d40df036a85e18"">63ca888</a>)</li>; <li>Add support for Python 3.11 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/145"">#145</a>) (<a href=""https://github.com/googleapis/python-api-common-protos/commit/b9dbb219ea46abd9851af1fc41ea37f9d5631c0b"">b9dbb21</a>)</li>; <li>added google.api.JwtLocation.cookie (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; <li>added google.api.Service.publishing and client libraries settings (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; <li>new fields in enum google.api.ErrorReason (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>deprecate google.api.BackendRule.min_deadline (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; <li><strong>deps:</strong> Require protobuf &gt;=3.19.5 (<a href=""https://github-redirect.dependabot.com/googleapis/python-api-common-protos/issues/141"">#141</a>) (<a href=""https://github.com/googleapis/python-api-common-protos/commit/9ea3530b459269e964fcc98db1c5025e05d6495f"">9ea3530</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>minor updates to comments (<a href=""https://github.com/googleapis/python-api-common-protos/commit/6af21322879cba158e0a5992c9799e68c1744fac"">6af2132</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/python-api-common-protos/commit/0ee8d805",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12514:3649,Error,ErrorReason,3649,https://hail.is,https://github.com/hail-is/hail/pull/12514,1,['Error'],['ErrorReason']
Availability,">; <li>Correctly create list of output files (even if the destination is the project's build directory)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:1838,down,downloaded,1838,https://hail.is,https://github.com/hail-is/hail/pull/12345,2,['down'],"['download', 'downloaded']"
Availability,">; <li>Optimize ImageStat.Stat.extrema <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7593"">#7593</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Handle pathlib.Path in FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7578"">#7578</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use list comprehensions to create transformed lists <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7597"">#7597</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Added support for reading DX10 BC4 DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7603"">#7603</a> [<a href=""https://github.com/sambvfx""><code>@​sambvfx</code></a>]</li>; <li>Optimized ImageStat.Stat.count <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7599"">#7599</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Moved error from truetype() to FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7587"">#7587</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Correct PDF palette size when saving <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7555"">#7555</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed closing file pointer with olefile 0.47 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7594"">#7594</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>ruff: Minor optimizations of list comprehensions, x in set, etc. <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7524"">#7524</a> [<a href=""https://github.com/cclauss""><code>@​cclauss</code></a>]</li>; <li>Build Windows wheels using cibuildwheel <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7580"">#7580</a> [<a href=""https://github.com/nulano""><code>@​nulano</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:8198,error,error,8198,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['error'],['error']
Availability,>; <li>api-change:<code>timestream-query</code>: [<code>botocore</code>] Documentation only update for SDK and CLI</li>; </ul>; <h1>1.21.11</h1>; <ul>; <li>api-change:<code>gamelift</code>: [<code>botocore</code>] Minor updates to address errors.</li>; <li>api-change:<code>cloudtrail</code>: [<code>botocore</code>] Add bytesScanned field into responses of DescribeQuery and GetQueryResults.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for S3 Object Ownership by allowing the S3 bucket owner full control canned ACL to be set when Athena writes query results to S3 buckets.</li>; <li>api-change:<code>keyspaces</code>: [<code>botocore</code>] This release adds support for data definition language (DDL) operations</li>; <li>api-change:<code>ecr</code>: [<code>botocore</code>] This release adds support for tracking images lastRecordedPullTime.</li>; </ul>; <h1>1.21.10</h1>; <ul>; <li>api-change:<code>mediapackage</code>: [<code>botocore</code>] This release adds Hybridcast as an available profile option for Dash Origin Endpoints.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] Documentation updates for Multi-AZ DB clusters.</li>; <li>api-change:<code>mgn</code>: [<code>botocore</code>] Add support for GP3 and IO2 volume types. Add bootMode to LaunchConfiguration object (and as a parameter to UpdateLaunchConfigurationRequest).</li>; <li>api-change:<code>kafkaconnect</code>: [<code>botocore</code>] Adds operation for custom plugin deletion (DeleteCustomPlugin) and adds new StateDescription field to DescribeCustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorizati,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:2036,avail,available,2036,https://hail.is,https://github.com/hail-is/hail/pull/11486,2,['avail'],['available']
Availability,">; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:2479,Mainten,Maintenance,2479,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['Mainten'],['Maintenance']
Availability,">; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraeme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:1760,Mainten,Maintenance,1760,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['Mainten'],['Maintenance']
Availability,"></li>; </ul>; <h2>pytest-asyncio 0.23.4a2</h2>; <h1>0.23.4 (UNRELEASED)</h1>; <ul>; <li>pytest-asyncio no longer imports additional, unrelated packages during test collection <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/729"">#729</a></li>; <li>Addresses further issues that caused an internal pytest error during test collection</li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/c34da04b82153ce052109bad31ccdbc0be7938e1""><code>c34da04</code></a> [docs] Mentioned pytest 8.2 compatibility fix in changelog.</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/143f745d279afc070cf5cf6144fbf34d960fae72""><code>143f745</code></a> Fix compatibility with pytest 8.2 FixtureDef.unittest removal</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/13d4b79f7ff0d9d0ea70880b3276f85dea7f1f15""><code>13d4b79</code></a> Remove unused function <code>_removesuffix</code></li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/cdd2c4906835b6f627d681fbee5d487554884e5f"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:6736,down,down,6736,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['down'],['down']
Availability,"><br /><h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiohttp 3.8.5 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **661/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 7.5 | Improper Neutralization of Special Elements in Data Query Logic <br/>[SNYK-PYTHON-MSAL-5904284](https://snyk.io/vuln/SNYK-PYTHON-MSAL-5904284) | `msal:` <br> `1.24.0 -> 1.24.1` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhYjlhNGM2ZS0xOTg1LTRmYTctYj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13753:1117,avail,available,1117,https://hail.is,https://github.com/hail-is/hail/pull/13753,1,['avail'],['available']
Availability,">@​ahg-g</code></a>)</li>; <li>Promote IdentifyPodOS feature to beta. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107859"">kubernetes/kubernetes#107859</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>Remove a v1alpha1 networking API for ClusterCIDRConfig (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109436"">kubernetes/kubernetes#109436</a>, <a href=""https://github.com/JamesLaverack""><code>@​JamesLaverack</code></a>)</li>; <li>Renamed metrics <code>evictions_number</code> to <code>evictions_total</code> and mark it as stable. The original <code>evictions_number</code> metrics name is marked as &quot;Deprecated&quot; and has been removed in kubernetes 1.23 . (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106366"">kubernetes/kubernetes#106366</a>, <a href=""https://github.com/cyclinder""><code>@​cyclinder</code></a>)</li>; <li>Skip x-kubernetes-validations rules if having fundamental error against the OpenAPIv3 schema. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108859"">kubernetes/kubernetes#108859</a>, <a href=""https://github.com/cici37""><code>@​cici37</code></a>)</li>; <li>Support for gRPC probes is now in beta. GRPCContainerProbe feature gate is enabled by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108522"">kubernetes/kubernetes#108522</a>, <a href=""https://github.com/SergeyKanzhelev""><code>@​SergeyKanzhelev</code></a>)</li>; <li>Suspend job to GA. The feature gate <code>SuspendJob</code> is locked and will be removed in 1.26. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108129"">kubernetes/kubernetes#108129</a>, <a href=""https://github.com/ahg-g""><code>@​ahg-g</code></a>)</li>; <li>The AnyVolumeDataSource feature is now beta, and the feature gate is enabled by default. In order to provide user feedback on PVCs with data s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:12664,error,error,12664,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['error'],['error']
Availability,">Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739d0400c333a72b32f5d514""><code>0fdebf3</code></a> Improve verify example</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/02b8e1a79d9e00acd61f9ac42e5555619fe2247a""><code>02b8e1a</code></a> Prevent duplicate destination files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0b65ca2f17c8890a3ec34cf80cde52ee5413cbec""><code>0b65ca2</code></a> Call eachFile action only once per source</li>; <li><a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:3586,down,download-task,3586,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,">As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have async fixtures that are set up and torn down for each module. If you're affected by this issue, please continue using the v0.21 release, until it is resolved.</p>; <h2>pytest-asyncio 0.23.5</h2>; <h1>0.23.5 (2024-02-09)</h1>; <ul>; <li>Declare compatibility with pytest 8 <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/737"">#737</a></li>; <li>Fix typing errors with recent versions of mypy <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/769"">#769</a></li>; <li>Prevent DeprecationWarning about internal use of <code>asyncio.get_event_loop()</code> from affecting test cases <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/757"">#757</a></li>; </ul>; <h2>Known issues</h2>; <p>As of v0.23, pytest-asyncio attaches an asyncio event loop to each item of the test suite (i.e. session, packages, modules, classes, functions) and allows tests to be run in those loops when marked accordingly. Pytest-asyncio currently assumes that async fixture scope is correlated with the new event loop scope. This prevents fixtures from being evaluated independently from the event loop scope and breaks some existing test suites (see <a href=""https://redirect.github.com/pytest-dev/pytest-asyncio/issues/706"">#706</a>). For example, a test suite may require all fixtures and tests to run in the same event loop, but have as",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507:2884,error,errors,2884,https://hail.is,https://github.com/hail-is/hail/pull/14507,1,['error'],['errors']
Availability,">Read more in the v2.0 Roadmap</a></p>; <p>:warning: <strong>This release will be the last release supporting Python 3.5. Please upgrade to a non-EOL Python version.</strong></p>; <ul>; <li>Added extra message to<code>urllib3.exceptions.ProxyError</code> when urllib3 detects that a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.</li>; <li>Added a mention of the size of the connection pool when discarding a connection due to the pool being full.</li>; <li>Added explicit support for Python 3.11.</li>; <li>Deprecated the <code>Retry.MAX_BACKOFF</code> class property in favor of <code>Retry.DEFAULT_MAX_BACKOFF</code> to better match the rest of the default parameter names. <code>Retry.MAX_BACKOFF</code> is removed in v2.0.</li>; <li>Changed location of the vendored <code>ssl.match_hostname</code> function from <code>urllib3.packages.ssl_match_hostname</code> to <code>urllib3.util.ssl_match_hostname</code> to ensure Python 3.10+ compatibility after being repackaged by downstream distributors.</li>; <li>Fixed absolute imports, all imports are now relative.</li>; </ul>; <h2>1.26.7</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed a bug with HTTPS hostname verification involving IP addresses and lack of SNI</li>; <li>Fixed a bug where IPv6 braces weren't stripped during certificate hostname matching</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; <h2>1.26.6</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Deprecated the <code>urllib3.contrib.ntlmpool</code> module. urllib3 is not able to support it pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11532:1574,down,downstream,1574,https://hail.is,https://github.com/hail-is/hail/pull/11532,1,['down'],['downstream']
Availability,">See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5/whatsnew/v1.5.0.html"">whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.5.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.5.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.4.4</h2>; <p>This is a patch release in the 1.4.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.4/whatsnew/v1.4.4.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/87cfe4e38bafe7300a6003a1d18bd80f3f77c763""><code>87cfe4e</code></a> RLS: 1.5.0</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/ecc700c8be8e4af2799dc18ce5f7e6328c80e976""><code>ecc700c</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48627"">#48627</a> on branch 1.5.x (DOC: Last changes to release notes for 1....</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/e726483d70938f3bff67e95358841a1f6271b149""><code>e726483</code></a> Backport PR <a href=""https://github-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12292:2004,avail,available,2004,https://hail.is,https://github.com/hail-is/hail/pull/12292,1,['avail'],['available']
Availability,">Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1da8f078e22412475b694ce07b890148b8a5e4fc""><code>1da8f07</code></a> Add comment</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/9703f764df56c52626f7d6f44bca8b1d51312389""><code>9703f76</code></a> Use pooling connection manager instead of basic one</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/306172e4c6532e185c8a6a9998bca7d22d2d0c63""><code>306172e</code></a> Bump up version number to 5.2.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/b9df0c0daa080450772c365f16a9406fe0ca607a""><code>b9df0c0</code></a> Document eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/05a4433770f7020ff845add9348bdc12c82793dd""><code>05a4433</code></a> Add eachFile action</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:3228,down,download-task,3228,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download-task']
Availability,"@cseed @danking it's a lot of lines of code, but I couldn't really figure out how to cut down on the boilerplate. Suggestions welcome.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3426:89,down,down,89,https://hail.is,https://github.com/hail-is/hail/pull/3426,1,['down'],['down']
Availability,@cseed @jigold I think this resolves our PVC issues modulo batch failure. We still need to add the batch refresh loop.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6166:65,failure,failure,65,https://hail.is,https://github.com/hail-is/hail/pull/6166,1,['failure'],['failure']
Availability,"@cseed Can you look over this for structure before I assign it randomly? There's three things I am not happy about or want double checked:. 1. I had to replicate the parsing code for `quoted_literal` etc. between the two parser classes. I couldn't figure out how to have the function in one place and be able to call it. 2. I debated back and forth what the `repsepUntil` and `repUntil` interface should look like. I decided to make it take the desired tokens instead of comparing to a string or any value because that seems more error proof and explicit. However, the users of the function have something like this now `repsepUntil(it, f, PunctuationToken("",""), PunctuationToken(""}""))`, which is quite verbose. I thought about making a second function that took string arguments and converted it to PunctuationToken and then called the functions that took tokens, but decided it was better to be explicit. 3. Can you double check the regex for `float_literal` is still correct? This was to fix the empty string match I had during our check-in last week.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4711:530,error,error,530,https://hail.is,https://github.com/hail-is/hail/pull/4711,1,['error'],['error']
Availability,@cseed I think this is a better organization. The pod specs in the database are static and can be inserted into the database upon job creation. So we now assert the job tasks are never null in the database. This doesn't change the problem of how to handle a pvc/pod creation error in the database. Should we delete the record upon failure? Poll and wait for creation to succeed up to N times?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6075:275,error,error,275,https://hail.is,https://github.com/hail-is/hail/pull/6075,2,"['error', 'failure']","['error', 'failure']"
Availability,"@danking I have a mostly completed draft for SAIGE in QoB. Can you take a look? I'm mainly looking for enough feedback to get a green light to actually start testing this end to end, fill in the remaining not implemented components, add documentation, add verbosity and possibly a dry run feature, and support VEP annotations natively. There are a couple of core concepts:; 1. Phenotypes - Set of phenotypes to test. I support the ability to group phenotypes together. This is in anticipation of a new version of SAIGE that Wei is going to release soon.; 2. VariantChunks - The set of variant intervals of data to test per job. If it's SAIGE-GENE, then there's also the ""groups"" to actually test within that interval.; 3. io - There's a bunch of wrappers that handle input and output files so all of that logic combined with the checkpointing logic is abstracted away from what is actually going on.; 4. steps - These are the SAIGE modules to run. They are all dataclasses with configuration options; 5. saige - There's a class that can be instantiated in Python or I started writing the framework for a CLI. This has the code that builds the DAG end to end. All configuration happens with a yaml file that can overwrite default parameters for each step such as whether to checkpoint or where the results should be written to. For the CLI, I envision you can either give a config file and/or specify `--overrides step1_null_glmm.use_checkpoint=true`. For every Saige run, I write out the configuration used to a file in the output directory as well as information about the input data and variant chunks and the batch information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13804:829,checkpoint,checkpointing,829,https://hail.is,https://github.com/hail-is/hail/pull/13804,2,['checkpoint'],"['checkpoint', 'checkpointing']"
Availability,"@danking I remember you wanted raise web.Response() here for a reason, so I'll let you decide if this is correct or not. ```; {""levelname"": ""ERROR"", ""asctime"": ""2020-03-13 15:53:52,139"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 635, in insert; jobs_args); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 172, in execute_many; return await cursor.executemany(sql, args_array); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 283, in executemany; self._get_db().encoding)); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8307:141,ERROR,ERROR,141,https://hail.is,https://github.com/hail-is/hail/pull/8307,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"@danking The latest version is the code in hail/methods/. I'm having trouble with all of the configs and how to instantiate that properly. After that, I need to figure out what inputs `run_saige` actually needs. Then I need to write util functions for creating the testing chunks and annotating the matrix table. Then I think after testing and cleaning it up, it will be sufficient for the workshop. To get it into main is going to be a lot more work to have helpful error messages, check MT is valid for this analysis, integrate it more carefully into QoB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13588:467,error,error,467,https://hail.is,https://github.com/hail-is/hail/pull/13588,1,['error'],['error']
Availability,@danking the lmm change can be considered a bug fix since delta should never be negative. The log change should make the tests more robust to which JVM. Let me know if this fixes the failures and I'll PR against 0.1 as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132:132,robust,robust,132,https://hail.is,https://github.com/hail-is/hail/pull/2132,2,"['failure', 'robust']","['failures', 'robust']"
Availability,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240:98,error,error,98,https://hail.is,https://github.com/hail-is/hail/issues/8240,2,['error'],['error']
Availability,"@jigold I made some changes to the annotation database web page, care to take a look?. Mainly got rid of the tree/query builder thing and moved that functionality to checkboxes in the documentation. Seemed redundant to have both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2144:206,redundant,redundant,206,https://hail.is,https://github.com/hail-is/hail/pull/2144,1,['redundant'],['redundant']
Availability,"@johnc1231 Had this idea for dealing with registry flakiness. I was hesitant at first because really I want buildkit to retry more transient errors, but maybe with its local cache it would be quick?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11666:141,error,errors,141,https://hail.is,https://github.com/hail-is/hail/pull/11666,1,['error'],['errors']
Availability,"@konradjk is getting bogged down by the single-core implementation of BlockMatrix diagonal. This implementation pulls out the diagonal of each diagonal block in parallel, and then collects and flattens the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3216:28,down,down,28,https://hail.is,https://github.com/hail-is/hail/pull/3216,1,['down'],['down']
Availability,"@tpoterba , can you take a look as well?. Notes:. 1. Azure uses Spark 3.0.2, so I need to build and publish a wheel for Spark 3.0.2.; 2. Azure provides Jupyter Notebooks already.; 3. hail/Makefile (for manual deploys) was missing some changes for deploy.sh, so I updated it.; 4. Azure sets the `AZURE_SPARK` environment variable inside hosted Jupyter Notebooks. 5. In Azure's Jupyter, if you set `extraClassPath` you break the extant classpath (e.g. you cannot load Scala stdlib classes). However, the JARs specified in `spark.jars` are added to the classpath properly, so, in Azure, it suffices to specify `spark.jars`. 6. Azure lacks requester pays, so I require Azure users download, untar, and upload the VEP files to their own bucket. 7. Instead of ""submit"", Azure installs Livy, a Java job-queue system. I have no idea how to set environment variables in Livy and Azure does not set AZURE_SPARK in Livy jobs; therefore, I search for `hdinsight` in the CLASSPATH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187:677,down,download,677,https://hail.is,https://github.com/hail-is/hail/pull/11187,1,['down'],['download']
Availability,"@tpoterba this builds on your recent PR https://github.com/hail-is/hail/pull/3882. partition counts are computed through Interpret on demand and memoized in the IR. fastPartitionCounts means get the partition counts if you have them (either because the IR know their partition counts, or they were previously computed by counting the RVD partitions). partitionCounts means compute (and memoize) if they aren't available the fast way. I think this is now optimal except that MatrixTable.count potentially runs things twice. We might be able to fix this with a MatrixLet in the case you're calling MatrixTable.count(). Next Table.index/MatrixTable.indexRows should use partitionCounts instead of zipWithIndex because computing the partition counts via the optimizer will potentially be much faster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3891:410,avail,available,410,https://hail.is,https://github.com/hail-is/hail/pull/3891,1,['avail'],['available']
Availability,"@tpoterba tried to run some jobs on the batch2 instance in the default namespace. He ran into two errors when trying to get log files (one while the job was running and the other when it terminated):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 49, in _read_logs; return await self.app['driver'].read_pod_logs(self._pod_name); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 160, in __getitem__; return self._state[key]; KeyError: 'driver'; ```. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 636, in download_to_file; self._do_download(transport, file_obj, download_url, headers, start, end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 574, in _do_download; download.consume(transport); File ""/usr/local/lib/python3.6/dist-pac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7412:98,error,errors,98,https://hail.is,https://github.com/hail-is/hail/pull/7412,1,['error'],['errors']
Availability,A KeyError's __str__ is just the key that failed. This prints the traceback and the full; error message. It is far more useful.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9022:90,error,error,90,https://hail.is,https://github.com/hail-is/hail/pull/9022,1,['error'],['error']
Availability,A Missing Default Case Should Have An Informative Error Message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3696:50,Error,Error,50,https://hail.is,https://github.com/hail-is/hail/issues/3696,1,['Error'],['Error']
Availability,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10738:118,error,errors,118,https://hail.is,https://github.com/hail-is/hail/pull/10738,3,"['down', 'error']","['downstream', 'error', 'errors']"
Availability,"A forthcoming change to the hail ci system will introduce deployment. This change adds `hail-ci-deploy.sh` which replicates the [""Deploy Website""](https://ci.hail.is/admin/editRunType.html?id=buildType:HailSourceCode_HailMainline_DeployWebsite&runnerId=RUNNER_29) and [""Deploy Google Cloud""](https://ci.hail.is/admin/editRunType.html?id=buildType:HailSourceCode_HailMainline_DeployDocsAndGoogleCloudSpark220&runnerId=RUNNER_10) TeamCity jobs. My general thinking for deploy jobs from the CI is that, for the time being, we'll hardcode a mapping from GitHub repository to [Kubernetes Secret](https://kubernetes.io/docs/concepts/configuration/secret/). That's where this `/secret/ci.hail.is-web-updater-rsa-key` will come from. Moreover, the CI will always authorize a gcloud account (again with a baked in mapping from GitHub repository to GCP service account) before calling the deploy script. I did not retest the master branch here. Should we do that even though a PR is only merged to master if it passes the tests? Even after locking down merging, there's still the possibility of CI bugs. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4220:1038,down,down,1038,https://hail.is,https://github.com/hail-is/hail/pull/4220,1,['down'],['down']
Availability,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112:839,reliab,reliable,839,https://hail.is,https://github.com/hail-is/hail/pull/7112,3,"['avail', 'error', 'reliab']","['available', 'error', 'reliable']"
Availability,A merge failure creates a funny batch that doesn't have an id. This handles that properly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8428:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/8428,1,['failure'],['failure']
Availability,"A mitigation, but not resolution, for #13402, this errors a job after 10 seconds of waiting on a network namespace, as it should never take that long to create one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13403:51,error,errors,51,https://hail.is,https://github.com/hail-is/hail/pull/13403,1,['error'],['errors']
Availability,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:382,down,down,382,https://hail.is,https://github.com/hail-is/hail/issues/6773,2,"['Avail', 'down']","['Available', 'down']"
Availability,"A revival of #12122, which was reverted because the web-based code path had an error. The last commit here fixes that bug and I tested that the web path works through a dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12177:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/pull/12177,1,['error'],['error']
Availability,"A simple pipeline to generate fingerprinting data for gnomad v4 failed. . ```python3; import hail as hl; from gnomad_qc.v4.resources.basics import get_gnomad_v4_vds; hl.init(default_reference='GRCh38'); ​; ht = hl.import_table(; ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.haplotype_database.txt"",; comment=(""@""),; ); ht = ht.key_by(locus=hl.locus(ht['#CHROMOSOME'], hl.int(ht['POSITION'])), alleles=hl.array([ht.MAJOR_ALLELE, ht.MINOR_ALLELE])); vds = get_gnomad_v4_vds(split=True, remove_hard_filtered_samples=False); vds = hl.vds.filter_variants(vds, ht); mt = hl.vds.to_dense_mt(vds); mt = mt.checkpoint(""gs://gnomad/v4.0/sample_qc/exomes/gnomad.exomes.v4.0.fingerprinting_variants.mt"", overwrite=True); ```. We were able to get it to succeed, but filtering the variant data first on locus, splitting it, filtering it on variants. Then proceeding. There may be an ugly interaction with joins and explode but it needs more investigation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11717:625,checkpoint,checkpoint,625,https://hail.is,https://github.com/hail-is/hail/issues/11717,1,['checkpoint'],['checkpoint']
Availability,"A user reported this error `concurrent.futures._base.TimeoutError` with no stack trace while copying files in a batch job. There's a comment in `is_transient_error` that we should catch this error, but I did not see it caught in the existing function.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/11817,2,['error'],['error']
Availability,"A user was getting an index out-of-bounds error on `cdf.values[idx]`. I can't reproduce it, but this should guarantee the index is in bounds, and is a simplification besides.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10624:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/10624,1,['error'],['error']
Availability,"A very small PR but here's the background and context behind this change. When talking to either GCP or Azure, hail chooses credentials in the following order from highest priority to lowest priority:. 1. An explicit `credential_file` argument passed to the relevant credentials class; 2. An environment variable containing the path to the credentials (`GOOGLE_APPLICATION_CREDENTIALS` or `AZURE_APPLICATION_CREDENTIALS`) (from this you can see why the code that was here is totally redundant); 3. The latent credentials present on the machine. This might be `gcloud` or `az` credentials, or the metadata server if you're on a cloud VM. I'm trying to rid the codebase of most explicit providing of credentials file paths, for two reasons:; - Quality of life. I'm already signed into the cloud with `gcloud` and `az`. I shouldn't need to download some file and provide `AZURE_APPLICATION_CREDENTIALS` to run this test. It should just use the latent credentials.; - We are trying to phase out credentials files altogether for security reasons. These files are long-lived secrets that you really don't want to leak and are currently exposed to users in Batch jobs, so they can be easily exfiltrated. Using the latent credentials on a cloud VM (the metadata server) has the benefit of only issuing short-lived access tokens which last for hours not months, so it's basically always better to use the latent credentials when possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13981:483,redundant,redundant,483,https://hail.is,https://github.com/hail-is/hail/pull/13981,2,"['down', 'redundant']","['download', 'redundant']"
Availability,"A/pylint/issues/5998"">#5998</a></p>; </li>; <li>; <p>Fix false positive for 'nonexistent-operator' when repeated '-' are; separated (e.g. by parens).</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5769"">#5769</a></p>; </li>; </ul>; <h1>What's New in Pylint 2.13.2?</h1>; <p>Release date: 2022-03-27</p>; <ul>; <li>; <p>Fix crash when subclassing a <code>namedtuple</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5982"">#5982</a></p>; </li>; <li>; <p>Fix false positive for <code>superfluous-parens</code> for patterns like; &quot;return (a or b) in iterable&quot;.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5803"">#5803</a></p>; </li>; <li>; <p>Fix a false negative regression in 2.13.0 where <code>protected-access</code> was not; raised on functions.</p>; <p>Fixes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5989"">#5989</a></p>; </li>; <li>; <p>Better error messages in case of crash if pylint can't write the issue template.</p>; <p>Refer to <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5987"">#5987</a></p>; </li>; </ul>; <h1>What's New in Pylint 2.13.1?</h1>; <p>Release date: 2022-03-26</p>; <ul>; <li>; <p>Fix a regression in 2.13.0 where <code>used-before-assignment</code> was emitted for; the usage of a nonlocal in a try block.</p>; <p>Fixes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5965"">#5965</a></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/7591ac04dcefc527c42fd7713c909d1319e83fab""><code>7591ac0</code></a> Bump pylint to 2.13.3, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/a880bd6d85d2487f509d1505b5146d608b15d870""><code>a880bd6</code></a> Change 'nonexistent-operator' to allow repeated unary ops (with space or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11702:1666,error,error,1666,https://hail.is,https://github.com/hail-is/hail/pull/11702,1,['error'],['error']
Availability,"AFAICT, labels are free. Without this, I cannot narrow the cost of, say, a Local SSD, down to one instance. https://cloud.google.com/resource-manager. Also, just, generally, I need to be able to know to which namespace a disk belongs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13053:86,down,down,86,https://hail.is,https://github.com/hail-is/hail/pull/13053,1,['down'],['down']
Availability,"AFAIK, the `retryTransientErrors` in `createNoCompression` would only retry any errors in creating the `ReadChannel`, but we still want to retry transient errors when calling read later down the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11832:80,error,errors,80,https://hail.is,https://github.com/hail-is/hail/pull/11832,3,"['down', 'error']","['down', 'errors']"
Availability,"ALLED,Number=1,Type=Integer,Description=""1 for variants that were called after phasing via splitting the bam into its component haplotypes and calling variants in haploid mode"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=BX,Number=.,Type=String,Description=""Barcodes and Associated Qual-Scores Supporting Alleles"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""ID of Phase Set for Variant"">; ##FORMAT=<ID=PQ,Number=1,Type=Integer,Description=""Phred QV indicating probability at this variant is incorrectly phased"">; ##FORMAT=<ID=JQ,Number=1,Type=Integer,Description=""Phred QV indicating probability of a phasing switch error in gap prior to this variant"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FILTER=<ID=UNSUPPORTED_GENOTYPE,Description=""If genotype field contains '.' we assume that this is due to making a single sample vcf from a multiple sample vcf in which this sample does not contain the variant."">; ##FILTER=<ID=10X_RESCUED_MOLECULE_HIGH_DIVERSITY,Description=""Set if true: (((RESCUED+NOT_RESCUED) > 0 & RESCUED/(RESCUED+NOT_RESCUED) > 0.1) & (MMD == -1 | MMD >= 3.0))"">; ##FILTER=<ID=10X_QUAL_FILTER,Description=""Set if true: (%QUAL <= 15 || (AF[0] > 0.5 && %QUAL < 50))"">; ##FILTER=<ID=10X_ALLELE_FRACTION_FILTER,Description=""Set if true: (AO[0] < 2 || AO[0]/(AO[0] + RO) < 0.15)"">; ##FILTER=<ID=10X_PHASING_INCONSISTENT,Description=""Uses haplotype information from the fragments and the alleles to filter some variants that are not consistent with phasing."">; ##FILTER=<ID=10X_H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:12086,error,error,12086,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['error'],['error']
Availability,Abstract properties need to be declared [this way](https://stackoverflow.com/questions/2736255/abstract-attributes-in-python) to be enforceable. If I delete `self.billing_manager` in one of the driver subclasses it still typechecks but would error at runtime. I also deleted the `create` method on the `CloudDriver` interface because it is never really used as an interface method (we only ever explicitly use the method from the concrete classes). As such it doesn't really enforce a contract and can be a little restrictive (in terra there is no sensible `credentials_file` input to such a method to create a terra driver.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13186:242,error,error,242,https://hail.is,https://github.com/hail-is/hail/pull/13186,1,['error'],['error']
Availability,"Add 2 new tutorials, document distributions as preferred way to run locally, small fixes to error messages and printouts",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1900:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/pull/1900,1,['error'],['error']
Availability,Add Error Handling Improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1739:4,Error,Error,4,https://hail.is,https://github.com/hail-is/hail/pull/1739,1,['Error'],['Error']
Availability,"Add a `keepRatio` parameter to the ApproxCDF aggregator. When compacting any level, always keep a fixed fraction of the smallest and largest values at that level. Also keep counts of the number of times each level is compacted, for use in downstream error estimates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6076:239,down,downstream,239,https://hail.is,https://github.com/hail-is/hail/pull/6076,2,"['down', 'error']","['downstream', 'error']"
Availability,Add a link to downloadable archive of tutorials to docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6520:14,down,downloadable,14,https://hail.is,https://github.com/hail-is/hail/issues/6520,1,['down'],['downloadable']
Availability,Add an a postiori error estimate for the approximate cdf aggregator. Use in new pdf plotting method than makes no assumption about the smoothness of the true distribution.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6039:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/6039,1,['error'],['error']
Availability,Add better error message for call_stats error mode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4380:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/4380,2,['error'],['error']
Availability,Add better sparse split multi error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6877:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/pull/6877,1,['error'],['error']
Availability,Add error message to genotype check assertions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1744:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/1744,1,['error'],['error']
Availability,Add error on matrixtable iter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3044:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/3044,1,['error'],['error']
Availability,Add error output to VEP exceptions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5224:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/5224,1,['error'],['error']
Availability,"Add errorId tracking to MakeNDArray, allowing user to get a good python error highlighting where an error is thrown from.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10095:4,error,errorId,4,https://hail.is,https://github.com/hail-is/hail/pull/10095,3,['error'],"['error', 'errorId']"
Availability,Add expr support and nicer error messages to key_by functions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2916:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/pull/2916,1,['error'],['error']
Availability,Add mendel error counts to variant and sample qc.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/148:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/148,1,['error'],['error']
Availability,Add more examples to the Expression.__nonzero__ error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2764:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/2764,1,['error'],['error']
Availability,Add retry infrastructure mirroring Python. This will hopefully fix the deploy issue. I think we'll have to grow another set of transient errors to retry related to lower-level networking issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8802:137,error,errors,137,https://hail.is,https://github.com/hail-is/hail/pull/8802,1,['error'],['errors']
Availability,Add selection to PCA to push down optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3888:29,down,down,29,https://hail.is,https://github.com/hail-is/hail/pull/3888,1,['down'],['down']
Availability,Add support for container checkpoint/restore,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11888:26,checkpoint,checkpoint,26,https://hail.is,https://github.com/hail-is/hail/pull/11888,1,['checkpoint'],['checkpoint']
Availability,"Add the `filter` and `find_replace` arguments to `import_table` and `import_vcf`. These are regex filter and substitution arguments, which will make it possible to tolerate some invalid inputs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5300:164,toler,tolerate,164,https://hail.is,https://github.com/hail-is/hail/pull/5300,1,['toler'],['tolerate']
Availability,Add the nice errors to struct / struct expression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2797:13,error,errors,13,https://hail.is,https://github.com/hail-is/hail/issues/2797,1,['error'],['errors']
Availability,"Added batch2_check step in build, fixed lint errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7208:45,error,errors,45,https://hail.is,https://github.com/hail-is/hail/pull/7208,1,['error'],['errors']
Availability,Added error message if keyspace or table don't exist in Cassandra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/680:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/680,1,['error'],['error']
Availability,Added logging line for ci error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6685:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/6685,1,['error'],['error']
Availability,Added nd array map2 to correspond to existing scala nd array map2 ir. Also used map2 to facilitate writing user facing features hl.nd.maximum and hl.nd.minimum which echo np.maximum and np.minimum.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10635:166,echo,echo,166,https://hail.is,https://github.com/hail-is/hail/pull/10635,1,['echo'],['echo']
Availability,"Added stats.LeveneHaldane and tests. Created doc folder, LeveneHaldane.tex, and bibfile.bib; Added docs/.gitignore. Added HWEPerVariant, test/resources/HWE_test.vcf and tests. Added Utils.time, now replaced by Utils.printTime. Used Option in ""r*"" (ratio) methods for missing values, now abstracted with Utils.divOption and Utils.someIf. Added rounding-error-tolerant comparison operators Utils.D_\* and used where appropriate. replaced closeEnough with D_==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/73:352,error,error-tolerant,352,https://hail.is,https://github.com/hail-is/hail/pull/73,1,['error'],['error-tolerant']
Availability,Adding checkpointing as a quality of life improvement for block matrices.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6933:7,checkpoint,checkpointing,7,https://hail.is,https://github.com/hail-is/hail/pull/6933,1,['checkpoint'],['checkpointing']
Availability,Adding support to nd arrays to include less than max dimension indexing and the use of ellipses to echo numpy behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10551:99,echo,echo,99,https://hail.is,https://github.com/hail-is/hail/pull/10551,1,['echo'],['echo']
Availability,"Adding this signature appeases `pyright` in my editor. Unfortunately, `pylint` still [gets confused](https://github.com/pylint-dev/pylint/issues/259) so I disabled this particular check. I made sure that `mypy` still errors when forgetting to provide a required parameter to a function call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13308:217,error,errors,217,https://hail.is,https://github.com/hail-is/hail/pull/13308,1,['error'],['errors']
Availability,Additional transient errors from GCS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14516:21,error,errors,21,https://hail.is,https://github.com/hail-is/hail/pull/14516,1,['error'],['errors']
Availability,"Addresses #1943 . [JVM Spec, Chapter 6](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html) states, regarding, DCMPG and DCMPL:; > NaN is unordered, so any double comparison fails if either or both of its operands are NaN. With both dcmpg and dcmpl available, any double comparison may be compiled to push the same result onto the operand stack whether the comparison fails on non-NaN values or fails because it encountered a NaN. For more information, see [§3.5](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-3.html#jvms-3.5). The `G` and `L` suffices refer to whether the presence of one or more `NaN`s should be indicated by returning ""greater than"" or ""less than"". Ergo, when we're checking `x > y` we use `DCMPL` so the NaN case produces `-1` with which the downstream comparison to `0` produces `false`. Confusingly, the simple intuition is, if you're checking **L**ess Than, you should use the **G** version. If you're checking **G**reater Than, you should use the **L** version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1952:261,avail,available,261,https://hail.is,https://github.com/hail-is/hail/pull/1952,2,"['avail', 'down']","['available', 'downstream']"
Availability,"Addresses this error:; ```; ERROR | 2019-06-17 09:41:59,615 | web_protocol.py | log_exception:355 | Error handling request; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 849, in create_batch; await create_job(batch.id, userdata, job_params); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 628, in create_job; pvc_size=pvc_size); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 398, in create_job; await job._create_pod(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 190, in _create_pod; self._pvc_name = await self._create_pvc(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 165, in _create_pvc; await self.mark_complete(None, failed=True, failure_reason=str(err)); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 526, in mark_complete; await self._mark_job_task_complete(task_name, pod_log, exit_code); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 288, in _mark_job_task_complete; assert self._pod_name is not None; AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6367:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/6367,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,Addresses this transient error I just encountered.; ```; + docker push gcr.io/hail-vdc/ci-intermediate:tn05m3kr79i2; The push refers to repository [gcr.io/hail-vdc/ci-intermediate]; Get https://gcr.io/v2/: dial tcp: lookup gcr.io: Temporary failure in name resolution; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8112:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/8112,2,"['error', 'failure']","['error', 'failure']"
Availability,Addresses user confusion in https://discuss.hail.is/t/potential-outdated-error-statement-for-hail-version-incompatibility/3562.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13494:73,error,error-statement-for-hail-version-incompatibility,73,https://hail.is,https://github.com/hail-is/hail/pull/13494,1,['error'],['error-statement-for-hail-version-incompatibility']
Availability,Adds [CADD](https://cadd.gs.washington.edu/download) v1.6 Hail Tables to datasets API/annotation DB.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10474:43,down,download,43,https://hail.is,https://github.com/hail-is/hail/pull/10474,1,['down'],['download']
Availability,"Adds `copy_spark_log_on_error` init configuration option. When true, driver logs are copied to the remote tmpdir if an error occurs. This is useful in support cases where users cannot copy logs off the dataproc server themselves as it has already shut down.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14447:119,error,error,119,https://hail.is,https://github.com/hail-is/hail/pull/14447,2,"['down', 'error']","['down', 'error']"
Availability,"Adds `writekudu` and `readkudu` commands for storing a VariantSampleMatrix in Kudu. Note that only biallelic variants can be stored at the moment, so `splitmulti` should be used. Sample run. ```; SPARK_MASTER=yarn-client; # chr1 in 6m52.6s; spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; importvcf -f vcf-1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf splitmulti writekudu -o file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com --drop; # read (2 min or so); spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; readkudu -i file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com count ; ```. To install Kudu on a cluster, see http://www.cloudera.com/documentation/betas/kudu/0-5-0/topics/kudu_installation.html#concept_u4s_tbq_dt_unique_1, and follow the instructions for installing from parcels. The CDS file is available at http://archive.cloudera.com/beta/kudu/csd/. You can also run Kudu locally using a VM, see http://getkudu.io/docs/quickstart.html. This is suitable for running unit tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242:1289,avail,available,1289,https://hail.is,https://github.com/hail-is/hail/pull/242,1,['avail'],['available']
Availability,"Adds a MatrixTable for all variant-gene cis-eQTL associations tested in each tissue (including non-significant associations) for GTEx v8. MatrixTable has columns keyed by tissue, and contain all available tissues from GTEx V8. The `GTEx_MatrixTables` notebook documents how the MatrixTable were generated. . The eQTL MatrixTable is ~220 GiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10535:195,avail,available,195,https://hail.is,https://github.com/hail-is/hail/pull/10535,1,['avail'],['available']
Availability,Adds a `pre-commit` linting rule to run on our html files that is jinja-aware. The ~ resilience ~ of html continues to astound me.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10031:85,resilien,resilience,85,https://hail.is,https://github.com/hail-is/hail/pull/10031,1,['resilien'],['resilience']
Availability,Adds optimization available in lowering process if number of rows per partition from child TableIR is known,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10798:18,avail,available,18,https://hail.is,https://github.com/hail-is/hail/pull/10798,1,['avail'],['available']
Availability,"Adds retry for specific 500 errors:. ```; aiodocker.exceptions.DockerError: DockerError(500, 'error creating overlay mount to /var/lib/docker/overlay2/545a1337742e0292d9ed197b06fe900146c85ab06e468843cd0461c3f34df50d/merged: device or resource busy'; ```. ```; aiodocker.exceptions.DockerError: DockerError(500, 'Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7715:28,error,errors,28,https://hail.is,https://github.com/hail-is/hail/pull/7715,2,['error'],"['error', 'errors']"
Availability,"Adds the available [GIANT 2018 Exome Array Summary Statistics](https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files#2018_Exome_Array_Summary_Statistics) datasets for WHR, BMI, and height as Hail Tables. For reproducibility, I added the notebook I used to generate the tables and schemas. The datasets were small in this case, and I ended up doing things locally on my machine. It didn't seem to make sense to try to redo things to fit into the older extract/load workflow once everything had already been generated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10235:9,avail,available,9,https://hail.is,https://github.com/hail-is/hail/pull/10235,1,['avail'],['available']
Availability,"After I set-up my spark context on my spark cluster, I ran hail.init() and ran into the following: ; `py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([null, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8423:130,error,error,130,https://hail.is,https://github.com/hail-is/hail/issues/8423,1,['error'],['error']
Availability,After a clean `make shadowJar` takes 2m33s on my laptop now. I just downloaded the latest Gradle from home-brew and ran `gradle wrapper` in the hail directory.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10245:68,down,downloaded,68,https://hail.is,https://github.com/hail-is/hail/pull/10245,1,['down'],['downloaded']
Availability,"After its singular additional dependency got shoved down into `hailtop`, auth no longer needs anything beyond the `hailtop/gear/web_common dependencies`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13655:52,down,down,52,https://hail.is,https://github.com/hail-is/hail/pull/13655,1,['down'],['down']
Availability,"After this, there are only 150ish warnings remaining, which shouldn't be too hard to fix by hand. Most are unused locals. Scalafix can delete unused locals, but I disable that, because there were too many cases where it left the rhs unnecessarily, e.g.; ```; ...; val idx = Symbol(genUID()); ...; ```; rewrites to; ```; ...; Symbol(genUID()); ...; ```; I'd rather just leave those as errors to be fixed manually. This is intended to replace #14103, which ended up mixing manual changes to fix warnings with scalafix changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14156:384,error,errors,384,https://hail.is,https://github.com/hail-is/hail/pull/14156,1,['error'],['errors']
Availability,"After updating from hail from v0.2.92 to v0.2.105 , I've started getting the ""Bucket is a requester pays bucket but no user project provided"" error (with further details in https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/hl.2Ehadoop_stat.20no.20longer.20works.20for.20requester.20pays.20bucket)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12540:142,error,error,142,https://hail.is,https://github.com/hail-is/hail/issues/12540,1,['error'],['error']
Availability,Allow `expect` in tests that don't kill and print multiple failures,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/169:59,failure,failures,59,https://hail.is,https://github.com/hail-is/hail/issues/169,1,['failure'],['failures']
Availability,Also added it to third-party images so we're not pulling from DockerHub. Turns out the \ufeff bug we were seeing is hitting a lot of people and is addressed in this release. I put this up in my namespace to see that I can load it without error (though didn't try copying over dashboards and such).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12300:238,error,error,238,https://hail.is,https://github.com/hail-is/hail/pull/12300,1,['error'],['error']
Availability,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8280:303,error,error,303,https://hail.is,https://github.com/hail-is/hail/pull/8280,1,['error'],['error']
Availability,"Also included useful changes that made it possible to diagnose and fix this problem including:. - No longer dropping metrics for test namespaces. I checked the prometheus disk and we have plenty of space to add these additional metrics. Very useful for diagnosing test time latencies.; - Add prometheus scraping for envoy pods. Gives us many great metrics like number of 2xx, 3xx, 4xx and 5xx requests per upstream, rate limit enforcement, even time until the cert expires; - Made `Connection reset` a retry-once error. A connection reset can sometimes be indistinguishable from non-transient errors when the client is not able to inspect the response code before the reset clears the TCP buffer. We take multiple consecutive resets to mean an intentional action from the server indicating that the client is doing something wrong and retrying will not help. I have put off adding gzip compression to gateway in this PR. It was working fine with all of our services except for grafana, in which it was messing up the websocket connection for some reason. I'll dig into that separately.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12425:513,error,error,513,https://hail.is,https://github.com/hail-is/hail/pull/12425,2,['error'],"['error', 'errors']"
Availability,Also moved runAssoc code down so it only executes when needed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1475:25,down,down,25,https://hail.is,https://github.com/hail-is/hail/pull/1475,1,['down'],['down']
Availability,"Also, make batch pods eviction-safe. This should allow the cluster autoscaler to scale the cluster down, according to: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7432:99,down,down,99,https://hail.is,https://github.com/hail-is/hail/pull/7432,1,['down'],['down']
Availability,"Also, make sure we always delete the disk despite any failures",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10429:54,failure,failures,54,https://hail.is,https://github.com/hail-is/hail/pull/10429,1,['failure'],['failures']
Availability,"Although Dataproc does not have a public Spark 3-based GA release schedule yet, it'd probably be helpful to start supporting a Spark 3 build; tagging @tpoterba for context. I'm not familiar with the release process internally, so let me know what other changes need to be made to accommodate this. In particular, this PR likely needs to change the PySpark requirements specified in https://github.com/hail-is/hail/blob/main/hail/python/requirements.txt. This PR builds on changes from #9199. The code changes are due to Scala 2.12 and Spark 3 changes:. - `y` in `x << y` must be an int; - `mutable.Stack` is deprecated; - `JavaConversions` is deprecated; - `addTaskCompletionListener` is overloaded; - `Row.merge()` is deprecated. The build changes are as follows:. - Upgraded Breeze from 1.0 to 1.1 due to a known bug: https://github.com/scalanlp/breeze/issues/772; - Downgraded from Json4s 3.7.0-M5 to 3.5.3 due to a known bug: https://github.com/json4s/json4s/issues/507; - Upgraded to `scalatest 3.0.5` for Scala 2.12 compatibility; - Update the `pyspark` version in `python/requirements.txt` to match `SCALA_VERSION` during `make install-deps`. The following testing commands pass (at least to the degree that `main` does):. - `make -j8 test SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5`; - `make -j8 test SCALA_VERSION=2.12.8 SPARK_VERSION=3.0.0`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524:869,Down,Downgraded,869,https://hail.is,https://github.com/hail-is/hail/pull/9524,1,['Down'],['Downgraded']
Availability,"Although we generally want unshared IRs for downstream passes, there are many instances where we construct IRs with shared nodes (common case: using the same Ref for the table row in TableMapRows expressions). . Requiredness correctness should not be affected by node-sharing, and deduplicating every time we want to run the requiredness analysis is potentially very expensive since the goal is to be able to run it at arbitrary points in the lowering stack, so I'm going to allow Requiredness to handle shared nodes. This could potentially mess up if there are instances where a ref is created and used to represent two separate values for whatever reason, but I think we should consider that to be a bug---e.g.; ```; val r = Ref(""foo"", TInt32); If(…, ; Let(""foo"", NA(TInt32), r + 3); Let(""foo"", I32(5), r + 5)); ```; should never exist. I've done the same for ComputeUsesAndDefs, which Requiredness needs, but same comment applies---correctness should not be affected by node sharing, unless we have improperly constructed IR with scoping issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8858:44,down,downstream,44,https://hail.is,https://github.com/hail-is/hail/pull/8858,1,['down'],['downstream']
Availability,And generate nice error message on Java 7.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/773:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/issues/773,1,['error'],['error']
Availability,"And separate preemptible and non-preemptible workloads to run in their own pools. This should fix the problem where the non-preemptible pool fills up with preemptible things but k8s can't evict. When this is ready to go in, I will remove the preemptible pool toleration and redeploy the infrastructure components by hand.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636:259,toler,toleration,259,https://hail.is,https://github.com/hail-is/hail/pull/7636,1,['toler'],['toleration']
Availability,Andrea didn't get an error on single quote string comparison,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/388:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/388,1,['error'],['error']
Availability,Annotates no variants without error. See gitter discussion with @lescai.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/838:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/838,1,['error'],['error']
Availability,"Another attempt at dropdown menus. I like this better, too, for the reasons you described. Changes:; - change dropdowns to ""caret"" style, with a triangle on the top of the dropdown that points to the header item it dropped down from; - move monitoring links into their own dropdown. To issues I'm not totally happy with:; - Monitoring can't be clicked on, so it is grayed out, but styling matches hover styling for active header items; - To center the triangle under the header item, I had to measure the width of the header items in the browser first. It would be nice to do this from within CSS, but I don't know how to do that: the caret and the header item are in different parts of the DOM, and I don't know how to communicate the width of the header item to the left property of the caret. It is deployed in my namespace if you want to take a look.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7196:223,down,down,223,https://hail.is,https://github.com/hail-is/hail/pull/7196,1,['down'],['down']
Availability,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:112,error,errors,112,https://hail.is,https://github.com/hail-is/hail/issues/8343,2,"['avail', 'error']","['available', 'errors']"
Availability,"Apologies for the size, this PR got a bit out of hand. Let me know if you want me to try to break it up. Changes:; - Use custom status for pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7354:460,down,down,460,https://hail.is,https://github.com/hail-is/hail/pull/7354,1,['down'],['down']
Availability,"Apparently, creating a zip file of all our dependencies and the Hail code takes ~30s. This change skips the shadow JAR for local development. We still need a shadow JAR to produce a shareable wheel file; however, when doing local development, we can simply tell Py4J (and the JVM) where to find our dependencies class files. Also notice that I made install-editable non-PHONY. It need not be PHONY as long as we can reliably determine if `hail/python` is the pip-installed version. To do so, we simply check if the `__init__.py` at the root of the pip package is newer than when we last installed. If its newer, then either:; 1. We edited `__init__.py`, or; 2. The pip location of Hail has changed since we last ran install-editable. I also deleted eggs because nobody uses eggs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13639:416,reliab,reliably,416,https://hail.is,https://github.com/hail-is/hail/pull/13639,1,['reliab'],['reliably']
Availability,"Around 0348 I was executing `curl ci.hail.is/status` and repeatedly getting error responses, unfortunately I lost the error responses (that curl was piping into something that blew up on non-json data). The most recent response was a gateway timeout. The most recent logs are:. ```; INFO	| 2018-10-23 03:41:29,166 	| prs.py 	| heal_target:139 | deploying Nealelab/cloudtools:master; INFO	| 2018-10-23 03:41:29,350 	| prs.py 	| try_deploy:179 | already deployed c49bb905d3ba4d791150c3627c3c9ebde006a55a; INFO	| 2018-10-23 03:41:29,351 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /heal HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:42:04,032 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""POST /test-ci-6oi3jysu.batch-pods/push HTTP/1.0"" 404 -; INFO	| 2018-10-23 03:42:04,196 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""POST /test-ci-6oi3jysu.batch-pods/pull_request HTTP/1.0"" 404 -; INFO	| 2018-10-23 03:42:04,677 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""POST /test-ci-6oi3jysu.batch-pods/pull_request_review HTTP/1.0"" 404 -; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ```; [hail-ci.log](https://github.com/hail-is/hail/files/2504423/hail-ci.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4607:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/4607,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Array[Int].sum() error message leaves something to be desired,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/933:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/933,1,['error'],['error']
Availability,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8667:139,error,error,139,https://hail.is,https://github.com/hail-is/hail/pull/8667,5,"['Error', 'echo', 'error']","['Error', 'echoes', 'error']"
Availability,"As described in #9600, `hl.hadoop_ls` currently raises a NullPointerException when called with a path that does not exist. The error originates in `is.hail.io.fs.HadoopFS.listStatus`. The [hadoop.fs.FileSystem docs](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html#globStatus-org.apache.hadoop.fs.Path-) list two forms of `globStatus`:; * public FileStatus[] globStatus(Path pathPattern); * public FileStatus[] globStatus(Path pathPattern, PathFilter filter). The first makes no mention of returning null. The second however, says:; > Returns: null if pathPattern has no glob and the path does not exist an empty array if pathPattern has a glob and no path matches it else an array of FileStatus objects matching the pattern. This matches the behavior seen with `hl.hadoop_ls`. This change checks for a null value returned from `globStatus` and raises a `FileNotFoundException` in that case. Resolves #9600",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10007:127,error,error,127,https://hail.is,https://github.com/hail-is/hail/pull/10007,1,['error'],['error']
Availability,"As discussed at team meeting today, filebeat needs a toleration to run on preemptibles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6820:53,toler,toleration,53,https://hail.is,https://github.com/hail-is/hail/pull/6820,1,['toler'],['toleration']
Availability,"As discussed in #14240, we emit warnings on database deadlocks, which there are enough of to trigger noisy alerts. Since there's nothing to be done operationally (and there's no current work underway to get rid of them), these alerts only contribute to alert fatigue and hide potential problems in the system that could be addressed. This demotes a deadlock to the `info` level so we can still see how often they occur but are not alerted by them. In the future when we resolve the current deadlock we can re-escalate this error so that we can catch new deadlocks that are introduced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14251:523,error,error,523,https://hail.is,https://github.com/hail-is/hail/pull/14251,1,['error'],['error']
Availability,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8239:70,error,error,70,https://hail.is,https://github.com/hail-is/hail/issues/8239,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"As mentioned in #14580, IR can get quite big, especially as it can contain an arbitrary amount of encoded literals from the user's python session. Tested manually, by making a very very large literal, running a pipeline with it on 0.2.132, observing the failure seen in #14650, then running the same pipeline with this change, and it succeeds as normal. Resolves #14650",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14651:254,failure,failure,254,https://hail.is,https://github.com/hail-is/hail/pull/14651,1,['failure'],['failure']
Availability,"As much as possible, avoid network requests. In particular, we know the type of tables that are read in checkpoint and Expression.persist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11677:104,checkpoint,checkpoint,104,https://hail.is,https://github.com/hail-is/hail/pull/11677,1,['checkpoint'],['checkpoint']
Availability,"As of [nest_asyncio 1.5.2](https://github.com/erdewit/nest_asyncio/commit/1856573ac86954d15b0f617c97c02844bcbc7ea4), we can initialize nest_asyncio inside running loops (e.g. Jupyter). If nest_asyncio is initialized after even one task is created, [users receive inscrutable errors](https://github.com/erdewit/nest_asyncio/issues/22\#issuecomment-874710264). This error happened during the QoB workshop I ran. This change takes advantage of nest_asyncio 1.5.2 to initialize nest_asyncio before *everything*, thus ensuring it can completely patch the event loop. You can reproduce the error yourself by running `python3 -c ""import hail as hl; hl.init(billing_project=\""not-a-real-billing-project\"")""` repeatedly in a terminal. I encounter the error ~50% of the time. With this change, I did not see the error after 5 invocations of that command.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12413:275,error,errors,275,https://hail.is,https://github.com/hail-is/hail/pull/12413,5,['error'],"['error', 'errors']"
Availability,"As part of our work with generating All of Us datasets, we needed to copy around a million gcs objects. Our `Copier` infrastructure 'should' be able to handle that, but it kept falling with robustness issues. What finally worked was using GCS's [rewrite](https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite) api. This allowed us to copy data without reading it, allowing the copies to complete in a fraction of the time while also reducing bandwidth needs. There are two components to this:; 1. Research what specific APIs we can take advantage of; 2. Update our code to use them when we can, for the `Copier`, and the new sync tool (#14248). Here's the code I used for making the rewrite requests for merging a set of matrix tables together, the progress bar code was for visibility. ```python3; async def rewrite(; gfs: GoogleStorageAsyncFS,; src: str,; dst: str,; progress: Optional[rich.progress.Progress] = None,; file_tid: Optional[rich.progress.TaskID] = None,; requests_tid: Optional[rich.progress.TaskID] = None,; ):; assert (progress is None) == (file_tid is None) == (requests_tid is None); src_bkt, src_name = gfs.get_bucket_and_name(src); dst_bkt, dst_name = gfs.get_bucket_and_name(dst); if not src_name:; raise IsABucketError(src); if not dst_name:; raise IsABucketError(dst); client = gfs._storage_client; path = (; f'/b/{src_bkt}/o/{urllib.parse.quote(src_name, safe="""")}/rewriteTo'; f'/b/{dst_bkt}/o/{urllib.parse.quote(dst_name, safe="""")}'; ); kwargs = {'json': '', 'params': {}}; client._update_params_with_user_project(kwargs, src_bkt); response = await retry_transient_errors(client.post, path, **kwargs); if progress is not None:; progress.update(requests_tid, advance=1); while not response['done']:; kwargs['params']['rewriteToken'] = response['rewriteToken']; response = await retry_transient_errors(client.post, path, **kwargs); if progress is not None:; progress.update(requests_tid, advance=1); if progress is not None:; progress.update(file_tid, advance=1)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14601:190,robust,robustness,190,https://hail.is,https://github.com/hail-is/hail/issues/14601,1,['robust'],['robustness']
Availability,"As the docs state, inParX should be ""true if in pseudo-autosomal region on chromosome X."" Likewise, inParY should be ""true if in pseudo-autosomal region on chromosome Y."" However, these flags are currently being applied regardless of chromosome. That is, currently any variant that meets (60001 <= start && start <= 2699520) || (154931044 <= start && start <= 155260560) gets inParX = true, and similarly for inParY. This is confusing and, I would guess, in error. inParX should only be able to be true if v.contig == ""X"" is true, and inParY should only be able to be true if v.contig == ""Y"" is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/452:458,error,error,458,https://hail.is,https://github.com/hail-is/hail/issues/452,1,['error'],['error']
Availability,Assertion Error when reading table,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4325:10,Error,Error,10,https://hail.is,https://github.com/hail-is/hail/issues/4325,1,['Error'],['Error']
Availability,Assertion error related to ndarray/extract,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/8325,1,['error'],['error']
Availability,Assertion error when filtering rows based on locus position,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['error'],['error']
Availability,Assertion error when using mt.annotate_rows() with different partition keys,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3119:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/3119,1,['error'],['error']
Availability,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4974:682,alive,alive,682,https://hail.is,https://github.com/hail-is/hail/pull/4974,5,"['alive', 'error']","['alive', 'error', 'errors']"
Availability,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6078:583,error,error,583,https://hail.is,https://github.com/hail-is/hail/pull/6078,1,['error'],['error']
Availability,"At some point (highly likely that it was the Ubuntu 20.04 -> 22.04 upgrade) Batch went from using cgroups v1 to cgroups v2 for setting containers' CPU and memory limits. We mostly don't touch cgroups, the container runtime handles that for us, but we poll the `cgroupfs` for recoding memory usage and CPU utilization. The accounting mechanism changed between v1 and v2 so batch was silently failing to collect these metrics. Deploying these changes into my namespace got me back the following plots (compiling hail):. <img width=""701"" alt=""Screenshot 2023-09-14 at 5 47 24 PM"" src=""https://github.com/hail-is/hail/assets/24440116/0f470e5a-7feb-4b9e-bac6-f560c8366d8e"">. The reason why we fail silently when the file doesn't exist is because we are letting the container runtime manage the cgroup, and there is a race condition between the container exiting + the cgroup getting destroyed and our polling of this file. We could probably do a better job reporting an error, like this though, perhaps logging errors if we fail to read this file more than X number of times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13626:965,error,error,965,https://hail.is,https://github.com/hail-is/hail/pull/13626,2,['error'],"['error', 'errors']"
Availability,Attempting to export NA loci gives bad error message.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4011:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/issues/4011,1,['error'],['error']
Availability,Attempting to install hail on dataproc using the `init_notebook.py` script errors out because hail depends on a more recent version of `jinja2` than the version of `notebook` pinned in the script allows for. This change upgrades the pinned version of `notebook` in the script to be compatible with hail's pinned `jinja2` version. Fixes #12926.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12965:75,error,errors,75,https://hail.is,https://github.com/hail-is/hail/pull/12965,1,['error'],['errors']
Availability,Azure default credentials will use the metadata server when available so we can just use those instead of manually reaching out to the metadata server.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13225:60,avail,available,60,https://hail.is,https://github.com/hail-is/hail/pull/13225,1,['avail'],['available']
Availability,"B, free 434.4 MiB); 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1575,Error,ErrorHandling,1575,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Error'],['ErrorHandling']
Availability,Bad error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9163:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/9163,1,['error'],['error']
Availability,Bad error message for filter_alleles exception,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['error'],['error']
Availability,Bad error message prints Java Bytecode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1705:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/1705,1,['error'],['error']
Availability,Bad error message when indexing a matrix table with itself,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9121:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/9121,1,['error'],['error']
Availability,Bad error message when passing `dict` with a `None` key,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5700:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/5700,1,['error'],['error']
Availability,Bad error message when using a non-aggregator expr in aggregate,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/4110,1,['error'],['error']
Availability,"Bad error message: MatrixTable and Table files are directories, path 'blah' is not a directory",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10843:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/10843,1,['error'],['error']
Availability,Bad error on different length division,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3653:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/3653,1,['error'],['error']
Availability,Bad error on key mismatch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4951:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/4951,1,['error'],['error']
Availability,Bad error on old agg.filter syntax,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4770:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/4770,1,['error'],['error']
Availability,Bad error on transmuting a key,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4085:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/4085,1,['error'],['error']
Availability,Bad error when using `annotate_cols` instead of `annotate_rows`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5415:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/5415,1,['error'],['error']
Availability,"Bascially grabbed the relevant bits from SparkBackend and ServiceBackend. Enabled by setting HAIL_QUERY_BACKEND=local. Needs HAIL_HOME and SPARK_HOME set to find jars, and hardcodes the py4j jar version that comes with Spark 2.4.x. Will have to work on ripping out Spark dependency. Currently uses HadoopFS for the file system in Java. GoogleFS in Python works with gs:// or local files, I just copied it and ripped out the Google stuff. Some some rough ideas from some of your old work, @johnc1231 (py4jbackend). Current results on the Python tests:. > == 470 failed, 245 passed, 87 skipped, 15 warnings, 1 error in 270.61 seconds ==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8860:608,error,error,608,https://hail.is,https://github.com/hail-is/hail/pull/8860,1,['error'],['error']
Availability,Based on https://clang.llvm.org/cxx_status.html and https://gcc.gnu.org/gcc-4.7/cxx0x_status.html Clang 3.3 and GCC 4.7 should be sufficient to prevent compile errors arising from not supporting C++11 features. FYI @cseed @tpoterba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1340:160,error,errors,160,https://hail.is,https://github.com/hail-is/hail/pull/1340,1,['error'],['errors']
Availability,"Based on https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/resource.20table.20query.20woes/near/338629272, merge to `9e0081c` and future commits (none of which are available now) need to be merged manually.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12744:184,avail,available,184,https://hail.is,https://github.com/hail-is/hail/pull/12744,1,['avail'],['available']
Availability,"Basically all naming, rids these test files of linting errors. We do a lot of reassigning a `BatchBuilder` variable to a `Batch` and so I consolidated around `bb` and `b`. A couple instances where I remove debug_info from an assert statement is because the associated `Batch` object would not exist, since that assert is triggered by an error that's raised before the `Batch` object is created.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12147:55,error,errors,55,https://hail.is,https://github.com/hail-is/hail/pull/12147,2,['error'],"['error', 'errors']"
Availability,Batch Driver Does not shut down tasks in the correct order,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13324:27,down,down,27,https://hail.is,https://github.com/hail-is/hail/issues/13324,1,['down'],['down']
Availability,Batch dropped a job that error'ed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4591:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/4591,1,['error'],['error']
Availability,"Batch page shows max 50 jobs paginated, with next page button. Search box supports search with a simple language of terms:; - k=v - match jobs with attribute key k and value v; - has:k - match jobs with attribute k, any value; - state - match jobs with the corresponding states. Search state terms are lower case (unlike actual job states, which I plan to change) and the recognized state terms are:. ```; state_query_values = {; 'pending': ['Pending'],; 'ready': ['Ready'],; 'running': ['Running'],; 'live': ['Ready', 'Running'],; 'cancelled': ['Cancelled'],; 'error': ['Error'],; 'failed': ['Failed'],; 'bad': ['Error', 'Failed'],; 'success': ['success'],; 'done': ['Cancelled', 'Error', 'Failed', 'Success']; }; ```; as you can see, some state search terms, like done, match multiple job states. ; - !term - match jobs that don't match term. To select specific names, you can do `name=foo`. Next steps:; - Make the corresponding changes to the API so you can iterate paginated through all jobs in a batch.; - Make batches page paginated, too.; - Help information about the search syntax.; - We'll probably want to order by fields other than just id.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7635:562,error,error,562,https://hail.is,https://github.com/hail-is/hail/pull/7635,4,"['Error', 'error']","['Error', 'error']"
Availability,Batches themselves can now have callbacks. You receive a callback for each member job's completion (success or failure). CI will use this to wait for completion of the entire DAG. Stacked on https://github.com/hail-is/hail/pull/4930,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5035:111,failure,failure,111,https://hail.is,https://github.com/hail-is/hail/pull/5035,1,['failure'],['failure']
Availability,"Because HttpResponseException is a subclass of IOException, the match clause for HttpResponseException in isRetryOnceError is unreachable. As a result, account-not-found errors for Google Cloud Storage are not handled correctly. This change reorders the match clauses accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12166:170,error,errors,170,https://hail.is,https://github.com/hail-is/hail/pull/12166,1,['error'],['errors']
Availability,"Because `HttpResponseException` is a subclass of `IOException`, the `match` clause for `HttpResponseException` in `isRetryOnceError` is unreachable. As a result, account-not-found errors for Google Cloud Storage are not handled correctly. This change reorders the `match` clauses accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12160:180,error,errors,180,https://hail.is,https://github.com/hail-is/hail/pull/12160,1,['error'],['errors']
Availability,"Before we can simplify the binding structure, we need to stop duplicating it all over the place. This PR rewrites `FreeVariables` so that it no longer needs special logic for particular nodes, hard coding binding structure (redundantly). To do this, it takes advantage of the new `Bindings`, which operates on a `GenericBindingEnv` interface. It adds a new implementation of this interface specifically for computing free variables, then simply does a generic traversal of the IR using this custom binging environment. While I find the new implementation far simpler and more obviously correct than the old, I do expect it to further simplify once I'm able to start modifying the core binding structure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14451:224,redundant,redundantly,224,https://hail.is,https://github.com/hail-is/hail/pull/14451,1,['redundant'],['redundantly']
Availability,"Below is the first part of the stack trace for for a job using pyhail with a user error in the expression language. The text on line 7 isn't in the `utils.py` from pyspark but in my provided `utils.py` (though the error certainly did come from pyspark `utils.py`). ```; Traceback (most recent call last):; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/genomes_qc.py"", line 161, in <module>; .export_variants(rf_path + "".va.txt.bgz"", "","".join(out_metrics)); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 489, in export_variants; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/utils.py"", line 209, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; expression.append('va.calldata.%(pop_upper)s = gs.filter(g => %(criterion)s == ""%(pop)s"").callStats(v)' % input_dict); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o93.run.; : org.broadinstitute.hail.utils.package$FatalException: `Struct' has no field `type'; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1184:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/issues/1184,3,['error'],['error']
Availability,"Ben came across an image in the wild with a null `Env` field in the manifest, which caused the following error:; ```; Error; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 868, in _run; timed_out = await self._run_until_done_or_deleted(self._run_container); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1010, in _run_until_done_or_deleted; return await run_until_done_or_deleted(self.deleted_event, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 680, in run_until_done_or_deleted; return step.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1066, in _run_container; await self._write_container_config(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1106, in _write_container_config; config = await self.container_config(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1166, in container_config; 'env': self._env(),; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1354, in _env; self.image.image_config['Config']['Env'] + self.env + CLOUD_WORKER_API.cloud_specific_env_vars_for_user_jobs; TypeError: unsupported operand type(s) for +: 'NoneType' and 'list'; ```. He fixed it by creating the following docker image:. ```docker; FROM jargene/hapice:1.0; ```. It could be that old versions of docker allowed this to be empty but have since made it `[]`, which would mean this would be unfortunately very annoying to test but nonetheless pretty trivial to fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13720:105,error,error,105,https://hail.is,https://github.com/hail-is/hail/pull/13720,2,"['Error', 'error']","['Error', 'error']"
Availability,Better Fam file error reporting,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/709:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/issues/709,1,['error'],['error']
Availability,Better HailContext error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3586:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/3586,1,['error'],['error']
Availability,Better Python Errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9398:14,Error,Errors,14,https://hail.is,https://github.com/hail-is/hail/pull/9398,1,['Error'],['Errors']
Availability,Better VEP error 2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4426:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/4426,1,['error'],['error']
Availability,Better catch-all error message for weird out-of-place exprs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1626:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/1626,1,['error'],['error']
Availability,Better error checking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1440:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/1440,1,['error'],['error']
Availability,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2330:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/2330,3,"['error', 'failure']","['error', 'failure']"
Availability,Better error message for Expression.__nonzero__,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2774:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/2774,1,['error'],['error']
Availability,Better error message for filtervariants,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/383:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/issues/383,1,['error'],['error']
Availability,Better error message for linear dependence of covariates in regression models,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1156:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/issues/1156,1,['error'],['error']
Availability,Better error message for spark.serializer unset,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3865:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/3865,1,['error'],['error']
Availability,"Better error message for unix files not preceded with ""file:///""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/381:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/issues/381,1,['error'],['error']
Availability,Better error message on interval parse failure,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5758:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/5758,2,"['error', 'failure']","['error', 'failure']"
Availability,Better error messages,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/163:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/issues/163,1,['error'],['error']
Availability,Better error messages for __getitem__,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3584:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/3584,1,['error'],['error']
Availability,Better error messages from returned aggregable types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1492:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/1492,1,['error'],['error']
Availability,"Better error on ""if expression""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4172:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/4172,1,['error'],['error']
Availability,Better errors for hl.utils.range_table / range_matrix_table,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3680:7,error,errors,7,https://hail.is,https://github.com/hail-is/hail/pull/3680,1,['error'],['errors']
Availability,Better errors for read,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3146:7,error,errors,7,https://hail.is,https://github.com/hail-is/hail/pull/3146,1,['error'],['errors']
Availability,Better errors on read_table/read_matrix_table for 0.1 formats,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3911:7,error,errors,7,https://hail.is,https://github.com/hail-is/hail/pull/3911,1,['error'],['errors']
Availability,Better fam file error messages,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/854:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/854,1,['error'],['error']
Availability,Better orderedRVD partition errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3997:28,error,errors,28,https://hail.is,https://github.com/hail-is/hail/pull/3997,1,['error'],['errors']
Availability,Both Cotton and I have PRs failing due to pod failures that are impossible to debug without this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9781:46,failure,failures,46,https://hail.is,https://github.com/hail-is/hail/pull/9781,1,['failure'],['failures']
Availability,"Both of these support a number of features not available in hail.plot:; - interactive legend (click to hide/show elements); - labelling with continuous expressions; - labelling using multiple expressions (will display a dropdown selection widget); - specifying color schemes; - hovering on points displays their coordinates, labels and additional source fields; - legend is also displayed outside of the plotting space to be unobtrusive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5601:47,avail,available,47,https://hail.is,https://github.com/hail-is/hail/pull/5601,1,['avail'],['available']
Availability,"Building from source - hail 0.2.74. `$ git clone https://github.com/hail-is/hail.git`; `$ cd hail`; `$ git checkout tags/0.2.74`; `$ cd hail`; `$ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12 SPARK_VERSION=3.1.2`; `...`; `curl -sSL https://storage.googleapis.com/hail-common/libsimdpp-2.1.tar.gz > libsimdpp-2.1.tar.gz; tar -xzf libsimdpp-2.1.tar.gz`. `c++ -o build/ibs.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include/darwin -MD -MF build/ibs.d -MT build/ibs.o -c ibs.cpp`. `make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.`; `make: *** [native-lib-prebuilt] Error 2`. I've installed lz4 and still get this error. . https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements no longer documents lz4 as required, but 0.2.74 is breaking on it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10747:838,Error,Error,838,https://hail.is,https://github.com/hail-is/hail/issues/10747,2,"['Error', 'error']","['Error', 'error']"
Availability,"Builds on #3500 . This PR introduces support for sparse block matrices. The only new command exposed in Python is `sparsify_row_intervals`. Matrix product currently always results in a dense block matrix. Sparse block matrices also support transpose, diagonal, and all non-mathematical operations except filtering. Element-wise mathematical operations are currently supported if and only if they cannot transform zeroed blocks to non-zero blocks. For example, all forms of element-wise multiplication are supported,; and element-wise multiplication results in a sparse block matrix with block support equal to the intersection of that of the operands. On the other hand, scalar addition is not supported, and matrix addition is supported only between block matrices with the same block sparsity. Once this is in, I'll expose a couple more sparsifiers (rectangles, band) and also bring in parallel export of many rectangles to TSV from another branch, so that users can proceed with LD / fine mapping applications. Down the line I plan to support for all operations by expanding to union of block support, or to all blocks, for some operations. And matrix multiplication ought to return the minimal number of blocks as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3501:1014,Down,Down,1014,https://hail.is,https://github.com/hail-is/hail/pull/3501,1,['Down'],['Down']
Availability,Builds on: https://github.com/hail-is/hail/pull/2825. added RVD (should be UnpartitionedRVD) and OrderedRVD; allows to add new rvd types (HashedRVD); added list of partition files to current specs (to support safe object storage write strategy in presence of failure),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2828:259,failure,failure,259,https://hail.is,https://github.com/hail-is/hail/pull/2828,1,['failure'],['failure']
Availability,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5121:445,Failure,Failures,445,https://hail.is,https://github.com/hail-is/hail/pull/5121,3,"['Failure', 'down', 'failure']","['Failures', 'down', 'failure']"
Availability,Bump de.undercouch.download from 3.2.0 to 5.2.1 in /hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:19,down,download,19,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['down'],['download']
Availability,Bump de.undercouch.download from 3.2.0 to 5.3.0 in /hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:19,down,download,19,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download']
Availability,Bump de.undercouch.download from 5.3.0 to 5.3.1 in /hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:19,down,download,19,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['down'],['download']
Availability,Bump de.undercouch.download from 5.3.1 to 5.4.0 in /hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:19,down,download,19,https://hail.is,https://github.com/hail-is/hail/pull/12893,1,['down'],['download']
Availability,"Bumps [aiodns](https://github.com/saghul/aiodns) from 2.0.0 to 3.0.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/saghul/aiodns/releases"">aiodns's releases</a>.</em></p>; <blockquote>; <h2>3.0.0</h2>; <ul>; <li>Release wheels and source to PyPI with GH actions</li>; <li>Try to make tests more resilient</li>; <li>Don't build universal wheels</li>; <li>Migrate CI to GH Actions</li>; <li>Fix TXT CHAOS test</li>; <li>Add support for CAA queries</li>; <li>Support Python &gt;= 3.6</li>; <li>Bump pycares dependency</li>; <li>Drop tasks.py</li>; <li>Allow specifying dnsclass for queries</li>; <li>Set URL to https</li>; <li>Add license args in setup.py</li>; <li>Converted Type Annotations to Py3 syntax Closes</li>; <li>Only run mypy on cpython versions</li>; <li>Also fix all type errors with latest mypy - pycares seems to have no typing / stubs so lets ignore it via <code>mypy.ini</code></li>; <li>setup: typing exists since Python 3.5</li>; <li>Fix type annotation of gethostbyname()</li>; <li>Updated README</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/saghul/aiodns/blob/master/ChangeLog"">aiodns's changelog</a>.</em></p>; <blockquote>; <h1>3.0.0</h1>; <ul>; <li>Release wheels and source to PyPI with GH actions</li>; <li>Try to make tests more resilient</li>; <li>Don't build universal wheels</li>; <li>Migrate CI to GH Actions</li>; <li>Fix TXT CHAOS test</li>; <li>Add support for CAA queries</li>; <li>Support Python &gt;= 3.6</li>; <li>Bump pycares dependency</li>; <li>Drop tasks.py</li>; <li>Allow specifying dnsclass for queries</li>; <li>Set URL to https</li>; <li>Add license args in setup.py</li>; <li>Converted Type Annotations to Py3 syntax Closes</li>; <li>Only run mypy on cpython versions</li>; <li>Also fix all type errors with latest mypy - pycares seems to have no typing / stubs so lets ignore it via <code>mypy.ini</code></li>; <li>setup:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11570:344,resilien,resilient,344,https://hail.is,https://github.com/hail-is/hail/pull/11570,2,"['error', 'resilien']","['errors', 'resilient']"
Availability,"Bumps [aiodocker](https://github.com/aio-libs/aiodocker) from 0.17.0 to 0.21.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiodocker/releases"">aiodocker's releases</a>.</em></p>; <blockquote>; <h2>aiodocker 0.18.0 release</h2>; <h2>Features</h2>; <ul>; <li>Improve the error text message if cannot connect to docker engine. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/411"">#411</a>)</li>; <li>Implement docker exec protocol. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/415"">#415</a>)</li>; <li>Implement container commit, pause and unpause functionality. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/418"">#418</a>)</li>; <li>Implement auto-versioning of the docker API by default. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/419"">#419</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiodocker/blob/master/CHANGES.rst"">aiodocker's changelog</a>.</em></p>; <blockquote>; <h1>0.21.0 (2021-07-23)</h1>; <h2>Bugfixes</h2>; <ul>; <li>Use ssl_context passsed to Docker constructor for creating underlying connection to docker engine. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/536"">#536</a>)</li>; <li>Fix an error when attach/exec when container stops before close connection to it. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/608"">#608</a>)</li>; </ul>; <h1>0.20.0 (2021-07-21)</h1>; <h2>Bugfixes</h2>; <ul>; <li>Accept auth parameter by <code>run()</code> method; it allows auto-pulling absent image from private storages. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/295"">#295</a>)</li>; <li>Fix passing of JSON params. (<a href=""https://github-redirect.dependabot.com/aio-libs/aiodocker/issues/543"">#543</a>)</li>; <li>Fix issue wit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11537:329,error,error,329,https://hail.is,https://github.com/hail-is/hail/pull/11537,1,['error'],['error']
Availability,"Bumps [aiohttp](https://github.com/aio-libs/aiohttp) from 3.8.1 to 3.8.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/releases"">aiohttp's releases</a>.</em></p>; <blockquote>; <h2>3.8.3</h2>; <p>.. attention::</p>; <p>This is the last :doc:<code>aiohttp &lt;index&gt;</code> release tested under; Python 3.6. The 3.9 stream is dropping it from the CI and the; distribution package metadata.</p>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Increased the upper boundary of the :doc:<code>multidict:index</code> dependency; to allow for the version 6 -- by :user:<code>hugovk</code>.</p>; <p>It used to be limited below version 7 in :doc:<code>aiohttp &lt;index&gt;</code> v3.8.1 but; was lowered in v3.8.2 via :pr:<code>6550</code> and never brought back, causing; problems with dependency pins when upgrading. :doc:<code>aiohttp &lt;index&gt;</code> v3.8.3; fixes that by recovering the original boundary of <code>&lt; 7</code>.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/6950"">#6950</a>)</p>; </li>; </ul>; <hr />; <h1>3.8.2 (2022-09-20, subsequently yanked on 2022-09-21)</h1>; <p>.. note::</p>; <p>This release has some compatibility fixes for Python 3.11 but it may; still have some quirks. Some tests are still flaky in the CI.</p>; <p>.. caution::</p>; <p>This release has been yanked from PyPI. Modern pip will not pick it; up automatically. The reason is that is has <code>multidict &lt; 6</code> set in; the distribution package metadata (see :pr:<code>6950</code>). Please, use; <code>aiohttp ~= 3.8.3, != 3.8.1</code> instead, if you can.</p>; <h2>Bugfixes</h2>; <ul>; <li>Added support for registering :rfc:<code>OPTIONS &lt;9110#OPTIONS&gt;</code>; HTTP method handlers via :py:class:<code>~aiohttp.web.RouteTableDef</code>.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/4663"">#4663</a>)</li>; <li>Started supporting :rfc:<code>authority-form &lt;9112#authority-form&gt;</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12296:929,recover,recovering,929,https://hail.is,https://github.com/hail-is/hail/pull/12296,1,['recover'],['recovering']
Availability,"Bumps [aiohttp](https://github.com/aio-libs/aiohttp) from 3.8.4 to 3.8.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/releases"">aiohttp's releases</a>.</em></p>; <blockquote>; <h2>3.8.5</h2>; <h2>Security bugfixes</h2>; <ul>; <li>; <p>Upgraded the vendored copy of llhttp_ to v8.1.1 -- by :user:<code>webknjaz</code>; and :user:<code>Dreamsorcerer</code>.</p>; <p>Thanks to :user:<code>sethmlarson</code> for reporting this and providing us with; comprehensive reproducer, workarounds and fixing details! For more; information, see; <a href=""https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w"">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-45c4-8wx5-qw6w</a>.</p>; <p>.. _llhttp: <a href=""https://llhttp.org"">https://llhttp.org</a></p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7346"">#7346</a>)</p>; </li>; </ul>; <h2>Features</h2>; <ul>; <li>; <p>Added information to C parser exceptions to show which character caused the error. -- by :user:<code>Dreamsorcerer</code></p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7366"">#7366</a>)</p>; </li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Fixed a transport is :data:<code>None</code> error -- by :user:<code>Dreamsorcerer</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/3355"">#3355</a>)</p>; </li>; </ul>; <hr />; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/v3.8.5/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.8.5 (2023-07-19)</h1>; <h2>Security bugfixes</h2>; <ul>; <li>; <p>Upgraded the vendored copy of llhttp_ to v8.1.1 -- by :user:<code>webknjaz</code>; and :user:<code>Dreamsorcerer</code>.</p>; <p>Thanks to :user:<code>sethmlarson</code> for reporting this and providing us with; comprehensive reproducer, workarounds and fixing details! For mo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13270:1055,error,error,1055,https://hail.is,https://github.com/hail-is/hail/pull/13270,5,['error'],['error']
Availability,"Bumps [async-timeout](https://github.com/aio-libs/async-timeout) from 3.0.1 to 4.0.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/async-timeout/releases"">async-timeout's releases</a>.</em></p>; <blockquote>; <h2>v4.0.2</h2>; <h2>Misc</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/259"">#259</a>, <a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a></li>; </ul>; <h2>v4.0.1</h2>; <ul>; <li>; <p>Fix regression:</p>; <ol>; <li>; <p>Don't raise TimeoutError from timeout object that doesn't enter into async context; manager</p>; </li>; <li>; <p>Use call_soon() for raising TimeoutError if deadline is reached on entering into; async context manager</p>; </li>; </ol>; <p>(<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/258"">#258</a>)</p>; </li>; <li>; <p>Make <code>Timeout</code> class available in <code>__all__</code>.</p>; </li>; </ul>; <h2>async-timeout 4.0.0</h2>; <h1>Changes</h1>; <ul>; <li>; <p>Implemented <code>timeout_at(deadline)</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/117"">#117</a>)</p>; </li>; <li>; <p>Supported <code>timeout.deadline</code> and <code>timeout.expired</code> properties.</p>; </li>; <li>; <p>Drooped <code>timeout.remaining</code> property: it can be calculated as; <code>timeout.deadline - loop.time()</code></p>; </li>; <li>; <p>Dropped <code>timeout.timeout</code> property that returns a relative timeout based on the; timeout object creation time; the absolute <code>timeout.deadline</code> should be used; instead.</p>; </li>; <li>; <p>Added the deadline modification methods: <code>timeout.reject()</code>,; <code>timeout.shift(delay)</code>, <code>timeout.update(deadline)</code>.</p>; </li>; <li>; <p>Deprecated synchronous context manager usage</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:961,avail,available,961,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['avail'],['available']
Availability,"Bumps [authlib](https://github.com/lepture/authlib) from 0.11 to 0.15.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/authlib/releases"">authlib's releases</a>.</em></p>; <blockquote>; <h2>Version 0.15.5</h2>; <ul>; <li>Make Authlib compatible with latest httpx</li>; <li>Make Authlib compatible with latest werkzeug</li>; <li>Allow customize RFC7523 <code>alg</code> value</li>; </ul>; <h2>Version 0.15.4</h2>; <p>Security fix when JWT claims is None.</p>; <p>For example, JWT payload has <code>iss=None</code>:</p>; <pre><code>{; &quot;iss&quot;: None,; ...; }; </code></pre>; <p>But we need to decode it with claims:</p>; <pre><code>claims_options = {; 'iss': {'essential': True, 'values': ['required']}; }; jwt.decode(token, key, claims_options=claims_options); </code></pre>; <p>It didn't raise an error before this fix.</p>; <h2>Version 0.15.3</h2>; <p>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/308"">lepture/authlib#308</a></p>; <h2>Version 0.15.2</h2>; <p>Fixed httpx authentication bug via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/283"">#283</a></p>; <h2>Version 0.15.1</h2>; <p>Backward compitable fix for using JWKs in JWT, via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/280"">#280</a>.</p>; <h2>Version 0.15</h2>; <p>This is the last release before v1.0. In this release, we added more RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK) Thumbprint</li>; </ul>; <p>We also fixed bugs for integrations:</p>; <ul>; <li>Fixed support for HTTPX&gt;=0.14.3</li>; <li>Added OAuth clients of HTTPX back via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/270"">#270</a></li>; <li>Fixed parallel t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:859,error,error,859,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['error'],['error']
Availability,"Bumps [azure-identity](https://github.com/Azure/azure-sdk-for-python) from 1.8.0 to 1.9.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Azure/azure-sdk-for-python/releases"">azure-identity's releases</a>.</em></p>; <blockquote>; <h2>azure-identity_1.9.0</h2>; <h2>1.9.0 (2022-04-05)</h2>; <h3>Features Added</h3>; <ul>; <li>Added PII logging if logging.DEBUG is enabled. (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23203"">#23203</a>)</li>; </ul>; <h3>Breaking Changes</h3>; <ul>; <li><code>validate_authority</code> support is not available in 1.9.0.</li>; </ul>; <h3>Bugs Fixed</h3>; <ul>; <li>Added check on <code>content</code> from msal response. (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/23483"">#23483</a>)</li>; <li>Fixed the issue that async OBO credential does not refresh correctly. (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/21981"">#21981</a>)</li>; </ul>; <h3>Other Changes</h3>; <ul>; <li>Removed <code>resource_id</code>, please use <code>identity_config</code> instead.</li>; <li>Renamed argument name <code>get_assertion</code> to <code>func</code> for <code>ClientAssertionCredential</code>.</li>; </ul>; <h2>azure-identity_1.9.0b1</h2>; <h2>1.9.0b1 (2022-03-08)</h2>; <h3>Features Added</h3>; <ul>; <li>Added <code>validate_authority</code> support for msal client (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22625"">#22625</a>)</li>; <li>Added <code>resource_id</code> support for user-assigned managed identity (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22329"">#22329</a>)</li>; <li>Added <code>ClientAssertionCredential</code> support (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/22328"">#22328</a>)</li>; <li>Updated App service API version to &quot;2019-08-01&quot; (<a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11752:618,avail,available,618,https://hail.is,https://github.com/hail-is/hail/pull/11752,1,['avail'],['available']
Availability,"Bumps [azure-storage-blob](https://github.com/Azure/azure-sdk-for-python) from 12.11.0 to 12.13.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Azure/azure-sdk-for-python/releases"">azure-storage-blob's releases</a>.</em></p>; <blockquote>; <h2>azure-storage-blob_12.13.1</h2>; <h2>12.13.1 (2022-08-04)</h2>; <h3>Bugs Fixed</h3>; <ul>; <li>Fixed two rare issues with ranged blob download when using client-side encryption V1 or V2.</li>; </ul>; <h2>azure-storage-blob_12.13.0</h2>; <h2>12.13.0 (2022-07-07)</h2>; <h3>Bugs Fixed</h3>; <ul>; <li>Stable release of features from 12.13.0b1.</li>; <li>Added support for deleting versions in <code>delete_blobs</code> by supplying <code>version_id</code>.</li>; </ul>; <h2>azure-storage-blob_12.13.0b1</h2>; <h2>12.13.0b1 (2022-06-15)</h2>; <h3>Features Added</h3>; <ul>; <li>Added support for service version 2021-08-06.</li>; <li>Added a new version of client-side encryption for blobs (version 2.0) which utilizes AES-GCM-256 encryption.; If you are currently using client-side encryption, it is <strong>highly recommended</strong> to switch to a form of server-side; encryption (Customer-Provided Key, Encryption Scope, etc.) or version 2.0 of client-side encryption. The encryption; version can be specified on any client constructor via the <code>encryption_version</code> keyword (<code>encryption_version='2.0'</code>).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/13989b5b1253e26f3f3ee24013a3013fea1bdf73""><code>13989b5</code></a> [Storage] Fix ranged download for client-side encryption (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-python/issues/25522"">#25522</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-python/commit/e90af4374bfd7c139737ad2888fcd269b3023520""><code>e90af43</code></a> DataLake funny dependency (<a href=""https://github-redirect.depen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12109:427,down,download,427,https://hail.is,https://github.com/hail-is/hail/pull/12109,1,['down'],['download']
Availability,"Bumps [boto3](https://github.com/boto/boto3) from 1.17.54 to 1.21.13.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.21.13</h1>; <ul>; <li>api-change:<code>synthetics</code>: [<code>botocore</code>] Allow custom handler function.</li>; <li>api-change:<code>transfer</code>: [<code>botocore</code>] Add waiters for server online and offline.</li>; <li>api-change:<code>devops-guru</code>: [<code>botocore</code>] Amazon DevOps Guru now integrates with Amazon CodeGuru Profiler. You can view CodeGuru Profiler recommendations for your AWS Lambda function in DevOps Guru. This feature is enabled by default for new customers as of 3/4/2022. Existing customers can enable this feature with UpdateEventSourcesConfig.</li>; <li>api-change:<code>macie</code>: [<code>botocore</code>] Amazon Macie Classic (macie) has been discontinued and is no longer available. A new Amazon Macie (macie2) is now available with significant design improvements and additional features.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] Documentation updates for Amazon EC2.</li>; <li>api-change:<code>sts</code>: [<code>botocore</code>] Documentation updates for AWS Security Token Service.</li>; <li>api-change:<code>connect</code>: [<code>botocore</code>] This release updates the *InstanceStorageConfig APIs so they support a new ResourceType: REAL_TIME_CONTACT_ANALYSIS_SEGMENTS. Use this resource type to enable streaming for real-time contact analysis and to associate the Kinesis stream where real-time contact analysis segments will be published.</li>; </ul>; <h1>1.21.12</h1>; <ul>; <li>api-change:<code>greengrassv2</code>: [<code>botocore</code>] Doc only update that clarifies Create Deployment section.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for data repository associations to use root (&quot;/&quot;) as the fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:980,avail,available,980,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['avail'],['available']
Availability,"Bumps [boto3](https://github.com/boto/boto3) from 1.26.7 to 1.26.17.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.17</h1>; <ul>; <li>bugfix:dynamodb: Fixes duplicate serialization issue in DynamoDB BatchWriter</li>; <li>api-change:<code>backup</code>: [<code>botocore</code>] AWS Backup introduces support for legal hold and application stack backups. AWS Backup Audit Manager introduces support for cross-Region, cross-account reports.</li>; <li>api-change:<code>cloudwatch</code>: [<code>botocore</code>] Update cloudwatch client to latest version</li>; <li>api-change:<code>drs</code>: [<code>botocore</code>] Non breaking changes to existing APIs, and additional APIs added to support in-AWS failing back using AWS Elastic Disaster Recovery.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for ECS Service Connect, a new capability that simplifies writing and operating resilient distributed applications. This release updates the TaskDefinition, Cluster, Service mutation APIs with Service connect constructs and also adds a new ListServicesByNamespace API.</li>; <li>api-change:<code>efs</code>: [<code>botocore</code>] Update efs client to latest version</li>; <li>api-change:<code>iot-data</code>: [<code>botocore</code>] This release adds support for MQTT5 properties to AWS IoT HTTP Publish API.</li>; <li>api-change:<code>iot</code>: [<code>botocore</code>] Job scheduling enables the scheduled rollout of a Job with start and end times and a customizable end behavior when end time is reached. This is available for continuous and snapshot jobs. Added support for MQTT5 properties to AWS IoT TopicRule Republish Action.</li>; <li>api-change:<code>iotwireless</code>: [<code>botocore</code>] This release includes a new feature for customers to calculate the position of their devices by adding three new APIs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:876,Recover,Recovery,876,https://hail.is,https://github.com/hail-is/hail/pull/12507,2,"['Recover', 'resilien']","['Recovery', 'resilient']"
Availability,"Bumps [click](https://github.com/pallets/click) from 8.1.1 to 8.1.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/releases"">click's releases</a>.</em></p>; <blockquote>; <h2>8.1.2</h2>; <p>This is a fix release for the <a href=""https://github.com/pallets/click/releases/tag/8.1.0"">8.1.0</a> feature release.</p>; <ul>; <li>Changes: <a href=""https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-2"">https://click.palletsprojects.com/en/8.1.x/changes/#version-8-1-2</a></li>; <li>Milestone: <a href=""https://github.com/pallets/click/milestone/17?closed=1"">https://github.com/pallets/click/milestone/17?closed=1</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/click/blob/main/CHANGES.rst"">click's changelog</a>.</em></p>; <blockquote>; <h2>Version 8.1.2</h2>; <p>Released 2022-03-31</p>; <ul>; <li>Fix error message for readable path check that was mixed up with the; executable check. :pr:<code>2236</code></li>; <li>Restore parameter order for <code>Path</code>, placing the <code>executable</code>; parameter at the end. It is recommended to use keyword arguments; instead of positional arguments. :issue:<code>2235</code></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/click/commit/030f53cf677ee1de534359c535d465eed0ec1d99""><code>030f53c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2238"">#2238</a> from pallets/release-8.1.2</li>; <li><a href=""https://github.com/pallets/click/commit/2f1c35a43652e565802c230dbc47a9a358a0c6fd""><code>2f1c35a</code></a> release version 8.1.2</li>; <li><a href=""https://github.com/pallets/click/commit/77dd30f8c54ebbdfbf461cedcd3d1fc1d7673f95""><code>77dd30f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/click/issues/2237"">#2237</a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11726:965,error,error,965,https://hail.is,https://github.com/hail-is/hail/pull/11726,1,['error'],['error']
Availability,"Bumps [comm](https://github.com/ipython/comm) from 0.2.1 to 0.2.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ipython/comm/releases"">comm's releases</a>.</em></p>; <blockquote>; <h2>v0.2.2</h2>; <h2>0.2.2</h2>; <p>(<a href=""https://github.com/ipython/comm/compare/v0.2.1...76149e7ee0f331772c964ae86cdb8bafebe6dfa2"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update Release Scripts <a href=""https://redirect.github.com/ipython/comm/pull/27"">#27</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Other merged PRs</h3>; <ul>; <li>chore: update pre-commit hooks <a href=""https://redirect.github.com/ipython/comm/pull/26"">#26</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/ipython/comm/graphs/contributors?from=2024-01-02&amp;to=2024-03-12&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Ablink1073+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Aipython%2Fcomm+involves%3Apre-commit-ci+updated%3A2024-01-02..2024-03-12&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ipython/comm/blob/main/CHANGELOG.md"">comm's changelog</a>.</em></p>; <blockquote>; <h2>0.2.2</h2>; <p>(<a href=""https://github.com/ipython/comm/compare/v0.2.1...76149e7ee0f331772c964ae86cdb8bafebe6dfa2"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update Release Scripts <a href=""https://redirect.github.com/ipython/comm/pull/27"">#27</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Other merged PRs</h3>; <ul>; <li>chore: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14492:395,Mainten,Maintenance,395,https://hail.is,https://github.com/hail-is/hail/pull/14492,1,['Mainten'],['Maintenance']
Availability,"Bumps [cryptography](https://github.com/pyca/cryptography) from 38.0.4 to 39.0.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>39.0.1 - 2023-02-07</p>; <pre><code>; * **SECURITY ISSUE** - Fixed a bug where ``Cipher.update_into`` accepted Python; buffer protocol objects, but allowed immutable buffers. **CVE-2023-23931**; * Updated Windows, macOS, and Linux wheels to be compiled with OpenSSL 3.0.8.; <p>.. _v39-0-0:</p>; <p>39.0.0 - 2023-01-01; </code></pre></p>; <ul>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Support for OpenSSL 1.1.0 has been removed.; Users on older version of OpenSSL will need to upgrade.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Dropped support for LibreSSL &lt; 3.5. The new; minimum LibreSSL version is 3.5.0. Going forward our policy is to support; versions of LibreSSL that are available in versions of OpenBSD that are; still receiving security support.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Removed the <code>encode_point</code> and; <code>from_encoded_point</code> methods on; :class:<code>~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicNumbers</code>,; which had been deprecated for several years.; :meth:<code>~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.public_bytes</code>; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.from_encoded_point</code>; should be used instead.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Support for using MD5 or SHA1 in; :class:<code>~cryptography.x509.CertificateBuilder</code>, other X.509 builders, and; PKCS7 has been removed.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Dropped support for macOS 10.10 and 10.11, macOS; users must upgrade to 10.12 or newer.</li>; <li><strong>ANNOUNCEMENT:</strong> The next version of <code>cryptography</code> (40.0) will change;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:966,avail,available,966,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['avail'],['available']
Availability,"Bumps [cryptography](https://github.com/pyca/cryptography) from 42.0.2 to 42.0.4.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>42.0.4 - 2024-02-20</p>; <pre><code>; * Fixed a null-pointer-dereference and segfault that could occur when creating; a PKCS#12 bundle. Credit to **Alexander-Programming** for reporting the; issue. **CVE-2024-26130**; * Fixed ASN.1 encoding for PKCS7/SMIME signed messages. The fields ``SMIMECapabilities``; and ``SignatureAlgorithmIdentifier`` should now be correctly encoded according to the; definitions in :rfc:`2633` :rfc:`3370`.; <p>.. _v42-0-3:</p>; <p>42.0.3 - 2024-02-15; </code></pre></p>; <ul>; <li>Fixed an initialization issue that caused key loading failures for some; users.</li>; </ul>; <p>.. _v42-0-2:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/fe18470f7d05f963e7267e34fdf985d81ea6ceea""><code>fe18470</code></a> Bump for 42.0.4 release (<a href=""https://redirect.github.com/pyca/cryptography/issues/10445"">#10445</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/aaa2dd06ed470695de818405a982d4c459869803""><code>aaa2dd0</code></a> Fix ASN.1 issues in PKCS#7 and S/MIME signing (<a href=""https://redirect.github.com/pyca/cryptography/issues/10373"">#10373</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/10442"">#10442</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/7a4d012991061974da5d9cb7614de65eac94f49b""><code>7a4d012</code></a> Fixes <a href=""https://redirect.github.com/pyca/cryptography/issues/10422"">#10422</a> -- don't crash when a PKCS#12 key and cert don't match (<a href=""https://redirect.github.com/pyca/cryptography/issues/10423"">#10423</a>) ...</li>; <li><a href=""https://github.com/pyca/cryptography/commit/df314bb182bdfd661333969a94325e4680d785f6""><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14332:836,failure,failures,836,https://hail.is,https://github.com/hail-is/hail/pull/14332,3,['failure'],['failures']
Availability,"Bumps [curlylint](https://github.com/thibaudcolas/curlylint) from 0.12.0 to 0.13.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/thibaudcolas/curlylint/releases"">curlylint's releases</a>.</em></p>; <blockquote>; <h2>v0.13.0 – Quality-of-life improvements</h2>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.13.0"">v0.13.0</a> 2021-04-24</h2>; <p>This release comes with a blog post! Read on <a href=""https://www.curlylint.org/blog/quality-of-life-improvements"">Quality-of-life improvements</a>.</p>; <h3>Added</h3>; <ul>; <li>Implement --template-tags CLI flag (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/25"">#25</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/77"">#77</a>).</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Add more descriptive error message for missing whitespace between HTML attributes (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/23#issuecomment-700622837"">#23 (comment)</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Move development dependencies from extras to separate <code>requirements.txt</code> (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; <li>Declare support for Python 3.9.</li>; <li>Tentatively declare support for Python 3.10 (tested with <code>Python 3.10.0a6+</code>).</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix Python 3.10 deprecation warning by importing Iterable from collections.abc (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/68"">#68</a>).</li>; </ul>; <h2>v0.12.2</h2>; <h2><a href=""https://github.com/thibaudcolas/curlylint/releases/tag/v0.12.2"">v0.12.2</a> 2021-03-06</h2>; <h3>Fixed</h3>; <ul>; <li>The <code>image_alt</code> rule no longer crashes when encountering template conditionals in img attributes (<a href=""https://github-redirect.dependa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11494:878,error,error,878,https://hail.is,https://github.com/hail-is/hail/pull/11494,2,['error'],['error']
Availability,"Bumps [de.undercouch.download](https://github.com/michel-kraemer/gradle-download-task) from 3.2.0 to 5.2.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.2.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:21,down,download,21,https://hail.is,https://github.com/hail-is/hail/pull/12332,6,"['Mainten', 'down']","['Maintenance', 'download', 'download-task', 'downloaded']"
Availability,"Bumps [de.undercouch.download](https://github.com/michel-kraemer/gradle-download-task) from 3.2.0 to 5.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.3.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>path</code> and <code>relativePath</code> properties to the <code>DownloadDetails</code> class so <code>eachFile</code> actions can also change the relative path of a target file and not only its name</li>; <li>Duplicate destination files are now prevented. Specifying a duplicate destination file (e.g. in an <code>eachFile</code> action) will lead to an exception being thrown.</li>; </ul>; <p>Bug fixes:</p>; <ul>; <li>Call <code>eachFile</code> action only once per source</li>; <li>Correctly create list of output files (even if the destination is the project's build directory)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Use pooling connection manager of Apache HttpClient instead of basic one. The basic one is not meant to be used by multiple threads. This fixes an issue that could cause an <code>IllegalStateException</code> with the message <code>Connection is still allocated</code>. Thanks to <a href=""https://github.com/dmarks2""><code>@​dmarks2</code></a> for spotting this.</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:21,down,download,21,https://hail.is,https://github.com/hail-is/hail/pull/12345,6,"['Down', 'Mainten', 'down']","['DownloadDetails', 'Maintenance', 'download', 'download-task']"
Availability,"Bumps [de.undercouch.download](https://github.com/michel-kraemer/gradle-download-task) from 5.3.0 to 5.3.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.3.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li>Allow <code>download</code> and <code>verify</code> extensions to be created on demand in custom tasks, so these tasks can be made compatible with Gradle's configuration cache (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:21,down,download,21,https://hail.is,https://github.com/hail-is/hail/pull/12707,8,"['Down', 'Mainten', 'down']","['Downgrade', 'Maintenance', 'download', 'download-task']"
Availability,"Bumps [de.undercouch.download](https://github.com/michel-kraemer/gradle-download-task) from 5.3.1 to 5.4.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.4.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to set request <code>method</code> and <code>body</code></li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 8.0.1</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4c983ed5cd229fa64912294737c858c2ba8486d6""><code>4c983ed</code></a> Bump up version number to 5.4.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/cc20442ab67bf37687c08e67af7e7de3a21c8fbe""><code>cc20442</code></a> Add integration tests for Gradle 8.0.2</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/472920e572e4cf45d321868874ced50ad8d1e2d5""><code>472920e</code></a> Add possibility to set request method and body</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/82e70cae2a8d48b4f5165a9b543d4e65bb793d88""><code>82e70ca</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86a15f1c16eb729dc71b6caf30237d07b8e0bb01""><code>86a15f1</code></a> Fix compiler warnings and deprecations</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/86363072c8239330b28976109a622bdd073507b6""><code>8636307</code></a> Negative timeouts are actually not allowed</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/4ff0ff0e63e0dd45f231990d0dcebffde6e6b709""><code>4ff0ff0</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12893:21,down,download,21,https://hail.is,https://github.com/hail-is/hail/pull/12893,7,"['Mainten', 'down']","['Maintenance', 'download', 'download-task']"
Availability,"Bumps [dictdiffer](https://github.com/inveniosoftware/dictdiffer) from 0.8.1 to 0.9.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/inveniosoftware/dictdiffer/releases"">dictdiffer's releases</a>.</em></p>; <blockquote>; <h2>Dictdiffer v0.9.0</h2>; <ul>; <li>Adds absolute tolerance feature for floats (<a href=""https://github.com/adrien-berchet""><code>@​adrien-berchet</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/152"">#152</a>)</li>; <li>Drops support of Python&lt;3.5 (<a href=""https://github.com/adrien-berchet""><code>@​adrien-berchet</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/160"">#160</a>)</li>; <li>Adds <code>assert_no_diff</code> helper to assist pytest users (<a href=""https://github.com/joesolly""><code>@​joesolly</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/153"">#153</a>)</li>; <li>Migrates CI to gh-actions (<a href=""https://github.com/ParthS007""><code>@​ParthS007</code></a> <a href=""https://github.com/diegodelemos""><code>@​diegodelemos</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/145"">#145</a>)</li>; <li>Removes dependency on pkg_resources (<a href=""https://github.com/eldruin""><code>@​eldruin</code></a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/inveniosoftware/dictdiffer/blob/master/CHANGES"">dictdiffer's changelog</a>.</em></p>; <blockquote>; <h1>Changes</h1>; <p>Version 0.9.0 (released 2021-07-22)</p>; <ul>; <li>Adds absolute tolerance feature for floats (<a href=""https://github.com/adrien-berchet""><code>@​adrien-berchet</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/152"">#152</a>)</li>; <li>Drops support of Python&lt;3.5 (<a href=""https://github.com/adrien-berchet""><code>@​adrien-be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11485:321,toler,tolerance,321,https://hail.is,https://github.com/hail-is/hail/pull/11485,1,['toler'],['tolerance']
Availability,"Bumps [docker](https://github.com/docker/docker-py) from 5.0.3 to 6.0.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/docker/docker-py/releases"">docker's releases</a>.</em></p>; <blockquote>; <h2>6.0.1</h2>; <h2>🐛 Bugfixes</h2>; <ul>; <li>Fix for <code>The pipe has been ended</code> errors on Windows (<a href=""https://github-redirect.dependabot.com/docker/docker-py/issues/3056"">#3056</a>)</li>; <li>Support floats for timestamps in Docker logs (<code>since</code> / <code>until</code>) (<a href=""https://github-redirect.dependabot.com/docker/docker-py/issues/3031"">#3031</a>)</li>; </ul>; <h2>What's Changed</h2>; <ul>; <li>docs: install package in ReadTheDocs build by <a href=""https://github.com/milas""><code>@​milas</code></a> in <a href=""https://github-redirect.dependabot.com/docker/docker-py/pull/3032"">docker/docker-py#3032</a></li>; <li>Use latest stable syntax for Dockerfiles by <a href=""https://github.com/thaJeztah""><code>@​thaJeztah</code></a> in <a href=""https://github-redirect.dependabot.com/docker/docker-py/pull/3035"">docker/docker-py#3035</a></li>; <li>feat: add support for floats to docker logs params since / until sinc… by <a href=""https://github.com/ArchiMoebius""><code>@​ArchiMoebius</code></a> in <a href=""https://github-redirect.dependabot.com/docker/docker-py/pull/3031"">docker/docker-py#3031</a></li>; <li>Change prune test to use anonymous volumes by <a href=""https://github.com/cpuguy83""><code>@​cpuguy83</code></a> in <a href=""https://github-redirect.dependabot.com/docker/docker-py/pull/3051"">docker/docker-py#3051</a></li>; <li>socket: handle npipe close by <a href=""https://github.com/nicks""><code>@​nicks</code></a> in <a href=""https://github-redirect.dependabot.com/docker/docker-py/pull/3056"">docker/docker-py#3056</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://github.com/ArchiMoebius""><code>@​ArchiMoebius</code></a> made their first contribution in <a href=""https://github-redirect.de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12475:333,error,errors,333,https://hail.is,https://github.com/hail-is/hail/pull/12475,1,['error'],['errors']
Availability,"Bumps [elasticsearch-spark-20_2.12](https://github.com/elastic/elasticsearch-hadoop) from 7.17.1 to 8.4.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12358:348,Down,Downloads,348,https://hail.is,https://github.com/hail-is/hail/pull/12358,6,"['Down', 'down']","['Downloads', 'downloads']"
Availability,"Bumps [elasticsearch-spark-20_2.12](https://github.com/elastic/elasticsearch-hadoop) from 8.4.3 to 8.6.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.0.html</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12601:347,Down,Downloads,347,https://hail.is,https://github.com/hail-is/hail/pull/12601,6,"['Down', 'down']","['Downloads', 'downloads']"
Availability,"Bumps [elasticsearch-spark-20_2.12](https://github.com/elastic/elasticsearch-hadoop) from 8.4.3 to 8.6.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-20_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.6.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.1.html</a></p>; <h2>Elasticsearch Hadoop 8.6.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.6/eshadoop-8.6.0.html</a></p>; <h2>Elasticsearch Hadoop 8.5.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.3.html</a></p>; <h2>Elasticsearch Hadoop 8.5.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.2.html</a></p>; <h2>Elasticsearch Hadoop 8.5.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.5/eshadoop-8.5.1.html</a></p>; <h2>Elasticsearch Hadoop 8.5.0</h2>; <p>Downloads: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12623:347,Down,Downloads,347,https://hail.is,https://github.com/hail-is/hail/pull/12623,6,"['Down', 'down']","['Downloads', 'downloads']"
Availability,"Bumps [elasticsearch-spark-30_2.12](https://github.com/elastic/elasticsearch-hadoop) from 8.0.0 to 8.4.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/elastic/elasticsearch-hadoop/releases"">elasticsearch-spark-30_2.12's releases</a>.</em></p>; <blockquote>; <h2>Elasticsearch Hadoop 8.4.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.3.html</a></p>; <h2>Elasticsearch Hadoop 8.4.2</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.2.html</a></p>; <h2>Elasticsearch Hadoop 8.4.1</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.1.html</a></p>; <h2>Elasticsearch Hadoop 8.4.0</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.4/eshadoop-8.4.0.html</a></p>; <h2>Elasticsearch Hadoop 8.3.3</h2>; <p>Downloads: <a href=""https://elastic.co/downloads/hadoop"">https://elastic.co/downloads/hadoop</a>; Release notes: <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html"">https://www.elastic.co/guide/en/elasticsearch/hadoop/8.3/eshadoop-8.3.3.html</a></p>; <h2>Elasticsearch Hadoop 8.3.2</h2>; <p>Downloads: <a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12319:347,Down,Downloads,347,https://hail.is,https://github.com/hail-is/hail/pull/12319,6,"['Down', 'down']","['Downloads', 'downloads']"
Availability,"Bumps [filelock](https://github.com/tox-dev/py-filelock) from 3.7.1 to 3.8.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/py-filelock/releases"">filelock's releases</a>.</em></p>; <blockquote>; <h2>3.8.0</h2>; <h2>What's Changed</h2>; <ul>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/py-filelock/pull/149"">tox-dev/py-filelock#149</a></li>; <li>Bump actions/upload-artifact from 2 to 3 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/py-filelock/pull/154"">tox-dev/py-filelock#154</a></li>; <li>Bump actions/download-artifact from 2 to 3 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/py-filelock/pull/152"">tox-dev/py-filelock#152</a></li>; <li>Bump pre-commit/action from 2.0.3 to 3.0.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/py-filelock/pull/151"">tox-dev/py-filelock#151</a></li>; <li>Bump actions/checkout from 2 to 3 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/py-filelock/pull/153"">tox-dev/py-filelock#153</a></li>; <li>Bump actions/setup-python from 2 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/py-filelock/pull/150"">tox-dev/py-filelock#150</a></li>; <li>Add timeout unit to docstrings by <a href=""https://github.com/jnordberg""><code>@​jnordberg</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/py-filelock/pull/148"">tox-dev/py-filelock#148</a></li>; <li>Unify badges style by <a href=""https://github.com/DeadNews""><code>@​DeadNews</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12157:784,down,download-artifact,784,https://hail.is,https://github.com/hail-is/hail/pull/12157,1,['down'],['download-artifact']
Availability,"Bumps [flake8](https://github.com/pycqa/flake8) from 3.8.3 to 4.0.1.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/flake8/commit/82b698e09996cdde5d473e234681d8380810d7a2""><code>82b698e</code></a> Release 4.0.1</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/0fac346d8437d205e508643253c7a7d5fdf5dee7""><code>0fac346</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1410"">#1410</a> from PyCQA/parallel-syntax-error</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/aa54693c9ec03368c6e592efff4dd4757dd72a47""><code>aa54693</code></a> fix parallel execution collecting a SyntaxError</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/d31c5356bbb0a884555662185697ddc6bb46a44c""><code>d31c535</code></a> Release 4.0.0</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/afd2399b4cc9b27c4e8a5c2dec8444df8f480293""><code>afd2399</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1407"">#1407</a> from asottile/setup-cfg-fmt</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/960cf8cf2044359d5fbd3454a2a9a1d7a0586594""><code>960cf8c</code></a> rerun setup-cfg-fmt (and restore comments)</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/d7baba5f14091e7975d2abb3ba9bf321b5be6102""><code>d7baba5</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1406"">#1406</a> from asottile/update-versions</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/d79021aafc809d999c4cbbc0a513a5ceb473efa2""><code>d79021a</code></a> update dependency versions</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/283f0c81241673221d9628beb11e2d7356826f00""><code>283f0c8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1404"">#1404</a> from PyCQA/drop-xdg-config</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/807904aebc20814ac595b0004ab526fff",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11456:503,error,error,503,https://hail.is,https://github.com/hail-is/hail/pull/11456,2,['error'],['error']
Availability,"Bumps [gidgethub](https://github.com/brettcannon/gidgethub) from 4.2.0 to 5.2.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/brettcannon/gidgethub/releases"">gidgethub's releases</a>.</em></p>; <blockquote>; <h2>5.2.1</h2>; <ul>; <li>; <p>Fix cgi and importlib_resources deprecations.; [PR <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/185"">#185</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/pull/185"">brettcannon/gidgethub#185</a>)</p>; </li>; <li>; <p>Add support for Python 3.11 and drop EOL Python 3.6; [PR <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/184"">#184</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/pull/184"">brettcannon/gidgethub#184</a>)</p>; </li>; </ul>; <h2>5.2.0</h2>; <ul>; <li>Make the minimum version of PyJWT be v2.4.0.</li>; </ul>; <h2>5.1.0</h2>; <ul>; <li>; <p>Use <code>X-Hub-Signature-256</code> header for webhook validation when available.; ([PR <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/160"">#160</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/pull/160"">brettcannon/gidgethub#160</a>)).</p>; </li>; <li>; <p>The documentation is now built using Sphinx v&gt;= 4.0.0.; ([Issue <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/143"">#143</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/143"">brettcannon/gidgethub#143</a>))</p>; </li>; <li>; <p><code>gidgethub.abc.GitHubAPI.getiter</code> now accepts <code>iterable_key</code> parameter; in order to support the Checks API.; ([Issue <a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/164"">#164</a>](<a href=""https://github-redirect.dependabot.com/brettcannon/gidgethub/issues/164"">brettcannon/gidgethub#164</a>))</p>; </li>; <li>; <p>Accept HTTP 202 ACCEPTED as successful.; ([PR <a href=""https://github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12328:1029,avail,available,1029,https://hail.is,https://github.com/hail-is/hail/pull/12328,1,['avail'],['available']
Availability,"Bumps [junixsocket-core](https://github.com/kohlschutter/junixsocket) from 2.3.2 to 2.6.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/kohlschutter/junixsocket/releases"">junixsocket-core's releases</a>.</em></p>; <blockquote>; <h2>junixsocket 2.6.1</h2>; <ul>; <li>Add AFSocket.checkConnectionClosed to probe connection status</li>; <li>Fix connection status checks and error handling</li>; <li>Fix bind behavior on Windows, support re-bind with reuseAddress</li>; <li>Fix and improve unit tests/selftests, remove several false-positive errors found in the wild (Azure Cloudshell/Microsoft CBL-Mariner 2.0, Amazon EC2, OpenBSD, etc.)</li>; <li>Fix SimpleTestServer demo, actually counting now to 5, not 6.</li>; <li>Make builds reproducible, align timestamps with git commit</li>; </ul>; <p>NOTE: If you're seeing unexpected errors in selftest, please verify with the attached <code>junixsocket-selftest-2.6.1-hotpatch-jar-with-dependencies.jar</code>. There may be false-positive socket timeout issues on very slow machines (e.g., qemu s390).</p>; <h2>junixsocket 2.6.0</h2>; <ul>; <li>Add support for GraalVM native-image</li>; <li>Add support for native-image selftest</li>; <li>Add support for AF_VSOCK (on Linux, and some macOS VMs)</li>; <li>Reintroduce deprecated legacy constructors for AFUNIXSocketAddress that were removed in 2.5.0.</li>; <li>Parent POM has been renamed from junixsocket-parent to junixsocket</li>; </ul>; <h2>junixsocket 2.5.2</h2>; <ul>; <li>Fix address handling in the Abstract Namespace</li>; <li>Fix support for very large datagrams (&gt; 1MB)</li>; <li>Fix InetAddress-wrapping of long addresses</li>; <li>Update Xcode support script, crossclang</li>; <li>Bump postgresql version in demo code</li>; <li>Fix dependency for custom architecture artifact</li>; </ul>; <h2>junixsocket 2.5.1</h2>; <ul>; <li>Add support for IBM z/OS (experimental, binary not included)</li>; <li>Add support for building from source on arm64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12483:420,error,error,420,https://hail.is,https://github.com/hail-is/hail/pull/12483,3,['error'],"['error', 'errors']"
Availability,"Bumps [jupyter-client](https://github.com/jupyter/jupyter_client) from 7.3.1 to 7.3.4.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/releases"">jupyter-client's releases</a>.</em></p>; <blockquote>; <h2>v7.3.4</h2>; <h2>7.3.4</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.3...ca4cb2d6a4b95a6925de85a47b323d2235032c74"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Revert latest changes to <code>ThreadedZMQSocketChannel</code> because they break Qtconsole <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/803"">#803</a> (<a href=""https://github.com/ccordoba12""><code>@​ccordoba12</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Fix sphinx 5.0 support <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/804"">#804</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/799"">#799</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-07&amp;to=2022-06-08&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-06-07..2022-06-08&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Accordoba12+updated%3A2022-06-07..2022-06-08&amp;type=Issues""><code>@​ccordoba12</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Apre-commit-ci+updated%3A2022-06-07..2022-06-08&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; <h2>v7.3.3</h2>; <h2>7.3.3</h2>; <p>(<a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:743,Mainten,Maintenance,743,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Mainten'],['Maintenance']
Availability,"Bumps [jupyter-client](https://github.com/jupyter/jupyter_client) from 7.4.4 to 7.4.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/releases"">jupyter-client's releases</a>.</em></p>; <blockquote>; <h2>v7.4.5</h2>; <h2>7.4.5</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.4...d27c8a497c6cbb1a232fbbe75cb1fd0f53faa9b0"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[7.x] Handle Jupyter Core Warning <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/875"">#875</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up 7.x workflows <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/865"">#865</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-10-25&amp;to=2022-11-10&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-10-25..2022-11-10&amp;type=Issues""><code>@​blink1073</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/v7.4.5/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>7.4.5</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.4.4...d27c8a497c6cbb1a232fbbe75cb1fd0f53faa9b0"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[7.x] Handle Jupyter Core Warning <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/875"">#875</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up 7.x workflows <a href=""https://github-redirect.dependabot.com/jupyter/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12459:445,Mainten,Maintenance,445,https://hail.is,https://github.com/hail-is/hail/pull/12459,1,['Mainten'],['Maintenance']
Availability,"Bumps [jupyter-client](https://github.com/jupyter/jupyter_client) from 7.4.8 to 8.0.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/releases"">jupyter-client's releases</a>.</em></p>; <blockquote>; <h2>v8.0.2</h2>; <h2>8.0.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.1...717d36edcd9ce595f727d8b5a27e270c2a6e2c46"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Add papermill downstream check and fix kernel client replies <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/925"">#925</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Adopt more ruff rules <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/924"">#924</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Prefer print in kernelspecapp <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/923"">#923</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-01-26&amp;to=2023-01-30&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-01-26..2023-01-30&amp;type=Issues""><code>@​blink1073</code></a></p>; <h2>v8.0.1</h2>; <h2>8.0.1</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.0...dc6113c360e05122430b8e130374e9f4e4b701d7"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Fix json_output in kernelspec app <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/921"">#921</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12656:486,down,downstream,486,https://hail.is,https://github.com/hail-is/hail/pull/12656,2,"['Mainten', 'down']","['Maintenance', 'downstream']"
Availability,"Bumps [jupyter-core](https://github.com/jupyter/jupyter_core) from 5.7.1 to 5.7.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_core/releases"">jupyter-core's releases</a>.</em></p>; <blockquote>; <h2>v5.7.2</h2>; <h2>5.7.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_core/compare/v5.7.1...1264a81fc834f18db2b41e136ec4ac9d1a4ad993"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update Release Scripts <a href=""https://redirect.github.com/jupyter/jupyter_core/pull/396"">#396</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Enforce pytest 7 <a href=""https://redirect.github.com/jupyter/jupyter_core/pull/393"">#393</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>chore: update pre-commit hooks <a href=""https://redirect.github.com/jupyter/jupyter_core/pull/392"">#392</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_core/graphs/contributors?from=2024-01-08&amp;to=2024-03-12&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_core+involves%3Ablink1073+updated%3A2024-01-08..2024-03-12&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_core+involves%3Apre-commit-ci+updated%3A2024-01-08..2024-03-12&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_core/blob/main/CHANGELOG.md"">jupyter-core's changelog</a>.</em></p>; <blockquote>; <h2>5.7.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_core/compare/v5.7.1...1264a81fc834f18db2b41e136ec4ac9d1a4ad993"">Full Changelog</a>)</p>; <h3>Maintenance and upkeep impr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14484:435,Mainten,Maintenance,435,https://hail.is,https://github.com/hail-is/hail/pull/14484,1,['Mainten'],['Maintenance']
Availability,"Bumps [jupyterlab](https://github.com/jupyterlab/jupyterlab) from 4.0.9 to 4.0.12.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/jupyterlab/jupyterlab/releases"">jupyterlab's releases</a>.</em></p>; <blockquote>; <h2>v4.0.12</h2>; <h2>4.0.12</h2>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/compare/v4.0.11...69079ec413cbe6d173f0a667c15802b76423ece5"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Fix jupyterlab downgrade issue on extension installation <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15650"">#15650</a> (<a href=""https://github.com/Sarthug99""><code>@​Sarthug99</code></a>)</li>; <li>Fix search highlights removal on clearing input box <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15690"">#15690</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Add scroll margin to headings for better alignment <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15703"">#15703</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix shortcut UI failing on filtering when empty command is given <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15695"">#15695</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; <li>Fix connection loop issue with standalone foreign document in LSP <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15262"">#15262</a> (<a href=""https://github.com/trungleduc""><code>@​trungleduc</code></a>)</li>; <li>Fix outputarea package from not detecting updates <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15642"">#15642</a> (<a href=""https://github.com/MFA-X-AI""><code>@​MFA-X-AI</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Backport PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15524"">#15524</a>: Fix visual tests <a href=""https://redirect.github.com/jupyter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14218:480,down,downgrade,480,https://hail.is,https://github.com/hail-is/hail/pull/14218,1,['down'],['downgrade']
Availability,"Bumps [nbsphinx](https://github.com/spatialaudio/nbsphinx) from 0.8.3 to 0.8.8.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/spatialaudio/nbsphinx/releases"">nbsphinx's releases</a>.</em></p>; <blockquote>; <h2>nbsphinx 0.8.8</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.8/"">https://pypi.org/project/nbsphinx/0.8.8/</a></p>; <ul>; <li>Support for the <code>sphinx_codeautolink</code> extension</li>; <li>Basic support for the <code>text</code> builder</li>; </ul>; <h2>nbsphinx 0.8.7</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.7/"">https://pypi.org/project/nbsphinx/0.8.7/</a></p>; <ul>; <li>Fix assertion error in LaTeX build with Sphinx 4.1.0+</li>; </ul>; <h2>nbsphinx 0.8.6</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.6/"">https://pypi.org/project/nbsphinx/0.8.6/</a></p>; <ul>; <li>Support for Jinja2 version 3</li>; </ul>; <h2>nbsphinx 0.8.5</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.5/"">https://pypi.org/project/nbsphinx/0.8.5/</a></p>; <ul>; <li>Freeze Jinja2 version to 2.11 (for now, until a bugfix is found)</li>; <li>Add <code>theme_comparison.py</code> tool for creating multiple versions (with different HTML themes) of the docs at once</li>; </ul>; <h2>nbsphinx 0.8.4</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.4/"">https://pypi.org/project/nbsphinx/0.8.4/</a></p>; <ul>; <li>Support for <code>mathjax3_config</code> (for Sphinx &gt;= 4)</li>; <li>Force loading MathJax on HTML pages generated from notebooks (can be disabled with <code>nbsphinx_assume_equations = False</code>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/spatialaudio/nbsphinx/blob/master/NEWS.rst"">nbsphinx's changelog</a>.</em></p>; <blockquote>; <p>Version 0.8.8 -- 2021-12-31 -- PyPI__ -- diff__</p>; <ul>; <li>Support for the <code>sphinx_codeautolink</code> extension</li>; <li>Basic support for the <code>text</code> b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:677,error,error,677,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['error'],['error']
Availability,"Bumps [nest-asyncio](https://github.com/erdewit/nest_asyncio) from 1.5.4 to 1.5.6.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/fe3616358ce3cd7fdd5d50acb50020ac6eb6902b""><code>fe36163</code></a> Remove old travis file</li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/5d3795ca0f2024d6e4d475101560e1dc96db93e9""><code>5d3795c</code></a> v1.5.6</li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/9dd34446e87ad5bc30d6f151932d7f26899f0a31""><code>9dd3444</code></a> Add Python 3.11 support</li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/ae731dcc2a779f4c27e0188100a72fb7ac2f129a""><code>ae731dc</code></a> Update test workflow, add mypy and flake8</li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/99d4ddde7c1df05537f6a31f5e9adf1c9c80fdb7""><code>99d4ddd</code></a> Fix flake8 and mypy errors</li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/8b5ec6c6fda3d45eab0cd08af1f9cf49855ebbcf""><code>8b5ec6c</code></a> v1.5.5</li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/3cfd2c8bc453174ec0be57cd3bb8ec16dbcde1b4""><code>3cfd2c8</code></a> Potential fix for issue <a href=""https://github-redirect.dependabot.com/erdewit/nest_asyncio/issues/65"">#65</a></li>; <li><a href=""https://github.com/erdewit/nest_asyncio/commit/616d9a5e15d8d75e3343422778e49af2e9ac80ea""><code>616d9a5</code></a> Patch asyncio.get_event_loop to not require a running loop, fixes <a href=""https://github-redirect.dependabot.com/erdewit/nest_asyncio/issues/70"">#70</a></li>; <li>See full diff in <a href=""https://github.com/erdewit/nest_asyncio/compare/v1.5.4...v1.5.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=nest-asyncio&package-manager=pip&previous-version=1.5.4&new-version=1.5.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12303:904,error,errors,904,https://hail.is,https://github.com/hail-is/hail/pull/12303,1,['error'],['errors']
Availability,"Bumps [notebook](https://github.com/jupyter/notebook) from 7.0.6 to 7.0.7.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/releases"">notebook's releases</a>.</em></p>; <blockquote>; <h2>v7.0.7</h2>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e86803"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Update to JupyterLab 4.0.11 <a href=""https://redirect.github.com/jupyter/notebook/pull/7215"">#7215</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:680,Mainten,Maintenance,680,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['Mainten'],['Maintenance']
Availability,"Bumps [numpy](https://github.com/numpy/numpy) from 1.21.6 to 1.23.4.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>; <blockquote>; <h2>v1.23.4</h2>; <h1>NumPy 1.23.4 Release Notes</h1>; <p>NumPy 1.23.4 is a maintenance release that fixes bugs discovered after; the 1.23.3 release and keeps the build infrastructure current. The main; improvements are fixes for some annotation corner cases, a fix for a; long time <code>nested_iters</code> memory leak, and a fix of complex vector dot; for very large arrays. The Python versions supported for this release; are 3.8-3.11.</p>; <p>Note that the mypy version needs to be 0.981+ if you test using Python; 3.10.7, otherwise the typing tests will fail.</p>; <h2>Contributors</h2>; <p>A total of 8 people contributed to this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li>Bas van Beek</li>; <li>Charles Harris</li>; <li>Matthew Barber</li>; <li>Matti Picus</li>; <li>Ralf Gommers</li>; <li>Ross Barnowski</li>; <li>Sebastian Berg</li>; <li>Sicheng Zeng +</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 13 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22368"">#22368</a>: BUG: Add <code>__array_api_version__</code> to <code>numpy.array_api</code> namespace</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22370"">#22370</a>: MAINT: update sde toolkit to 9.0, fix download link</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22382"">#22382</a>: BLD: use macos-11 image on azure, macos-1015 is deprecated</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22383"">#22383</a>: MAINT: random: remove <code>get_info</code> from &quot;extending with Cython&quot;...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12441:307,mainten,maintenance,307,https://hail.is,https://github.com/hail-is/hail/pull/12441,1,['mainten'],['maintenance']
Availability,"Bumps [numpy](https://github.com/numpy/numpy) from 1.21.6 to 1.23.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>; <blockquote>; <h2>v1.23.5</h2>; <h1>NumPy 1.23.5 Release Notes</h1>; <p>NumPy 1.23.5 is a maintenance release that fixes bugs discovered after; the 1.23.4 release and keeps the build infrastructure current. The; Python versions supported for this release are 3.8-3.11.</p>; <h2>Contributors</h2>; <p>A total of 7 people contributed to this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li><a href=""https://github.com/DWesl""><code>@​DWesl</code></a></li>; <li>Aayush Agrawal +</li>; <li>Adam Knapp +</li>; <li>Charles Harris</li>; <li>Navpreet Singh +</li>; <li>Sebastian Berg</li>; <li>Tania Allard</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 10 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22489"">#22489</a>: TST, MAINT: Replace most setup with setup_method (also teardown)</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22490"">#22490</a>: MAINT, CI: Switch to cygwin/cygwin-install-action@v2</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22494"">#22494</a>: TST: Make test_partial_iteration_cleanup robust but require leak...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22592"">#22592</a>: MAINT: Ensure graceful handling of large header sizes</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22593"">#22593</a>: TYP: Spelling alignment for array flag literal</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22594"">#22594</a>: BUG: Fix bounds checking for <code>random.logseries</code></li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22595"">#22595</a>: DEV: Update ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12515:307,mainten,maintenance,307,https://hail.is,https://github.com/hail-is/hail/pull/12515,1,['mainten'],['maintenance']
Availability,"Bumps [numpy](https://github.com/numpy/numpy) from 1.21.6 to 1.24.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>; <blockquote>; <h2>v1.24.2</h2>; <h1>NumPy 1.24.2 Release Notes</h1>; <p>NumPy 1.24.2 is a maintenance release that fixes bugs and regressions; discovered after the 1.24.1 release. The Python versions supported by; this release are 3.8-3.11.</p>; <h2>Contributors</h2>; <p>A total of 14 people contributed to this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li>Bas van Beek</li>; <li>Charles Harris</li>; <li>Khem Raj +</li>; <li>Mark Harfouche</li>; <li>Matti Picus</li>; <li>Panagiotis Zestanakis +</li>; <li>Peter Hawkins</li>; <li>Pradipta Ghosh</li>; <li>Ross Barnowski</li>; <li>Sayed Adel</li>; <li>Sebastian Berg</li>; <li>Syam Gadde +</li>; <li>dmbelov +</li>; <li>pkubaj +</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 17 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22965"">#22965</a>: MAINT: Update python 3.11-dev to 3.11.</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22966"">#22966</a>: DOC: Remove dangling deprecation warning</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22967"">#22967</a>: ENH: Detect CPU features on FreeBSD/powerpc64*</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22968"">#22968</a>: BUG: np.loadtxt cannot load text file with quoted fields separated...</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22969"">#22969</a>: TST: Add fixture to avoid issue with randomizing test order.</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22970"">#22970</a>: BUG: Fix fill violating read-only flag. (<a href=""https://redirect.github.com/numpy/numpy/issues/22959"">#22959</a>)</li>; <li><a href=""https://redirect.github.com/numpy/numpy/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12898:307,mainten,maintenance,307,https://hail.is,https://github.com/hail-is/hail/pull/12898,1,['mainten'],['maintenance']
Availability,"Bumps [orjson](https://github.com/ijl/orjson) from 3.6.4 to 3.6.7.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.6.7</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd64 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:959,error,error,959,https://hail.is,https://github.com/hail-is/hail/pull/11572,1,['error'],['error']
Availability,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.0 to 1.4.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.4.1</h2>; <p>This is the first patch release in the 1.4.x series and includes some regression fixes and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.1/whatsnew/v1.4.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.0/whatsnew/v1.4.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install -c conda-forge pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.4.0. If all goes well, we'll release pandas 1.4.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/whatsnew/v1.4.0.html"">whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11539:600,avail,available,600,https://hail.is,https://github.com/hail-is/hail/pull/11539,1,['avail'],['available']
Availability,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.5 to 1.5.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.5.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.5.0. If all goes well, we'll release pandas 1.5.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5/whatsnew/v1.5.0.html"">whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.5.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.5.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.4.4</h2>; <p>This is a patch release in the 1.4.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.4/whatsnew/v1.4.4.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12292:625,avail,available,625,https://hail.is,https://github.com/hail-is/hail/pull/12292,1,['avail'],['available']
Availability,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.5 to 1.5.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.5.2</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.2/whatsnew/v1.5.2.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.1</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.1/whatsnew/v1.5.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whats",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:586,avail,available,586,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['avail'],['available']
Availability,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.5 to 1.5.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.5.3</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.3/whatsnew/v1.5.3.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.2</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.2/whatsnew/v1.5.2.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <h2>Pandas 1.5.1</h2>; <p>This is a patch release in the 1.5.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.1/whatsnew/v1.5.1.html"">full",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12610:586,avail,available,586,https://hail.is,https://github.com/hail-is/hail/pull/12610,1,['avail'],['available']
Availability,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 9.5.0 to 10.0.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>; <blockquote>; <h2>10.0.0</h2>; <p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/10.0.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/10.0.0.html</a></p>; <h2>Changes</h2>; <ul>; <li>Fixed deallocating mask images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7246"">#7246</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added ImageFont.MAX_STRING_LENGTH <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7244"">#7244</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix Windows build with pyproject.toml <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7230"">#7230</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Do not close provided file handles with libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7199"">#7199</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Convert to HSV if mode is HSV in getcolor() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7226"">#7226</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added alpha_only argument to getbbox() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7123"">#7123</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Prioritise speed in <em>repr_png</em> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7242"">#7242</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Limit size even if one dimension is zero in decompression bomb check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7235"">#7235</a> [<a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:466,mask,mask,466,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['mask'],['mask']
Availability,"Bumps [psutil](https://github.com/giampaolo/psutil) from 5.8.0 to 5.9.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/giampaolo/psutil/blob/master/HISTORY.rst"">psutil's changelog</a>.</em></p>; <blockquote>; <h1>5.9.0</h1>; <p>2021-12-29</p>; <p><strong>Enhancements</strong></p>; <ul>; <li>1851_, [Linux]: <code>cpu_freq()</code>_ is slow on systems with many CPUs. Read current; frequency values for all CPUs from <code>/proc/cpuinfo</code> instead of opening many; files in <code>/sys</code> fs. (patch by marxin)</li>; <li>1992_: <code>NoSuchProcess</code>_ message now specifies if the PID has been reused.</li>; <li>1992_: error classes (<code>NoSuchProcess</code><em>, <code>AccessDenied</code></em>, etc.) now have a better; formatted and separated <code>__repr__</code> and <code>__str__</code> implementations.</li>; <li>1996_, [BSD]: add support for MidnightBSD. (patch by Saeed Rasooli)</li>; <li>1999_, [Linux]: <code>disk_partitions()</code>_: convert <code>/dev/root</code> device (an alias; used on some Linux distros) to real root device path.</li>; <li>2005_: <code>PSUTIL_DEBUG</code> mode now prints file name and line number of the debug; messages coming from C extension modules.</li>; <li>2042_: rewrite HISTORY.rst to use hyperlinks pointing to psutil API doc.</li>; </ul>; <p><strong>Bug fixes</strong></p>; <ul>; <li>1456_, [macOS], <strong>[critical]</strong>: <code>cpu_freq()</code>_ <code>min</code> and <code>max</code> are set to; 0 if can't be determined (instead of crashing).</li>; <li>1512_, [macOS]: sometimes <code>Process.connections()</code>_ will crash with; <code>EOPNOTSUPP</code> for one connection; this is now ignored.</li>; <li>1598_, [Windows]: <code>disk_partitions()</code>_ only returns mountpoints on drives; where it first finds one.</li>; <li>1874_, [SunOS]: swap output error due to incorrect range.</li>; <li>1892_, [macOS]: <code>cpu_freq()</code>_ broken on Apple M1.</li>; <li>1901_, [macOS]: diff",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11459:674,error,error,674,https://hail.is,https://github.com/hail-is/hail/pull/11459,1,['error'],['error']
Availability,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.12.2 to 2.13.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.13.0?</h1>; <p>Release date: 2022-03-24</p>; <ul>; <li>; <p>Add missing dunder methods to <code>unexpected-special-method-signature</code> check.</p>; </li>; <li>; <p>No longer emit <code>no-member</code> in for loops that reference <code>self</code> if the binary operation that; started the for loop uses a <code>self</code> that is encapsulated in tuples or lists.</p>; <p>Ref <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1360"">PyCQA/astroid#1360</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4826"">#4826</a></p>; </li>; <li>; <p>Output better error message if unsupported file formats are used with <code>pyreverse</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5950"">#5950</a></p>; </li>; <li>; <p>Fix pyreverse diagrams type hinting for classmethods and staticmethods.</p>; </li>; <li>; <p>Fix pyreverse diagrams type hinting for methods returning None.</p>; </li>; <li>; <p>Fix matching <code>--notes</code> options that end in a non-word character.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5840"">#5840</a></p>; </li>; <li>; <p>Updated the position of messages for class and function defintions to no longer cover; the complete definition. Only the <code>def</code> or <code>class</code> + the name of the class/function; are covered.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5466"">#5466</a></p>; </li>; <li>; <p><code>using-f-string-in-unsupported-version</code> and <code>using-final-decorator-in-unsupported-version</code> msgids; were renamed from <code>W1601</code> and <code>W1602</code> to <code>W2601</code> and <code>W2602</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11674:875,error,error,875,https://hail.is,https://github.com/hail-is/hail/pull/11674,1,['error'],['error']
Availability,"Bumps [pytest](https://github.com/pytest-dev/pytest) from 7.1.1 to 7.1.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest/releases"">pytest's releases</a>.</em></p>; <blockquote>; <h2>7.1.3</h2>; <h1>pytest 7.1.3 (2022-08-31)</h1>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10060"">#10060</a>: When running with <code>--pdb</code>, <code>TestCase.tearDown</code> is no longer called for tests when the <em>class</em> has been skipped via <code>unittest.skip</code> or <code>pytest.mark.skip</code>.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10190"">#10190</a>: Invalid XML characters in setup or teardown error messages are now properly escaped for JUnit XML reports.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/10230"">#10230</a>: Ignore <code>.py</code> files created by <code>pyproject.toml</code>-based editable builds introduced in <a href=""https://pip.pypa.io/en/stable/news/#v21-3"">pip 21.3</a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/3396"">#3396</a>: Doctests now respect the <code>--import-mode</code> flag.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9514"">#9514</a>: Type-annotate <code>FixtureRequest.param</code> as <code>Any</code> as a stop gap measure until <code>8073</code>{.interpreted-text role=&quot;issue&quot;} is fixed.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9791"">#9791</a>: Fixed a path handling code in <code>rewrite.py</code> that seems to work fine, but was incorrect and fails in some systems.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9917"">#9917</a>: Fixed string representation for <code>pytest.approx</code>{.interpreted-text role=&quot;func&quot;} when used to compare tuples.</li>; </ul>; <h2>Imp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12187:768,error,error,768,https://hail.is,https://github.com/hail-is/hail/pull/12187,1,['error'],['error']
Availability,"Bumps [regex](https://github.com/mrabarnett/mrab-regex) from 2023.3.23 to 2023.5.5.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/mrabarnett/mrab-regex/blob/hg/changelog.txt"">regex's changelog</a>.</em></p>; <blockquote>; <p>Version: 2023.5.5</p>; <pre><code>Removed semicolon after 'else' in 'munge_name'.; </code></pre>; <p>Version: 2023.5.4</p>; <pre><code>Fixed pyproject.toml and setup.py.; </code></pre>; <p>Version: 2023.5.3</p>; <pre><code>pyproject.toml was missing.; </code></pre>; <p>Version: 2023.5.2</p>; <pre><code>Added pyproject.toml.; </code></pre>; <p>Version: 2023.3.23</p>; <pre><code>Git issue 495: Running time for failing fullmatch increases rapidly with input length; Re-enabled modified repeat guards due to regression in speed caused by excessive backtracking.; </code></pre>; <p>Version: 2023.3.22</p>; <pre><code>Git issue 494: Backtracking failure matching regex `^a?(a?)b?c\1$` against string `abca`; Disabled repeat guards. They keep causing issues, and it's just simpler to rely on timeouts.; </code></pre>; <p>Version: 2022.10.31</p>; <pre><code>Updated text for supported Unicode and Python versions.; </code></pre>; <p>Version: 2022.9.13</p>; <pre><code>Updated to Unicode 15.0.0.; </code></pre>; <p>Version: 2022.9.11</p>; <pre><code>Updated version.; </code></pre>; <p>Version: 2022.8.17</p>; <pre><code>Git issue 477: \v for vertical spacing; <p>Added \p{HorizSpace} (\p{H}) and \p{VertSpace} (\p{V}).; </code></pre></p>; <p>Version: 2022.7.25</p>; <pre><code>Git issue 475: 2022.7.24 improperly released; <p>The file <a href=""https://pypi.org/pypi/regex/2022.7.24/json"">https://pypi.org/pypi/regex/2022.7.24/json</a> was missing references to most of the wheels, so this is a new release in the hope that it was just a glitch in GitHub Actions.; </code></pre></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12989:913,failure,failure,913,https://hail.is,https://github.com/hail-is/hail/pull/12989,1,['failure'],['failure']
Availability,"Bumps [rich](https://github.com/Textualize/rich) from 12.6.0 to 13.5.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/releases"">rich's releases</a>.</em></p>; <blockquote>; <h2>v13.5.2</h2>; <p>Bugfix</p>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>Mostly cake, one or two puppies</h2>; <p><a href=""https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/"">https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/</a></p>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Spa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:361,error,error,361,https://hail.is,https://github.com/hail-is/hail/pull/13380,2,['error'],['error']
Availability,"Bumps [rich](https://github.com/Textualize/rich) from 12.6.0 to 13.5.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/releases"">rich's releases</a>.</em></p>; <blockquote>; <h2>Markdown fixes</h2>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>v13.5.2</h2>; <p>Bugfix</p>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>Mostly cake, one or two puppies</h2>; <p><a href=""https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/"">https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/</a></p>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:756,error,error,756,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['error'],['error']
Availability,"Bumps [rich](https://github.com/Textualize/rich) from 12.6.0 to 13.6.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/releases"">rich's releases</a>.</em></p>; <blockquote>; <h2>The Python 3.12 release</h2>; <p>Mostly a meta update in readiness for the release of Python3.12</p>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>Markdown fixes</h2>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>v13.5.2</h2>; <p>Bugfix</p>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>Mostly cake, one or two puppies</h2>; <p><a href=""https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/"">https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/</a></p>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs not expanding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13758:967,error,error,967,https://hail.is,https://github.com/hail-is/hail/pull/13758,2,['error'],['error']
Availability,"Bumps [sphinx-rtd-theme](https://github.com/readthedocs/sphinx_rtd_theme) from 1.3.0 to 2.0.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/readthedocs/sphinx_rtd_theme/blob/master/docs/changelog.rst"">sphinx-rtd-theme's changelog</a>.</em></p>; <blockquote>; <h1>2.0.0</h1>; <h2>Added</h2>; <ul>; <li>Support for Sphinx versions <code>6.x</code> and <code>7.x</code></li>; <li>Support for docutils <code>&lt;=0.20</code></li>; </ul>; <h2>Deprecations</h2>; <ul>; <li>The HTML4 writer is now officially deprecated. An error will be thrown if your; project configuration still uses the HTML4 writer.</li>; <li>Support for Sphinx versions &lt; 5.0 was removed.</li>; <li>In addition, our supported dependencies will match the dependencies from our; lowest supported Sphinx release, version 5.0: Python &gt;= 3.6 and docutils &gt; 0.14 and &lt; 0.19</li>; </ul>; <p>.. _release-1.3.0:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/7c9b1b5d391f6d7fae72274393eb25d1df96e546""><code>7c9b1b5</code></a> Release 2.0 final (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1544"">#1544</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/c1044107602faf9be43e4358bc4f8b6abff9b420""><code>c104410</code></a> Bump for next potential release, 2.0.0rc5 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1539"">#1539</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/53ca116ef64123735e5e445258b8b103ad31a26e""><code>53ca116</code></a> Release 2.0.0rc4 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1538"">#1538</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/4498e97b462688bac2ff3615ac1da1b867b21842""><code>4498e97</code></a> Fix AttributeError when one of <code>css_files</code> is a string (<a href=""https://redire",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14502:562,error,error,562,https://hail.is,https://github.com/hail-is/hail/pull/14502,1,['error'],['error']
Availability,Bytecode failed verification error with group_by,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4853:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/4853,1,['error'],['error']
Availability,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534:69,down,download,69,https://hail.is,https://github.com/hail-is/hail/pull/10534,4,"['down', 'error']","['download', 'downloads', 'error']"
Availability,CHANGELOG: Add `hl.die` function that can be used to generate errors. Useful in data validation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8865:62,error,errors,62,https://hail.is,https://github.com/hail-is/hail/pull/8865,1,['error'],['errors']
Availability,"CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. Stacked on #12212 . This PR threads through region requests from the user and feeds that information into the scheduler. The architecture of a pool per machine type has not changed. We explicitly chose not to have a new pool per region x machine_type. Instead, the control loop looks at the front of the job queue and tries to predict which jobs are likely to be scheduled. From those jobs, we then find which regions the jobs can run in and create the number of corresponding instances. We use the fair share calculation to estimate how many jobs per user can be scheduled in 2.5 minutes assuming the scheduling loop runs once per second. We then grab this many jobs from the queue for each user and estimate the ""scheduling iteration"" at which each iteration of the scheduler each chunk of user jobs would be scheduled. We sort the overall set of jobs that we've chosen by the ""scheduling iteration"". We also include the regions as part of the sorting queries with None (any region) being sorted last. This is to compact the free cores across jobs so as to avoid fragmentation of instances created and for jobs with no region specifications to fill in the remaining cores in any region. For the hailtop.batch client, I added a new setting in `~/.config/hail` to set the default regions for all jobs in the ServiceBackend and a new method on `Job` that sets the list of regions to run in. Things to double check once everything is working is the sort orders on the scheduling queries are correct. . Once this PR goes in, then we can merge #11840 with some minor changes. There will also be a follow-up PR that gets rid of the CI-specific code in the scheduler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221:193,avail,available,193,https://hail.is,https://github.com/hail-is/hail/pull/12221,1,['avail'],['available']
Availability,"CHANGELOG: BatchPoolExecutor now raises an informative error message for a variety of ""system"" errors, such as missing container images. If the main container fails for reasons beyond BatchPoolExecutor's control, such; as a missing container image, we previously did not report these errors. In; fact, we encountered errors when trying to load the output file that cannot; exist if the main container errors. Smaller included changes:; - directly use the asynchronous, low-level client instead of the synchronous,; low-level client; - introduce an `async_cancel` now that we have access to the async client.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9543:55,error,error,55,https://hail.is,https://github.com/hail-is/hail/pull/9543,5,['error'],"['error', 'errors']"
Availability,CHANGELOG: Bokeh 3.x.x is now required which allows for compatibility with pandas 2.x.x. There were a lot of issues in this file that I tried to clean up. The mypy errors are now just things that are poorly typed by Bokeh. I also changed how manhattan plot hides irrelevant tooltip information. I made it an explicit argument to the `_get_scatter_plot_elements`. I verified the plots in the GWAS Tutorial look the same by eye. I also verified the tooltips look the same with a few spot checks. I also built the docs to ensure those were fine.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12927:164,error,errors,164,https://hail.is,https://github.com/hail-is/hail/pull/12927,1,['error'],['errors']
Availability,CHANGELOG: Changed cost per instance from $0.02170 to $0.021935 from switching to using local SSDs. - Added 1 local SSD (375 GB) and formatted it in the worker run script.; - Changed the resource for boot-disk to just disk and modified the worker config. I figured there was no reason to have a separate boot disk in the resources as long as all disks are assumed to be fractions of the instance based on the number of cores being used.; - Changed the worker boot disk from 100 GB to 20 GB; - Changed the worker to move all docker files and batch files to the Local SSD from the boot disk. Can you double check my math for the documentation?. Is it possible it takes longer for an instance to boot up with a local SSD? One of my earlier tests had workers stuck in STAGING. This resolved itself later on so I'm assuming it was a Google error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8844:835,error,error,835,https://hail.is,https://github.com/hail-is/hail/pull/8844,1,['error'],['error']
Availability,"CHANGELOG: Fix #13356 and fix #13409. In QoB pipelines with 10K or more partitions, transient ""Corrupted block detected"" errors were common. This was caused by incorrect retry logic. That logic has been fixed. I now assume we cannot reuse a ReadChannel after any exception occurs during read. We also do not assume that the ReadChannel ""atomically"", in some sense, modifies the ByteBuffer. In particular, if we encounter any error, we blow away the ByteBuffer and restart our read entirely. As I described in [this comment to #13409](https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184), I have a 10K partition pipeline which was reliably producing this error but now reliably *does not* produce this error (it produces another one, #13721, fix forthcoming for that too).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13730:121,error,errors,121,https://hail.is,https://github.com/hail-is/hail/pull/13730,6,"['error', 'reliab']","['error', 'errors', 'reliably']"
Availability,CHANGELOG: Fix #13937 caused by faulty library code in the Google Cloud Storage API Java client library.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14022:32,fault,faulty,32,https://hail.is,https://github.com/hail-is/hail/pull/14022,1,['fault'],['faulty']
Availability,"CHANGELOG: Fix #13979, affecting Query-on-Batch and manifesting most frequently as ""com.github.luben.zstd.ZstdException: Corrupted block detected"". This PR upgrades google-cloud-storage from 2.29.1 to 2.30.1. The google-cloud-storage java library has a bug present at least since 2.29.0 in which simply incorrect data was returned. https://github.com/googleapis/java-storage/issues/2301 . The issue seems related to their use of multiple intremediate ByteBuffers. As far as I can tell, this is what could happen:. 1. If there's no channel, open a new channel with the current position.; 2. Read *some* data from the input ByteChannel into an intermediate ByteBuffer.; 3. While attempting to read more data into a subsequent intermediate ByteBuffer, an retryable exception occurs.; 4. The exception bubbles to google-cloud-storage's error handling, which frees the channel and loops back to (1). The key bug is that the intermediate buffers have data but the `position` hasn't been updated. When we recreate the channel we will jump to the wrong position and re-read some data. Lucky for us, between Zstd and our assertions, this usually crashes the program instead of silently returning bad data. This is the third bug we have found in Google's cloud storage java library. The previous two:. 1. https://github.com/hail-is/hail/issues/13721; 2. https://github.com/hail-is/hail/issues/13937. Be forewarned: the next time we see bizarre networking or data corruption issues, check if updating google-cloud-storage fixes the problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14080:832,error,error,832,https://hail.is,https://github.com/hail-is/hail/pull/14080,1,['error'],['error']
Availability,"CHANGELOG: Fix `RuntimeError: This event loop is already running` error when running hail in a Jupyter Notebook. Man this is really complicated. OK, so, things I learned:. 1. [asyncio will not create a new event loop if `set_event_loop` has been called even if `set_event_loop(None)` has since been called.](https://github.com/python/cpython/blob/main/Lib/asyncio/events.py#L676); 2. [asyncio will not create a new event loop in a thread other than the main thread.](https://github.com/python/cpython/blob/main/Lib/asyncio/events.py#L677); 3. `aiohttp.ClientSession` stashes a copy of the event loop present when it starts. This can cause all manner of extremely confusing behavior if you later change the event loop or use that session from a different thread. The fix, in the end, wasn't that complicated. Anywhere Hail explicitly asks for an event loop (so that we can run async code), we apply nest asyncio if the event loop is already running. Otherwise we do nothing. Nest asyncio appears to [no longer require](https://github.com/erdewit/nest_asyncio/tree/master#usage) `apply` to be called before the event loop starts running. This PR *does not* address:; 1. Hail nesting async code in sync code in async code. I think we should avoid this, but the `hailtop.fs` and `hailtop.batch` APIs, among others, need async versions before we can do that.; 2. This `aiohttp.ClientSession` nonsense. We really should take pains to ensure we create one `ClientSession` per loop and we never mix loops.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13899:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/13899,1,['error'],['error']
Availability,"CHANGELOG: Fix a major correctness bug ocurring when calling `BlockMatrix.transpose` on sparse BlockMatrices. Symmetric matrices are not affected. It seems like `BlockMatrix.transpose` has been broken for a while when the matrix is sparse. . I added `PerBlockMatrixSparsifier` as a way to sparsify particular blocks of a `BlockMatrix` from python. This is just so we can write tests / diagnose user errors based on sparsity patterns. I also wrote a helper function to sparsify numpy matrices for testing purposes. . The crucial fix here is to `GridPartitioner.transpose`. That function is supposed to return a pair of the form `(GridPartitioner, Int => Int)`, where the first of the pair is the new `GridPartitioner` for the transposed thing, and the second of the pair is a function that takes in a partition number and returns the partition number of its parent partition. Crucially, it's a function from new partition ids to old partition ids. I believe that code I'm removing did the opposite. Refresher on `GridPartitioner`: There are 3 coordinate systems:. There's ""coordinate"", which is (row, column). There's ""blockIndex"", which is the column major numbering of all blocks. And there's ""partitionIndex"", which is similar to numbering by blockIndex but it skips the sparse blocks",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8867:399,error,errors,399,https://hail.is,https://github.com/hail-is/hail/pull/8867,1,['error'],['errors']
Availability,CHANGELOG: Fix bug made more likely by 0.2.101 in which Hail errors when interacting with a NumPy integer or floating point type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12278:61,error,errors,61,https://hail.is,https://github.com/hail-is/hail/pull/12278,1,['error'],['errors']
Availability,"CHANGELOG: Fix long-standing bug wherein `hl.agg.collect_as_set` and `hl.agg.counter` error when applied to types which, in Python, are unhashable. For example, `hl.agg.counter(t.list_of_genes)` will not error when `t.list_of_genes` is a list. Instead, the counter dictionary will use `FrozenList` keys from the `frozenlist` package. Hey @iris-garden ! I figured this was good reviewing practice for you and also a chance to see how we convert data to/from JSON and to/from the JVM (by way of this ""encoded"" representation which is a binary one). The details of that are not super important to this PR, but you might take a peek to understand the change. The main issue here is that in Python, you can't write:; ```; {[1]}; ```; Because sets must contain ""hashable"" data. Python lists are not hashable because they're mutable. This is transitively a problem. For example, the following also fails with the same error because the list inside the tuple is mutable thus the tuple is not (safely) hashable.; ```; {(""tuples"", ""are"", ""immutable"", [""lists"", ""are"", ""not""])}; ```. Hail's internal language is fully immutable, so every type can be placed inside a set (or used as the keys of a dict). When we convert from Hail's internal representation to Python, we cannot use mutable types in hashable positions. Unfortunately, we also need to maintain backwards compatibility with the way the code currently works. You can see this pretty clearly in the difference between `hl.agg.collect` and `hl.agg.collect_as_set`:; ```; t = hl.utils.range_table(1); t = t.annotate(ls = [1, 2, 3]); collected_ls = t.aggregate(hl.agg.collect(t.ls)); collected_as_set_ls = t.aggregate(hl.agg.collect_as_set(t.ls)); ```; `collected_ls` should be `[[1, 2, 3]]` whereas `collected_as_set_ls` necessarily uses hashable types: `{frozenlist([1, 2, 3])}`. Things are particularly subtle with dictionaries whose keys must always be hashable and whose values need only be hashable if the dictionary itself must be hashable. For exa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12265:86,error,error,86,https://hail.is,https://github.com/hail-is/hail/pull/12265,3,['error'],['error']
Availability,"CHANGELOG: Fixed #13346. Previously, when parsing VCFs, Hail failed on INFO or FORMAT fields with missing elements because the meaning of ""."" could be ambiguous. Hail now resovles the ambiguity, when possible, using the number of alleles. If the meaning is still ambiguous after considering the number of alleles, Hail uses a new `hl.import_vcf` parameter to resolve the ambiguity. See the `hl.import_vcf` docs for details. See https://github.com/hail-is/hail-rfcs/pull/8 for details on the problem and the solution. I assessed the effect of removing the `array_elements_required=True` fast path by evaluating the following code against this PR's tip commit `cd06c248e4` and `0.2.120` (`f00f916faf`). I ran it three times per commit and report each individual time as well as the average. ```; In [1]: import hail as hl. In [2]: %%time; ...: mt = hl.import_vcf(; ...: '/Users/dking/projects/hail-data/ALL.chr21.raw.HC.vcf.bgz'; ...: ); ...: mt._force_count_rows(); ```. | commit | run 1 (s) | run 2 (s) | run 3 (s) | average (s) | warm average (s) |; |--------------------------|-----------|-----------|-----------|-------------|------------------|; | `cd06c248e4` (this PR) | 116s | 80s | 77s | 91+-18 | 78.5 +- 1.5 |; | `f00f916faf` (`0.2.120`) | 112s | 80s | 79s | 90+-15 | 79.5 +- 0.5 |. This is what I expected. For a VCF with no ambiguity and few instances of ""."", we've added a very minor amount of new work. ---. Note that I had to specifically override the Number setting for certain FORMAT and INFO fields because they were set to `.` in the benchmarked VCF. If this error appears in a 1KG VCF, it must be fairly common.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13465:1577,error,error,1577,https://hail.is,https://github.com/hail-is/hail/pull/13465,1,['error'],['error']
Availability,"CHANGELOG: Fixed a bug in `hail.ggplot.scale_color_continuous` that sometimes caused errors by generating invalid colors. For example, this code:. ```python; import hail as hl; from hail.ggplot import ggplot, aes, geom_point; t = hl.utils.range_table(10); t = t.annotate(x=hl.rand_unif(), y=hl.rand_unif(), z=hl.rand_unif(1, 10)); fig = ggplot(t, aes(x=t.x, y=t.y, color=t.z)) + geom_point(); fig.show(); ```. Produces an error like this:. ```; ValueError:; Invalid element(s) received for the 'color' property of scatter.marker; Invalid elements include: ['rgb(-84, -187, 123)', 'rgb(-116, -227, 131)', 'rgb(-29, -120, 109)', 'rgb(-33, -125, 110)', 'rgb(15, -65, 97)', 'rgb(10, -71, 99)', 'rgb(-112, -223, 130)', 'rgb(-63, -162, 117)', 'rgb(-81, -185, 122)', 'rgb(31, -45, 93)']; ```. But after this change is applied, it produces a plot like this:. ![newplot(2)](https://github.com/hail-is/hail/assets/84595986/0da82782-2409-46f6-b924-eea72eb6ce97)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13609:85,error,errors,85,https://hail.is,https://github.com/hail-is/hail/pull/13609,2,['error'],"['error', 'errors']"
Availability,CHANGELOG: Fixed bug in reading tables/matrixtables with partition intervals that led to error or segfault.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12277:89,error,error,89,https://hail.is,https://github.com/hail-is/hail/pull/12277,1,['error'],['error']
Availability,"CHANGELOG: Fixed bugs in the identity by descent implementation for Query on Batch. This PR fixes #14052. There were two bugs in how we compute IBD. In addition, the tests weren't running in QoB and the test dataset we were using doesn't have enough variability to catch errors. I used Balding Nichols generated data instead. Do we need to set the seed in the tests here?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14062:271,error,errors,271,https://hail.is,https://github.com/hail-is/hail/pull/14062,1,['error'],['errors']
Availability,CHANGELOG: Fixed compiler errors related to interval filter pushdown.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9179:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/9179,1,['error'],['errors']
Availability,"CHANGELOG: Fixed incorrect error message when incorrect type specifid with hl.loop. I added a test that gave a bad error message, then rearranged code in `hl.loop` to improve the error message. Prior to this change, the error a user would get here is that they wrote a loop that isn't tail recursive, because hail would insert an implicit cast when trying to unify types, and the casting would wrap the recursive loop call. Now, we check to make sure the loop's return type is correct before analyzing whether it's tail recursive, which I believe removes the possibility of getting a tail recursion error when you should be getting a type error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10174:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/pull/10174,6,['error'],['error']
Availability,"CHANGELOG: Fixed issue where ndarrays being sliced and indexed into in one expression didn't have sufficient bounds checks. Fixes #9144 by checking if indices provided in a slicing expression that mixes slices and single indices are out of bounds. . Alex, please tell me if you're able to get anymore errors from #9144 with this change. I wasn't able to, but that issue was not super well defined at first so it's unclear to me if this covers all of your problems. I retitled the issue to reflect my understanding of the problems. . More detail:. In numpy, it is ok for the upper bound of a slice to go past the end of an array, but it is not ok for an indexing operation to do the same. For example:. ```; n = np.array([[1,2,3,4], [1,2,3,4]]); n[0:1000, 0:1000]; ```. is allowed, the bounds just get clamped. However, this is not allowed:. ```; n[1000, 1000]; ```. nor is:. ```; n[1000, 0:1000]; ```. Hail was not handling that last case with a mix of slices and indices correctly. Namely, it was not throwing an out bounds error on the first axis index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9223:301,error,errors,301,https://hail.is,https://github.com/hail-is/hail/pull/9223,2,['error'],"['error', 'errors']"
Availability,CHANGELOG: Fixed the error 'Argument list too long: '/bin/sh'' when using the LocalBackend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10508:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/10508,1,['error'],['error']
Availability,"CHANGELOG: Fixes #13697, a long standing issue with QoB, in which a failing partition job or driver job is not failed in the Batch UI. I am not sure why we did not do this this way in the first place. If a JVMJob raises an exception, Batch will mark the job as failed. Ergo, we should raise an exception when a driver or a worker fails!. Here's an example: I used a simple pipeline that write to a bucket to which I have read-only access. You can see an example Batch (where every partition fails): https://batch.hail.is/batches/8046901. [1]. ```python3; import hail as hl; hl.utils.range_table(3, n_partitions=3).write('gs://neale-bge/foo.ht'); ```. NB: I removed the `log.error` in `handleForPython` because that log is never necessary. That function converts a stack of exceptions into a triplet of the short message, the full exception with stack trace, and a Hail error id (if present). That triplet is always passed along to someone else who logs the exception. (FWIW, the error id indicates a Python source location that is associated with the error. On the Python-side, we can look up that error id and provide a better stack trace.). [1] You'll notice the logs are missing. I noticed this as well, it's a new bug. I fixed it in https://github.com/hail-is/hail/pull/13729.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13715:674,error,error,674,https://hail.is,https://github.com/hail-is/hail/pull/13715,5,['error'],['error']
Availability,"CHANGELOG: Fixes #13704, in which Hail could encounter an IllegalArgumentException if there are too many transient errors. I need to do the multiplication in 64-bits so that it does not wrap around to a large negative value. Then I can use `math.min` with the maxDelayMs to get us back into 32-bits. I'm just pushing through a bunch of bugs to get Wenhan unblocked today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13713:115,error,errors,115,https://hail.is,https://github.com/hail-is/hail/pull/13713,1,['error'],['errors']
Availability,"CHANGELOG: Hail Query-on-Batch previously used Class A Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I fell down the rabbit hole of not issuing ""list"" operations unless we really need to know if a file is a directory. This should partly help reduce the flakiness in Azure (which is tracked in #13351) as well as high costs in Azure both of which are at least partly suspected to be caused by our frequent use of ""list"" operations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13450:236,down,down,236,https://hail.is,https://github.com/hail-is/hail/pull/13450,2,['down'],['down']
Availability,"CHANGELOG: Hail no longer officially supports Python 3.7. Combines https://github.com/hail-is/hail/pull/12927 and https://github.com/hail-is/hail/pull/12908. Many changes. All seem to be necessary together. ---. I fixed any new mypy errors or deprecation warnings. I also cleaned up plots.py (which isn't CI'd by mypy) because I was in there and it was a mess. I unified all our pip-tools versions. Require pandas 2 now. That requires Bokeh 3.x.x. Fix the pinned-requirements.txt dependencies so they reflect the actual necessary runtime harmony. Upgraded Sphinx. The method names lose their fixed-width styling but I think it looks fine. Version policy added. Updating everything means Jinja2 can jump to the latest version too. Numpy deprecated some of its types, using `_` gets rid of the warning.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12929:233,error,errors,233,https://hail.is,https://github.com/hail-is/hail/pull/12929,1,['error'],['errors']
Availability,"CHANGELOG: Hail supports identity_by_descent on Apple M1 and M2 chips; however, your Java installation must be an arm64 installation. Using x86_64 Java with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an Apple M1 or Apple M2 and `/usr/libexec/java_home -V` does not include `(arm64)`, you must switch to an arm64 version of the JVM. Fixes (hail#14000). Fixes #14000. Hail has never supported its native functionality on Mac OS X Apple M1 chips. In particular, we only built x86_64 compatible dylibs. M1 chips will try to simulate a very basic x86_64 ISA using Rosetta 2 but our x86_64 dylibs expect the ISA of at least sandybridge, which includes some SIMD instructions not supported by Rosetta 2. This PR bifurcates our native build into x86_64 and arm64 targets which live in build/x86_64 and build/arm64, respectively. In Linux, this moves where the object files live, but should otherwise have no effect. The test and benchmark targets use the ""native"" build which always points at the x86_64 object files. The shared object targets, LIBBOOT & LIBHAIL, explicitly depend on x86_64 because that is the only linux architecture we support. In OS X, we only test and benchmark the ""native"" build, which is detected using `uname -m`. For the shared objects (the dylibs) we have four new files: libboot and libbhail for x86_64 and for arm64. Each pair files is placed in `darwin/x86_64/` and `darwin/arm64/`, respectively. Those dylibs are never meant to escape the src/main/c world. The LIBBOOT and LIBHAIL targets (which are invoked by hail/Makefile) combine the two architecture-specific dylibs into a ""universal"" dylib. You can verify this by running `file` on the dylibs. Here I run them on the new ""prebuilt"" files which are in this PR:. ```; (base) dking@wm28c-761 hail % file hail/prebuilt/lib/darwin/libboot.dylib; hail/prebuilt/lib/darwin/libboot.dylib: Mach-O universal binary with 2 architectures: [x86_64:Mach-O 64-bit dynamically linked shared library x86_64] [arm64:Mach-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14006:198,error,errors,198,https://hail.is,https://github.com/hail-is/hail/pull/14006,1,['error'],['errors']
Availability,"CHANGELOG: Improve error message when combining incompatibly indexed fields in certain operations including array indexing. See test cases for straightforward examples. In main, none of the test code triggers errors. You have to execute the table to actually trigger an error in IR serialization which will reference `""sa""` which is totally meaningless to the user (let alone many Hail engineers).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12566:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/12566,3,['error'],"['error', 'errors']"
Availability,"CHANGELOG: In Query-on-Batch, retries of certain errors has been increased from once to five times. This should reduce the occurrence of transient errors such as ""Connection reset"" and `SocketException`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13012:49,error,errors,49,https://hail.is,https://github.com/hail-is/hail/pull/13012,2,['error'],['errors']
Availability,"CHANGELOG: In Query-on-Batch, retries of certain errors has been increased from once to five times. This should reduce the occurrence of transient errors such as ""Connection reset"" and `SocketException`. ---. The old approach doesn't work because it doesn't have the retry logic around the invocation. Moreover, the old approach wouldn't retry transient errors encountered after a retry once error. The new approach address both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13021:49,error,errors,49,https://hail.is,https://github.com/hail-is/hail/pull/13021,4,['error'],"['error', 'errors']"
Availability,"CHANGELOG: Introduce `hailctl fs sync` which robustly transfers one or more files between Amazon S3, Azure Blob Storage, and Google Cloud Storage. There are really two distinct conceptual changes remaining here. Given my waning time available, I am not going to split them into two pull requests. The changes are:. 1. `basename` always agrees with the [`basename` UNIX utility](https://en.wikipedia.org/wiki/Basename). In particular, the folder `/foo/bar/baz/`'s basename is *not* `''` it is `'baz'`. The only folders or objects whose basename is `''` are objects whose name literally ends in a slash, e.g. an *object* named `gs://foo/bar/baz/`. 2. `hailctl fs sync`, a robust copying tool with a user-friendly CLI. `hailctl fs sync` comprises two pieces: `plan.py` and `sync.py`. The latter, `sync.py` is simple: it delegates to our existing copy infrastructure. That copy infastructure has been lightly modified to support this use-case. The former, `plan.py`, is a concurrent file system `diff`. `plan.py` generates and `sync.py` consumes a ""plan folder"" containing these files:. 1. `matches` files whose names and sizes match. Two columns: source URL, destination URL. 2. `differs` files or folders whose names match but either differ in size or differ in type. Four columns: source URL, destination URL, source state, destination state. The states are either: `file`, `dif`, or a size. If either state is a size, both states are sizes. 3. `srconly` files only present in the source. One column: source URL. 4. `dstonly` files only present in the destination. One column: destination URL. 5. `plan` a proposed set of object-to-object copies. Two columns: source URL, destination URL. 6. `summary` a one-line file containing the total number of copies in plan and the total number of bytes which would be copied. As described in the CLI documentation, the intended use of these commands is:. ```; hailctl fs sync --make-plan plan1 --copy-to gs://gcs-bucket/a s3://s3-bucket/b; hailctl fs sync --use",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14248:45,robust,robustly,45,https://hail.is,https://github.com/hail-is/hail/pull/14248,3,"['avail', 'robust']","['available', 'robust', 'robustly']"
Availability,"CHANGELOG: Mitigate new transient error from Google Cloud Storage which manifests as `aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548)`. As of around 1500 ET 2023-10-16, this exception happens whenever we issue a lot of requests to GCS. See [Zulip thread](https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/cluster.20size/near/396777320).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13817:34,error,error,34,https://hail.is,https://github.com/hail-is/hail/pull/13817,1,['error'],['error']
Availability,"CHANGELOG: Raise an error instead of silently returning bad results when using a negative index with an `hl.nd.array`. In practice, I always saw this return a positive value very close to zero. We should fix this to just support Python-style indexing, but I wanted to fix ASAP given the severity.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12803:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/12803,1,['error'],['error']
Availability,"CHANGELOG: Requester pays buckets now work in `hailtop.fs` and `hl.hadoop_*`. This has been broken since at least 0.2.115. I first check for an explicit argument or the standard hailctl configuration. If neither of those exist, I try to parse spark-defaults.conf with lots of error handling and warning.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13089:276,error,error,276,https://hail.is,https://github.com/hail-is/hail/pull/13089,1,['error'],['error']
Availability,"CHANGELOG: Since 0.2.110, `hailctl dataproc` set the heap size of the driver JVM dangerously high. It is now set to an appropriate level. This issue manifests in a variety of inscrutable ways including RemoteDisconnectedError and socket closed. See issue #13960 for details. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced [""memory protection""](https://cloud.google.com/dataproc/docs/support/troubleshoot-oom-errors#memory_protection) which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to 41g (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14066:429,error,errors,429,https://hail.is,https://github.com/hail-is/hail/pull/14066,1,['error'],['errors']
Availability,"CHANGELOG: Teach `hailctl dataproc start` about `--expiration-time`. Teach `hailctl dataproc modify` about `--no-max-idle`, `no-max-age`, `--max-age`, and `--expiration-time`. These flags were always available as pass-throughs. This change adds them to the; help documentation and improves error messages in the case that an incompatible; set of arguments are provided together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9263:200,avail,available,200,https://hail.is,https://github.com/hail-is/hail/pull/9263,2,"['avail', 'error']","['available', 'error']"
Availability,"CHANGELOG: Teach `hailctl dataproc` to use new `gcloud` flag names which suppresses the warnings about `--num-secondary-workers`. `hailctl` now requires at least gcloud 284.0.0. Run `gcloud components update` to update. This has been available since March, it seems high time to fix this and recommend people update gcloud.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9262:234,avail,available,234,https://hail.is,https://github.com/hail-is/hail/pull/9262,1,['avail'],['available']
Availability,"CHANGELOG: The Batch LocalBackend now supports `always_run` jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded. See [Zulip thread](https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/.60always_on.60.20for.20local.20backend/near/341620608) for some discussion. The question to whether we can support `always_run` in the local backend depends on whether to make a breaking change in the local backend where a failure in a job does not immediately raise an exception. Seemed there was no issue with this on zulip.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12780:110,error,error,110,https://hail.is,https://github.com/hail-is/hail/pull/12780,2,"['error', 'failure']","['error', 'failure']"
Availability,CHANGELOG: The Resource class is now also available at hailtop.batch.Resource. I screwed up my branches. This is just #9515.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9575:42,avail,available,42,https://hail.is,https://github.com/hail-is/hail/pull/9575,1,['avail'],['available']
Availability,CHANGELOG: The `Resource` class is now also available at hailtop.batch.Resource.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9515:44,avail,available,44,https://hail.is,https://github.com/hail-is/hail/pull/9515,1,['avail'],['available']
Availability,CHANGELOG: The `batch_size` parameter of `vds.new_combiner` is deprecated in favor of `gvcf_batch_size`. This avoids a confusing error message: https://dev.hail.is/t/vds-new-combiner/269/4?u=dking. @tpoterba do you think this is better?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12213:129,error,error,129,https://hail.is,https://github.com/hail-is/hail/pull/12213,1,['error'],['error']
Availability,"CHANGELOG: Use indexed VEP cache files for GRCh38 on both dataproc and QoB. Fixes #13989. In this PR, I did the following:; 1. Installed samtools into the Docker image to get rid of errors in the log output; 2. Added the `--merged` flag so that VEP will use the directory `homo_sapiens_merged` for the cache. Outstanding Issues:; 1. The FASTA files that are in `homo_sapiens/` were not present in the merged dataset. Do we keep both the `homo_sapiens` and `homo_sapiens_merged/` directories in our bucket or do we transfer the FASTA files to the merged directory?; 2. Once we decide the answer to (1), then I can fix this in dataproc. The easiest thing to do is to add the tar file with the `_merged` data to the dataproc vep folders and use the `--merged` flag. However, that will double the startup time for VEP on a worker node in dataproc. Before:; <img width=""617"" alt=""Screenshot 2023-12-05 at 12 42 16 PM"" src=""https://github.com/hail-is/hail/assets/1693348/bee7fff5-782c-4f19-aa88-26383ed386b7"">. After:; <img width=""619"" alt=""Screenshot 2023-12-05 at 12 46 30 PM"" src=""https://github.com/hail-is/hail/assets/1693348/3d731759-6c69-4f1c-9c73-92bfb05c239a"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071:182,error,errors,182,https://hail.is,https://github.com/hail-is/hail/pull/14071,1,['error'],['errors']
Availability,"CHANGELOG: `PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run. inspect.Signature allows you to inspect the signature of some function, and [inspect.Signature.bind](https://docs.python.org/3/library/inspect.html#inspect.Signature.binds) takes arguments to the function and maps those arguments to method parameters. If the invocation is invalid, i.e. would raise a TypeError when *actually* invoking the function with those arguments, this method raises a similar error. This can help a user immediately catch something like not passing the correct number of arguments to a PythonJob call without having to submit and run it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12734:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/pull/12734,3,['error'],"['error', 'erroring', 'errors']"
Availability,"CHANGELOG: `hailctl dataproc` now creates clusters using Dataproc version 2.1.33. It previously used version 2.1.2. Dataproc verifications:; 1. Memory available after startup: `mem avail: 42959 of 52223 MiB`. OK.; 2. SparkMonitor still shows up.; 3. Native code (`hl.identity_by_descent`) still works. The Dataproc version list only keeps the most recent five, sadly. Lucky for us, archive.org happens to have a capture with 2.1.2. I also requested a capture of the current state of the page. - https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1. - 2023-03-07: https://web.archive.org/web/20230307225815/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1. - 2023-12-12: https://web.archive.org/web/20231212001716/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1. | Component | 2.1.33-debian11 | 2.1.2-debian11 |; | --- | --- | --- |; | Apache Atlas | 2.2.0 | 2.2.0 |; | Apache Flink | 1.15.4 | 1.15.0 |; | Apache Hadoop | 3.3.6 | 3.3.3 |; | Apache Hive | 3.1.3 | 3.1.3 |; | Apache Hive WebHCat | 3.1.3 | 3.1.3 |; | Apache Hudi | 0.12.3 | 0.12.0 |; | Apache Kafka | 3.1.0 | 3.1.0 |; | Apache Pig | 0.18.0-SNAPSHOT | 0.18.0-SNAPSHOT | | Apache Spark | 3.3.2 | 3.3.0 |; | Apache Sqoop v1 | 1.5.0-SNAPSHOT | 1.5.0-SNAPSHOT | | Apache Sqoop v2 | N/A | 1.99.6 |; | Apache Tez | N/A | 0.10.1 |; | Cloud Storage Connector | hadoop3-2.2.18 | hadoop3-2.2.9 | | Conscrypt | 2.5.2 | 2.5.2 |; | Docker | 20.10 | 20.10 |; | Hue | 4.10.0 | 4.10.0 |; | Java | 11 | 11 |; | JupyterLab Notebook | 3.4 | 3.4 |; | Oozie | 5.2.1 | 5.2.1 |; | Trino | 376 | 376 |; | Python | Python 3.10 | Python 3.10 |; | R | R 4.1 | R 4.1 |; | Ranger | 2.2.0 | 2.2.0 |; | Scala | 2.12.18 | 2.12.14 |; | Solr | 9.0.0 | 9.0.0 |; | Zeppelin Notebook | 0.10.1 | 0.10.1 |; | Zookeeper | 3.8.3 | 3.8.0 |. Here's a diff. I think the things that may affect us are:. 1. Scala patch version; 4. Cloud Storage Connector patch version (this is the motivation fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14093:151,avail,available,151,https://hail.is,https://github.com/hail-is/hail/pull/14093,2,['avail'],"['avail', 'available']"
Availability,"CHANGELOG: `hl.import_table` is up to twice as fast for small tables. The big change is optimizing for the single file, no filters case in which; we need not scan for the first extant row, that row *must* be in the first; partition, if it exists at all. Unfortunately there is no zero-RPC way to; determine the number of partitions in a table, so I must catch an error; about the lack of a zeroth partition. I also did some refactoring:. 1. Move some functions to a utility file and add lots of indents and newlines to make them readable.; 2. Use `hl.format` for constructing strings.; 3. Make `should_filter_line` into `should_remove_line` for clarity of name.; 4. Modify `should_remove_line` to use short-circuiting and/or instead of array folds.; 5. Modify `should_remove_line` to indicate (via returning None) when there are no filters enabled.; 6. Add types.; 7. Fix a bug where we assumed that `.collect()[0]` would be `None` if there were no values in the table. (It raises an error); 8. Deduplicate `hail.utils.deduplicate` (haha: I mean, there is already code for doing field dedupe)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11782:363,error,error,363,https://hail.is,https://github.com/hail-is/hail/pull/11782,2,['error'],['error']
Availability,"CHANGELOG: `hl.logistic_regression_rows`, `hl.poisson_regression_rows`, and `hl.skat` all now support configuration of the maximum number of iterations and the tolerance. I had to make some fixes to how we count the number of iterations. It was quite screwy. Now it should reliably report the correct number of iterations regardless of failure, non-convergence, or explosion. This was requested [on Zulip](https://hail.zulipchat.com/#narrow/stream/127634-Feature-Requests/topic/Convergence.20issues.20with.20hl.2Elogistic_regression_rows).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11759:160,toler,tolerance,160,https://hail.is,https://github.com/hail-is/hail/pull/11759,3,"['failure', 'reliab', 'toler']","['failure', 'reliably', 'tolerance']"
Availability,CHANGELOG: `hl.nd.vstack` and `hl.nd.hstack` provide clear error messages when used with incompatible matrices or with no arguments.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12636:59,error,error,59,https://hail.is,https://github.com/hail-is/hail/pull/12636,1,['error'],['error']
Availability,"CHANGELOG: fix LocalBackend.run() succeeding when intermediate command fails. Stacked on #9219 as that PR is essentially approved, and to avoid a merge conflict. The commit in this PR is https://github.com/hail-is/hail/pull/9297/commits/cbc3bbe7f14c01d44c89995a03375d983fc14f4f. Caused by associativity of the ternary conditional ('set -e' + 'x' is the operand `a` in `a if cond else b`). Easy reproduction case on main:. ```python; def test_single_job_with_mixed_shells(self):; b = self.batch(); j = b.new_job(); j.command(f'echoddd ""hello""'); j2 = b.new_job(); j2.command(f'echo ""world""'). self.assertRaises(Exception, b.run); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9297:526,echo,echoddd,526,https://hail.is,https://github.com/hail-is/hail/pull/9297,2,['echo'],"['echo', 'echoddd']"
Availability,"CHANGELOG: make hail's optimization rewriting filters to interval-filters smarter and more robust. Completely rewrites ExtractIntervalFilters. Instead of matching against very specific patterns, and failing completely for things that don't quite match (e.g. an input is let bound, or the fold implementing ""locus is contained in a set of intervals"" is written slightly differently), this uses a standard abstract interpretation framework, which is almost completely insensitive to the form of the IR, only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:91,robust,robust,91,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['robust'],['robust']
Availability,"CI is currently erroring on line 555 because when it starts up `last_known_github_status` is empty. Pre #11156, `last_known_github_status` initialized to `None` so using `get` here should have the same effect, and allow CI to post statuses to github.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11222:16,error,erroring,16,https://hail.is,https://github.com/hail-is/hail/pull/11222,1,['error'],['erroring']
Availability,"CI was getting 422s from GitHub. Using a; `raise_for_status=True` ClientSession circumvented gidgethubs native; error handling logic smothering the HTTP response body where github; places critical debugging information. Aiohttp is aware that; `raise_for_status` provides no access to the response body. They addressed; this in https://github.com/aio-libs/aiohttp/pulls/3892, but that has not; been released because 4.0.0 has not yet been released. Moreover, `gidgethub` incorrectly handles the too many statuses response. I'll PR a fix into their repo. For now, I've added a bit more information the logs and fixed the main issue, the missing `['status']`. Another relevant issue: https://github.com/aio-libs/aiohttp/issues/4600.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8480:112,error,error,112,https://hail.is,https://github.com/hail-is/hail/pull/8480,1,['error'],['error']
Availability,Call htsjdk's disableOnTheFlyModifications do disable header repairs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6013:61,repair,repairs,61,https://hail.is,https://github.com/hail-is/hail/pull/6013,1,['repair'],['repairs']
Availability,"Calling `fb.result()` generates stalled execution, no warnings or errors raised. Should generate some error message, or potentially the result (memory address). cc @catoverdrive . Test case. ```scala; def testString() {; val rt = PString(); val input = ""hello""; val fb = FunctionBuilder.functionBuilder[Region, String, Long]; val srvb = new StagedRegionValueBuilder(fb, rt). fb.emit(; Code(; srvb.start(),; srvb.addString(fb.getArg[String](2)),; srvb.end(); ); ). val region = Region(); val rv = RegionValue(region). val res1 = fb.result()()(region, input); println(""Past res1""). val res2 = fb.result()()(region, input); // never reached; println(""Past res2""); }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7384:66,error,errors,66,https://hail.is,https://github.com/hail-is/hail/issues/7384,2,['error'],"['error', 'errors']"
Availability,"Can't really test this easily, unfortunately. CHANGELOG: Fix integer overflow error when reading files >2G with `hl.import_plink`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8948:78,error,error,78,https://hail.is,https://github.com/hail-is/hail/pull/8948,1,['error'],['error']
Availability,"CaseBuilder has an `or_error` method to throw an error if no `when` conditions are true. Currently, SwitchBuilder does not have an equivalent method: it only supports returning a default value or missing. The option to throw an error on an unhandled value can be useful for making sure that all possible values for an enum expression have been accounted for in switch cases.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9749:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/pull/9749,2,['error'],['error']
Availability,"CastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:62:35: required from ‘simdpp::arch_avx2::int16<8>& simdpp::arch_avx2::int16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8>]’; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:55:73: required from ‘simdpp::arch_avx2::int16<8>::int16(const simdpp::arch_avx2::uint16<8, E>&) [with E = void]’; libsimdpp-2.0-rc2/simdpp/detail/insn/i_shift_r.h:42:28: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int16<8>’ with ‘private’ member ‘simdpp::arch_avx2::int16<8>::d_’ from an array of ‘const class simdpp::arch_avx2::uint16<8>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:33:7: note: ‘class simdpp::arch_avx2::int16<8>’ declared here; class int16<8, void> : public any_int16<8, int16<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint16<16>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:104987,error,error,104987,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,"Caveats:; * the copy-to-GS at the end is crashing without a good error message,; but probably permissions, even though I've given my service account; access to that bucket.; * this runs all benchmarks in replicate. We should split them up; in a randomized (deterministic?) way so that the wall time is; shorter.; * needs to dump into a database instead of json files on GS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6908:65,error,error,65,https://hail.is,https://github.com/hail-is/hail/pull/6908,1,['error'],['error']
Availability,"Change ""=="" and ""!="" to throw error message for off-type comparisons",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/405:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/pull/405,1,['error'],['error']
Availability,"Changes to make sure that only the annotation datasets are visible on the docs page, now that the `datasets.json` config file contains all available datasets. Overview:. - In `datasets.json`, moved ""key_properties"" inside an ""annotation_db"" field, like `""annotation_db"": {""key_properties"": []}`, so that only the datasets with the ""annotation_db"" key are shown in the annotation DB docs page. Removed ""key_properties"" from non-annotation datasets. - Minor reformatting changes to docs page, added a reference genome column to the HTML table. - Updated deploy script to reflect the filename change from `annotation_db.json` to `datasets.json`. - Modified checks for keys in dicts from `assert key in doc, doc` to `assert key in doc` in `DatasetVersion.from_json()` and `Dataset.from_name_and_json()`. Since the `doc` that is passed to these methods from the checked in JSON file is just a dict like `doc = {""annotation_db"": {""key_properties"": [...]}, ""description"": ..., ""url"": ..., ""versions"": [...]}` this seems to work fine. Let me know if `key in doc, doc` form was used for other reasons I've overlooked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9546:139,avail,available,139,https://hail.is,https://github.com/hail-is/hail/pull/9546,1,['avail'],['available']
Availability,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8420:249,error,error,249,https://hail.is,https://github.com/hail-is/hail/pull/8420,2,['error'],['error']
Availability,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6746:86,alive,alive,86,https://hail.is,https://github.com/hail-is/hail/pull/6746,3,['alive'],['alive']
Availability,Cheatsheets page with a link to the download,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7570:36,down,download,36,https://hail.is,https://github.com/hail-is/hail/pull/7570,1,['down'],['download']
Availability,"Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3MThjYjgyZC1jNGU3LTRlNWEtODgzZi02NjQ0NjlmYzA4MGEiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjcxOGNiODJkLWM0ZTctNGU1YS04ODNmLTY2NDQ2OWZjMDgwYSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""718cb82d-c4e7-4e5a-883f-664469fc080a"",""prPublicId"":""718cb82d-c4e7-4e5a-883f-664469fc080a"",""dependencies"":[{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.11.2""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-JUPYTERSERVER-6099119""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[461],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Generation of Error Message Containing Sensitive Information](https://learn.snyk.io/lesson/error-message-with-sensitive-information/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14070:3691,Error,Error,3691,https://hail.is,https://github.com/hail-is/hail/pull/14070,2,"['Error', 'error']","['Error', 'error-message-with-sensitive-information']"
Availability,"Christina reports https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/initialization.20action.20failed.20in.20starting.20cluster:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [d0a8142009bf49a1a51f5276576aeddb] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-c3e3c3c1-4a54-41e4-aa06-83d5d2ce80ec-us/google-cloud-dataproc-metainfo/c174dc73-2817-4ec1-8c2d-ae4e0c4f91ae/jobs/d0a8142009bf49a1a51f5276576aeddb/driveroutput'.; Traceback (most recent call last):; File ""/Users/cchen/anaconda/envs/hail-env/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565:160,ERROR,ERROR,160,https://hail.is,https://github.com/hail-is/hail/issues/6565,4,"['ERROR', 'avail', 'error', 'failure']","['ERROR', 'available', 'error', 'failure']"
Availability,Clarify language about data download in GWAS tutorial,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5060:28,down,download,28,https://hail.is,https://github.com/hail-is/hail/pull/5060,1,['down'],['download']
Availability,ClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:2824,Heartbeat,HeartbeatReceiver,2824,https://hail.is,https://github.com/hail-is/hail/issues/4780,2,['Heartbeat'],['HeartbeatReceiver']
Availability,"Closes https://github.com/hail-is/hail/issues/14652. See https://github.com/populationgenomics/hail/pull/346. Thanks for the contribution @illusional!. Gives Dataproc clusters started via `hailctl dataproc start` internet access by default, since we need it to install some of our dependencies, per the error message in the linked issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14653:303,error,error,303,https://hail.is,https://github.com/hail-is/hail/pull/14653,1,['error'],['error']
Availability,"Code:; ```python3; import hail as hl; from hail import ir. hl.init(); mt = hl.import_vcf('path/to/vcf'); mt = hl.MatrixTable(ir.MatrixKeyRowsBy(mt._mir, ['locus'], is_sorted=True)); ht = mt._localize_entries('_e', '_c'); j = hl.Table._multi_way_zip_join([ht, ht], 'd', 'g'); j.write('tst.ht'); ```. Java Stack Trace:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretJSON.; : java.util.NoSuchElementException: key not found: alleles; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:24); 	at is.hail.expr.ir.TableMultiWayZipJoin.execute(TableIR.scala:669); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:775); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:93); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:63); 	at is.hail.expr.ir.Interpret$.interpretJSON(Interpret.scala:22); 	at is.hail.expr.ir.Interpret.interpretJSON(Interpret.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.Reflec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5396:355,error,error,355,https://hail.is,https://github.com/hail-is/hail/issues/5396,1,['error'],['error']
Availability,Codegen cast errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446:13,error,errors,13,https://hail.is,https://github.com/hail-is/hail/issues/3446,1,['error'],['errors']
Availability,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/367:460,Error,Error,460,https://hail.is,https://github.com/hail-is/hail/issues/367,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/309:341,Error,Error,341,https://hail.is,https://github.com/hail-is/hail/issues/309,1,['Error'],['Error']
Availability,"Commandline:. ```; hail -l /xchip/cga_home/gtiao/Hail/hail.re-import.log importvcf $VCF \; filtervariants all \; count \; filtersamples list -i 'file:///xchip/cga_home/gtiao/37k/germline_cancer_joint_calling.restricted_samples.sample_list' --remove \; count \; exportsamples -c 's.id' -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; ```. Error message:. ```; hail: info: running: exportsamples -c s.id -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; hail: exportsamples: fatal: does not support multiallelics.; Run `splitmulti' first.; ```. It works if I insert ""splitmulti"" after the import command, but then I drop all variants, so this seems a very silly requirement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/382:370,Error,Error,370,https://hail.is,https://github.com/hail-is/hail/issues/382,1,['Error'],['Error']
Availability,Commandline:. ```; hail-new -l /home/unix/gtiao/hail.rename.log \; read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds \; renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt \; write -o /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.no_spaces.vds. ```. Error message:. ```; hail: info: running: read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds; [Stage 0:=============================> (1 + 1) / 2]hail: info: running: renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt; hail: renamesamples: caught exception: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt at 175616 exp: -1352655701 got: 441984571. ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/347:405,Error,Error,405,https://hail.is,https://github.com/hail-is/hail/issues/347,2,"['Error', 'error']","['Error', 'error']"
Availability,Compile results in fatal error: lz4.h: No such file or directory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4880:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/4880,1,['error'],['error']
Availability,Compiling NativeBoot.cpp fails with error message about -march=sandybridge flag,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4305:36,error,error,36,https://hail.is,https://github.com/hail-is/hail/issues/4305,1,['error'],['error']
Availability,"Consider an annotate variants like this:. ```; va.annot.annot = if (isMissing(va.annot.annot.gene) && isDefined(va.annot_MESA.gene)) va.annot_MESA; else if (isMissing(va.annot.annot.gene) && isDefined(va.annot_FinEst.annot.gene)) va.annot_FinEst.annot; else va.annot.annot; ```. If `va.annot_MESA` and `va.annot_FinEst.annot` are not the same the following error arrises, but doesn't give much information as to what went wrong. ```; hail: fatal: annotatevariants expr: expected same-type `then' and `else' clause, got `Struct' and `Struct'; <input>:2: else if (isMissing(va.annot.annot.gene) && isDefined(va.annot_FinEst.annot.gene)) va.annot_FinEst.annot ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1167:357,error,error,357,https://hail.is,https://github.com/hail-is/hail/issues/1167,1,['error'],['error']
Availability,Consider that `hl.len(mt.AD) == hl.len(mt.alleles)`. We should `die` if this is unsatisfied instead of throw index out of bounds errors when we index into arrays.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5157:129,error,errors,129,https://hail.is,https://github.com/hail-is/hail/issues/5157,1,['error'],['errors']
Availability,"Consider, for example, https://ci.hail.is/batches/7490668/jobs/240 and https://cloudlogging.app.goo.gl/BZfqkc6SCM5RfPs79 in which a PR fails because hello does not come alive fast enough. Presumably the 10mCPU is a little limiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13123:169,alive,alive,169,https://hail.is,https://github.com/hail-is/hail/pull/13123,1,['alive'],['alive']
Availability,"Consider, for example, this deploy: https://ci.hail.is/batches/7956812. `test-dataproc-37` succeeded but `test-dataproc-38` failed (it timed out b/c the master failed to come online). You can see the error logs for the cluster here: https://cloudlogging.app.goo.gl/t1ux8oqy11Ba2dih7. It states a certain file either did not exist or we did not have permission to access it. [`test_dataproc-37`](https://batch.hail.is/batches/7956812/jobs/193) and [`test_dataproc-38`](https://batch.hail.is/batches/7956812/jobs/194) started around the same time and both uploaded four files into:. gs://hail-30-day/hailctl/dataproc/ci_test_dataproc/0.2.121-7343e9c368dc/. And then set it to public read/write. The public read/write means that permissions are not the issue. Instead, the issue is that there must be some sort of race condition in GCS which means that if you ""patch"" (aka overwrite) an existing file, it is possible that a concurrent reader will see the file as not existing. Unfortunately, I cannot confirm this with audit logs of the writes and read because [public objects do not generate audit logs](https://cloud.google.com/logging/docs/audit#data-access).; > Publicly available resources that have the Identity and Access Management policies [allAuthenticatedUsers](https://cloud.google.com/iam/docs/overview#allauthenticatedusers) or [allUsers](https://cloud.google.com/iam/docs/overview#allusers) don't generate audit logs. Resources that can be accessed without logging into a Google Cloud, Google Workspace, Cloud Identity, or Drive Enterprise account don't generate audit logs. This helps protect end-user identities and information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13573:200,error,error,200,https://hail.is,https://github.com/hail-is/hail/pull/13573,2,"['avail', 'error']","['available', 'error']"
Availability,Consistent scala.MatchError: ArrayBuffer error while running pipeline on Cray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1151:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/issues/1151,1,['error'],['error']
Availability,"Context (which however conflates a few different issues): https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Error.20__C1188collect_distributed_array/near/276162426. Running the `prepare_gnomad_v3_variants` step of the https://github.com/broadinstitute/gnomad-browser pipeline using Hail 0.2.105 results in the following error:. ```; Traceback (most recent call last):; File ""/tmp/bc960dc2a938406794277b1c01c577dd/gnomad_v3_variants.py"", line 53, in <module>; run_pipeline(pipeline); File ""/tmp/bc960dc2a938406794277b1c01c577dd/pyfiles_2z60b0es.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/bc960dc2a938406794277b1c01c577dd/pyfiles_2z60b0es.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/bc960dc2a938406794277b1c01c577dd/pyfiles_2z60b0es.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1066>"", line 2, in write; File ""/opt/conda/default/lib/python3.8/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1335, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 105, in execute; raise e.maybe_user_error(ir) from None; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 99, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: IllegalArgumentException: requirement failed: Invalid method, methods may have at most 255 arguments, found 1163; Retu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:138,Error,Error,138,https://hail.is,https://github.com/hail-is/hail/issues/12533,2,"['Error', 'error']","['Error', 'error']"
Availability,"Context (which however conflates a few different issues): https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Error.20__C1188collect_distributed_array/near/276162426. Running the `prepare_gtex_v7_expression_data` step of the https://github.com/broadinstitute/gnomad-browser pipeline using Hail 0.2.105 results in the following error:. ```; Traceback (most recent call last):; File ""/tmp/dc052bbb66544877a73c723ac0a0dc46/genes.py"", line 327, in <module>; run_pipeline(pipeline); File ""/tmp/dc052bbb66544877a73c723ac0a0dc46/pyfiles_h7wv4zl3.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/dc052bbb66544877a73c723ac0a0dc46/pyfiles_h7wv4zl3.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/dc052bbb66544877a73c723ac0a0dc46/pyfiles_h7wv4zl3.zip/data_pipeline/pipeline.py"", line 132, in run; File ""/tmp/dc052bbb66544877a73c723ac0a0dc46/pyfiles_h7wv4zl3.zip/data_pipeline/data_types/gtex_tissue_expression.py"", line 14, in prepare_gtex_expression_data; File ""<decorator-gen-1060>"", line 2, in export; File ""/opt/conda/default/lib/python3.8/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1098, in export; Env.backend().execute(; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 105, in execute; raise e.maybe_user_error(ir) from None; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 99, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: MethodTooLargeException: Method too large: __C444collect_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:138,Error,Error,138,https://hail.is,https://github.com/hail-is/hail/issues/12531,2,"['Error', 'error']","['Error', 'error']"
Availability,"Context (which however conflates a few different issues): https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Error.20__C1188collect_distributed_array/near/276162426. Running the `prepare_pext` step of the https://github.com/broadinstitute/gnomad-browser pipeline using Hail 0.2.105 results in the following error:. ```; 2022-12-01 05:56:34.825 Hail: INFO: Loading <StructExpression of type struct{ensg: str, symbol: str, max_pexts: str, Spleen: str, Brain_FrontalCortex_BA9_: str, SmallIntestine_TerminalIleum: str, Artery_Coronary: str, Skin_SunExposed_Lowerleg_: str, Brain_Hippocampus: str, Esophagus_Muscularis: str, Brain_Nucleusaccumbens_basalganglia_: str, Artery_Tibial: str, Brain_Hypothalamus: str, Adipose_Visceral_Omentum_: str, Brain_CerebellarHemisphere: str, Nerve_Tibial: str, Breast_MammaryTissue: str, Liver: str, Skin_NotSunExposed_Suprapubic_: str, AdrenalGland: str, Pancreas: str, Lung: str, Pituitary: str, Muscle_Skeletal: str, Colon_Transverse: str, Artery_Aorta: str, Heart_AtrialAppendage: str, Adipose_Subcutaneous: str, Esophagus_Mucosa: str, Heart_LeftVentricle: str, Brain_Cerebellum: str, Brain_Cortex: str, Thyroid: str, Stomach: str, WholeBlood: str, Brain_Anteriorcingulatecortex_BA24_: str, Brain_Putamen_basalganglia_: str, Brain_Caudate_basalganglia_: str, Colon_Sigmoid: str, Esophagus_GastroesophagealJunction: str, Brain_Amygdala: str, mean_proportion: str}> fields. Counts by type:; str: 42; Traceback (most recent call last): (0 + 1) / 1]; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/genes.py"", line 327, in <module>; run_pipeline(pipeline); File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1066>"", line 2, in write",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:138,Error,Error,138,https://hail.is,https://github.com/hail-is/hail/issues/12532,2,"['Error', 'error']","['Error', 'error']"
Availability,"Copied over from the Zulip thread:. Dan and I still have work to figure out the authentication strategy for browser-based REST requests, but as a workaround I've added a tiny aiohttp proxy that uses the python client library to fulfill the requests, which could enable local frontend work while we figure out the right way to do authentication and streaming data through websockets. Implementing the polling and separating it logically from the view components was actually a nice little case study in how to do this in React/Svelte, but is far from an honest or thorough comparison. If you want to run it for yourself, you can pull down the branch in that PR and then do the following (which I'll write dev docs for if this is something that we actually want to check in):. Install node if you do not have it; Run npm install in the $HAIL, $HAIL/js_common, and $HAIL/batch2/react-batch (or svelte-batch) directories; In one terminal in $HAIL/batch2, run python proxy.py; In another terminal in one of the react-batch or svelte-batch directories, run npm run dev; Go to localhost:3000 in your browser if it didn't pop up automatically",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10504:633,down,down,633,https://hail.is,https://github.com/hail-is/hail/pull/10504,1,['down'],['down']
Availability,"Copy pattern from other services: tolerate preemptibles, min 3 replicas, auto-scale up to 10.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7287:34,toler,tolerate,34,https://hail.is,https://github.com/hail-is/hail/pull/7287,1,['toler'],['tolerate']
Availability,Couldn't build with sbt after googleFS changes. Couldn't download the proper LZ4 dependency.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8475:57,down,download,57,https://hail.is,https://github.com/hail-is/hail/pull/8475,1,['down'],['download']
Availability,Couldn't reopen #9074. This PR errors if any of the input or output paths have unescaped wildcard characters in them.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9087:31,error,errors,31,https://hail.is,https://github.com/hail-is/hail/pull/9087,1,['error'],['errors']
Availability,"Current State; ---. When a commit is merged into a deployable target branch, the [CI deploys that commit](https://github.com/hail-is/hail/blob/master/ci/ci/prs.py#L166-L242). If the deploy job fails, we just [log the failure and change nothing](https://github.com/hail-is/hail/blob/master/ci/ci/prs.py#L295-L313). Since `PRS.latest_deployed` for the given target ref is not changed, the CI will attempt to deploy the latest SHA at the next heal point. We heal periodically, when master changes, when review statuses change, and probably elsewhere. Anywhere we call `PRS.heal_target`. Desired State; ---. Instead, we should track the last successful deploy as well as all the failing deploys since then. This enables us to a) not redeploy a failing deploy and b) find the most recent successful deploy and re-deploy that one. If the most recent successful deploy fails again, we should probably error very loudly. Note that when the CI first comes up there will be no most recent successful deploy. The possible situations are:. - most recent deploy succeeded. - no deploy has ever succeeded. - a deploy has succeeded, but some number of SHAs since then have all failed . Motivation; ---. We want to ensure there is a deployed artifact. For some projects a deploy failure does not leave the universe in a bad state. For example, hail itself updates the latest-hash file after all artifact uploads have succeed. For some projects, a half-way passing deployment will interrupt our users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4435:217,failure,failure,217,https://hail.is,https://github.com/hail-is/hail/issues/4435,3,"['error', 'failure']","['error', 'failure']"
Availability,"Current State; ---. When a new PR is created or the source SHA for a PR changes, a build is unconditionally started for that SHA merged with the latest target SHA. When the target SHA changes, all PR builds for that target SHA are killed. When a target SHA changes, the CI heals that target. When healing a target, the CI attempts to avoid n^2 unnecessary builds. It achieves this by serializing the build+merge of approved PRs for a given target. When there are no approved PRs, the CI will build every remaining PR with pending/`Buildable` status. If a PR is unapproved and there are a number of approved PRs, it is likely the PR will spend a significant amount of time as ""pending"" as it waits for the approved PRs to be merged. Desired State; ---. The CI should track if a source SHA has ever been tested (success or failure). If the target SHA changes, a build should only be killed if the source SHA has been successfully tested before. If the source SHA changes, a PR build should be killed regardless of whether the old source SHA has been successfully built before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4438:821,failure,failure,821,https://hail.is,https://github.com/hail-is/hail/issues/4438,1,['failure'],['failure']
Availability,Current behavior is it's testing both D_== and abs(d1 -d2) <= tolerance. Now `absolute=True` specifies use `abs(d1-d2)`. Behavior used to be D_== for everything until I changed it for the BGEN test. So `absolute=True` should only be in the BGEN tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3364:62,toler,tolerance,62,https://hail.is,https://github.com/hail-is/hail/pull/3364,1,['toler'],['tolerance']
Availability,"Currently if there are duplicated chr:pos:ref:alt in an annotation file, for example read by `annotatevariants table`, also the variants in the vds get duplicated. This is clearly not nice. So instead think on a better behaviour. But do not just issue an error, because otherwise is a pain.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/389:255,error,error,255,https://hail.is,https://github.com/hail-is/hail/issues/389,1,['error'],['error']
Availability,"Currently the binding structure is redundantly specified in two places: Binds.scala, and the parser. We need the binding structure in the parser to propagate the environment, so we can annotate `Ref` nodes (and a few other things) with their types. But we can't use Binds.scala because we don't yet have an IR. This PR removes environment maintenance from the parser by deferring type annotation to a separate pass (which is simple, because it can use the Binds.scala infrastructure). One consequence is that we can't assign types to nodes like `Ref` during parsing, which means we can't ask for the type of any node during parsing, and by extension we can't ask for types of children in IR node constructors. Instead, all typechecking logic is moved to the `TypeCheck` pass. Some benefits of this change:; * The parser is simpler, as it doesn't have to maintain a typing environment.; * Binds.scala is now the single source of truth on the binding structure of the IR.; * Instead of typechecking being split in an ad-hoc way between IR constructors and the `TypeCheck` pass, all typechecking and type error reporting logic is in one place.; * The parser parses a context-free grammar, no more and no less. If the input is gramatically correct, the parser succeeds.; * We can round trip IR with type errors through the text representation. For instance, if we log an IR that fails TypeCheck, we can copy the IR from the log, parse it, then debug. This change was motivated by my work in progress to convert the parser to use the SSA grammar, which this should greatly simplify. I chose to make the type annotation pass after parsing mutate the IR in place (with the unfortunate exception of `Apply`, which can change into an `ApplyIR` or `ApplySpecial`. Do these really need to be separate nodes?). The type of a `Ref` node was already mutable to allow this sort of deferred annotation, and I've had to make a few other things mutable as well. Alternatively we could rebuild the entire IR to include t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13990:35,redundant,redundantly,35,https://hail.is,https://github.com/hail-is/hail/pull/13990,2,"['mainten', 'redundant']","['maintenance', 'redundantly']"
Availability,"Currently there are multiple compatible versions of the `CADD` (1.4 and 1.6) and `gnomad_genome_sites` (2.1.1 and 3.1) annotation datasets, which leads to an error in `index_compatible_version`. This PR addresses that by annotating with the highest version number if there are multiple compatible versions of the same dataset. Will likely add fix in future to allow user to specify the version they would like to use, this was just a fairly quick and easy fix in the meantime.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10868:158,error,error,158,https://hail.is,https://github.com/hail-is/hail/pull/10868,1,['error'],['error']
Availability,"Currently there is a race condition in which we can shut down the filestore before; the aiohttp app stops accepting connections. If that happens, jobs will get; partially scheduled on the worker, but there will be no working file store so the; jobs cannot complete successfully. The partially scheduled jobs in turn leave the; worker in a bad, non-idle state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10949:57,down,down,57,https://hail.is,https://github.com/hail-is/hail/pull/10949,1,['down'],['down']
Availability,"Currently, `ggplot` expects a `Table` to be passed in as its data, and errors if passed a `MatrixTable`. With this change, the following code:. ```python; import hail as hl; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_entries(entry_idx = mt.row_idx + mt.col_idx); mt.show(); ```. Which produces a `MatrixTable` that looks like this:. ```; +---------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+; | row_idx | 0.entry_idx | 1.entry_idx | 2.entry_idx | 3.entry_idx | 4.entry_idx | 5.entry_idx | 6.entry_idx | 7.entry_idx |; +---------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+; | int32 | int32 | int32 | int32 | int32 | int32 | int32 | int32 | int32 |; +---------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+; | 0 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |; | 1 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |; | 2 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |; | 3 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |; | 4 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |; | 5 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |; | 6 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 |; | 7 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 |; | 8 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |; | 9 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |; +---------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+; showing the first 8 of 10 columns; ```. Can be plugged into `ggplot` like so:. ```python; from hail.ggplot import *; fig = ggplot(mt, aes(x=mt.row_idx, y=mt.entry_idx)) + geom_point(); fig.show(); ```. To produce this plot:. <img width=""1512"" alt=""Screen Shot 2022-10-07 at 15 30 50"" src=""https://user-images.githubusercontent.com/84595986/194639655-e88de8fa-2992-4e57-ad06-5f6164dc4d84.png"">. This is accomplished using `Expr._to_table` to transform the `MatrixTable` such that the fields used to generate the plot can be straig",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12293:71,error,errors,71,https://hail.is,https://github.com/hail-is/hail/pull/12293,1,['error'],['errors']
Availability,"Currently, an instance that is `inactive` in batch and `Terminated` per the cloud will enter the second branch and we will call `deactivate`, which since the instance is already inactive will be a no-op. We really want the third branch to be executed in which we call delete on the instance, so that the inactive instances don't hang around forever. Vedant and I paired on this and did some case analysis to restructure the conditions here a little bit. The order of the conditions now doesn't matter and is hopefully more explicit. We also decided that an unspoken case (deleted but not terminated) should be an error scenario, let me know if you think that is an appropriate exception to log here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11392:613,error,error,613,https://hail.is,https://github.com/hail-is/hail/pull/11392,1,['error'],['error']
Availability,"Currently, attempting to bit shift by zero bits throws an error. ```; $ hl.eval(hl.bit_rshift(4, 0)); HailUserError: Error summary: HailException: cannot shift by a negative value: 4 >> 0; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9843:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/pull/9843,2,"['Error', 'error']","['Error', 'error']"
Availability,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8791:135,error,error,135,https://hail.is,https://github.com/hail-is/hail/pull/8791,3,['error'],['error']
Availability,"Currently, constructing a `hl.Struct` is slower than I'd like, since it requires copying every field from the input `kwargs` into the `Struct` object's `__dict__`. This PR's main goal was to avoid the need to do that, by just using the input `kwargs` dict we were already saving as `_fields`. . When working on this, I also noticed that we allowed users to mutate fields of a struct, and we didn't do so in a consistent way (`s.a` vs `s[""a""]` could return different answers). I avoid that by throwing an error from now on if a user tries to modify one of the ""main"" fields of the struct (i.e. the ones that are in the `_fields` array).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10968:504,error,error,504,https://hail.is,https://github.com/hail-is/hail/pull/10968,1,['error'],['error']
Availability,"Currently, if the type of the `default` argument to `dict.get` does not match the dictionary's value type, the error message contains the dictionary's type and the `default` argument's type. However, the `default` argument's type should be compared to the dictionary's **value** type. This can be particularly confusing when dealing with nested dictionaries. For example:; ```python; d = hl.dict({""foo"": {""foo"": 1}}); d.get(""somekey"", hl.dict({""bar"": {""bar"": 2}})); ```; results in:; ```; TypeError: 'get' expects parameter 'default' to have the same type as the dictionary ; value type, found 'dict<str, dict<str, int32>>' and 'dict<str, dict<str, int32>>'; ```. This change puts the value type instead of the dictionary type in the error message and slightly rewords the message to be clearer about which type is which.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7377:111,error,error,111,https://hail.is,https://github.com/hail-is/hail/pull/7377,2,['error'],['error']
Availability,"Currently, if we have a file structure like:. a/; b/; aa/; bb/. A glob pattern like `*/b` will raise FileNotFoundError beacuse; we try to list the file or folder named ""b"" inside `aa`. We should; not error. We should return `['a/b']`. It is insufficient to avoid FileNotFoundError altogether because; the Hadoop API treats paths without globs differently. In particular,; listing the path `aa/b` should raise an error. This change fixes behavior in the first case and treats the second; case explicitly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11905:200,error,error,200,https://hail.is,https://github.com/hail-is/hail/pull/11905,2,['error'],['error']
Availability,"Currently, jobs in hail batch can only be run on n1 machines but with the rise of deep learning in bioinformatics, the ability to run jobs on g2 machines, as well as other GPU supported machines, is an important and exciting addition to hail batch. This PR highlights the steps needed to add new machine types into hail batch and could be used as a template for further development support. . The changes in this PR can broadly be divided into additions to the job crun container and insertion of g2 resources (CPU, RAM, L4 Accelerator) into the resources table for billing. This PR uses the NVIDIA Container Toolkit, which allows the creation of GPU accelerated containers. This toolkit is integrated with docker via the parameters —runtime=nvidia and the specification of GPUs is made through —gpus all. The toolkit is installed in the batch worker VM startup script and the corresponding docker parameters are configured if the machine type is g2, so there is no change to the docker configuration for n1 machines. For the toolkit to work there is a nvidia hook that needs to be injected into the crun config. These modifications are also done based on machine type. On the billing side, the existing pricing setup was expanded to include g2 machines. The g2 instance cores and RAM are inserted into the database, and the SKUs are hard coded. For future machine type incorporation or updates, [https://cloud.google.com/skus/?currency=USD&filter=](https://cloud.google.com/skus/?currency=USD&filter=) may serve as a useful resource to identify relevant SKU ids. A new resource type was also added for the accelerator, including preemptible and non-preemtible. Finally, g2 machines mount the worker data disk under the name nvme0n2 so the code is updated to reflect this. Future work may want to investigate a way to automatically detect what the proper disk name is or make the disk naming logic more robust.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430:1903,robust,robust,1903,https://hail.is,https://github.com/hail-is/hail/pull/13430,1,['robust'],['robust']
Availability,"Currently, on mobile browsers, the home page of [hail.is](https://hail.is/) is shrunk down to the point where it's difficult to read without zooming in. This adds a [viewport tag](https://developer.mozilla.org/en-US/docs/Mozilla/Mobile/Viewport_meta_tag) to improve that layout. ## Before:. ![before](https://user-images.githubusercontent.com/1156625/59926804-12a60f00-9409-11e9-80c5-52fe4c8ddd66.png). ## After:. ![after](https://user-images.githubusercontent.com/1156625/59926813-16d22c80-9409-11e9-958c-8553ef583be8.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6432:86,down,down,86,https://hail.is,https://github.com/hail-is/hail/pull/6432,1,['down'],['down']
Availability,"Currently, tasks to schedule new instances are put on the event loop inside the `Pool` and `JobPrivateInstanceManager` constructors. `Pool.create` and `JobPrivateInstanceManager.create` first instantiate an object of their respective type and then load existing instances from the database into the in-memory instance collection. This could potentially cause the create instances loop to trigger while we're drawing ""existing"" instances, which causes the assertion error in https://github.com/hail-is/hail-tasks/issues/24 when the create instances loop and load instances query race to add the instance to the in-memory data structure. This change moves the task creation from the constructor to the `create` method, so we don't start creating instances until all existing instances are accounted for. I think I would have liked to simply pass the constructor a list of instances, but we can't create an `Instance` without an `InstanceCollection`. Resolves hail-is/hail-tasks#24. I also threw in a bit of cleanup, i.e. removing some variable assignments that didn't seem very helpful and resolving a lint issue where we used `items` where we could just use `values`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11766:465,error,error,465,https://hail.is,https://github.com/hail-is/hail/pull/11766,1,['error'],['error']
Availability,"Currently, the Hail datasets API only has the pan-UKB datasets available via the Amazon S3 bucket. This PR makes the pan-UKB datasets available via GCS as well (from the `gs://ukb-diverse-pops-public` bucket), and updates the summary statistics and meta-analysis MatrixTables to reflect the most up-to-date release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12073:63,avail,available,63,https://hail.is,https://github.com/hail-is/hail/pull/12073,2,['avail'],['available']
Availability,"Currently, the MJS and MJC requests from the worker to the driver for a given job can race, as they are run as independent asyncio tasks. This results in unnecessary database load and deadlocks between the MJS and MJC SQL procedures. Rather than address the procedures directly, we enforce that we will never run MJS and MJC concurrently. The system is resilient to never receiving an MJS (as MJC will add any attempt data if not present), so we can make the following changes to the worker:; - Serialize the submission of MJS and MJC requests by having the MJC task wait on the MJS future; - Give up retrying MJS once the job has completed because we will instead just send an MJC. This could potentially reduce the database load for very short jobs. I ran a load test of 10k `true` jobs and `sleep 5` jobs a few times against my namespace and saw 0 deadlocks 🎉",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11824:353,resilien,resilient,353,https://hail.is,https://github.com/hail-is/hail/pull/11824,1,['resilien'],['resilient']
Availability,"Currently, the `_csrf` cookie is made available to all subdomains of `.hail.is`. This means that if I first visit `batch.hail.is` I get a `_csrf` cookie set for `.hail.is`. That cookie is then reused if I visit `ci.hail.is`. Even more awkward, the same value of the cookie will get reused if I then visit `batch.azure.hail.is`. This isn't that big of a deal, these can all be considered part of the same application that the hail team delivers and secures, but it is very little work to set stricter bounds on where this cookie is sent. By removing the `domain` attribute and using `samesite='strict'`, the cookie's domain will be set by the browser to the domain of the request whose response included the `Set-Cookie` header, e.g. `batch.hail.is` or `internal.hail.is`. `Strict` mode then ensures that the cookie will only be sent to that exact domain, meaning that each application is guaranteed to receive the `_csrf` token that it itself delivered, and a `_csrf` token from CI cannot be used to take actions against Batch. This should not have an adverse impact on existing users' browser sessions. In `render_template` we preserve the value of an existing `_csrf` cookie so this change should do the following:; - Logged in user visits a page with an existing widely scoped (`.hail.is`) `_csrf` cookie; - The server returns a `Set-Cookie` header with a new `_csrf` cookie for strictly the `batch.hail.is` domain but with the same token value as the original `_csrf` cookie; - The user now has two cookies and the browser could send either one on a given request, but it does not matter because they have the same value; - If the user logs out and back in, their old widely scoped cookie will be cleared and they only get the strict cookie from now on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14180:38,avail,available,38,https://hail.is,https://github.com/hail-is/hail/pull/14180,1,['avail'],['available']
Availability,"Currently, the only way to check if Hail can read URLs with a given scheme (`gs`, `s3`, etc) is to attempt to read a URL with that scheme. However, the same exception type is thrown whether the scheme is not supported or the file doesn't exist or something else went wrong and the error message is the only way to determine what went wrong. This adds a `hl.hadoop_scheme_supported` function, which returns a boolean indicating whether or not a URL scheme is supported. Discussed on Zulip: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Get.20supported.20URL.20schemes. @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10555:281,error,error,281,https://hail.is,https://github.com/hail-is/hail/pull/10555,1,['error'],['error']
Availability,"Currently, when a QoB worker job fails in a way that prevents it from writing a result file with the error's stack trace in it, the Batch worker that started the job tries to get the result file anyway, and when it fails, raises a 404. This change retrieves the stack trace from the QoB job and raises an error with the stack trace included.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12468:101,error,error,101,https://hail.is,https://github.com/hail-is/hail/pull/12468,2,['error'],['error']
Availability,"Currently, when a job-deletion event arrives on a worker, it removes the job from its running list of jobs and enqueues a future to run that job's deletion/cleanup. If a deletion removes all running jobs from a worker, and the worker does not receive new work, it could potentially idle out and shutdown before the job has finished cleanup in another task. One specific example of where this could go wrong is that worker shutdown can delete the compute client that the job's cleanup step needs to detach/delete a disk. The worker should never shut down while operations are still in progress, so we should only ever remove a job once its deletion has completed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11802:549,down,down,549,https://hail.is,https://github.com/hail-is/hail/pull/11802,1,['down'],['down']
Availability,"Customize export variant- and genotype-level fields.; Upgrade to SorlJ 6, add dependencies.; Support SolrCloud.; Retry on Solr error.; Only insert non-null fields.; Only export called non-ref genotypes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/497:127,error,error,127,https://hail.is,https://github.com/hail-is/hail/pull/497,1,['error'],['error']
Availability,"D$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:7626,failure,failure,7626,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['failure'],['failure']
Availability,"Danfeng was seeing this error trying to unpersist a matrix table:. ```; >>> mt = hl.utils.range_matrix_table(10, 10); >>> mt = mt.persist(); >>> mt = mt.unpersist(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/wang/code/hail/hail/python/hail/matrixtable.py"", line 2896, in unpersist; return Env.backend().unpersist_matrix_table(self); TypeError: unpersist_matrix_table() missing 1 required positional argument: 'storage_level'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5246:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/pull/5246,1,['error'],['error']
Availability,"Dear hail team,. I am trying to get familiar with hail by filtering and doing simple stuff I usually do on VCFs with softwares like bcftools.; I am working with Hail version 0.2.57-582b2e31b8bd; I splitted my VCF from multiallelic to biallelic with:; `data_tmp_bi = hl.split_multi_hts(data_tmp)`; Then I want to update the allele counts the same way as what I saw in your documentation:; `data_tmp_bi = data_tmp_bi.annotate_rows(info = hl.struct(AC=data_tmp_bi.info.AC[data_tmp_bi.a_index - 1],**data_tmp_bi.info))`; but I get this error:; ```; File ""<ipython-input-29-3595a23add68>"", line 1, in <module>; data_tmp_bi = data_tmp_bi.annotate_rows(info = hl.struct(AC=data_tmp_bi.info.AC[data_tmp_bi.a_index - 1],**data_tmp_bi.info)). TypeError: struct() got multiple values for keyword argument 'AC'; ```; The workaround I have been using is then to create a new info field called 'AC2', to then drop the 'AC' field and then recreate the 'AC' field with `annotate_rows` with to finally drop 'AC2'. Which is a long workaround:; ```; data_tmp_bi = data_tmp_bi.annotate_rows(info = hl.struct(AC2=data_tmp_bi.info.AC[data_tmp_bi.a_index - 1],**data_tmp_bi.info)); data_tmp_bi = data_tmp_bi.annotate_rows(info=data_tmp_bi.info.drop('AC')); data_tmp_bi = data_tmp_bi.annotate_rows(info = hl.struct(AC=data_tmp_bi.info.AC2, **data_tmp_bi.info)); data_tmp_bi = data_tmp_bi.annotate_rows(info=data_tmp_bi.info.drop('AC2')); ```. On top of this, I filter by column some samples with `filter_cols`, so then, I want to update fields like allele count again, so I would still need to use the same workaround as above, otherwise I get the same error. Do you have an idea of what the problem might be? Or a better way of doing this than what I am using?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9685:532,error,error,532,https://hail.is,https://github.com/hail-is/hail/issues/9685,2,['error'],['error']
Availability,"Dear hail team,. I would like to delete the gcvf information in the matrix table. I run:. mt = mt.transmute_entries(**mt.gvcf_info). **1. error in the hail terminal:** ; 2022-03-24 14:51:45 Hail: ERROR: Analysis exception: 'MatrixTable.transmute_entries': name collision with field indexed by ['row']: 'AS_MQRankSum'; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1132>"", line 2, in transmute_entries; File ""/PATH_OMIT/.local/lib/python3.7/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/PATH_OMIT/.local/lib/python3.7/site-packages/hail/matrixtable.py"", line 1952, in transmute_entries; check_annotate_exprs(caller, named_exprs, self._entry_indices, set()); File ""/PATH_OMIT/.local/lib/python3.7/site-packages/hail/utils/misc.py"", line 468, in check_annotate_exprs; check_collisions(caller, list(named_exprs), indices); File ""/PATH_OMIT/.local/lib/python3.7/site-packages/hail/utils/misc.py"", line 363, in check_collisions; raise ExpressionException(msg); hail.expr.expressions.base_expression.ExpressionException: 'MatrixTable.transmute_entries': name collision with field indexed by ['row']: 'AS_MQRankSum'. **2. Details in mt.gcvf_info:**; <StructExpression of type struct{AC: array<int32>, AF: array<float64>, AN: int32, AS_BaseQRankSum: array<float64>, AS_FS: array<float64>, AS_InbreedingCoeff: array<float64>, AS_MQ: array<float64>, AS_MQRankSum: array<float64>, AS_QD: array<float64>, AS_QUALapprox: array<int32>, AS_RAW_BaseQRankSum: str, AS_RAW_MQ: array<float64>, AS_RAW_MQRankSum: array<tuple(float64, int32)>, AS_RAW_ReadPosRankSum: array<tuple(float64, int32)>, AS_ReadPosRankSum: array<float64>, AS_SB_TABLE: array<array<int32>>, AS_SOR: array<float64>, AS_VarDP: array<int32>, BaseQRankSum: float64, ExcessHet: float64, FS: float64, InbreedingCoeff: float64, MQ: float64, MQRankSum: float64, MQ_DP: int32, QD: float64, QUALapprox: int32, RAW_GT_COUNT: array<int32>, RAW_MQ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11660:138,error,error,138,https://hail.is,https://github.com/hail-is/hail/issues/11660,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Default Mathjax CDN is no longer active. https://www.mathjax.org/cdn-shutting-down/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2145:78,down,down,78,https://hail.is,https://github.com/hail-is/hail/pull/2145,1,['down'],['down']
Availability,"Depends on #3539. (first new commit is ""filled out block-sparse matrix support""). Third and final major PR to introduce block-sparse matrices. All block matrix operations are now supported apart from the following cases that are most likely user error (and even these can be forced by applying the new method `densify` first):; - Division between two block matrices.; - Multiplication by a scalar or broadcasted vector which includes an infinite or ``nan`` value.; - Division by a scalar or broadcasted vector which includes a zero, infinite or ``nan`` value.; - Division of a scalar or broadcasted vector by a block matrix.; - Exponentiation by a negative exponent. The following operations are newly supported:; - Addition and subtraction of block matrices, resulting in ""union"" of realized blocks.; - Addition and subtraction of a scalar or broadcasted vector, resulting in a block-dense matrix.; - Slicing, and more generally row/column filtering, resulting in a block-dense matrix. New infrastructure includes:; - `supersetPartitions` in RichRDD, analogue of `subsetPartitions`, used to densify/realizeBlocks; - `BlockMatrixUnionOpRDD`, so far applied for `+` and `-` but able to support more general ops. Throughout I've aimed to support the block-sparse case while minimizing overhead on the more common block-dense case, particularly with `maybeSparse = None` when block-dense. Docs and tests updated accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3636:246,error,error,246,https://hail.is,https://github.com/hail-is/hail/pull/3636,1,['error'],['error']
Availability,"Depends on #3992. Resolves https://github.com/hail-is/hail/issues/878. @tpoterba @catoverdrive Could you please double check I handled the joins, indices, and aggregations properly? Everything I tried besides just copying the indices and aggregations from `agg_expr` resulted in an index mismatch error. My current code generates the right output. I'm just skeptical whether it is robust to other faulty inputs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4004:297,error,error,297,https://hail.is,https://github.com/hail-is/hail/pull/4004,3,"['error', 'fault', 'robust']","['error', 'faulty', 'robust']"
Availability,"Depends on #4049 and #4121. . @danking I'm concerned I changed the meaning of your `file_row_index` field and the code you specialized for Caitlin is broken with this PR. That field is always sorted now from 0 to nVariants. Before, the indices were the order in the BGEN file. Now they're the variant index into the actual index file. I had to change/delete some of the Python tests because they didn't make sense anymore. Next PR will support push down by locus, alleles rather than row index and this shouldn't be a concern anymore (and we can delete file_row_index). If you think the change in behavior will cause problems, I'll close this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4102:449,down,down,449,https://hail.is,https://github.com/hail-is/hail/pull/4102,1,['down'],['down']
Availability,Dir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:244); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:77); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at __C1310collect_distributed_array_table_native_writer.apply_region1_87(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:6052,Error,ErrorHandling,6052,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Error'],['ErrorHandling']
Availability,"Dir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3488,failure,failure,3488,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['failure'],['failure']
Availability,"Do a tree reduce instead of a linear reduce. This means that the java; stack depth is log2(N) instead of N, and prevents stack overflow errors; when unioning hundreds of tables together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6943:136,error,errors,136,https://hail.is,https://github.com/hail-is/hail/pull/6943,1,['error'],['errors']
Availability,Docker also reports transient DNS resolution failure as 500.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8189:45,failure,failure,45,https://hail.is,https://github.com/hail-is/hail/pull/8189,1,['failure'],['failure']
Availability,"Docker jobs were cleaning up after themselves by including the entire directory for that job which removed the old container files. This PR makes it so `Container.remove()` removes the directory for the container. Before we merge this, I want to check the logs for this PR and make sure there aren't error messages that don't show up in tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12397:300,error,error,300,https://hail.is,https://github.com/hail-is/hail/pull/12397,1,['error'],['error']
Availability,Docker just always returns 500s for images that don't exist and has a slightly different error message for each thing that could go wrong (project / repository / image / tag don't exist) and that message varies across registries.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12594:89,error,error,89,https://hail.is,https://github.com/hail-is/hail/pull/12594,1,['error'],['error']
Availability,Docs formatting error in multi-way-zip-join,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7945:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/issues/7945,1,['error'],['error']
Availability,Documentation for this field's use is availible at the end of the linked; section. We can maybe version the docs url in order to host the docs to; the released versions. https://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8549:38,avail,availible,38,https://hail.is,https://github.com/hail-is/hail/pull/8549,1,['avail'],['availible']
Availability,Don't construct intermediate string.; Print full exception on metadata load failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/899:76,failure,failure,76,https://hail.is,https://github.com/hail-is/hail/pull/899,1,['failure'],['failure']
Availability,Downsample Aggregator Needs Examples,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8241:0,Down,Downsample,0,https://hail.is,https://github.com/hail-is/hail/issues/8241,1,['Down'],['Downsample']
Availability,DownsampleAggregator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4010:0,Down,DownsampleAggregator,0,https://hail.is,https://github.com/hail-is/hail/pull/4010,1,['Down'],['DownsampleAggregator']
Availability,DownsampleAggregator with label,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4090:0,Down,DownsampleAggregator,0,https://hail.is,https://github.com/hail-is/hail/pull/4090,1,['Down'],['DownsampleAggregator']
Availability,"Due to a typo I tried to read a vds that doesn't exist. I got the following error message:; hail: fatal: read: corrupt VDS: no metadata.ser file. Recreate VDS. It should probably say something more specific, ideally:; hail: fatal: read: VDS does not exist. Or at least:; hail: fatal: read: non-existent or corrupt VDS: no metadata.ser file. Please (re)create VDS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/327:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/327,1,['error'],['error']
Availability,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8958:605,recover,recover,605,https://hail.is,https://github.com/hail-is/hail/pull/8958,1,['recover'],['recover']
Availability,Due to most of these failures being due to service tests. Feel free to revert when you see fit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10178:21,failure,failures,21,https://hail.is,https://github.com/hail-is/hail/pull/10178,1,['failure'],['failures']
Availability,"EDIT: this was an issue with the input data. When using Hail 0.2.85, getting the following error (with code that worked with 0.2.78). ; Wondering what changes have been made between these versions that can help point to a solution?; Thanks!. code:; ```; phenotypes = phenotype_df.columns[1:]. y = list(map(lambda p: hl.float64(mt[p]), phenotypes)). covs = list(map(lambda cov: mt[cov], covariate_df.columns[1:])). x = mt.GT.n_alt_alleles(). hl_res = hl.methods.linear_regression_rows(y,x,[1]+covs). hl_res.checkpoint(quantitative_gwas_results_chk, overwrite=True); ```. error. `is.hail.utils.HailException: 1 samples and 12 covariates (including x) implies -11 degrees of freedom.; `. stacktrace. ```; Py4JJavaError Traceback (most recent call last); <[command-1581524108362637]()> in <module>; 4 covs = list(map(lambda cov: mt[cov], covariate_df.columns[1:])); 5 x = mt.GT.n_alt_alleles(); ----> 6 hl_res = hl.methods.linear_regression_rows(y,x,[1]+covs); 7 hl_res.checkpoint(quantitative_gwas_results_chk, overwrite=True); 8 n_filtered_variants = hl_res.count(). <decorator-gen-1734> in linear_regression_rows(y, x, covariates, block_size, pass_through, weights). /databricks/python/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /databricks/python/lib/python3.8/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through, weights); 374 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 375 ; --> 376 return ht_result.persist(); 377 ; 378 . <decorator-gen-1132> in persist(self, storage_level). /databricks/python/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413:91,error,error,91,https://hail.is,https://github.com/hail-is/hail/issues/11413,3,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"ERROR: org.broadinstitute.hail.annotations.AnnotationPathException,throw new AnnotationPathException()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/454:0,ERROR,ERROR,0,https://hail.is,https://github.com/hail-is/hail/issues/454,1,['ERROR'],['ERROR']
Availability,"Each webpage should have a single `h1` matching the `title` text. Individual command documentation begins with `#`/`h1`. The single doc page has a `title`/`h1` Hail Documentation, so headings in individual command docs should get shifted down a level, `hi => h(i+1)`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/635:238,down,down,238,https://hail.is,https://github.com/hail-is/hail/issues/635,1,['down'],['down']
Availability,"Environment:; - Spark 3.2.0; - Scala 2.12.15. Running: ; ```; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.2.0; ```; I get the error:; ```BUILD SUCCESSFUL in 2m 5s; 3 actionable tasks: 3 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; make: *** No rule to make target 'check-pip-lockfiles', needed by 'install-on-cluster'. Stop.; ```. Issue is fixed for me by renaming `install-on-cluster: $(WHEEL) check-pip-lockfiles` -> `install-on-cluster: $(WHEEL) check-pip-lockfile` on line 344 of hail/Makefile. Many thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12568:167,error,error,167,https://hail.is,https://github.com/hail-is/hail/issues/12568,1,['error'],['error']
Availability,"Envoy by default responds with a 403 if it fails to make an authorization check. We should treat the failure to contact `auth` as a transient error, so that clients can know to retry whatever request they were trying to make instead of failing. In particular, in dev namespaces the current behavior can trigger a QoB client to cancel an ongoing pipeline while polling for completion. [status_on_error](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/ext_authz/v3/ext_authz.proto#extensions-filters-http-ext-authz-v3-extauthz) allows us to configure that default behavior to return a 503 unavailable. I deployed this in Azure by running `make -C gateway deploy NAMESPACE=default`. I poked around to see that I could access my dev namespace and production pages through the browser but did not otherwise try to prove that this failure mode no longer exists.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13735:101,failure,failure,101,https://hail.is,https://github.com/hail-is/hail/pull/13735,3,"['error', 'failure']","['error', 'failure']"
Availability,Error building jar for Hail 0.2.11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['Error'],['Error']
Availability,Error compiling HAIL on Mac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/1274,1,['Error'],['Error']
Availability,"Error estimates for approx quantiles, better PDF plots.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6039:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/6039,1,['Error'],['Error']
Availability,Error for importvcf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['Error'],['Error']
Availability,Error if something goes wrong generating build info,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1994:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/1994,1,['Error'],['Error']
Availability,"Error in ""transmute_entries"" method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11660:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/11660,1,['Error'],['Error']
Availability,Error in hl.split_multi_hts documentation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10676:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/10676,1,['Error'],['Error']
Availability,Error in import vcf/write vds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/913:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/913,1,['Error'],['Error']
Availability,"Error in parsing vcf files to hail (""cannot set missing field for required type +PFloat64"")",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Error'],['Error']
Availability,Error is rather obvious as is fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8403:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/8403,1,['Error'],['Error']
Availability,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2881:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/2881,1,['Error'],['Error']
Availability,"Error message was this:. ```; + kubectl -n pr-12053-default-chqnmsebv8fq scale deployment batch-driver --replicas=0; Error from server (Forbidden): deployments.apps ""batch-driver"" is forbidden: User ""system:serviceaccount:pr-12053-default-chqnmsebv8fq:admin"" cannot get resource ""deployments"" in API group ""apps"" in the namespace ""pr-12053-default-chqnmsebv8fq""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12059:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/12059,2,['Error'],['Error']
Availability,Error message: hail: variantqc: caught exception: org.apache.spark.SparkException,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/660,1,['Error'],['Error']
Availability,"Error messages from GCR and AR are different, and sometimes it looks like this from GCR:. ```; ""unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12708:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/12708,1,['Error'],['Error']
Availability,Error of using Nirvana in the HAIL,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5657:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5657,1,['Error'],['Error']
Availability,Error on spark-submit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['Error'],['Error']
Availability,"Error page was too aggressive: it would delete any pod even if there was just a temporary network hiccup. Error page means a connection to the notebook failed. That means the state isn't Ready anymore, it is at most initializing (but maybe the pod is gone). In that case, probe k8s to figure out the current state, but don't probe the notebook, let /notebook call /wait to poll for the notebook to come back up (or report whatever state is there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7158:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/7158,2,['Error'],['Error']
Availability,Error reading negative integer in CN field,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Error'],['Error']
Availability,Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}},MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['Error'],['Error']
Availability,"Error summary: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. ```; In [4]: tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-3a6cee08b392> in <module>; ----> 1 tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['Error'],['Error']
Availability,Error using approx_median aggregation on column grouped matrix table,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7824:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/7824,1,['Error'],['Error']
Availability,Error when executing operations using entry fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['Error'],['Error']
Availability,Error when exporting variants,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/367:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/367,1,['Error'],['Error']
Availability,Error when importing /broad/hptmp/tpoterba/dbNSFP_3.2a_variant.filtered.all.txt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/391:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/391,1,['Error'],['Error']
Availability,Error when importing large WGS study (merck),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/301:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/301,1,['Error'],['Error']
Availability,Error when iterating over a set literal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4078:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/4078,1,['Error'],['Error']
Availability,Error when running `hl.agg.hist` with value `-0.0`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['Error'],['Error']
Availability,Error when running filter_intervals,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12920:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/12920,1,['Error'],['Error']
Availability,Error when running variant annotation pipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/320:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/320,1,['Error'],['Error']
Availability,Error when subsetting newly imported ExAC file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/309:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/309,1,['Error'],['Error']
Availability,Error when trying to subset the newly imported ExAC,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/304:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/304,1,['Error'],['Error']
Availability,Error when union'ing tabes from order_by,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4080:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/4080,1,['Error'],['Error']
Availability,Error when using `agg.group_by` with `agg.filter`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5296:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5296,1,['Error'],['Error']
Availability,Error when using hail in spark,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/2076,1,['Error'],['Error']
Availability,Error when variantsqc and exporting to plink,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/325:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/325,1,['Error'],['Error']
Availability,"Error with export(), write(), to_pandas() when using spark-submit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/2527,1,['Error'],['Error']
Availability,Error：Execution failed for task ':compileScala',MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/453,1,['Error'],['Error']
Availability,"Every suite is allocated by the gradle test framework, and then only those matching the requested filters are executed. Ergo, non-lazy fields on a suite will be executed (and may trigger errors) even though the suite wasn't requested by the gradle user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2262:187,error,errors,187,https://hail.is,https://github.com/hail-is/hail/pull/2262,1,['error'],['errors']
Availability,"Ex.v7.max_expression_per_gene_per_tissue.031318.kt""). <decorator-gen-1046> in read_table(path). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in read_table(path); 1865 :class:`.Table`; 1866 """"""; -> 1867 return Table(Env.hc()._jhc.readTable(path)); 1868 ; 1869 @typecheck(t=Table,. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.types.TableType.<init>(TableType.scala:15); 	at is.hail.expr.Parser$$anonfun$table_type_expr$7.apply(Parser.scala:308); 	at is.hail.expr.Parser$$anonfun$table_type_expr$7.apply(Parser.scala:307); 	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:137); 	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136); 	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237); 	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237); 	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4325:1830,Error,Error,1830,https://hail.is,https://github.com/hail-is/hail/issues/4325,1,['Error'],['Error']
Availability,Example failure https://ci.azure.hail.is/batches/1675765/jobs/112,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12153:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/12153,1,['failure'],['failure']
Availability,"Example from Zulip:. ```; mt_one.count(); # (239279, 153); mt_two.count(); # (522049, 188); mt_merge = mt_one.union_cols(mt_two); mt_merge.count(); (242190, 341); ```. We should warn or error in this case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5993:186,error,error,186,https://hail.is,https://github.com/hail-is/hail/issues/5993,1,['error'],['error']
Availability,Example stack trace:; ```; 2022-02-08 18:09:30 root: ERROR: IllegalArgumentException: requirement failed; From java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:52); 	at is.hail.rvd.RVDPartitioner.extendKeySamePartitions(RVDPartitioner.scala:141); 	at is.hail.expr.ir.LoweredTableReader$$anon$2.coerce(TableIR.scala:382); 	at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); 	at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1790); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:581); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1304); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1035); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:394); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:547); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:69); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77). ```. called from here:; https://github.com/hail-is/hail/blob/d2f87d81dd1af43617740309e354d4bac8c672e0/hail/src/main/scala/is/hail/expr/ir/TableIR.scala#L382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11335:53,ERROR,ERROR,53,https://hail.is,https://github.com/hail-is/hail/issues/11335,1,['ERROR'],['ERROR']
Availability,"Example that currently fails:. ```; >>> mt = hl.import_vcf('sample.vcf'); >>> mt.annotate_entries(PL = hl.cond(mt.DP > 20, mt.PL, hl.map(lambda x: x, mt.PL))); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/cotton/hail/python/hail/matrixtable.py"", line 975, in annotate_entries; return self._select_entries(caller, s=self.entry.annotate(**e)); File ""/home/cotton/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/home/cotton/hail/python/hail/matrixtable.py"", line 2887, in _select_entries; return cleanup(MatrixTable(base._jvds.selectEntries(str(s._ir)))); File ""/home/cotton/opt/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/cotton/hail/python/hail/utils/java.py"", line 212, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed: mismatch:; array<int32>; array<int32>; ApplyComparisonOp(GT(int32,int32),GetField(ArrayRef(GetField(Ref(va,struct{locus: locus<GRCh37>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{NEGATIVE_TRAIN_SITE: bool, HWP: float64, AC: array<int32>, culprit: str, MQ0: int32, ReadPosRankSum: float64, AN: int32, InbreedingCoeff: float64, AF: array<float64>, GQ_STDDEV: float64, FS: float64, DP: int32, GQ_MEAN: float64, POSITIVE_TRAIN_SITE: bool, VQSLOD: float64, ClippingRankSum: float64, BaseQRankSum: float64, MLEAF: array<float64>, MLEAC: array<int32>, MQ: float64, QD: float64, END: int32, DB: bool, HaplotypeScore: float64, MQRankSum: float64, CCC: int32, NCC: int32, DS: bool}, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{GT: call, AD: array<int32>, DP: int32, GQ: int32, PL: array<int32>}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]),Ref(i,int32)),DP),I32(20)). Java stack trace:; java.lang.AssertionError: assertion failed: mismatch:; array<int32>; array<int32>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4134:840,Error,Error,840,https://hail.is,https://github.com/hail-is/hail/issues/4134,1,['Error'],['Error']
Availability,Example transient error:. ```; E hail.utils.java.FatalError: batch id was 2301842; E IllegalStateException: Timeout on blocking read for 30000000000 NANOSECONDS; E java.lang.IllegalStateException: Timeout on blocking read for 30000000000 NANOSECONDS; E at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:123); E at reactor.core.publisher.Mono.block(Mono.java:1727); E at com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:130); E at com.azure.storage.blob.specialized.BlobClientBase.downloadStreamWithResponse(BlobClientBase.java:731); E at is.hail.io.fs.AzureStorageFS$$anon$1.fill(AzureStorageFS.scala:152); E at is.hail.io.fs.FSSeekableInputStream.read(FS.scala:141); E at java.io.DataInputStream.read(DataInputStream.java:149); E at is.hail.utils.richUtils.RichInputStream$.readRepeatedly$extension0(RichInputStream.scala:21); E at is.hail.utils.richUtils.RichInputStream$.readFully$extension1(RichInputStream.scala:12); E at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:546); E at is.hail.io.LZ4SizeBasedCompressingInputBlockBuffer.readBlock(InputBuffers.scala:608); E at is.hail.io.BlockingInputBuffer.readBlock(InputBuffers.scala:382); E at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:388); E at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:405); E at is.hail.io.LEB128InputBuffer.readByte(InputBuffers.scala:217); E at is.hail.io.LEB128InputBuffer.readInt(InputBuffers.scala:223); E at __C173548Compiled.__m174060INPLACE_DECODE_r_binary_TO_r_binary(Emit.scala); E at __C173548Compiled.__m174059DECODE_r_struct_of_r_binaryEND_TO_SBaseStructPointer(Emit.scala); E at __C173548Compiled.__m174058begin_group_0(Emit.scala); E at __C173548Compiled.__m174057begin_group_0(Emit.scala); E at __C173548Compiled.__m173566split_Let(Emit.scala); E at __C173548Compiled.apply(Emit.scala); E at is.hail.expr.ir.lowering.LowerToCDA$.$anonfun$lower$2(LowerToCDA.scala:51,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12261:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/12261,2,"['down', 'error']","['downloadStreamWithResponse', 'error']"
Availability,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3545:300,error,errors,300,https://hail.is,https://github.com/hail-is/hail/pull/3545,3,"['error', 'failure']","['errors', 'failures']"
Availability,"Execute TableWriter and MatrixWriter via lowering pipeline instead of spark execution.; Removed support for checkpoint files for now - plan is to implement something more general purpose somewhat akin to call-caching. Plot of top 20 affected benchmarks, none of which use writing, interestingly...; ![image](https://user-images.githubusercontent.com/8223952/231855633-84ddbe64-1dba-4e62-bfa0-b9e2b041d588.png); You can view these results yourself in [benchmarks.zip](https://github.com/hail-is/hail/files/11225644/benchmarks.zip)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12879:108,checkpoint,checkpoint,108,https://hail.is,https://github.com/hail-is/hail/pull/12879,1,['checkpoint'],['checkpoint']
Availability,ExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:15638,Error,Error,15638,https://hail.is,https://github.com/hail-is/hail/issues/8944,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"Existing tables and matrix tables that we don't own/didn't create but are accessible via the datasets API are not always available on both GCS and S3 (e.g. pan UKB datasets are only on AWS S3), or are not available in an `eu` bucket on GCS (e.g. gnomAD datasets). . This just adds a column `cloud: [regions]` to the tables on the datasets API and annotation DB docs pages to be more explicit about what datasets/versions are available on which cloud platform. . So, for example, if a version of a dataset is in `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1`, then under `cloud: [regions]` we would see `gcp: [eu,us], aws: [us]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10358:121,avail,available,121,https://hail.is,https://github.com/hail-is/hail/pull/10358,3,['avail'],['available']
Availability,Expands the interface to permit importing with no entry fields (previously was error),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3449:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/pull/3449,1,['error'],['error']
Availability,"Export_elasticsearch encounter ""Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/issues/5643,1,['error'],['error']
Availability,Extending the idea of better error messages from #9398 to include `NDArrayRef`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9444:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/9444,1,['error'],['error']
Availability,"F; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:1182,avail,available,1182,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['avail'],['available']
Availability,"F=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiV",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466:2890,Toler,Tolerations,2890,https://hail.is,https://github.com/hail-is/hail/issues/6466,1,['Toler'],['Tolerations']
Availability,"F>\""]},\""includeStart\"":true,\""includeEnd\"":true},{\""start\"":{\""locus\"":{\""contig\"":\""chr20\"",\""position\"":17994753},\""alleles\"":[\""A\"",\""<NON_REF>\""]},\""end\"":{\""locus\"":{\""cont...""] ; !s20 = ToStream(%29) [False]; !s21 = StreamMap(!s20) { (%elt10) =>; SelectFields(%elt10) [; (partitionCounts distinctlyKeyed firstKey; lastKey)]; }; !37 = ToArray(!s21); !38 = WriteMetadata(!37) [""{\""name\"":\""TableSpecWriter\"",\""path\"":\""/tmp/foo.ht\"",\""typ\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],data:Array[Struct{}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{new_globals:Array[Struct{}]}\""},\""rowRelPath\"":\""rows\"",\""globalRelPath\"":\""globals\"",\""refRelPath\"":\""references\"",\""log\"":true}""]; !39 = Begin(!34, !36, !38); WriteMetadata(!39) [""{\""name\"":\""RelationalWriter\"",\""path\"":\""/tmp/foo.ht\"",\""overwrite\"":true,\""maybeRefs\"":{\""references\"":[\""GRCh38\""]}}""]. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:17); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:15665,Error,ErrorHandling,15665,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,"FO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:1867,failure,failure,1867,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['failure'],['failure']
Availability,"FYI @cseed @danking: I found a bad bug in the Kubernetes deserializer that we use to deserialize the pod spec from the request from the client. Any variable with a compound name like `generate_name` or `cluster_name` was being set to None regardless of what the input was. This is because their code has a mapping from the Python style with underscores to camel case and it was using the camel case to look up the attributes instead of the underscore names. @akotlar was going to make a PR to correct it in their repo. For now, I'm including the correct code in our repo. Without it, I was getting errors trying to deserialize pod templates with metadata from the MySQL database back to Python. The other horrible behavior I found with the deserializer is if you pass `None` to the deserialize function, you get a dictionary with all attributes set to `None` instead of just `None`. Something to be aware of or we should fix it in this PR to return None instead. @akotlar: I tried to make everything just adhere to Python3, but can you make sure I did the conversions correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5738:598,error,errors,598,https://hail.is,https://github.com/hail-is/hail/pull/5738,1,['error'],['errors']
Availability,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10860:732,error,errors,732,https://hail.is,https://github.com/hail-is/hail/pull/10860,1,['error'],['errors']
Availability,"FYI @danking . I wasn't sure if removing a previously deleted user should be an error. I think it should be, but I left it as idempotent for now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9941:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/pull/9941,1,['error'],['error']
Availability,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6772:114,ping,ping,114,https://hail.is,https://github.com/hail-is/hail/pull/6772,3,"['down', 'echo', 'ping']","['down', 'echo', 'ping']"
Availability,"FYI @tpoterba. I have a few pending PRs now. I will run through this when they are all in. If anything else changes, we should probably do it again a couple days before the workshop and fix issues. Remaining workshop todo items:; - My error handling is too aggressive and frozen pod (2.B) won't work right now; - Still need to add memory/CPU settings to the workshop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7156:235,error,error,235,https://hail.is,https://github.com/hail-is/hail/pull/7156,1,['error'],['error']
Availability,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303:203,Error,Error,203,https://hail.is,https://github.com/hail-is/hail/issues/303,1,['Error'],['Error']
Availability,"Failing pipeline with data available here: gs://hail-jigold/ibd-exomes-part4471.mt. [Zulip conversation](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/subject/Working.20on.20translating.20a.20QC.20step.20from.200.2E1.20to.200.2E2). ```; import hail as hl. mt = hl.read_matrix_table('/Users/jigold/ibd-exomes-part4471.mt'). vars_of_interest = hl.set([""frameshift_variant"", ""inframe_deletion"", ""inframe_insertion"", ""stop_lost"", ""stop_gained"", ""start_lost"",\; ""splice_acceptor_variant"", ""splice_donor_variant"", ""splice_region_variant"", ""missense_variant"", ""synonymous_variant""]). mt = mt.annotate_globals(x = vars_of_interest); filtered = mt.filter_rows(mt.x.contains(mt.vep.most_severe_consequence)). filtered.write('/tmp/guhan.mt', overwrite=True); ```. This causes a segmentation fault that can be replicated with the first variant only `head(1)`. Filtering out rows where `mt.vep.most_severe_consequence` is missing will make the pipeline succeed. Tried this IR in IRSuite to replicate error with no success:; ```; @Test def debugGuhan() {; val s = ToSet(MakeArray(FastIndexedSeq(Str(""frameshift_variant""), Str(""inframe_deletion""), Str(""inframe_insertion""),; Str(""stop_lost""), Str(""stop_gained""), Str(""start_lost""), Str(""splice_acceptor_variant""), Str(""splice_donor_variant""),; Str(""splice_region_variant""), Str(""missense_variant""), Str(""synonymous_variant"")), TArray(TString()))). assertEvalsTo(LowerBoundOnOrderedCollection(s, NA(TString()), onKey = false), 11); }; ```. Separately, @tpoterba thinks the IR needs to have a Let statement for `set` in the IR implementation. I tried this, but it didn't fix the segmentation fault.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522:27,avail,available,27,https://hail.is,https://github.com/hail-is/hail/issues/4522,4,"['avail', 'error', 'fault']","['available', 'error', 'fault']"
Availability,Failure during installing Hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14235:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/issues/14235,1,['Failure'],['Failure']
Availability,Failure in vep annotation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/issues/303,1,['Failure'],['Failure']
Availability,Failure to annotate variants with dbNSFP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/317:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/issues/317,1,['Failure'],['Failure']
Availability,Fatal error when using `hl.sum` over Array of Double,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3436:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/issues/3436,1,['error'],['error']
Availability,"FatalError: IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 21, ap01-sw-wdp1.c.topmed-kathiresan-lipids-wgs.internal, executor 7): java.lang.IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null; at htsjdk.samtools.liftover.LiftOver.liftOver(LiftOver.java:137); at is.hail.io.reference.LiftOver.queryInterval(LiftOver.scala:61); at is.hail.variant.ReferenceGenome.liftoverLocusInterval(ReferenceGenome.scala:435); at is.hail.codegen.generated.C11.method1(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174:175,failure,failure,175,https://hail.is,https://github.com/hail-is/hail/issues/5174,2,['failure'],['failure']
Availability,"File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:77); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:49038,error,error,49038,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['error'],['error']
Availability,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10696:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/10696,2,"['error', 'failure']","['error', 'failure']"
Availability,First step towards relative error approx cdf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6025:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/6025,1,['error'],['error']
Availability,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7445:108,error,errors,108,https://hail.is,https://github.com/hail-is/hail/pull/7445,2,['error'],['errors']
Availability,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8819:456,down,downstream,456,https://hail.is,https://github.com/hail-is/hail/pull/8819,2,['down'],['downstream']
Availability,Fix #3475: Expression error gets output in stacktrace,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3497:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/3497,1,['error'],['error']
Availability,"Fix Die, add better require_biallelic error",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4865:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/4865,1,['error'],['error']
Availability,Fix Solr cast error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/560:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/560,1,['error'],['error']
Availability,Fix Type Error When PAR is Empty,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3897:9,Error,Error,9,https://hail.is,https://github.com/hail-is/hail/pull/3897,1,['Error'],['Error']
Availability,Fix VCF error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3076:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/3076,1,['error'],['error']
Availability,Fix aggregate rows key error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8322:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/8322,1,['error'],['error']
Availability,Fix an error in the MatrixTable tutorial,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14239:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/14239,1,['error'],['error']
Availability,Fix asm4s test failures on Cray.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1372:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/1372,1,['failure'],['failures']
Availability,Fix bad error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2789:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/2789,2,['error'],['error']
Availability,"Fix bad error message in Table.order_by, extend functionality",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4927:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/4927,1,['error'],['error']
Availability,Fix bad error message in unify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3801:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/3801,1,['error'],['error']
Availability,"Fix bad error message, make 'analyze' errors better",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4045:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/4045,2,['error'],"['error', 'errors']"
Availability,Fix bgen index error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2499:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/2499,1,['error'],['error']
Availability,Fix downcode docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4802:4,down,downcode,4,https://hail.is,https://github.com/hail-is/hail/pull/4802,1,['down'],['downcode']
Availability,Fix empty struct write error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1272:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/1272,1,['error'],['error']
Availability,Fix error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1040:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/1040,2,['error'],['error']
Availability,Fix error message for import_gen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2537:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/2537,1,['error'],['error']
Availability,Fix error message when Code[T] emission happens on worker,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3525:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/3525,1,['error'],['error']
Availability,Fix error message when indexing into (Matrix)Tables without keys,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6736:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/6736,1,['error'],['error']
Availability,Fix error message when no files found,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2850:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/2850,1,['error'],['error']
Availability,Fix error message with null values for export vcf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4156:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/4156,1,['error'],['error']
Availability,Fix error messages for MatrixTable / Table join syntax with __getitem__,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2694:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/2694,1,['error'],['error']
Availability,Fix error messages in union_cols,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5241:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/5241,1,['error'],['error']
Availability,Fix error messages related to bad indexing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5171:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/5171,1,['error'],['error']
Availability,"Fix error messages to be consistent about using quotes, not backticks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5761:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/5761,1,['error'],['error']
Availability,"Fix error, a bit of cleanup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2889:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/2889,1,['error'],['error']
Availability,Fix errors in FormatParser when array elements are missing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5170:4,error,errors,4,https://hail.is,https://github.com/hail-is/hail/pull/5170,1,['error'],['errors']
Availability,Fix failure after is.hail rename.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1268:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/1268,1,['failure'],['failure']
Availability,Fix manhattan plot downsampling,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4704:19,down,downsampling,19,https://hail.is,https://github.com/hail-is/hail/pull/4704,1,['down'],['downsampling']
Availability,Fix match error on import VCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2232:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/2232,1,['error'],['error']
Availability,Fix merge failure loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6045:10,failure,failure,10,https://hail.is,https://github.com/hail-is/hail/pull/6045,1,['failure'],['failure']
Availability,Fix method not found error due to relocation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1086:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/1086,1,['error'],['error']
Availability,Fix py4j error handling,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1210:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/1210,1,['error'],['error']
Availability,Fix python error messages,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1206:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/1206,1,['error'],['error']
Availability,Fix require biallelic error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2742:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/2742,1,['error'],['error']
Availability,Fix serialization error of $MapLike$$ produced by TDict.mapValues,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/613:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/613,1,['error'],['error']
Availability,Fix some aggregator parse errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2953:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/2953,1,['error'],['errors']
Availability,Fix splitmulti error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3433:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/3433,1,['error'],['error']
Availability,Fix struct errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2900:11,error,errors,11,https://hail.is,https://github.com/hail-is/hail/pull/2900,1,['error'],['errors']
Availability,Fix tc error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1838:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/1838,1,['error'],['error']
Availability,Fix typo in Table.checkpoint docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6431:18,checkpoint,checkpoint,18,https://hail.is,https://github.com/hail-is/hail/pull/6431,1,['checkpoint'],['checkpoint']
Availability,Fix up echoes after Table/MatrixTable/BlockMatrix writes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4801:7,echo,echoes,7,https://hail.is,https://github.com/hail-is/hail/pull/4801,1,['echo'],['echoes']
Availability,Fix warning in Table/MatrixTable checkpoint docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8938:33,checkpoint,checkpoint,33,https://hail.is,https://github.com/hail-is/hail/pull/8938,1,['checkpoint'],['checkpoint']
Availability,Fixed bad error message in Variant ctor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1545:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/pull/1545,1,['error'],['error']
Availability,Fixed error messages (!),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/324:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/324,1,['error'],['error']
Availability,Fixed error messages caused by invalid VCF input,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/369:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/369,1,['error'],['error']
Availability,Fixed error reporting problem in Jenkinsfile.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/540:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/540,1,['error'],['error']
Availability,Fixed failure on bgen import/hardcalls/write [0.1],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2114:6,failure,failure,6,https://hail.is,https://github.com/hail-is/hail/pull/2114,1,['failure'],['failure']
Availability,Fixed serialization error in binary op,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/940:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/940,1,['error'],['error']
Availability,"Fixes #11335. @tpoterba made the replicating test, with that it was easy to find the source of the bug. The error was in; ```; pkPartitioned; .strictify(); ...; .changePartitionerNoRepartition(partitioner.extendKeySamePartitions(keyType)); ```; where `pkPartitioned` is keyed by the partition key. In the test case, all rows have the same partition key, so the partitioner looks like `[x, x], [x, x], ...`. In that case, `strictify` correctly collapses all those partitions into one, but `partitioner.extendKeySamePartitions(keyType)` tries to extend the key type without changing the partitioning, which in this case creates an invalid partitioner. The fix is to use `pkPartitioned.extendKeyPreservesPartitioning(key)`, which does the `strictify` and creates the correct partitioner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11355:108,error,error,108,https://hail.is,https://github.com/hail-is/hail/pull/11355,1,['error'],['error']
Availability,"Fixes #11899 . To make [GENCODE v35](https://www.gencodegenes.org/human/release_35.html) available via the datasets API, as requested in issue https://github.com/hail-is/hail/issues/11899. . The Hail Table was created from the [comprehensive gene annotation (on the reference chromosomes only) GTF file](https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_35/gencode.v35.annotation.gtf.gz).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11946:89,avail,available,89,https://hail.is,https://github.com/hail-is/hail/pull/11946,1,['avail'],['available']
Availability,"Fixes #12976. I do not fully understand the Azure git tagging scheme, but [this commit](https://github.com/Azure/azure-sdk-for-java/commit/054df3fb74098f4ee30eeb1df70df1e40438d169) appears to have made `close` idempotent. It was merged in June of 2022. That commit resolved [an issue](https://github.com/Azure/azure-sdk-for-java/issues/24782) reporting an error very similar to our own. All the azure version changes update the Azure packages to their latest versions as of 2023-05-09 1713 ET. Unfortunately, newer Azure versions and Spark 3.3.0 have inconsistent `io.netty` requirements. Spark is stuck back in February 2022. Spark 3.4.0 does support a compatible version of `io.netty`, but we're months from seeing that in Dataproc. This change packages up Azure and all its dependencies into one JAR of relocated classes. We must refer to those classes by their relocated names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032:356,error,error,356,https://hail.is,https://github.com/hail-is/hail/pull/13032,1,['error'],['error']
Availability,"Fixes #13346. Another user was confused by this: https://github.com/hail-is/hail/issues/14102. Unfortunately, the world appears to have embraced missing values in VCF array fields even though the single element case is ambiguous. In #13346, I proposed a scheme by which we can disambiguate many of the cases, but implementing it ran into challenges because LoadVCF.scala does not expose whether or not an INFO field was a literal ""."" or elided entirely from that line. Anyway, this error message actually points users to the fix. I also changed some method names such that every method is ArrayType and never TypeArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14105:482,error,error,482,https://hail.is,https://github.com/hail-is/hail/pull/14105,1,['error'],['error']
Availability,"Fixes #13441 . Usage: `python3 devbin/generate_gcp_ar_cleanup_policy.py > my_policy_file.txt`; Reference: https://cloud.google.com/artifact-registry/docs/repositories/cleanup-policy; Note, we have a maximum of 10 policies available.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13489:222,avail,available,222,https://hail.is,https://github.com/hail-is/hail/pull/13489,1,['avail'],['available']
Availability,Fixes #13696.; `n_divisions` is a downsampling factor. It conflicts with `collect_all` when `collect_all=True` is specified.; Deprecate `collect_all` and issue warning when users supply a non-`None` `collect_all` argument.; Use `n_divisions=None` to not downsample (implies `collect_all=True`).; Throw when `collect_all=True` and `n_divisions` is not `None` - this is a conflict.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13738:34,down,downsampling,34,https://hail.is,https://github.com/hail-is/hail/pull/13738,2,['down'],"['downsample', 'downsampling']"
Availability,"Fixes #14262. Ever since starting to control job network namespaces ourselves, we run the worker container with `--network host`. But running with the host's network namespace means there's no need (nor meaning) to use port forwarding rules with `-p`. Docker safely ignores this redundant setting but emit some log messages like:. ```; WARNING: Published ports are discarded when using host network mode; ```. The solution here is to just remove the old port publishing settings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14252:279,redundant,redundant,279,https://hail.is,https://github.com/hail-is/hail/pull/14252,1,['redundant'],['redundant']
Availability,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2173:13,Error,Error,13,https://hail.is,https://github.com/hail-is/hail/pull/2173,3,['Error'],"['Error', 'ErrorHandling']"
Availability,"Fixes #2921. Example:. ```; In [3]: mt = mt.annotate_entries(PL=[0]). In [4]: hl.split_multi_hts(mt)._force_count_rows(). ... hundreds of lines of java trace ... Hail version: 0.2.30-bae67f384161; Error summary: HailException: array index out of bounds: index=1, length=1; ----------; Python traceback:; File ""<ipython-input-4-ba11ec1cd68e>"", line 1, in <module>; hl.split_multi_hts(mt)._force_count_rows(). File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2246, in split_multi_hts; (hl.range(0, 3).map(lambda i:. File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2250, in <lambda>; ).map(lambda j: split.PL[j])))))). File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2250, in <lambda>; ).map(lambda j: split.PL[j])))))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7792:197,Error,Error,197,https://hail.is,https://github.com/hail-is/hail/pull/7792,1,['Error'],['Error']
Availability,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3921:630,error,errors,630,https://hail.is,https://github.com/hail-is/hail/pull/3921,1,['error'],['errors']
Availability,Fixes #5293. @tpoterba I decided not to throw an error if the key is null and instead just let the `flatMap` in `restrictTo` take care of it. Let me know if you think we should error out.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5294:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/pull/5294,2,['error'],['error']
Availability,Fixes #6888. Also simplify code to remove redundant checks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6897:42,redundant,redundant,42,https://hail.is,https://github.com/hail-is/hail/pull/6897,1,['redundant'],['redundant']
Availability,"Fixes #7584. We already require this in Scala, but since we don't require it in python the error message is bad.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7656:91,error,error,91,https://hail.is,https://github.com/hail-is/hail/pull/7656,1,['error'],['error']
Availability,"Fixes #8083 . The format of the status is ; ```; # {; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int; # }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8088:196,error,error,196,https://hail.is,https://github.com/hail-is/hail/pull/8088,2,['error'],['error']
Availability,"Fixes #8343. The Google Storage Hadoop connector introduced; a backwards incompatible change in 2.1.0 which relies on; a new method in Hadoop 2.8.3 that is not present in Hadoop; 2.7.3. There are no Spark releases that include Hadoop 2.8.3; yet, so we choose to downgrade to the last compatible; connector library version. Randomly picked a hail query person.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8344:262,down,downgrade,262,https://hail.is,https://github.com/hail-is/hail/pull/8344,1,['down'],['downgrade']
Availability,"Fixes #8944. CHANGELOG: Fixed crash (error 134 or SIGSEGV) in `MatrixTable.annotate_cols`, `hl.sample_qc`, and more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9169:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/9169,1,['error'],['error']
Availability,Fixes `bash: line 10: yum: command not found` failures in `delete_azure_batch_instances`. ### Security Assessment; This change has no security impact,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14720:46,failure,failures,46,https://hail.is,https://github.com/hail-is/hail/pull/14720,1,['failure'],['failures']
Availability,"Fixes https://github.com/hail-is/hail/issues/14130. We pervasively assume:; 1. That our entire system is used within a single Python thread.; 2. That once an event loop is created that's the only event loop that will exist forever. Pytest (and newer version of IPython, afaict) violate this pretty liberally. ~~pytest_asyncio has [explicit instructions on how to run every test in the same event loop](https://pytest-asyncio.readthedocs.io/en/latest/how-to-guides/run_session_tests_in_same_loop.html). I've implemented those here.~~ [These instructions don't work](https://github.com/pytest-dev/pytest-asyncio/issues/744). It seems that the reliable way to ensure we're using one event loop everywhere is to use pytest-asyncio < 0.23 and to define an event_loop fixture with scope `'session'`. I also switched test_batch.py into pytest-only style. This allows me to use session-scoped fixtures so that they exist exactly once for the entire test suite execution. Also:; - `RouterAsyncFS` methods must either be a static method or an async method. We must not create an FS in a sync method. Both `parse_url` and `copy_part_size` now both do not allocate an FS.; - `httpx.py` now eagerly errors if the running event loop in `request` differs from that at allocation time. Annoying but much better error message than this nonsense about timeout context managers.; - `hail_event_loop` either gets the current thread's event loop (running or not, doesn't matter to us) or creates a fresh event loop and sets it as the current thread's event loop. The previous code didn't guarantee we'd get an event loop b/c `get_event_loop` fails if `set_event_loop` was previously called.; - `conftest.py` is inherited downward, so I lifted fixtures out of test_copy.py and friends and into a common `hailtop/conftest.py`; - I added `make -C hail pytest-inter-cloud` for testing the inter cloud directory. You still need appropriate permissions and authn.; - I removed extraneous pytest.mark.asyncio since we use auto mo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14097:641,reliab,reliable,641,https://hail.is,https://github.com/hail-is/hail/pull/14097,1,['reliab'],['reliable']
Availability,Fixes the test where 'ERROR: could not find file' is in the message.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11692:22,ERROR,ERROR,22,https://hail.is,https://github.com/hail-is/hail/pull/11692,1,['ERROR'],['ERROR']
Availability,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6152:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/6152,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8230:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/8230,1,['error'],['error']
Availability,"Fixes this error message in logs:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 302, in batch_callback; await asyncio.shield(batch_callback_handler(request)); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 276, in batch_callback_handler; await wb.notify_batch_changed(); TypeError: notify_batch_changed() missing 1 required positional argument: 'app'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7399:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/7399,1,['error'],['error']
Availability,Fixes this error: ```except Py4JJavaError as e:; NameError: global name 'Py4JJavaError' is not defined```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1166:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/1166,1,['error'],['error']
Availability,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7660-default-ne0pow1za1v4' '--format=value(name)'; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7661:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/7661,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7663-default-ccnvti4s750b' '--format=value(name)' --project hail-vdc; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; Deleted [https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-o2tmf].; ERROR: (gcloud.compute.instances.delete) Could not fetch resource:; - The resource 'projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-8z4dd' was not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7666:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/7666,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Fixes this error:. ```; Traceback (most recent call last):; File ""/Users/tpoterba/anaconda3/envs/hail/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 83, in __del__; File ""/Users/tpoterba/anaconda3/envs/hail/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 79, in close; File ""/Users/tpoterba/anaconda3/envs/hail/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 453, in _close; AttributeError: 'ServiceBackend' object has no attribute '_batch_client'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11337:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/11337,1,['error'],['error']
Availability,Fixes this problem:. ```; + xargs -r gcloud -q compute instances delete --zone; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; You may set it for your current workspace by running:. $ gcloud config set project VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7520:80,ERROR,ERROR,80,https://hail.is,https://github.com/hail-is/hail/pull/7520,1,['ERROR'],['ERROR']
Availability,"Fixes this:. ```; WARNING	| 2019-04-30 23:31:34,751 	| server.py 	| handler:818 | callback for batch 33, job 993 failed due to an error, I will not retry. Error: Invalid URL 'ci2/batch_callback': No schema supplied. Perhaps you meant http://ci2/batch_callback?; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6008:130,error,error,130,https://hail.is,https://github.com/hail-is/hail/pull/6008,2,"['Error', 'error']","['Error', 'error']"
Availability,Fixes: #14559; `hl.nd.array`s constructed from stream pipelines can cause out of memory exceptions owing to a limitation in the python CSE algorithm that does not eliminate partially redundant expressions in if-expressions. Explicitly `let`-binding the input collection prevents it from being evaluated twice: once for the flattened data stream and once for the original shape.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14571:183,redundant,redundant,183,https://hail.is,https://github.com/hail-is/hail/pull/14571,1,['redundant'],['redundant']
Availability,Follow-up PRs will use this for better error messages in places like require_biallelic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4845:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/4845,1,['error'],['error']
Availability,"For #604: I changed the max-width to 80em from 45em. If this is not wide enough, then we should probably remove the max-width property. For #605: It was extremely difficult to replicate the issue, but I believe it's because the mathjax and jquery operations are running asynchronously and the mathjax finishes before the jquery code has finished populating the DOM. I added a ""defer"" attribute to the mathjax script loading, so the script downloads in the background, but doesn't get executed until the DOM has been populated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/630:439,down,downloads,439,https://hail.is,https://github.com/hail-is/hail/pull/630,1,['down'],['downloads']
Availability,"For added context see: https://github.com/hail-is/hail/issues/13351. cc: @patrick-schultz @chrisvittal @daniel-goldstein @ehigham @iris-garden . I actually don't think we need to notify anyone because we'll still get errors if something goes wrong in default. If main starts failing, we have the slightly annoying situation of needing to run tests in a one-off manner to verify and we might need to block a release until we can revert.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13353:217,error,errors,217,https://hail.is,https://github.com/hail-is/hail/pull/13353,1,['error'],['errors']
Availability,"For attributes, same as /jobs, and state queries: open, closed, running, success, failure, cancelled, and complete. Also added an `open` batch state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7645:82,failure,failure,82,https://hail.is,https://github.com/hail-is/hail/pull/7645,1,['failure'],['failure']
Availability,"For example, see error messages when batch-driver shuts down: https://cloudlogging.app.goo.gl/kKgJdv34s3a6Vd8n6.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13418:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/13418,2,"['down', 'error']","['down', 'error']"
Availability,"For my SAIGE implementation, it would be nice to be able to use the `{BATCH_TMPDIR}` environment variable within a file name so that I can give user-specified file path names for output files to write that can be used downstream in globs when importing temporary files in Hail without having to localize all of the files (could be 4K+ files). I also thought this could solve Konrad's region bucket request where we copy data from a region-specific location.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14340:218,down,downstream,218,https://hail.is,https://github.com/hail-is/hail/pull/14340,1,['down'],['downstream']
Availability,"For my pipeline code, I need a way to iterate through the list of jobs submitted and collect their error codes to determine if a pipeline failed and if so print out the log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5200:99,error,error,99,https://hail.is,https://github.com/hail-is/hail/pull/5200,1,['error'],['error']
Availability,"For rrm and grm, I've pushed overall rescaling to the faster block matrix side and thereby removed the action that counts the number of variants after filtering. The behavior is identical except that we no longer warn when constant variants are dropped, but I've clarified the behavior in the docs. In the unfathomable case that all variants are constant, the user will receive the error message `HailException: block matrix must have at least one row` when the BlockMatrix is being created. For rrm, I've also simplified the filtering, as well as the expression algebra for std_dev from; ```; hl.sqrt((mt.__ACsq + (n_samples - mt.__n_called) * mt.__mean_gt ** 2) / n_samples - mt.__mean_gt ** 2); ```; to; ```; hl.sqrt(mt.__ACsq - (mt.__AC ** 2) / mt.__n_called); ```; with the added benefit of combining two annotates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3944:382,error,error,382,https://hail.is,https://github.com/hail-is/hail/pull/3944,1,['error'],['error']
Availability,For some reason either artifact registry or aiodocker returns a 500 instead of a 403 when a service account does not have access to pull an image. Had to add another special case for handling this error. https://console.cloud.google.com/logs/query;query=%22ys6od%22;pinnedLogId=2022-10-03T13:09:09.430766581Z%2Fyw46w2divo5eqk0vv;cursorTimestamp=2022-10-03T13:09:09.430766581Z?project=hail-vdc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12257:197,error,error,197,https://hail.is,https://github.com/hail-is/hail/pull/12257,1,['error'],['error']
Availability,"Found this bug in my dev environment with randomly changing the version names. The error is a catastrophic failure trying to insert duplicate rows, so there shouldn't be any issues with the current database data.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12721:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/pull/12721,2,"['error', 'failure']","['error', 'failure']"
Availability,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3447:363,failure,failure,363,https://hail.is,https://github.com/hail-is/hail/issues/3447,2,['failure'],['failure']
Availability,From @berylc - possibly due to empty partitions? This was after 2 import_table's and a join:; ```; Java stack trace:; java.lang.UnsupportedOperationException: empty.min; at scala.collection.TraversableOnce$class.min(TraversableOnce.scala:222); at scala.collection.mutable.ArrayOps$ofRef.min(ArrayOps.scala:186); at is.hail.rvd.OrderedRVD$.calculateKeyRanges(OrderedRVD.scala:615); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:184); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:19); at is.hail.table.Table.repartition(Table.scala:1003); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-3959178; Error summary: UnsupportedOperationException: empty.min; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3515:1279,Error,Error,1279,https://hail.is,https://github.com/hail-is/hail/issues/3515,1,['Error'],['Error']
Availability,"From Akhil:. ```; Traceback (most recent call last):; File ""/tmp/e9b3538af9b14d6ba440edbdf89e1ef1/phewas_rvas_jhs_unrel.py"", line 55, in <module>; print(mt.describe()); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 85, in describe; rowstr = ""\nRows: \n"" + ""\n "".join([""{}: {}"".format(k, v._type) for k, v in self._row_keys.items()]); AttributeError: 'list' object has no attribute 'items'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [e9b3538af9b14d6ba440edbdf89e1ef1] failed with error:; Job failed with message [AttributeError: 'list' object has no attribute 'items']. Additional details can be found in 'gs://dataproc-37b10a9c-3ebd-47a4-9228-bf38bdfba439-us/google-cloud-dataproc-metainfo/e432509c-1bea-4839-9cb8-4d404d195a35/jobs/e9b3538af9b14d6ba440edbdf89e1ef1/driveroutput'.; ```; ```; mt = hl.read_matrix_table('gs://jhs_data_topmed/HCLOF_JHS_AF_01_unrel.mt'); mt.describe(); print(mt.count()); mt = mt.annotate_rows(genes = mt.vep.transcript_consequences.gene_symbol); mt = mt.group_rows_by(mt.genes); print(mt.describe()); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7410:424,ERROR,ERROR,424,https://hail.is,https://github.com/hail-is/hail/issues/7410,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1983:73,error,error,73,https://hail.is,https://github.com/hail-is/hail/issues/1983,5,"['Error', 'error']","['Error', 'error']"
Availability,"From the logs:; ```; WARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9771:160,error,error,160,https://hail.is,https://github.com/hail-is/hail/pull/9771,1,['error'],['error']
Availability,"Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Correct <code>Any</code> type annotations. <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/791"">#791</a> (<a href=""https://github.com/joouha""><code>@​joouha</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/main/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>7.3.4</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.3...ca4cb2d6a4b95a6925de85a47b323d2235032c74"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Revert latest changes to <code>ThreadedZMQSocketChannel</code> because they break Qtconsole <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/803"">#803</a> (<a href=""https://github.com/ccordoba12""><code>@​ccordoba12</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Fix sphinx 5.0 support <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/804"">#804</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/799"">#799</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-07&amp;to=2022-06-08&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-06-07..2022-06-08&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Accordoba12+updated%3A2022-06-07..2022-06-08&amp;type=Issues""><code>@​cc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:4002,Mainten,Maintenance,4002,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Mainten'],['Maintenance']
Availability,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3038:5,error,error,5,https://hail.is,https://github.com/hail-is/hail/issues/3038,1,['error'],['error']
Availability,"Function2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:1078,error,error,1078,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['error'],['error']
Availability,GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:2099,Heartbeat,HeartbeatReceiver,2099,https://hail.is,https://github.com/hail-is/hail/issues/4780,2,['Heartbeat'],['HeartbeatReceiver']
Availability,"GCP Logging attempts to infer the severity of a log entry and defaults to labelling everything written to `stdout` as `INFO` and everything written to `stderr` as `ERROR`. There are some [LogEntry fields](https://cloud.google.com/logging/docs/structured-logging) that can be overwritten by fields in our JSON output, including `severity`. Until now we had been logging the severity level as `levelname`, which is the expectation of `jsonlogger`, but this means GCP's levels and ours do not necessarily match. This adds another `severity` field to the logs so GCP can pick up the level. This should get rid of a swath of non-error log messages written to `stderr` (only JSON though) and lets other levels like `WARNING` get marked as such. GCP does quite literally extract the severity field though, so it does not appear in the `jsonPayload`. I've kept the `levelname` in then as a sanity check but am also happy to try to remove it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9910:164,ERROR,ERROR,164,https://hail.is,https://github.com/hail-is/hail/pull/9910,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"GENCODE GTF files have contigs ""chr1"", ""chr2"", ... , ""chrX"", ""chrY"", ""chrM"". Currently, when `reference_genome` is set to GRCh37, `import_gtf` removes the ""chr"" prefix. This works for chromosomes 1-22, X, and Y. However, for the mitochondrial contig, Hail expects ""MT"" instead of ""M"" and attempting to do anything with the imported table results in an error: `HailException: Invalid interval '[...]' found. Contig 'M' is not in the reference genome 'GRCh37'.`. With this change, `import_gtf` recodes ""M"" and ""chrM"" as ""MT"" so that the intervals are valid for GRCh37.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9694:352,error,error,352,https://hail.is,https://github.com/hail-is/hail/pull/9694,1,['error'],['error']
Availability,"GIAB VCF import fails because htsjdk ""repairs"" header in vcf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6012:38,repair,repairs,38,https://hail.is,https://github.com/hail-is/hail/issues/6012,1,['repair'],['repairs']
Availability,GRM failure with N >50k,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1004:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/issues/1004,1,['failure'],['failure']
Availability,"GWAS. Everything is OK until the line to do GWAS. The error I have is as follows:. -------------- start of error ------------------; 2021-01-25 12:36:11 Hail: INFO: linear_regression_rows: running on 250 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:44859); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-40-d6d936b012db>"", line 3, in <module>; covariates=[1.0]); File ""<decorator-gen-1697>"", line 2, in linear_regression_rows; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/methods/statgen.py"", line 370, in linear_regression_rows; return ht_result.persist(); File ""<decorator-gen-1111>"", line 2, in persist; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:1749,Error,Error,1749,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['Error'],['Error']
Availability,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8058:210,down,downstream,210,https://hail.is,https://github.com/hail-is/hail/pull/8058,1,['down'],['downstream']
Availability,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:24,failure,failure,24,https://hail.is,https://github.com/hail-is/hail/issues/3053,2,"['error', 'failure']","['error', 'failure']"
Availability,"Getting the following errors when compiling on a Mac. Any suggestions?. ./gradlew shadowJar ; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/farrell/github/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/darwin; c++ -fvisibility=hidden -dynamiclib -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/darwin/libibs.dylib; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:23:no such instruction: `vmovd %xmm0, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:38:no such instruction: `vpextrq $1, %xmm0,%rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:79:no such instruction: `vpcmpeqd %xmm5, %xmm5,%xmm5'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:96:no such instruction: `vpxor %xmm1, %xmm0,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:114:no such instruction: `vpxor %xmm5, %xmm1,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:135:no such instruction: `vpand %xmm3, %xmm2,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:150:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:167:no such instruction: `vpxor %xmm5, %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:193:no such instruction: `vpor LC1(%rip), %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:212:no such instruction: `vpand %xmm1, %xmm0,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:225:no such instruction: `vpandn %xmm4, %xmm3,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:241:no such instruction: `vmovd %xmm4, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:255:no such instruction: `vpxor %xmm1, %xmm0,%xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:277:no such instruction: `v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274:22,error,errors,22,https://hail.is,https://github.com/hail-is/hail/issues/1274,1,['error'],['errors']
Availability,"Gidgethub does not properly handle this error response. The right answer is to fix gidgethub, but, alas, too much to do and too little time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8540:40,error,error,40,https://hail.is,https://github.com/hail-is/hail/pull/8540,1,['error'],['error']
Availability,Google considers any message written to stderr as an error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9845:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/9845,1,['error'],['error']
Availability,"Got this error in a migration step when dev deploying. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 598, in _read_bytes; data = await self._reader.readexactly(num_bytes); File ""/usr/lib/python3.7/asyncio/streams.py"", line 679, in readexactly; await self._wait_for_data('readexactly'); File ""/usr/lib/python3.7/asyncio/streams.py"", line 473, in _wait_for_data; await self._waiter; File ""/usr/lib/python3.7/asyncio/selector_events.py"", line 804, in _read_ready__data_received; data = self._sock.recv(self.max_size); ConnectionResetError: [Errno 104] Connection reset by peer. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""create_database.py"", line 263, in <module>; loop.run_until_complete(async_main()); File ""/usr/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""create_database.py"", line 259, in async_main; await migrate(database_name, db, i, m); File ""create_database.py"", line 201, in migrate; (to_version, to_version, name, script_sha1)); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 26, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 229, in just_execute; async with self.start() as tx:; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 114, in __aenter__; await tx.async_init(self.db_pool, self.read_only); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 135, in async_init; await cursor.execute('START TRANSACTION;'); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8761:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/8761,1,['error'],['error']
Availability,Got this error message:. [Stage 0:> (0 + 120) / 2941]hail: write: fatal: hdfs://dataflow01.broadinstitute.org:8020/user/aganna/DIABETES.vcf.bgz: caught java.lang.IllegalArgumentException: requirement failed; offending line: 1 23735206 var_1_23735206 C T 4044.57PASS AC=1;AF=1.971e-0… . which according to Tim can be improved (there is no separation between QUAL and INFO filed). My main point however is that for these malformed files we should be able to remove the malformed lines instead of just not allowing to import the .vcf. This might be an important vcf and cannot / or difficult to find a better version.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/361,1,['error'],['error']
Availability,"Grace requested this for gnomAD purposes. In Azure, we can determine this; using the classpath or envionrment variables (the env var is only available; inside Jupyter). In GCP, I added an environment variable to our Spark env; / Jupyter env.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11230:141,avail,available,141,https://hail.is,https://github.com/hail-is/hail/pull/11230,1,['avail'],['available']
Availability,HAIL 0.1: export vcf hadoop error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['error'],['error']
Availability,"HON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14211:2790,avail,available,2790,https://hail.is,https://github.com/hail-is/hail/pull/14211,1,['avail'],['available']
Availability,"HON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `40.5.0 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:2860,avail,available,2860,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['avail'],['available']
Availability,HTTP 404 error at https://hail.is/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14616:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/14616,1,['error'],['error']
Availability,"HTTPError: 413 Client Error: Request Entity Too Large. FYI, cranking up the relevant setting will only sort of fix the problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5898:22,Error,Error,22,https://hail.is,https://github.com/hail-is/hail/issues/5898,1,['Error'],['Error']
Availability,"HY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:4157,avail,available,4157,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"HY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:4149,avail,available,4149,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263:29,failure,failure,29,https://hail.is,https://github.com/hail-is/hail/pull/6263,2,"['error', 'failure']","['errors', 'failure']"
Availability,Hail Literal Throws Weird Errors on Hail Objects,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3708:26,Error,Errors,26,https://hail.is,https://github.com/hail-is/hail/issues/3708,1,['Error'],['Errors']
Availability,"Hail Version: 0.2.108. Running the line w/ Zstd compressed UKB bgen files:; `ht = hl.experimental.pc_project(; mt.GT,; loadings_ht.loadings,; loadings_ht.pca_af,; ); `; I get the error at the end of spark execution: ; `Exception in thread ""map-output-dispatcher-0"" Exception in thread ""map-output-dispatcher-1"" java.lang.UnsatisfiedLinkError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.recommendedCOutSize()J; `; (full error attached); [error.txt](https://github.com/hail-is/hail/files/10458606/error.txt). Any ideas?; Many Thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12608:179,error,error,179,https://hail.is,https://github.com/hail-is/hail/issues/12608,4,['error'],['error']
Availability,"Hail Version: `0.2.53-8140f17d9262`. I've got a matrix table where I have added a row and col index to, and am trying to select a field from the table using the following syntax and getting the following error:. `mt[1, 1].GT`. ```; The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); <ipython-input-65-47ed69d0881f> in <module>; 1 for header in headers:; ----> 2 ext = ext.annotate(newHeader = filtered[header.row_idx, ext.idx].GT); 3 ext = ext.rename({ 'newHeader': header.header }). /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in __getitem__(self, item); 626 return self.index_entries(row_key, col_key); 627 except TypeError as e:; --> 628 raise invalid_usage from e; 629 raise invalid_usage; 630. TypeError: MatrixTable.__getitem__: invalid index argument(s); Usage 1: field selection: mt['field']; Usage 2: Entry joining: mt[mt2.row_key, mt2.col_key]. To join row or column fields, use one of the following:; rows:; mt.index_rows(mt2.row_key); mt.rows().index(mt2.row_key); mt.rows()[mt2.row_key]; cols:; mt.index_cols(mt2.col_key); mt.cols().index(mt2.col_key); mt.cols()[mt2.col_key]; ```. Please let me know if you need any further information. The zulip topic name is `Error accessing entry by row/col key`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9704:204,error,error,204,https://hail.is,https://github.com/hail-is/hail/issues/9704,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hail appears to have executed the exact same write command twice. The first write driver ends at 2023-10-13T01:17:55Z and the next write driver starts at 2023-10-13T01:18:11Z, just 16 seconds later. Batch: https://batch.hail.is/batches/8058522; Just the drivers: https://batch.hail.is/batches/8058522?q=name%3Dexecute%28...%29_driver. Driver & frontend logs indicate the first driver job completed and was almost immediately followed by a resubmission of the entire pipeline. https://cloudlogging.app.goo.gl/1344nayXTgaqKhCz8. # OLD. ### What happened?. NB: This is a development build 87398e1b514e. I think my comments below might be misleading. We purposely `WriteMetadata` multiple times, but with different `MetadataWriter`s. Unfortunately, this information does not appear in the SSA IR for some reason?. ---. The ""Relevant log output"" contains the last IR printed before the code was executed. The observed error was:. <details>; <summary>Expand me for the full trace. ```; Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```. </summary>. ```; Traceback (most recent call last):; File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 345, in <module>; ); File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 297, in main; mt = mt.filter_rows(mt.locus.in_autosome()); File ""<decorator-gen-1358>"", line 2, in write; File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/typecheck/check.py"", line 587, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/matrixtable.py"", line 2738, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 541, in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=time",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:913,error,error,913,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['error'],['error']
Availability,Hail context error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3102:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/3102,1,['error'],['error']
Availability,Hail improve error message for pip,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7081:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/pull/7081,1,['error'],['error']
Availability,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7063:657,error,error,657,https://hail.is,https://github.com/hail-is/hail/issues/7063,1,['error'],['error']
Availability,Hail should not signal an error if the PAR is empty. This change provides an explicit type to the PAR before handing it to filter_intervals. . @tpoterba assigning you because I want to make sure I'm doing this right.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3897:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/3897,1,['error'],['error']
Availability,Hail should produce a legible error message when reading an out-of-date VDS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2159:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/2159,1,['error'],['error']
Availability,Hail should report an error to the user immediately when an input file is malformed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4100:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/4100,1,['error'],['error']
Availability,Hail tests should better describe their failures,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1286:40,failure,failures,40,https://hail.is,https://github.com/hail-is/hail/issues/1286,1,['failure'],['failures']
Availability,Hail throws a confusing error message on a weird CSV,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5222:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/5222,1,['error'],['error']
Availability,"Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@562ad221{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 14:46:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 14:46:39 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:39 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 14:46:40 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 14:46:40 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 14:46:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 14:46:40 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 14:4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:32033,AVAIL,AVAILABLE,32033,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,HailContext(local=local[1]) uses all available cores,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1564:37,avail,available,37,https://hail.is,https://github.com/hail-is/hail/issues/1564,1,['avail'],['available']
Availability,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8636:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/pull/8636,1,['error'],['error']
Availability,"HailException: error while checking subtype:; super: struct{trait_type: str, phenocode: str, pheno_sex: str, coding: str, modifier: str, pheno_data: array<struct{}>, description: str, description_more: str, coding_description: str, category: str, n_cases_full_cohort_both_sexes: int64, n_cases_full_cohort_females: int64, n_cases_full_cohort_males: int64, col_array: tuple(1:struct{n_cases: int32, n_controls: int32, heritability: struct{estimates: struct{ldsc: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; sub: struct{trait_type: str, phenocode: str, pheno_sex: str, coding: str, modifier: str, pheno_data: array<struct{}>, description: str, description_more: str, coding_description: str, category: str, n_cases_full_cohort_both_s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10858:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/issues/10858,1,['error'],['error']
Availability,Handle failure in SamplePCA when matrix has fewer than k non-zero singular values,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2008:7,failure,failure,7,https://hail.is,https://github.com/hail-is/hail/pull/2008,1,['failure'],['failure']
Availability,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/430:338,failure,failure,338,https://hail.is,https://github.com/hail-is/hail/issues/430,2,['failure'],['failure']
Availability,"Happy to sit down and go through what this is about. This is current running on the cluster:. ```; $ kubectl get pods; NAME READY STATUS RESTARTS AGE; ...; spark-master-ffcfbf95c-gth5s 1/1 Running 0 4m; spark-worker-699db74c7-lsd9v 1/1 Running 0 11h; spark-worker-699db74c7-plgdd 1/1 Running 0 11h; ```. but I haven't automated deployment yet. I'm currently building the hail image from a distribution I hand built, but I'll switch over to the standard distribution once this goes in: https://github.com/hail-is/hail/pull/4554 (it fixed some bugs that showed up in this deployment). That's the `gs://hail-cseed/hail-test.zip` stuff. This was surprisingly difficult to get working. The main culprit, I think, is that Spark makes it impossible to bind and advertise different addresses for the Spark master. In the end I faked it out with:. > echo ""0.0.0.0 spark-master"" >> /etc/hosts. which works but seems a bit dubious.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4560:13,down,down,13,https://hail.is,https://github.com/hail-is/hail/pull/4560,2,"['down', 'echo']","['down', 'echo']"
Availability,"Having some issues with `map` in `annotateglobal`. The following command:. ```; annotateglobal expr -c 'global.pops = [""AFR"", ""NFE""]' \; annotateglobal expr -c 'global.pop_counts = global.pops.map(x => samples.count(sa.meta_test.POP == x))' \; showglobals; ```. return this error:. ```; hail: fatal: annotateglobal expr: symbol `x' not found; <input>:1:global.nfes = global.pops.map(x => samples.count(sa.meta_test.POP == x)); ```. @tpoterba seemed to suggest aggregators don't work inside map?. Anyway, to provide context: what I'd love to do is eventually be able to iterate through arrays to summarize data (rather than having to specify all my criteria once in each annotate command). Perhaps this is better done in some `group_by` type functionality, but I'm not sure where that is on the roadmap.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/549:274,error,error,274,https://hail.is,https://github.com/hail-is/hail/issues/549,1,['error'],['error']
Availability,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8908:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/issues/8908,4,"['Error', 'down', 'error']","['Error', 'downstream', 'error']"
Availability,"Hello, I have installed hail using pycharm not from github. Now getting the attached error. My command was:; ```; import hail as hl; hl.init(); import os; from hail.plot import show; [hail.err.txt](https://github.com/hail-is/hail/files/3570397/hail.err.txt). from pprint import pprint; hl.plot.output_notebook(). hl.utils.get_1kg('data/'); hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). mt = hl.read_matrix_table('data/1kg.mt'); mt.rows().select().show(5); ```; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6982:85,error,error,85,https://hail.is,https://github.com/hail-is/hail/issues/6982,1,['error'],['error']
Availability,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/issues/825,1,['error'],['error']
Availability,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453:733,error,errors,733,https://hail.is,https://github.com/hail-is/hail/issues/453,2,"['FAILURE', 'error']","['FAILURE', 'errors']"
Availability,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1151:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1151,5,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:587,error,errors,587,https://hail.is,https://github.com/hail-is/hail/issues/1525,2,['error'],['errors']
Availability,"Hi - I receive the following error when running the `variantqc` example from the [Getting Started documentation](https://hail.is/getting_started.html):. `$ ./build/install/hail/bin/hail read ~/sample.vds splitmulti variantqc -o ~/variantqc.tsv sampleqc -o ~/sampleqc.tsv; `. `hail: fatal: variantqc: parse error: ""-o"" is not a valid option`. Leaving `variantqc` out runs without error and generates the sampleqc output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1017:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/1017,3,['error'],['error']
Availability,"Hi Hail developers,; I am a new hail user and was struggling to process my multi-sample vcf file with hail. I first tried to read in my file with the code below and create annotate another column:. **import hail as hl; rt = hl.import_vcf('chr1_biallelic.vcf.gz',force_bgz=True,reference_genome=""GRCh38"",drop_samples=True).rows(); rt = rt.annotate(variant=rt.CHROM + ':' + rt.POSITION + "":"" + rt.REF + "":"" + rt.ALT)''**. However, running the third line gives me the following error:; **AttributeError: Table instance has no field, method, or property 'CHROM'**. Even though the CHROM header is present in my vcf file, it's not being read. Is it because of the metadata headers? Sorry if this is a basic question, I don't have any other resources to rely on and couldn't find any solutions online.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10978:475,error,error,475,https://hail.is,https://github.com/hail-is/hail/issues/10978,1,['error'],['error']
Availability,"Hi all,; I used hail 0.2 and made my own hail referene genome for a plant other than human by faking the X, Y and Par. But I can not find anywhere to setup my own reference genome even I know there is hl.init method which can set different version of human reference genome, and there is no set_reference method which is the counterpart of hl.get_reference().; How can I make my own reference genome available in hail0.2? Thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9837:400,avail,available,400,https://hail.is,https://github.com/hail-is/hail/issues/9837,1,['avail'],['available']
Availability,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419:274,ERROR,ERROR,274,https://hail.is,https://github.com/hail-is/hail/issues/1419,12,['ERROR'],['ERROR']
Availability,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6012:275,error,error,275,https://hail.is,https://github.com/hail-is/hail/issues/6012,2,"['error', 'repair']","['error', 'repair']"
Availability,"Hi!. Trying to calculate polygenic risk score with code from the [Polygenic Score Calculation](https://hail.is/docs/0.2/guides/genetics.html#polygenic-score-calculation), getting error with stacktrace:. `2022-05-14 12:09:07 Hail: INFO: Running Hail version 0.2.94-f0b38d6c436f; 2022-05-14 12:09:08 SparkContext: WARN: Using an existing SparkContext; some configuration may not take effect.; 2022-05-14 12:09:08 root: INFO: RegionPool: initialized for thread 30: Thread-4; 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 34.3 KiB, free 434.4 MiB); 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:179,error,error,179,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['error'],['error']
Availability,"Hi, . I'm using the concordance function to compare two sets of data, and I feel the n_discordant (last column) is not correct. . For example: ; ```; chr1:930314 [""C"",""T""] {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} [[0,0,0,0,0],[0,0,21,1,0],[0,0,2057,0,0],[0,0,0,91,0],[0,0,0,0,3]] 2172; chr1:946538 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,5,3,0],[0,0,1868,1,1],[0,0,0,279,0],[0,0,0,0,16]] 2170; chr1:946653 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,856,275,66],[0,0,386,74,7],[0,0,16,415,33],[0,0,0,3,42]] 1898; ```. In the first example, I thought the n_discordant should be 0 if the `concordance` field is correct, isn't it?. The code I was using: ; `global_GA_both, samples_GA_both, SNPs_GA_both = hl.concordance(mt_exome, mt_GAsP_ft)`. The Hail version:; ```Running on Apache Spark version 2.4.3; SparkUI available at http://spark-master:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.26-2dcc3d963867; LOGGING: writing to Concordance_2019_11_28_hail.log; ```. When I was using google Terra Hail 0.2.11-daed180b84d8, I didn't have this issue. The output didn't have `left_row` or `right_row`. Cheers,; Qinqin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7632:1137,avail,available,1137,https://hail.is,https://github.com/hail-is/hail/issues/7632,1,['avail'],['available']
Availability,"Hi, I am a staff in DCH. Now, we are testing the hail software and meet some test errors .; 1. How to check the results of some commands? such as, annotatesamples. For example, if you run this command:; hail importvcf sample.vcf annotatesamples expr -c 'sa.nHet = gs.count(g.isHet)‘ exportsamples –c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnota",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/683:82,error,errors,82,https://hail.is,https://github.com/hail-is/hail/issues/683,2,['error'],['errors']
Availability,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:412,echo,echo,412,https://hail.is,https://github.com/hail-is/hail/issues/2076,2,"['echo', 'error']","['echo', 'errors']"
Availability,"Hi, I found hl.init(sc=sc) returns error since hail-0.2.92.; It can reproduce simply run as following.; Is it a bug ??; Or should I run other ways ?. - Environments I tested. ; Hail version : 0.2.92 or later.; Mac book air (M1) , spark local mode; Rocky Linux 8.5 , spark local mode; Rocky Linux 8.5 , spark yarn cluster mode. - how to reproduce; ```; import os; os.environ['PYSPARK_SUBMIT_ARGS'] = ' \; --jars \; /Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/hail/backend/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; pyspark-shell '. from pyspark import SparkContext; sc=SparkContext.getOrCreate(). import hail as hl; hl.init(sc=sc); ```. - Error logs ; ```; 22/05/11 14:31:21 WARN Utils: Your hostname, spacerider.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en6); 22/05/11 14:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 22/05/11 14:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use set",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/issues/11827,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299:285,down,downgrade,285,https://hail.is,https://github.com/hail-is/hail/issues/7299,2,['down'],"['downgrade', 'downgrades']"
Availability,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003:425,ERROR,ERROR,425,https://hail.is,https://github.com/hail-is/hail/issues/1003,5,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"Hi, I'm facing an error when importing a bgen file and I was wondering if you can help me. I'd made a small .bgen file using qctools to select only a few samples from the original file and now I'm trying to import this smaller file into hail, like this:. `hl.import_bgen('subsetted_chr22.bgen', entry_fields=['GT', 'GP', 'dosage'], sample_file='samples_test.sample').write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/issues/8545,2,['error'],['error']
Availability,"Hi,. I tried to compile Hail version 852f92aac4532abc2fc743e0629840a7f6d86496; but it failed with:. >$ ./gradlew -Dspark.version=2.2.0 shadowJar archiveZip; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++11 -Ilibsimdpp-2.1 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -c NativeBoot.cpp; NativeBoot.cpp:1:0: error: bad value (sandybridge) for -march= switch; #include <jni.h>; ^; make: *** [build/NativeBoot.o] Error 1; :nativeLib FAILED. I'm compiling on Amazon's EMR, emr-5.10.0, where I install miniconda to get python 3.6 installed (as a bootstrap action), and run manually:. > sudo yum update -y; sudo yum install g++ cmake git -y; sudo mkdir -p /etc/alternatives/jre; sudo ln -s /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-7.b10.37.amzn1.x86_64/include /etc/alternatives/jre/include; git clone https://github.com/broadinstitute/hail.git; cd hail/; git checkout 852f92aac4532abc2fc743e0629840a7f6d86496; ./gradlew -Dspark.version=2.2.0 shadowJar archiveZip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4305:500,error,error,500,https://hail.is,https://github.com/hail-is/hail/issues/4305,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hi,. Thanks so much for setting up a tutorial for Batch, and for all your work on Hail as a service - it's a really great project and I'm excited to try it out!. I'm following the tutorial and was wondering if this line is there by mistake?. https://github.com/hail-is/hail/blob/8140f17d926235470b1ed1cdefd591c3b41838a5/hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py#L73. It throws `AttributeError: 'InputResourceFile' object has no attribute 'add_extension'`, and the method is not defined for `InputResourceFile` indeed. However it is defined for `JobResourceFile`, which, if I understand, Batch uses to find the job output? If so, I guess, for the input resource with an explicitly defined name, calling `add_extension` is redundant? . I'm on Hail `0.2.59-63cf625e29e5`. Vlad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9645:743,redundant,redundant,743,https://hail.is,https://github.com/hail-is/hail/issues/9645,1,['redundant'],['redundant']
Availability,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/715:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/issues/715,3,"['error', 'failure']","['error', 'failure']"
Availability,"Hi:; I come form DCH in China. Now I try to use the following commands to import table in tutorial;; ```; from hail import *; hs= HailContext(); table=hc.import_table('1000Genomes.sample_annotations',impute=True).key_by('Sample'); ```. However, an error appeared like this :; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'import_table' is not defined. Should I import other modules to use import_table command? Thanks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818:248,error,error,248,https://hail.is,https://github.com/hail-is/hail/issues/1818,1,['error'],['error']
Availability,Hist error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/944:5,error,error,5,https://hail.is,https://github.com/hail-is/hail/pull/944,1,['error'],['error']
Availability,Horrible diff but it's mainly just a de-indentation of the `insert_jobs_into_db` function and lifting the try/except block to the callsite of the transaction. By handling that error externally to the transaction we don't stifle errors that would be caught by the `@transaction` retry logic.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14370:176,error,error,176,https://hail.is,https://github.com/hail-is/hail/pull/14370,2,['error'],"['error', 'errors']"
Availability,I accidentally copy pasted the line. If you look a few lines down you can see the actual name/value pair for that environment variable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13628:61,down,down,61,https://hail.is,https://github.com/hail-is/hail/pull/13628,1,['down'],['down']
Availability,"I accidentally passed a list instead of a string as the hb.Batch name and got this error; ```; Traceback (most recent call last):; File ""outrider_batch_pipeline.py"", line 216, in <module>; main(); File ""outrider_batch_pipeline.py"", line 212, in main; logger.info(f""Output: {output_file}""); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py"", line 119, in __exit__; next(self.gen); File ""/Users/weisburd/code/methods/batch/batch_utils.py"", line 66, in run_batch; batch.run(dry_run=args.dry_run, verbose=args.verbose); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/batch.py"", line 423, in run; return self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 435, in _run; bc_batch = bc_batch.submit(disable_progress_bar=disable_progress_bar); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 167, in submit; async_batch = async_to_blocking(self._async_builder.submit(*args, **kwargs)); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/lib/python3.7/site-packages/nest_asyncio.py"", line 63, in run_until_complete; return self._run_until_complete_orig(future); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/aioclient.py"", line 492, in submit; batch = aw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9050:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/issues/9050,1,['error'],['error']
Availability,"I added a cheatsheets page to the docs. It has a link to the single current cheat sheet. I just used the fact that it's already on github to create a download link, if that's bad practice I'm happy to do something else.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7570:150,down,download,150,https://hail.is,https://github.com/hail-is/hail/pull/7570,1,['down'],['download']
Availability,I also added SocketException and SSLException as transient errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9404:59,error,errors,59,https://hail.is,https://github.com/hail-is/hail/pull/9404,1,['error'],['errors']
Availability,"I also added `batch/ignored-pylint-errors` which we diff the output of pylint against. If pylint differs from the `ignored-pylint-errors`, then either we fixed an ignored error or we added a new unignored error. In either case, the developer should address the issue by changing the code or the `ignored-pylint-errors` file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4789:35,error,errors,35,https://hail.is,https://github.com/hail-is/hail/pull/4789,5,['error'],"['error', 'errors']"
Availability,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/561:183,error,error,183,https://hail.is,https://github.com/hail-is/hail/issues/561,3,"['down', 'error']","['download', 'error']"
Availability,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1807:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/1807,1,['error'],['error']
Availability,"I am not really sure why $? is not set to 1 if these parentheses are absent. Nonetheless,; this fixes CI to actually complain about mypy failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9335:137,failure,failures,137,https://hail.is,https://github.com/hail-is/hail/pull/9335,1,['failure'],['failures']
Availability,"I am not sure what it should look like, but right now trying to show a table with no row fields causes an error. To replicate:. ```; hl.utils.range_table(10).key_by().select().show(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7545:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/issues/7545,1,['error'],['error']
Availability,"I am not sure what persist should mean in the service backend. For some linear; algebra work, persist appears to be used to store the results of an expensive; query. In that setting, we should clearly checkpoint the dataset. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11936:201,checkpoint,checkpoint,201,https://hail.is,https://github.com/hail-is/hail/pull/11936,1,['checkpoint'],['checkpoint']
Availability,"I am running AWS EMR 6.3.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.1.1; * Scala 2.12.10. On that cluster, I try to build HAIL version 0.2.60-de1845e1c2f6 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.1.1; ```. I got the error: . ```; Task :compileScala; Pruning sources from previous analysis, due to incompatible CompileSetup. /opt/broad-hail/hail/src/main/scala/is/hail/backend/service/ServiceBackend.scala:37: method toString in class IOUtils is deprecated: see corresponding Javadoc for more information.; new GoogleStorageFS(IOUtils.toString(is)); one error found; ```. FYI the same version of hail (0.2.60) on EMR 6.2.0 (same stack but with Spark 3.0.1) works without issue using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831:317,error,error,317,https://hail.is,https://github.com/hail-is/hail/issues/10831,2,['error'],['error']
Availability,"I am testing hail build on spark3 (v0.2.89, spark 3.1.2) and getting the following error with jinja2 (see below).; From the error it seems like this is due to Hail's dependency of bokeh using the latest version of jinja2. Downgrading jinja2 to 3.0.0 solves the problem, and it seems like other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.ja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/issues/11705,6,"['Down', 'error']","['Downgrading', 'error']"
Availability,"I am trying to convert the matrix table output from `vcf_combiner `to vcf format. ; I get the following error on running `export_vcf `: ; ```; Hail version: 0.2.64-1ef70187dc78; Error summary: HailException: Invalid type for format field 'gvcf_info'. Found 'struct{BaseQRankSum: float64, ExcessHet: float64, InbreedingCoeff: float64, MLEAC: array<int32>, MLEAF: array<float64>, MQRankSum: float64, RAW_MQandDP: array<int32>, ReadPosRankSum: float64}'.; ```; Description of the matrix table : ; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'LA': array<int32>; 'LGT': call; 'LAD': array<int32>; 'LPGT': call; 'LPL': array<int32>; 'RGQ': int32; 'END': int32; 'gvcf_info': struct {; BaseQRankSum: float64, ; ExcessHet: float64, ; InbreedingCoeff: float64, ; MLEAC: array<int32>, ; MLEAF: array<float64>, ; MQRankSum: float64, ; RAW_MQandDP: array<int32>, ; ReadPosRankSum: float64; }; 'DP': int32; 'GP': array<float64>; 'GQ': int32; 'MIN_DP': int32; 'PG': array<float64>; 'PID': str; 'PS': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; ```; The gVCF was produced by GATK4.2 dragen-mode.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10644:104,error,error,104,https://hail.is,https://github.com/hail-is/hail/issues/10644,2,"['Error', 'error']","['Error', 'error']"
Availability,"I am using Hail 0.2.54. However, I also tested with the latest build.gradle file. I run the following make install command:; `make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.2`. However, I got this error message which did not appear before. ` > Could not resolve org.scalanlp:breeze-natives_2.11:+.; Required by:; project :; > Failed to list versions for org.scalanlp:breeze-natives_2.11.; > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error; * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.; * Get more help at https://help.gradle.org. BUILD FAILED in 29s; make: *** [build/libs/hail-all-spark.jar] Error 1`. It seems that is caused by https://repo.hortonworks.com/content/repositories/releases/ server is done.; I am wondering whether there is any maven substitute can be used temporarily to compile hail.jar?. Thanks in advance for your help.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9419:226,error,error,226,https://hail.is,https://github.com/hail-is/hail/issues/9419,3,"['Error', 'error']","['Error', 'error']"
Availability,"I argue `fatalIf(p, msg)` is less readable than `if (p) fatal(msg)` and not any shorter. It also causes an error since you can't control which fatal to call, e.g., `Utils.fatal` vs `Line.fatal`. @tpoterba Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/262:107,error,error,107,https://hail.is,https://github.com/hail-is/hail/issues/262,1,['error'],['error']
Availability,I assume the local file reads are somehow more tolerant to being closed? I don't know why this doesn't fail like every single matrix read test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3512:47,toler,tolerant,47,https://hail.is,https://github.com/hail-is/hail/pull/3512,1,['toler'],['tolerant']
Availability,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564:1001,down,down,1001,https://hail.is,https://github.com/hail-is/hail/issues/5564,1,['down'],['down']
Availability,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10379:88,error,error,88,https://hail.is,https://github.com/hail-is/hail/pull/10379,1,['error'],['error']
Availability,"I changed ci.hail.is to point to kubernetes, so this won't work any more. The new web site is ready to go (live at test.hail.is) and I will switch it over hail.is over late tonight. It needs to go down for a short while to get new Let's Encrypt credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4399:197,down,down,197,https://hail.is,https://github.com/hail-is/hail/pull/4399,1,['down'],['down']
Availability,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962:591,checkpoint,checkpoint,591,https://hail.is,https://github.com/hail-is/hail/pull/7962,1,['checkpoint'],['checkpoint']
Availability,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7647:235,error,error,235,https://hail.is,https://github.com/hail-is/hail/issues/7647,4,"['Error', 'error']","['Error', 'ErrorHandling', 'error']"
Availability,"I created this bug when I added exception handling. I incorrectly put the exception handling *after* the `rmdir`. If an exception occurred while removing one of the children of a directory, in all likelihood, the directory is non-empty. With this change, we raise any exceptions before unlinking the directory. It is the responsibility of the caller to decide what to do if we were unable to remove one of our children. I saw this while debugging another transient error on another PR: https://github.com/hail-is/hail/issues/13361.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13362:465,error,error,465,https://hail.is,https://github.com/hail-is/hail/pull/13362,1,['error'],['error']
Availability,"I decided to break off this chunk from another PR that has stalled. That PR will ultimately build on this to add all developers automatically to dev AND test namespaces, but this should be an improvement for now. A few things in here:. - Deleted all the `DatabaseResource` stuff in the auth driver. Since databases now are created and destroyed with the namespace and not the developer, this is basically dead code.; - Added the ability to add a user for an existing hail identity. This is only permitted in dev namespaces and serves as a way for developers to use the same hail identity across namespaces. There is one caveat here: `create_initial_account.py` tries to copy the `<dev-name>-gsa-key` secret from default into the developer namespace and this code will *not* do that anymore. For the developer to submit jobs to the namespace, they must first manually copy in the secret from `default` if it does not already exist inside the namespace. This is awkward, but IMO acceptable because:; - the copying code in `create_initial_account.py` is already broken anyway because when that script is run in a dev deploy it does not have access to production secrets; - I hope that when we eventually go keyless we can delete the gsa key secrets and this whole problem goes away.; - I feel like it's not too bad to do this manual one time copy as opposed to maintaining code that is privileged enough to reach across namespaces. Seems error prone and like a security headache.; - Deletes `create_initial_account.py` in favor of using our actual API to create the dev user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13180:1435,error,error,1435,https://hail.is,https://github.com/hail-is/hail/pull/13180,1,['error'],['error']
Availability,"I did the following ; ```; import is.hail._; val hc = HailContext(sc); ```. When I did so, I got this error message . ```; java.util.NoSuchElementException: spark.hadoop.io.compression.codecs; at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.SparkConf.get(SparkConf.scala:235); at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:94); at is.hail.HailContext$.apply(HailContext.scala:171); ... 50 elided; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1447:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/issues/1447,1,['error'],['error']
Availability,"I do not fully understand the Azure git tagging scheme, but [this commit](https://github.com/Azure/azure-sdk-for-java/commit/054df3fb74098f4ee30eeb1df70df1e40438d169) appears to have made `close` idempotent. It was merged in June of 2022. That commit resolved [an issue](https://github.com/Azure/azure-sdk-for-java/issues/24782) reporting an error very similar to our own. All the azure version changes update the Azure packages to their latest versions as of 2023-05-09 1713 ET. Fixes #12976",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13018:342,error,error,342,https://hail.is,https://github.com/hail-is/hail/pull/13018,1,['error'],['error']
Availability,"I do not fully understand why, but I was getting errors about df.columns not accepting string indexes:. ```; IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices. ```. I think `df.columns` is not intended to let us get the Series for a column from the DataFrame. I think we are supposed to use `df` directly for this purpose, which is what I do here. I also noticed an unfortunate issue where `''` is false-y so it is overlooked by the `or` in favor of the default value. It would be nice to have some kind of lazy `None` coalescing operator in Python :/.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12337:49,error,errors,49,https://hail.is,https://github.com/hail-is/hail/pull/12337,1,['error'],['errors']
Availability,"I do not fully understand why, but pytest sometimes imports Hail in threads; other than the main thread. Asyncio raises an error if you try to start an; event loop in a non-main thread. This PR only runs nest_asyncio if there is already a running event loop. If; there is no running event loop, we do not need to run nest_asyncio anyway! If; there is a running event loop, we can modify it to be nest-able even from; a non-main thread.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10364:123,error,error,123,https://hail.is,https://github.com/hail-is/hail/pull/10364,1,['error'],['error']
Availability,"I do not know why but /etc triggers errors about:; ```; archive/tar: write too long; ```; Even though /etc is not very large (1.4MB). I suspect there is some symlink; or other nonsense which is breaking Kaniko. The solution, after much trial and error, was simple: copy over directories that do not; cause issues and copy only the necessary files out of etc. A mix of speculation and; binary search lead me to the conclusion that /etc/ld.so.* are the only files necessary; from /etc for python to run correctly. These files tell the kernel how to link python3.7; to the various libraries on which it depends (which live in lib and lib64). Anyway, I've tested that this image can build itself, so it should be good enough for; our purposes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10399:36,error,errors,36,https://hail.is,https://github.com/hail-is/hail/pull/10399,2,['error'],"['error', 'errors']"
Availability,"I do not know why the retries setting in pip.conf did not catch https://ci.hail.is/batches/167314/jobs/27, but more retries never hurt anyone. Another CI-related PR. This one changes the base image of everything else: hail-ubuntu. It's an ubuntu image with two scripts that make pip and apt more resilient. Take a look at docker/hail-ubuntu.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906:296,resilien,resilient,296,https://hail.is,https://github.com/hail-is/hail/pull/9906,1,['resilien'],['resilient']
Availability,"I do not think we frequently get errors in `storage.reader`, but I think `storage.writer` was always flaky and we were protected by the `retryTransientErrors` on `createNoCompression`. My change to fix requester pays delayed the error until either the first `write` or the `close` which do not have a `retryTransientErrors` (and it is not obvious to me that it is safe to retry a `flush`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12868:33,error,errors,33,https://hail.is,https://github.com/hail-is/hail/pull/12868,2,['error'],"['error', 'errors']"
Availability,I don't think it is used anymore. Builds are failing because it is returning 500. > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9418:548,Error,Error,548,https://hail.is,https://github.com/hail-is/hail/pull/9418,1,['Error'],['Error']
Availability,"I encountered an error in an external package when I used a Hail-generated pandas data frame, which is due to an unsupported dtype [`pandas.StringDtype`](https://pandas.pydata.org/docs/reference/api/pandas.StringDtype.html#pandas.StringDtype).; https://github.com/biocore-ntnu/pyranges/pull/264. Given it's still experimental in pandas, can we have an option to generate a data frame that have `dtype=object` string columns? or maybe, we should make `dtype=object` default.; https://github.com/hail-is/hail/blob/c4b09953f62cea090c8ab2026bc81851b9f4d64a/hail/python/hail/table.py#L3345-L3346",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11738:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/11738,1,['error'],['error']
Availability,"I figure we should keep this up to date with our supported python images. AFAIK, we currently only use the mirror of python:3.7. We don't expose any of them as publicly available images, but perhaps we should due to dockerhub rate limits? I suppose that's a question for another day. We probably want to base our python-dill images on these so that docker hub can't just force push a new version of a tag and break our builds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12255:169,avail,available,169,https://hail.is,https://github.com/hail-is/hail/pull/12255,1,['avail'],['available']
Availability,I followed the rabbit hole form https://github.com/hail-is/hail/pull/7922 and was a bit concerned that we weren't verifying the stream met our expectations. I don't think we need to handle errors more gracefully (these assertions should only fail on malformed files).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7932:189,error,errors,189,https://hail.is,https://github.com/hail-is/hail/pull/7932,1,['error'],['errors']
Availability,"I found that aggregating sorted arrays gives incorrect results and crashes when trying to do so without converting the array to string. This code explains what I found pretty well:. ```python; > mt = hl.balding_nichols_model(1, 10, 10). # Aggregate concatenated alleles (works fine); > mt.aggregate_rows(hl.agg.counter(hl.delimit(mt.alleles, '|'))); {'A|C': 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076:412,error,error,412,https://hail.is,https://github.com/hail-is/hail/issues/8076,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:168,error,error,168,https://hail.is,https://github.com/hail-is/hail/issues/8029,2,"['Error', 'error']","['Error', 'error']"
Availability,"I get this error when using the last hail version built on Spark 1.6. ``` hail: annotatevariants vds: caught exception: java.lang.IllegalArgumentException: requirement failed: nPartitions = 2847, ranges = 2844; 	at scala.Predef$.require(Predef.scala:233); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner.<init>(OrderedPartitioner.scala:27); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner$.read(OrderedPartitioner.scala:110); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:185); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.utils.richUtils.RichHadoopConfiguration$.readObjectFile$extension(RichHadoopConfiguration.scala:205); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:9); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:61); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:9)```. however, the same commands work on the hail version ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1213:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1213,1,['error'],['error']
Availability,"I got an email saying the activity logs are no longer supported after September 30th. Here's the [migration instructions](https://cloud.google.com/compute/docs/logging/migrating-from-activity-logs-to-audit-logs#log_entry_field_mappings). I figured out how to map the fields mostly by trial and error looking at the JSON for an event. The only thing that didn't map at all was the operationType. I hardcoded that as 'insert'. There are different event_subtype names such as 'v1.compute.instances.insert' or 'beta.compute.instances.insert'. So I did what they suggested and looked for a partial match such as 'compute.instances.insert'. I can send you the full JSON for the events if you want to double check anything. I also double checked that the activity logs aren't used anywhere else in the repo, but it might be good for you to confirm that since you wrote a lot of this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9439:294,error,error,294,https://hail.is,https://github.com/hail-is/hail/pull/9439,1,['error'],['error']
Availability,"I got sick of having my PRs fail due to these ""rare"" errors. This PR adds; a type of error which we will retry exactly once. Hopefully this; reduces the frequency of these errors sufficiently that we are no; longer plagued by them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11917:53,error,errors,53,https://hail.is,https://github.com/hail-is/hail/pull/11917,3,['error'],"['error', 'errors']"
Availability,I got the error message while importing VCFs in dataflow01. `hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.test.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.fold,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['error'],['error']
Availability,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5418:359,error,error,359,https://hail.is,https://github.com/hail-is/hail/pull/5418,1,['error'],['error']
Availability,I had to change the return type and override delete for Azure as they just return a status code unless there's an error. The `resp.json()` didn't work for the Azure response type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10936:114,error,error,114,https://hail.is,https://github.com/hail-is/hail/pull/10936,1,['error'],['error']
Availability,"I have a GATK-generated VCF that has been passed through vt normalize. During the vt stage, a ""NaN"" value in the VCF was changed to ""nan"", which causes write to fail:. `2016-11-07 15:13:31 ERROR Hail:101 - hail: fatal: write: file:###.vcf.bgz: variant 5:49429187:G:C,T: INFO field InbreedingCoeff:; unable to convert nan (of class java.lang.String) to Double:; caught java.lang.NumberFormatException: For input string: ""nan""; offending line: 5 49429187 rs59402528 G C,T 2.28455e+07 VQSRTrancheSNP99.90t...`. The original ""NaN"" value is processed fine. As the VCF spec is pretty quiet on exact floating point representation in VCFs, could code to handle ""nan"" be added?. Apologies for edit spam -- clumsy fingers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1065:189,ERROR,ERROR,189,https://hail.is,https://github.com/hail-is/hail/issues/1065,1,['ERROR'],['ERROR']
Availability,I have no idea how this ever worked. It should have triggered UTF-8 decoding errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10381:77,error,errors,77,https://hail.is,https://github.com/hail-is/hail/pull/10381,1,['error'],['errors']
Availability,I have yet to successfully create a VCF that doesn't hit another error before hitting this one. But user hit this here: https://discuss.hail.is/t/assertionerror-exception/1700,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9494:65,error,error,65,https://hail.is,https://github.com/hail-is/hail/pull/9494,1,['error'],['error']
Availability,"I hope this is the last one. Instead of assert (since the catch doesn't catch assertion errors), I just use an `if`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1383:88,error,errors,88,https://hail.is,https://github.com/hail-is/hail/pull/1383,1,['error'],['errors']
Availability,"I implemented `cancelled` as a join from the batch table rather than storing redundant information for each job. @akotlar The issue was that all jobs were getting set to cancelled at the same time (and thus notifying children), so always run jobs were all getting run at once neglecting the hierarchy of job dependencies. This is the purpose of having the `cancelled` flag as separate from the `Cancelled` state and is checked in `create_if_ready`. @cseed Can you look and see if this solves the cancel problem we discussed earlier? The faulty PR was this one: https://github.com/hail-is/hail/pull/6128/files",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6341:77,redundant,redundant,77,https://hail.is,https://github.com/hail-is/hail/pull/6341,2,"['fault', 'redundant']","['faulty', 'redundant']"
Availability,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1377:131,error,error,131,https://hail.is,https://github.com/hail-is/hail/issues/1377,2,['error'],['error']
Availability,I just added a slightly more informative error message. Do you want something more than this?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9733:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/pull/9733,1,['error'],['error']
Availability,"I know why it happened (was on an earlier version) but this is a general thing I've seen:; ```; Traceback (most recent call last):; File ""saige_pan_ancestry.py"", line 247, in <module>; main(args); File ""saige_pan_ancestry.py"", line 210, in main; n_threads=n_threads); File ""/home/konradk/ukb_common/utils/saige_pipeline.py"", line 238, in load_results_into_hail; load_data_task.always_run().depends_on(*tasks_to_hold); TypeError: 'TaskResourceFile' object is not callable; ```; say you did `Task.always_runn()` - it would give this error bc it creates a file and then can't call it as a function. Not sure what the long term solution is, but just thought I'd raise it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8011:531,error,error,531,https://hail.is,https://github.com/hail-is/hail/issues/8011,1,['error'],['error']
Availability,"I left the changes to Query and Batch in separate commits for ease of review. I put these in the same PR because we don't really have standalone testing for JVM Jobs outside of Query-on-Batch so the FASTA use-case serves as a test here that cloudfuse is working properly for JVM Jobs. Would be great if Jackie you could review the batch commit and Tim could review the query commit. ## Hail Query; - Added support for the `FROM_FASTA_FILE` rpc and the service backend now passes sequence file information from RGs in every rpc; - Refactored the liftover handling in service_backend to not redundantly store liftover maps and just take them from the ReferenceGenome objects like I did for sequence files. This means that add/remove liftover/sequence functions on the Backend are just intended to sync up the backend with python, which is a no-op for the service backend.; - Don't localize the index file on fromFASTAFile/addSequence before creating the index object. `FastaSequenceIndex` just loads the whole file on construction so might as well stream it in from whatever storage it's in.; - FASTA caching is left alone because those files will be mounted and unmounted from the jvm container over the life of the job. JVM doesn't have to worry about disk usage because that's handled by Batch XFS quotas, so long as the service backend requests enough storage to fit the FASTA file. Batch will make sure that a given bucket (and therefore a given FASTA file) is mounted once per-user on a batch worker. ## Hail Batch; - Added support for read-only cloudfuse mounts for JVM jobs; - These mounts are shared between jobs on the same machine from the same user; - I did not change DockerJobs, but they could be very easily adapted to use this new mount-sharing code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:589,redundant,redundantly,589,https://hail.is,https://github.com/hail-is/hail/pull/12736,1,['redundant'],['redundantly']
Availability,"I made any command that is larger than 10KB is written to a file instead and is downloaded as an input file to be run (same as what we do for Python jobs). I also added a new field `user_code` that represents the user's code that they want to run versus the command we have Docker run. I formatted the `user_code` for Python jobs to show the actual function definition and the arguments used to call the function. It's probably not perfect (i.e. JobResourceFile), but better than nothing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10664:80,down,downloaded,80,https://hail.is,https://github.com/hail-is/hail/pull/10664,1,['down'],['downloaded']
Availability,"I need this extra debugging information to understand what is going on in Azure with deleted VMs still showing up in the portal with ResourceNotFound errors. Miah and Greg are running into this same problem in their deployment. My guess is what is happening is the worker is active and working fine, but then the deployment gets ""Canceled"" because the OMSAgent takes too long to deploy. So our loop then cancels the deployment which messes up the state in Azure of the already deployed and running VM. I popped the parameters from the deployment result in case it contains sensitive data (I'm mainly worried about any private SSH keys).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13231:150,error,errors,150,https://hail.is,https://github.com/hail-is/hail/pull/13231,1,['error'],['errors']
Availability,"I need to get GKE costs down further. The driver is now 3 CPU anyway. @daniel-goldstein, I am not sure how to modify the Azure terraform. It appears this; is controlled by a variable which is already set to a 4 core machine?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11985:24,down,down,24,https://hail.is,https://github.com/hail-is/hail/pull/11985,1,['down'],['down']
Availability,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7523:97,resilien,resilient,97,https://hail.is,https://github.com/hail-is/hail/pull/7523,4,"['failure', 'resilien']","['failure', 'resilient']"
Availability,"I needed to make the read function in MatrixRead into a class so I can print/parse as part of the work to replace AST with IR in the Python interface. Also, push down required type into MT decoder. I also use the compiler to build the function to add the entries to the row values (no RegionValueBuilder).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3819:162,down,down,162,https://hail.is,https://github.com/hail-is/hail/pull/3819,1,['down'],['down']
Availability,"I never tested that PR that got merged (whoops!) and CI tests are insufficient; to catch this case (we should beef those up, asana task added). The issue was that I thought the method to issue an HTTP get request was `get`,; but it was `getitem`. This PR fixes that. This error occured during `update` and; thus prevented all forward progress of CI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402:272,error,error,272,https://hail.is,https://github.com/hail-is/hail/pull/8402,1,['error'],['error']
Availability,"I noticed that jobs in test deployments were deadlocking because we weren't spinning up extra instances (compared to the production version of Batch). Although each job could fit on an open instance, its allocated share is still less than the core request for that job. This PR aims to increase the probability in which we ignore an exceed shares error the more we have these errors such that at a certain point the rate will be 100% and we'll be able to continue scheduling.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9464:347,error,error,347,https://hail.is,https://github.com/hail-is/hail/pull/9464,2,['error'],"['error', 'errors']"
Availability,"I noticed that the gnomAD mitochondria datasets were pointing at the `gs://gnomad-public-requester-pays` bucket. The Google Cloud Public Datasets version should be up to date now. Also updated the documented schema for chrM sites. Some information about those changes is available in the gnomAD change log: https://gnomad.broadinstitute.org/news/2021-08-rename-filter-in-mitochondria-dataset-and-minor-format-changes/. And finally, since these were the only two datasets that reference `gnomad-public-requester-pays`, removed `gnomad-public-requester-pays` from the list of annotation DB buckets used by `hailctl dataproc`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11282:271,avail,available,271,https://hail.is,https://github.com/hail-is/hail/pull/11282,1,['avail'],['available']
Availability,I noticed that this step blew up in the benchmarking PR and thought I'd provide a better error message.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12866:89,error,error,89,https://hail.is,https://github.com/hail-is/hail/pull/12866,1,['error'],['error']
Availability,"I observed a cluster with it set to the default idle time of 30 seconds in Azure and the workers were continuously thrashing leading up to 49 instances being created over the course of a PR. With an idle time of 120 seconds, there was no thrashing and 28 instances were created over the course of the PR (16 standard + job private etc.). The cluster nicely scaled down at the end of the PR. It looked like a couple of times the `standard-np` pool scaled up and then scaled down so I assume the `standard` pool wasn't at full capacity while that was happening. It might be worth configuring the `standard-np` pool to be 4 or 5 standing instances with 16 cores and see what happens -- that might help as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13314:364,down,down,364,https://hail.is,https://github.com/hail-is/hail/pull/13314,2,['down'],['down']
Availability,I often observe the JVM getting stuck in a high memory state unable to recover from after an OOM. Best to just kill the JVM.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12805:71,recover,recover,71,https://hail.is,https://github.com/hail-is/hail/pull/12805,1,['recover'],['recover']
Availability,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8229:56,Down,Downside,56,https://hail.is,https://github.com/hail-is/hail/pull/8229,2,['Down'],['Downside']
Availability,I ran into an error on another branch where ArrayFor doesn't work if the object being aggregated is a Set. I'm assuming here the ToArray is free for all container objects.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3796:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/3796,1,['error'],['error']
Availability,"I ran into issues in the development of the query service wherein, if the; aiohttp `ClientSession` is not created in the same event pool as the one; from which the request is made, then unusual errors occur. This change makes it harder to accidentally create a BatchClient in the; wrong event loop because the factory method, `BatchClient.create` is; itself an async function. It is still possible to create the BatchClient; in one event loop and use it in another (do not do this!!), but that seems; to be an unlikely mistake. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10923:194,error,errors,194,https://hail.is,https://github.com/hail-is/hail/pull/10923,1,['error'],['errors']
Availability,"I really borked our SQL linting. This PR is short but it catches a few critical problems. 1. The point of `check-sql.sh` is to detect modifications or deletions of SQL files in PRs and fail if such a change occurs. Currently on `main` it does not detect modifications. In #13456, I removed the `delete-<service>-tables.sql` files (intentionally), so added the `^D` to the `grep` regex to indicate that it is OK to have a deletion. What I inadvertently did though is change the rule from ""It's ok to have Additions of any file OR Modifications of estimated-current.sql / delete-<service>-tables.sql"" to ""It's ok to have Additions OR Modifications OR Deletions of estimated-current.sql / delete-<service>-tables.sql"". Really this should have been ""It's ok to have Additions OR Modifications of estimated-current.sql OR Deletions of delete-<service>-tables.sql"". I've changed it to reflect that rule. 2. Rules currently silently *pass* in CI with an error message that git is not installed. In #13437 I changed the image used to run the linters and inadvertently didn't include `git` which `check-sql.sh` needs to run. Here's how it failed in a sneaky way:; - Since `git` is not installed, all calls to `git` fail, but the script is not run with `set -e` so every line of the script is executed; - Since `git` lines fail, `modified_sql_file_list` remains empty; - Since `modified_sql_file_list` remains empty, it appears to the check at the end that everything checked out; - The if statement runs successfully and the script returns with error code 0. To fix this I do a few things:; - installed `git` in the linting image; - `set -e` by default and only enable `set +e` later on when necessary (because we don't want a failed `git diff` to immediately exit); - Do away with the file checking and instead check the error code of the grep. If nothing survives the grep filter, which means there were no illegal changes made, grep will return with exit code 1. So we treat that exit code as a success.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13745:947,error,error,947,https://hail.is,https://github.com/hail-is/hail/pull/13745,3,['error'],['error']
Availability,"I run into this ~1/20,000 QoB just when writing result files. I'm going to make a separate issue with the ABS client repo but I suspect this might be difficult to track down and fix. Example stack trace:. ```; RuntimeException: java.lang.IllegalStateException: Faulted stream due to underlying sink write failure	Gjava.lang.RuntimeException: java.lang.IllegalStateException: Faulted stream due to underlying sink write failure; 	at is.hail.shadedazure.com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:92); 	at is.hail.shadedazure.com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:102); 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:308); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:241); 	at is.hail.utils.package$.using(package.scala:669); 	at is.hail.io.index.IndexWriterUtils.writeMetadata(IndexWriter.scala:253); 	at __C511collect_distributed_array_table_native_writer.apply_region3_328(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:198); 	at is.hail.services.package$.retryTransientErrors(package.scala:187); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:197); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.service.Worker$.main(Worker.scala:195); 	at is.hail.backend.service.Main$.main(Main.scala:9); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAcces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14430:169,down,down,169,https://hail.is,https://github.com/hail-is/hail/pull/14430,5,"['Fault', 'down', 'failure']","['Faulted', 'down', 'failure']"
Availability,"I saw an error in a CI job that was copying in some files. To be clear,; this change would not restart a copy of the entire directory. It just; retries the directory listing. We currently use O(N_FILES) memory to; store the list of files in a directory *before* we start copying. ```; Traceback (most recent call last):; File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 110, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1501, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 104, in main; files=files; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 76, in copy_from_dict; transfers=transfers; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 50, in copy; bytes_listener=make_tqdm_listener(byte_pbar)); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/fs/copier.py"", line 439, in copy; await copier._copy(sema, copy_report, transfer, return_exceptions); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/fs/copier.py"", line 532, in _copy; raise e; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/fs/copier.py"", line 527, in _copy; ], return_exceptions=return_exceptions, cancel_on_error=True); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 519, in bounded_gather2; return await bounded_gather2_raise_exceptions(sema, *pfs, cancel_on_error=cancel_on_error); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 504, in bounded_gather2_raise_exceptions; return await asyncio.gather(*tasks); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", lin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11511:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/11511,1,['error'],['error']
Availability,I see the docs for the PyHail API but is there a getting started guide available yet? Also are there any plans to make a PyHail package available for installation through PyPI?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218:71,avail,available,71,https://hail.is,https://github.com/hail-is/hail/issues/1218,2,['avail'],['available']
Availability,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5817:75,FAILURE,FAILURES,75,https://hail.is,https://github.com/hail-is/hail/issues/5817,2,"['FAILURE', 'echo']","['FAILURES', 'echo']"
Availability,"I set a `.curlrc` file to ensure we use the right options by default. In a few places that are; intended to be run on a developer's laptop, I had to explicitly specify the necessary; options. I replaced all the short flags with long flags, the important conversions to know:; - `-L` `--location`, follow redirects; - `-O` `--remote-name`, use the basename of the URL as the name of a local file in which to save the data; - `-s` `--silent`, do not print anything to stdout/stderr; - `-S` `--show-error`, when used with `-s`, shows errors on stderr; - `-f` `--fail`, return non-zero exit code if the response is 4xx or 5xx (normally the response would be printed / saved and curl will exit successfully)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11183:496,error,error,496,https://hail.is,https://github.com/hail-is/hail/pull/11183,2,['error'],"['error', 'errors']"
Availability,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8878:400,down,down,400,https://hail.is,https://github.com/hail-is/hail/pull/8878,3,"['down', 'failure']","['down', 'failures']"
Availability,I tested this by adding the check_aggregated_resources loop which didn't throw any errors. I tested the cost by looking at the UI and submitting one job that cost $0.0004 and then two of that same job in a batch and it cost $0.0008. The database looked like this:. mysql> select * FROM aggregated_batch_resources where batch_id > 46;; +----------+--------------------------+-------------+-------+; | batch_id | resource | usage | token |; +----------+--------------------------+-------------+-------+; | 47 | compute/n1-preemptible/1 | 25867000 | 11 |; | 47 | compute/n1-preemptible/1 | 0 | 115 |; | 47 | compute/n1-preemptible/1 | 0 | 132 |; | 47 | disk/local-ssd/1 | 9932928000 | 11 |; | 47 | disk/local-ssd/1 | 0 | 54 |; | 47 | disk/local-ssd/1 | 0 | 132 |; | 47 | disk/pd-ssd/1 | 529756160 | 11 |; | 47 | disk/pd-ssd/1 | 0 | 132 |; | 47 | disk/pd-ssd/1 | 0 | 186 |; | 47 | ip-fee/1024/1 | 26487808 | 11 |; | 47 | ip-fee/1024/1 | 0 | 132 |; | 47 | ip-fee/1024/1 | 0 | 188 |; | 47 | memory/n1-preemptible/1 | 99329280 | 11 |; | 47 | memory/n1-preemptible/1 | 0 | 26 |; | 47 | memory/n1-preemptible/1 | 0 | 132 |; | 47 | service-fee/1 | 25867000 | 11 |; | 47 | service-fee/1 | 0 | 110 |; | 47 | service-fee/1 | 0 | 132 |; | 48 | compute/n1-preemptible/1 | 0 | 31 |; | 48 | compute/n1-preemptible/1 | 0 | 76 |; | 48 | compute/n1-preemptible/1 | 27659000 | 94 |; | 48 | compute/n1-preemptible/1 | 26520000 | 122 |; | 48 | compute/n1-preemptible/1 | 0 | 156 |; | 48 | compute/n1-preemptible/1 | 0 | 168 |; | 48 | disk/local-ssd/1 | 10621056000 | 94 |; | 48 | disk/local-ssd/1 | 10183680000 | 122 |; | 48 | disk/local-ssd/1 | 0 | 125 |; | 48 | disk/local-ssd/1 | 0 | 154 |; | 48 | disk/local-ssd/1 | 0 | 156 |; | 48 | disk/local-ssd/1 | 0 | 168 |; | 48 | disk/pd-ssd/1 | 0 | 69 |; | 48 | disk/pd-ssd/1 | 566456320 | 94 |; | 48 | disk/pd-ssd/1 | 0 | 102 |; | 48 | disk/pd-ssd/1 | 543129600 | 122 |; | 48 | disk/pd-ssd/1 | 0 | 156 |; | 48 | disk/pd-ssd/1 | 0 | 168 |; | 48 | ip-fee/1024/1 | 0 | 57 |; | 48 ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9346:83,error,errors,83,https://hail.is,https://github.com/hail-is/hail/pull/9346,1,['error'],['errors']
Availability,"I think old repos still work, which is why CI is mostly fine, but I was seeing errors when testing locally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5798:79,error,errors,79,https://hail.is,https://github.com/hail-is/hail/pull/5798,1,['error'],['errors']
Availability,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6011:153,down,down,153,https://hail.is,https://github.com/hail-is/hail/pull/6011,3,"['down', 'downtime', 'failure']","['down', 'downtime', 'failure']"
Availability,I think the syntax change is needed because numpy versions older than 1.12 don't let you filter a numpy array on a boolean mask in this way.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2940:123,mask,mask,123,https://hail.is,https://github.com/hail-is/hail/pull/2940,1,['mask'],['mask']
Availability,I think there is some unresolved issue with asyncgen shutdown that is keeping; workers alive. This is not an issue in worker because worker calls sys.exit; which forcibly stops execution. cc: @daniel-goldstein @jigold.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10136:87,alive,alive,87,https://hail.is,https://github.com/hail-is/hail/pull/10136,1,['alive'],['alive']
Availability,"I think this can go in instead of #8730. I ran dev deploy with master and then didn't delete the database and ran the tests with the new version. The billing UI page reported the correct values. I also ran the new version with the check functions in the background and got no errors. I can probably double check the UI batches cost are correct, but let's wait until we're happy with the code before I do anymore testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8759:276,error,errors,276,https://hail.is,https://github.com/hail-is/hail/pull/8759,1,['error'],['errors']
Availability,"I think this is probably a bug, since we write out the reference genome and seem to support it just fine in Scala. I want to be able to do e.g.:. ```; >>> import hail as hl; >>> rg = hl.ReferenceGenome(""foo"", ['a', 'b'], {'a': 4, 'b': 6}); >>> t = hl.utils.range_table(10); >>> t = t.annotate(locus=hl.locus_from_global_position(t.idx, reference_genome='foo')); >>> t.write('test.t'); ```. and then, in a separate instance of hail, do:. ```; >>> import hail as hl; >>> t = hl.read_table('test.t'); ```. Currently, I get the following error:; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/Users/wang/code/hail/hail/python/hail/expr/type_parsing.py"", line 80, in visit_locus; return hl.tlocus(gr); File ""<decorator-gen-56>"", line 2, in __init__; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; KeyError: 'foo'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in read_table; File ""/Users/wang/code/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6907:534,error,error,534,https://hail.is,https://github.com/hail-is/hail/issues/6907,1,['error'],['error']
Availability,I think this is the last one. The tests passed 30 times without a failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10044:66,failure,failure,66,https://hail.is,https://github.com/hail-is/hail/pull/10044,1,['failure'],['failure']
Availability,"I think this may help users discover `group_by` and also help users; who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6856:722,failure,failure,722,https://hail.is,https://github.com/hail-is/hail/pull/6856,1,['failure'],['failure']
Availability,"I think we need to figure out how to get cblas on the broad cluster. ```; # use UGER; # ish -l os=RedHat7; # use Anaconda3; # use Java-1.8; # use OpenBLAS; # source activate hail; # ipython; In [1]: import hail as hl . In [2]: mt = hl.balding_nichols_model(3, 100, 100) ; Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:693,avail,available,693,https://hail.is,https://github.com/hail-is/hail/issues/5559,1,['avail'],['available']
Availability,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862:10,down,down,10,https://hail.is,https://github.com/hail-is/hail/issues/3862,1,['down'],['down']
Availability,"I tried out the tiebreaking_expr with some dummyish data:. ```from hail import *; from hail.representation import *; import subprocess; import os; hc = HailContext(). vds = hc.read('gs://daly_atgu_finnish_swedish_exomes_v8/daly_atgu_finnish_swedish_exomes_99prosAndPSC.vds'). vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). intervals = KeyTable.import_interval_list('gs://ibd-exomes/5k.txt'); vds = vds.filter_variants_table(intervals, keep=True). vds = vds.variant_qc(); vds = vds.filter_variants_expr('va.qc.AF>0.01 && va.qc.callRate>0.99', keep=True). table = hc.import_table('gs://daly_atgu_finnish_swedish_exomes_v8/finnibdPSC_isCase.txt', impute=True).key_by('s'); vds = vds.annotate_samples_table(table, root='sa.pheno'). vds = vds.repartition(100); vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno.iscase) 1 else 0""). vds.export_samples('gs://daly_atgu_finnish_swedish_exomes_v8/99pros_psc_relBelowPIHAT35.tsv','s=s'). ```. But this results in an error:. ```hail: info: Reading table to impute column types; hail: info: Finished type imputation; Loading column `s' as type String (imputed); Loading column `case' as type Int (imputed); Struct{; pheno: Int; }; [Stage 6:====================================================>(3347 + 1) / 3348]Verify Output for is/hail/codegen/generated/C3:; Traceback (most recent call last):; File ""/tmp/0a89b0df-1299-4db1-9e90-0efc77501684/99pros_psc_relatedness_short.py"", line 26, in <module>; vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno) 1 else 0""); File ""<decorator-gen-322>"", line 2, in ibd_prune; File ""/home/ec2-user/BuildAgent/work/179f3a9ad532f105/python/hail/java.py"", line 112, in handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2178:983,error,error,983,https://hail.is,https://github.com/hail-is/hail/issues/2178,1,['error'],['error']
Availability,"I tried to achieve this within the CSS flex system, but I had trouble making the buttons equal-sized. The CSS grid system seemed more suited to our needs. The `grid-gap` property tells the browser to leave at least that much space between elements (the current code achieved that with this funky `a + a` rule). The key new rules are:; ```css; grid-template-columns: repeat(auto-fit, minmax(130px, 1fr));; width: 100%;; ```; The first rule tells the browser to put as many columns in the grid as possible, except that:; 1. no column may be less than 130px (approximately the length of the phrase ""Hail Query""); 2. no column may occupy more than one ""fraction"" of the horizontal space (which is to say: each column gets an equal portion of the horizontal space). The second rule tells the browser how much space is available for the grid. Without that rule, the browser tries to make the grid as small as possible, i.e. one column wide. <img width=""1020"" alt=""Screen Shot 2021-12-13 at 4 09 29 PM"" src=""https://user-images.githubusercontent.com/106194/145889522-af0c2367-0f37-43e2-8451-720a25981460.png"">; <img width=""331"" alt=""Screen Shot 2021-12-13 at 4 09 17 PM"" src=""https://user-images.githubusercontent.com/106194/145889525-b6a12ecc-0e7c-4af1-9b0f-5a6fdda322ab.png"">; <img width=""379"" alt=""Screen Shot 2021-12-13 at 4 09 11 PM"" src=""https://user-images.githubusercontent.com/106194/145889526-05773377-1ada-4a5d-81de-749ddac412e1.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11152:813,avail,available,813,https://hail.is,https://github.com/hail-is/hail/pull/11152,1,['avail'],['available']
Availability,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:77,error,errors,77,https://hail.is,https://github.com/hail-is/hail/issues/4733,3,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009:143,error,error,143,https://hail.is,https://github.com/hail-is/hail/issues/8009,2,['error'],['error']
Availability,"I use a Mac and try to install hail.; I use Mojave; I installed pyenv to modify my python versions.; I installed Python 3.7.9 since you recommend to use Python 3.7 as the latest version.; I then did a pip install hail, and it fails with pyspark:. Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1.tar.gz (215.7 MB); ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-pip-egg-info-vlaj8k6d; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:325,ERROR,ERROR,325,https://hail.is,https://github.com/hail-is/hail/issues/9742,2,"['ERROR', 'error']","['ERROR', 'errored']"
Availability,I use these functions to monitor the k8s cluster. These are useful in the interim while we; move towards more robust monitoring solutions. To make these accessible modify your ~/.bashrc; or ~/.zshrc to have this line:. source /path/to/hail-repository/devbin/functions.sh. cc: services-team: @jigold @CDiaz96 @catoverdrive @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10049:110,robust,robust,110,https://hail.is,https://github.com/hail-is/hail/pull/10049,1,['robust'],['robust']
Availability,I verified the test failed with the same [error as KC's](https://discuss.hail.is/t/repartition-on-read/2148/2) before my change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10697:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/10697,1,['error'],['error']
Availability,"I want the function to exit in error if any step fails. I achieve this by; starting a sub-shell and setting -e inside that sub shell. That causes the; sub-shell to exit if any individual command fails. Previously, the `git clone` could fail but `clone` would still return exit code; zero.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8190:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/pull/8190,1,['error'],['error']
Availability,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209:200,down,down,200,https://hail.is,https://github.com/hail-is/hail/pull/2209,1,['down'],['down']
Availability,"I want to have a functionality when import VCF, disable the filter based on `sum(AD) != DP`. Although this is a good sanity check, but since we are not clear if this error will lead to inaccurate genotype calls, there will be an option for analyst to keep those calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/427:166,error,error,166,https://hail.is,https://github.com/hail-is/hail/issues/427,1,['error'],['error']
Availability,"I was confused at how these could work because `self.conn.commit` and `self.conn.rollback` are async functions, but then I couldn't find any invocations of these methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11367:81,rollback,rollback,81,https://hail.is,https://github.com/hail-is/hail/pull/11367,1,['rollback'],['rollback']
Availability,"I was trying to filtervariants with an interval_list, and I wrote:. filtervariants list --keep -i /user/satterst/exome_evaluation_regions.v1.interval_list count; (when clearly I should have done filtervariants intervals... instead of filtervariants list...). and it kept getting most of the way done and then failing, with the error message:; hail: count: fatal: invalid variant. Invalid variant? I double-checked that my dataset was OK by running count on the whole thing, telling me that the filtering was the problem, so I created different versions of the interval_list file, trying to figure out if something was specified incorrectly in the file... I was literally troubleshooting the interval_list file for over half an hour before I realized what the problem was. And it's not the first time I've made this mistake. . I humbly submit that a better error message here would be helpful, something like:; filtervariants: fatal: list expected but intervals encountered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/383:327,error,error,327,https://hail.is,https://github.com/hail-is/hail/issues/383,2,['error'],['error']
Availability,"I will submit a larger batch PR soon, not sure this is worth addressing until then, because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to addres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5118:955,error,error,955,https://hail.is,https://github.com/hail-is/hail/issues/5118,1,['error'],['error']
Availability,"I wonder if related to #4754?; ```; ht = hl.experimental.import_gtf('gs://konradk/gencode.v19.annotation.gtf.bgz', 'GRCh37', True, min_partitions=12); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0],; transcript_id=ht.transcript_id.split('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4799:743,error,error,743,https://hail.is,https://github.com/hail-is/hail/issues/4799,2,['error'],['error']
Availability,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:184,error,error,184,https://hail.is,https://github.com/hail-is/hail/issues/2070,3,"['Error', 'error']","['Error', 'error']"
Availability,"I'd think this: `ht.transmute(ref=ht.alleles[0], alt=ht.alleles[1])` should error out since it's modifying a key field, but it doesn't seem to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3673:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/3673,1,['error'],['error']
Availability,"I'll admit I didn't test this since I'm currently debugging the deadlock PR, but this should quiet down the error logs that are being emitted whenever a test runs that intentionally throws an exception in Query.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11376:99,down,down,99,https://hail.is,https://github.com/hail-is/hail/pull/11376,2,"['down', 'error']","['down', 'error']"
Availability,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520:176,down,downloaded,176,https://hail.is,https://github.com/hail-is/hail/issues/1520,2,"['down', 'error']","['downloaded', 'error']"
Availability,I'm down to about 350 issues. The next one will probably be the last one.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8804:4,down,down,4,https://hail.is,https://github.com/hail-is/hail/pull/8804,1,['down'],['down']
Availability,"I'm not 100% sure if this is true, but I've seen someone say on Stack Overflow that Macs only get BLAS and LAPACK natives after XCode is installed. We say on our Getting Started that these should automatically work on OSX, but we've also always required a C compiler as a getting started step. Now that we are going to distribute prebuilt packages, it may be the case that users won't download XCode because they don't need to and as such won't get the natives. We should verify whether or not natives are present prior to XCode installation and update docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1913:385,down,download,385,https://hail.is,https://github.com/hail-is/hail/issues/1913,1,['down'],['download']
Availability,"I'm not 100% sure this will fix Ben's out of memory error when copying input files. It's possible there's a memory leak somewhere else. My guess on why this change could solve the issue is that because we open the files in ""r+b"" mode when writing to a local file in parallel, the kernel was caching the data as much as possible based on the available amount of memory to the container because it was anticipating sequential reads. I'd think the kernel is smart enough to evict data when needed, but maybe when using an attached disk with a network drive, the contention is high if the disk starts to lag and it couldn't write all of the data in the write buffer quickly enough and evict unneeded data to meet demand??? At the very least, hopefully this change will allow us to see if there is a true memory leak beyond the kernel using all of the available memory for caching potential reads. <img width=""652"" alt=""Screen Shot 2023-01-26 at 3 52 38 PM"" src=""https://user-images.githubusercontent.com/1693348/214948120-e6bd9671-8cda-48e9-994c-6bab93ddb11a.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12625:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/pull/12625,3,"['avail', 'error']","['available', 'error']"
Availability,"I'm not sure if this is the right change, but I'm pretty sure the Azure deployment was stuck because `_heal` kept aborting early on a GitHub post error. See #13050 for context.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13115:146,error,error,146,https://hail.is,https://github.com/hail-is/hail/pull/13115,1,['error'],['error']
Availability,I'm not sure if this is the right fix here or if we can get rid of the deprecated FS but this is the lint error that's breaking main.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11806:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/pull/11806,1,['error'],['error']
Availability,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8038:58,error,errors,58,https://hail.is,https://github.com/hail-is/hail/pull/8038,1,['error'],['errors']
Availability,"I'm not sure this is the only reason why we're getting worker log errors when a user deletes jobs, but this code is definitely wrong in the case a container hasn't been started. I'm conflicted on whether we should do nothing or write empty files though.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13727:66,error,errors,66,https://hail.is,https://github.com/hail-is/hail/pull/13727,1,['error'],['errors']
Availability,I'm not sure why my exception didn't get picked up by the aiohttp.ClientOSError block. I added a plain OSError just in case. Feel free to push back on that. It's possible I didn't have updated is_transient_error code when I got the original error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7340:241,error,error,241,https://hail.is,https://github.com/hail-is/hail/pull/7340,1,['error'],['error']
Availability,"I'm repeatedly getting this error on when attempting to run vep GRCh38 on a dataproc cluster started using hailctl with 25 preemptible nodes, and otherwise default params.; ```; 2019-07-14 20:54:55 TaskSetManager: INFO: Finished task 1611.1 in stage 8.0 (TID 21696) in 49183 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1601/10000); 2019-07-14 20:54:57 TaskSetManager: INFO: Starting task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1, partition 1559, PROCESS_LOCAL, 8800 bytes); 2019-07-14 20:54:57 TaskSetManager: INFO: Finished task 1570.1 in stage 8.0 (TID 21697) in 45412 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1602/10000); 2019-07-14 20:55:04 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 1.; 2019-07-14 20:55:04 DAGScheduler: INFO: Executor lost: 1 (epoch 0); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-07-14 20:55:04 TransportClient: ERROR: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['error'],['error']
Availability,"I'm running the following script: . /psych/genetics_data/working/cseed/bin/hail read -i ${input_vds} \; annotatevariants tsv file:///medpop/esp2/mzekavat/Estonia/UPDATED_TOOLS/dbNSFPv3.2/dbNSFP3.2a.ALLChr.bgz \; -r va.dbNSFP \; -t 'SIFT_pred: String, PROVEAN_pred: String, Polyphen2_HDIV_pred: String, Polyphen2_HVAR_pred: String, LRT_pred: String, MutationTaster_pred: String, MutationAssessor_pred: String, FATHMM_pred: String, MetaSVM_pred: String, MetaLR_pred: String, CADD_phred: Double, `Eigen-raw`: Double, `Eigen-phred`: Double, `Eigen-raw_rankscore`: Double' \; -v ""#chr,pos(1-based),ref,alt"" \; -m ""."" \; annotatevariants expr -c 'va.of8 = (if (""D"" ~ va.dbNSFP.SIFT_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.PROVEAN_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.LRT_pred) 1 else 0) + (if (""H"" ~ va.dbNSFP.MutationAssessor_pred || ""M"" ~ va.dbNSFP.MutationAssessor_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.MutationTaster_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.FATHMM_pred) 1 else 0)' \; exportvariants -c 'v.contig,v.start,v.ref,v.alt,va.of8,va.dbNSFP.MetaSVM_pred,va.dbNSFP.MetaLR_pred,va.dbNSFP.CADD_phred,va.dbNSFP.`Eigen-raw`,va.dbNSFP.`Eigen-phred`,va.dbNSFP.`Eigen-raw_rankscore`' -o /user/mzekavat/MiGen/dbNSFP.MiGen.tsv. and I'm getting an error here: /medpop/esp2/mzekavat/MiGen/Annotation/hail.log; Would greatly appreciate thoughts on this as soon as possible!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/317:1352,error,error,1352,https://hail.is,https://github.com/hail-is/hail/issues/317,1,['error'],['error']
Availability,I'm seeing a PR failure that I can't debug further without the status information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9942:16,failure,failure,16,https://hail.is,https://github.com/hail-is/hail/pull/9942,1,['failure'],['failure']
Availability,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6093:18,failure,failures,18,https://hail.is,https://github.com/hail-is/hail/pull/6093,1,['failure'],['failures']
Availability,"I'm seeing this in the driver logs:. ```; ERROR 2020-06-16 23:37:18,446 in event loop Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 500, in event_loop; await self.handle_event(event); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 430, in handle_event; timestamp = event.timestamp.timestamp() * 1000; AttributeError: 'dict' object has no attribute 'timestamp'; ```. `event['timestamp']` is in RFC3339 Zulu format with nanosecond precision, for example: 2020-06-08T16:49:53.374657381Z, see: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry. There is no native RFC3339 Python parser. RFC3339 is nearly identicaly to ISO 8601, except maybe some timezone differences which aren't relevant in Zulu format, see: https://en.wikipedia.org/wiki/ISO_8601. There isn't a native Python ISO 8601 parser. dateutil.parser.isoparse is a ISO 8601 parser (and is maybe also supports RFC3339? I can't quite tell.). Note, Python datetime only has microsecond accuracy, but that's fine because we only store millisecond accuracy. Time is the worst.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8975:42,ERROR,ERROR,42,https://hail.is,https://github.com/hail-is/hail/pull/8975,1,['ERROR'],['ERROR']
Availability,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/321:628,error,error,628,https://hail.is,https://github.com/hail-is/hail/issues/321,2,"['Avail', 'error']","['Available', 'error']"
Availability,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10702:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/10702,2,['error'],['error']
Availability,"I've been working on an R interface to Hail through the sparklyr package, with some minor success. However, a recent commit (e7552fd55a9d) is somehow causing Spark to stop prematurely when R calls the `is.hail.table.Table.count()` method. Any clues as to why this might be happening?. <details>; <summary>Stack trace</summary>. 	Error: org.apache.spark.SparkException: Job 3 cancelled because SparkContext was shut down; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); 	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); 	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:329,Error,Error,329,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,"['Error', 'down']","['Error', 'down']"
Availability,"IJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZJIJZJJIJZZJJIJZZJZJZJZJZJZJZJZJZJIJJIJJZJZJZJZLis/hail/io/OutputBuffer;; ```. Presumably this used to work fine in earlier Hail versions. However, it seems impossible to revert to such a version at the moment, as 0.2.81 is the oldest version that one can still start a Dataproc cluster with -- earlier versions use a Debian image without a fix to the `log4j` vulnerability. 0.2.81 yields a different error (`Class too large`):. ```; Traceback (most recent call last):; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/gnomad_v3_variants.py"", line 53, in <module>; run_pipeline(pipeline); File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1123>"", line 2, in write; File ""/opt/conda/default/lib/python3.8/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:14101,error,error,14101,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['error'],['error']
Availability,IR Dict lookups should include key/dict in error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6158:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/issues/6158,1,['error'],['error']
Availability,IR types are always available,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3332:20,avail,available,20,https://hail.is,https://github.com/hail-is/hail/pull/3332,1,['avail'],['available']
Availability,"Ideally, a stream would be able to recover from a transient error by; seeking, but until we have that functionality, this avoids having; one failure out of 5000 (which I have now seen twice). Example: https://batch.hail.is/batches/1531518/jobs/2094.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11716:35,recover,recover,35,https://hail.is,https://github.com/hail-is/hail/pull/11716,3,"['error', 'failure', 'recover']","['error', 'failure', 'recover']"
Availability,"If I can GET the URL I'm about to redirect to, then the only point of failure; remaining is the gateway nginx, and I trust that to work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4692:70,failure,failure,70,https://hail.is,https://github.com/hail-is/hail/pull/4692,1,['failure'],['failure']
Availability,"If a KeyTable has no keys, then the KeyedRDD should have an empty row as the key rather than throwing a fatal error. This was causing problems when using `same` for KeyTables with no keys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2115:110,error,error,110,https://hail.is,https://github.com/hail-is/hail/pull/2115,1,['error'],['error']
Availability,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8830:92,error,errors,92,https://hail.is,https://github.com/hail-is/hail/pull/8830,4,['error'],"['error', 'errors']"
Availability,"If a compatible annotation dataset can't be found in `index_compatible_version`, show the user the available versions and reference genome builds of the requested annotation dataset in the raised `ValueError`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10515:99,avail,available,99,https://hail.is,https://github.com/hail-is/hail/pull/10515,1,['avail'],['available']
Availability,"If a job is ""Running"", but the pod will always enter ""CrashLoopBackOff"" (due to a bad image), it will never finish. If a batch is deleted, the job is not marked cancelled so it is returned at the top of the refresh loop, but when the updated job is looked up using `undeleted` records, it is missing. This causes an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6737:316,error,error,316,https://hail.is,https://github.com/hail-is/hail/issues/6737,1,['error'],['error']
Availability,"If a many-partitions heavily-filtered matrix table is converted to a block matrix with `write_from_entry_expr`, parallelism is lost and kills performance. In the extreme case, imagine a MT with 4096 partitions, each with 1M rows, which are filtered to 1 row. There will be 1-way parallelism in the write. . We need to checkpoint the intermediate matrix if the loss of parallelism is above some threshold.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6995:318,checkpoint,checkpoint,318,https://hail.is,https://github.com/hail-is/hail/issues/6995,1,['checkpoint'],['checkpoint']
Availability,"If a pod is unreachable for any reason, we previously retried forever. However,; a pods are ephemeral; we cannot assume they will return. Instead, if we fail to; contact a pod, we remove it from the pods list and log the error. If the pod; really does exist, the monitor_pods loop will attempt to initialize it again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9858:221,error,error,221,https://hail.is,https://github.com/hail-is/hail/pull/9858,1,['error'],['error']
Availability,"If instance name isn't active, look to see whether it existed in the database before printing an error message about an unknown instance.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10325:97,error,error,97,https://hail.is,https://github.com/hail-is/hail/pull/10325,1,['error'],['error']
Availability,"If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run ./configure. The ./configure script queries the user for sparkVersion and generates a valid gradle.properties file. Afterwards, the user can execute gradle normally without any -D parameters. Users may still override the sparkVersion variable on the command line by specifying -PsparkVersion=2.1.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613:63,error,errors,63,https://hail.is,https://github.com/hail-is/hail/pull/1613,1,['error'],['errors']
Availability,"If there is a true issue, we raise the exception which is caught and printed by; docker_call_retry, or, if that re-raises, it is stored as an error and serialized; back to the driver in Container.run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9980:142,error,error,142,https://hail.is,https://github.com/hail-is/hail/pull/9980,1,['error'],['error']
Availability,"If we cannot authenticate the user, we should send them to a publicly accessible page where the error message can be presented.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12401:96,error,error,96,https://hail.is,https://github.com/hail-is/hail/pull/12401,1,['error'],['error']
Availability,"If we close the db pool before shutting down the task manager and worker pool, then we get coroutines trying to `acquire` a connection after the connection pool is already closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11433:40,down,down,40,https://hail.is,https://github.com/hail-is/hail/pull/11433,1,['down'],['down']
Availability,"If we hit an exception and exit the iterator early, then we are no longer iterating. We need to record that fact so that we can retry transient errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12492:144,error,errors,144,https://hail.is,https://github.com/hail-is/hail/pull/12492,1,['error'],['errors']
Availability,"If we miss an event, the refresh loop won't find it because it errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6703:63,error,errors,63,https://hail.is,https://github.com/hail-is/hail/pull/6703,1,['error'],['errors']
Availability,"If you dev deploy right now you'll likely see warnings like this:. ```; (hail) dgoldste@wmce3-cb7 hail % hailctl dev deploy -b hail-is/hail:main -s merge_code; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fb64d58f1f0>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7fb64d5a1520>, 1.908669784)]']; connector: <aiohttp.connector.TCPConnector object at 0x7fb64d579040>; Created deploy batch, see https://ci.hail.is/batches/7992015; (hail) dgoldste@wmce3-cb7 hail %; ```. `HailCredentials` recently changed such that now they contain resources (GCP or Azure credentials) that require closing, so `hail_credentials()` needs to be used as a context manager or you get those unclosed errors on exit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13602:757,error,errors,757,https://hail.is,https://github.com/hail-is/hail/pull/13602,1,['error'],['errors']
Availability,"If you invoke 'hail read -i vds write -o file.tsv', hail will delete that tsv and throw a requirement error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/322:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/issues/322,1,['error'],['error']
Availability,"If you're encountering this issue the quick fix is to use `array_elements_required=False`. ```; hl.import_vcf(..., array_elements_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:257,checkpoint,checkpoint,257,https://hail.is,https://github.com/hail-is/hail/issues/13346,6,"['Error', 'checkpoint', 'error']","['ErrorHandling', 'checkpoint', 'error']"
Availability,"Implements existing query service endpoints using websockets instead of long-running http requests. I open a new socket for each request instead of attempting to hold one open for each client to send all its requests; not sure if one is preferable to the other, but this one seemed easier to handle and more in line with what we were doing before. ~~I haven't put any sort of heartbeat on either end for now for simplicity; the `blocking_to_async` wrapper around the jvm execution interferes with the server's ability to send/receive pings and pongs, and I'm not currently handling retries for timeouts anyways; would love suggestions on how to make this more robust.~~. Since nginx's default behavior is to close the websocket after 60s of non-activity (which seems pretty reasonable), I have a 30s heartbeat on the server-side websocket connection. This meant rewriting the flow to split a task off to execute the blocking JVM function and keeping the websocket task open to handle the heartbeat. Currently, we rely on the client to close the connection once the jvm task is completed and the result response is received; if the connection is closed/something errors for some other reason, we check to see if the task is completed and cancel it if it's not. The client side still doesn't poll the server for existence, but if the socket is unexpectedly closed we'll retry the request.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636:376,heartbeat,heartbeat,376,https://hail.is,https://github.com/hail-is/hail/pull/9636,6,"['error', 'heartbeat', 'ping', 'robust']","['errors', 'heartbeat', 'pings', 'robust']"
Availability,Importing several single-sample VCFs with different IDs doesn't generate an error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1989:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/1989,1,['error'],['error']
Availability,Improve assert error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1878:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/1878,1,['error'],['error']
Availability,Improve error message for incorrect type for dict.get default argument,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7377:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/7377,1,['error'],['error']
Availability,Improve error message for starting a cluster without a region,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8791:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/8791,1,['error'],['error']
Availability,Improve error message when bgen.idx does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2412:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/2412,1,['error'],['error']
Availability,Improve robustness of export_plink and export_gen tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4528:8,robust,robustness,8,https://hail.is,https://github.com/hail-is/hail/pull/4528,1,['robust'],['robustness']
Availability,Improved Variant.parse error message.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2026:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/2026,1,['error'],['error']
Availability,"In 0.2, there is Table.to_spark but Table.from_spark is missing. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2938:225,error,error,225,https://hail.is,https://github.com/hail-is/hail/issues/2938,1,['error'],['error']
Availability,"In `SparkBackend`, TableIRs get lowered into SparkStages and value IRs get lowered into SparkPipelines. `SparkStage` represents the necessary computation on a partition of a table, as well as the partitioning information. This can either directly represent a TableIR, in which case the partition IR (`body`) is an array of all the rows of that given partition, or whatever the downstream ValueIR needs---e.g. for `TableCount`, the length of that array; for `TableWrite`, the filename that the partition was written out to, etc. `SparkPipeline` represents a local value that can use the results from the referenced stages. One assumption that I've made in this PR is that none of the bindings across all `SparkStage.globals` will have the same name, and none of them will be named ""context"". (I think this is a fairly reasonably assumption, since we'll just use genUID() to generate unique IDs for all of them and then use unique symbols once #5080 goes in.). In this PR, I've lowered:; - TableCount; - TableCollect; - TableGetGlobals; - TableRange; - TableMapGlobals; - TableMapRows. in order to write some tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5127:377,down,downstream,377,https://hail.is,https://github.com/hail-is/hail/pull/5127,1,['down'],['downstream']
Availability,"In a TableValue, the RVD key may be longer than the TableType key, so it's wrong for the row type of the result of a TableIR execute to depend on the RVD key. I tried to find all cases of this error in TableIR execute methods, and only found these two.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8890:193,error,error,193,https://hail.is,https://github.com/hail-is/hail/pull/8890,1,['error'],['error']
Availability,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7942:91,ERROR,ERROR,91,https://hail.is,https://github.com/hail-is/hail/pull/7942,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"In brief: . Computing an LD (X @ X.T) matrix using hail's BlockMatrices that have been normalized using the default function seems to produce values >> 1 (~1.00000001, but much larger than floating point error). This can cause problems in downstream applications. Example:. # Normalize genotypes; BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(), ; out_dir + out_name + ""_norm"" + ""_bm"", ; mean_impute = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6351:204,error,error,204,https://hail.is,https://github.com/hail-is/hail/issues/6351,2,"['down', 'error']","['downstream', 'error']"
Availability,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9259:301,error,error,301,https://hail.is,https://github.com/hail-is/hail/pull/9259,3,['error'],['error']
Availability,"In main, we assumed that if `self._batch is not None` then there is a Batch that needs cancelling, but that is no longer true. Now, we must track whether we have called `submit` or not. I made a few other minor cleanups of `ServiceBackend` while I was in there. There is no longer any need to have a `None` batch b/c there is no distinction between a builder and a batch now. Example error:; ```; INFO hailtop.aiocloud.aiogoogle.credentials:credentials.py:92 using credentials file /test-gsa-key/key.json: GoogleServiceAccountCredentials for testns-test-418@hail-vdc.iam.gserviceaccount.com; _________ ERROR at teardown of Tests.test_loop_with_struct_of_strings __________. init_hail = None; request = <SubRequest 'set_query_name' for <TestCaseFunction test_loop_with_struct_of_strings>>. @pytest.fixture(autouse=True); def set_query_name(init_hail, request):; backend = current_backend(); if isinstance(backend, ServiceBackend):; backend.batch_attributes = dict(name=request.node.name); yield; backend.batch_attributes = dict(); references = list(backend._references.keys()); for rg in references:; backend.remove_reference(rg); backend.initialize_references(); if backend._batch:; report: Dict[str, CollectReport] = request.node.stash[test_results_key]; if any(r.failed for r in report.values()):; > log.info(f'cancelling failed test batch {backend._batch.id}'). test/hail/conftest.py:81: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; /usr/local/lib/python3.9/dist-packages/hailtop/batch_client/aioclient.py:347: in id; self._raise_if_not_created(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <hailtop.batch_client.aioclient.Batch object at 0x7ffae6f11bb0>. def _raise_if_not_created(self):; if not self.is_created:; > raise BatchNotCreatedError; E hailtop.batch_client.aioclient.BatchNotCreatedError. ```. https://batch.hail.is/batches/7950601/jobs/156",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13564:384,error,error,384,https://hail.is,https://github.com/hail-is/hail/pull/13564,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"In the following hail call, the `sa` binding is not available in the filter's lambda argument. In almost all modern programming languages, bindings are lexical, descending all the way into nested code. We would like to support the same intuitive notion of binding in hail. ```; vds.annotate_samples_expr(; ""sa.mendel = gs.filter(g => va.mendel.filter(x => x.fam == sa.fam).length).count()""); ```. ### Design Suggestion. As we move towards the compiler, this should become more natural because these filters will always be inlined. We need only not reset the environment when descending into a lambda.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1351:52,avail,available,52,https://hail.is,https://github.com/hail-is/hail/issues/1351,1,['avail'],['available']
Availability,"In this PR, I rewrite `linear_regression_rows_nd` to use `_map_partitions` instead of `_group_within_partitions`. By doing this, I've eliminated the need to do a `key_by` at the end of `linear_regression_rows_nd`. I also think this makes the code clearer. . This PR also makes a few seemingly random changes that are actually bug fixes:. 1. When emitting `Apply` nodes, we were grabbing the `Code[Region]` from the first argument to the `MethodBuilder`. However, the assumption that the first argument will always be a `Region` seems to no longer be true. As such, we just construct a `CodeParam` from the `StagedRegion` we have available. . 2. In the NDArrayEmitter, I want to make sure I call the local `emit` method that passes off to `emitWithRegion`, for the same reason as 1: (Can't trust first argument to be a `Region`). 3. In `EmitStream`, I need to use `memoizeField` instead of `memoize`, because regular `memoize` saves to a `LocalRef`, and that will get reset to 0 when `next` is called on a stream. Lesson: don't trust locals for things that must live between elements of a stream. I feel like you have a better idea of how the Stream stuff gets emitted than I do Patrick. I'm curious if what I wrote in `process_block` could be written in a way that would lead to better code getting emitted, as I still need to figure out how to squeeze more performance out of this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9469:629,avail,available,629,https://hail.is,https://github.com/hail-is/hail/pull/9469,1,['avail'],['available']
Availability,Inbreeding errors out on multiallelic GTs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4373:11,error,errors,11,https://hail.is,https://github.com/hail-is/hail/pull/4373,1,['error'],['errors']
Availability,"Includes a general version of Ward's algorithm, a common hierarchical clustering technique important for implementing the UNICORN model. . Specifically pinging @alexb-3 for code review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/593:152,ping,pinging,152,https://hail.is,https://github.com/hail-is/hail/pull/593,1,['ping'],['pinging']
Availability,Incorrect array indexing into a genotype should have a better error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3713:62,error,error,62,https://hail.is,https://github.com/hail-is/hail/issues/3713,1,['error'],['error']
Availability,"Increase memory and cpu for test_hail_services_java to match java query tests. This contains tests of the shuffler IR, which runs the hail compiler, so it seems it should have the same resource limits as the other java query tests. #9401 is getting an out of memory error in `testShuffleIR`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9423:266,error,error,266,https://hail.is,https://github.com/hail-is/hail/pull/9423,1,['error'],['error']
Availability,"Installing certbot is hard so I stopped doing that. Instead, I use the cerbot docker image. I also eliminated `sed` use in the letsencrypt directory. We now maintain the options-ssl-nginx.conf file ourselves. I copied the settings from certbot; GitHub. They're straightforward, as a part of regular package versioning maintenance we should also; reconsider our cipher suites and TLS versions. We now have a `make run DRY_RUN=true` option which can be run repeatedly without affecting the default certs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9971:318,mainten,maintenance,318,https://hail.is,https://github.com/hail-is/hail/pull/9971,1,['mainten'],['maintenance']
Availability,Instead of throwing match errors when trying to parse format fields.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3008:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/issues/3008,1,['error'],['errors']
Availability,"Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the source, so stat'ing the sources and destinations are all overlapping. This avoids dependencies where I have to e.g. stat the input, decide what to do, and then perform a second action. I approached testing two ways: First, hand test common operations and errors (copy file, copy dir, overwrite, overwrite dir with file and vice versa, the various treat_dest_as settings, large files, detecting copy-and-files on input on Google Storage, etc.) Second, I enumerated essentially all single input ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:1518,error,error,1518,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['error'],['error']
Availability,"Interface change:. ``` scala; abstract class Type[T] extends BaseType {; def coerce(a: Any): T; // ...; }; ```. Note the two major changes:; - Every `Type` now must correspond to a Scala type; - Every `Type` must know how to convert appropriate values to their associated Scala type. We may then naturally modify methods like `evalCompose`:. ``` scala; def evalCompose[T](ec: EvalContext, typ: Type[T])(subexpr: AST); (g: (T) => Any): () => Any = {; val f = subexpr.eval(ec); () => {; val x = f(); if (x != null); g(typ.coerce(x)); else; null; }; }; ```. which will hopefully induce or enable downstream simplifications.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/624:593,down,downstream,593,https://hail.is,https://github.com/hail-is/hail/issues/624,1,['down'],['downstream']
Availability,"Introduce FASTAReaderConfig to act as a kind of factory for FASTAReader,; while all FASTAReaders themselves are confined to ThreadLocals (except; in tests). Furthermore, add a lock around the fasta file map to prevent more than; one fasta from being copied per jvm. The can be lock contention on the; map, but if there is large amounts of waiting for said lock, then it; usually means that a fasta is downloading and we definitely should be; waiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9435:401,down,downloading,401,https://hail.is,https://github.com/hail-is/hail/pull/9435,1,['down'],['downloading']
Availability,Is there a way for me to test this further? My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. This should seed up the feedback substantially for anyone working on an image that transitively depends on other images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7534:100,down,download,100,https://hail.is,https://github.com/hail-is/hail/pull/7534,1,['down'],['download']
Availability,"Issue came up on doctest branch. Reproducible example:; ```; assoc_vds = hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'). kinship_vds = assoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1368:702,Error,Error,702,https://hail.is,https://github.com/hail-is/hail/issues/1368,1,['Error'],['Error']
Availability,"It also fixes numeric promotion of TInt to TLong, which threw an; assertion error before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/748:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/pull/748,2,['error'],['error']
Availability,It appears (see [1] and [2]) that compiling AVX2 instructions (which hail uses to calculate IBD quickly) on a Mac using some versions of MacPorts GCC doesn't work. The Hail team recommends compiling with Clang when on Mac OS X. We _do not recommend_ removing AVX2 compatibility (either by adding `-mno-avx` or removing `-march=native`) because the AVX2 instructions are vital to IBD performance. [1] http://stackoverflow.com/questions/10327939/error-no-such-instruction-while-assembling-project-on-mac-os-x; [2] https://github.com/Theano/Theano/issues/1980,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1341:444,error,error-no-such-instruction-while-assembling-project-on-mac-os-x,444,https://hail.is,https://github.com/hail-is/hail/issues/1341,1,['error'],['error-no-such-instruction-while-assembling-project-on-mac-os-x']
Availability,"It causes errors on GRCh38. I propose that the default argument is None, and we if None we use a default dict for GRCh37 or GRCh38 and an empty dictionary for other references.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4925:10,error,errors,10,https://hail.is,https://github.com/hail-is/hail/issues/4925,1,['error'],['errors']
Availability,"It definitely looks like ""ZONE_RESOURCE_POOL_EXHAUSTED"" is the cause of these GPU test failures. In this case it looks like it took ~4 minutes to successfully get a VM (after two exhaustion errors) & schedule the job. By then, our uniform 6 minute timeout per test left us with just two minutes. It looks like the job actually did succeed in the worker (seems to have taken ~2 minutes, seems long, does testing for CUDA do some kind of initialization work?). Looks like backing that off to 10 minutes might be just enough to eventually get us a GPU. Might be worth pulling that into its own build.yaml test job so that it does not block the queue of other tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739:87,failure,failures,87,https://hail.is,https://github.com/hail-is/hail/pull/13739,2,"['error', 'failure']","['errors', 'failures']"
Availability,"It is able to execute a trivial pipeline without the JVM on the client. The countdown down to a fully functional Hail Query service begins now. I will start running the Python tests against the service to benchmark our progress. The main blockers are:; - Table lowering @tpoterba @patrick-schultz @catoverdrive ; - The shuffle service @tpoterba @danking ; - Reading, writing and threading the (per-user, for the query service) filesystem through execution. I'll be working on this.; - A Batch backend for distributed execution. I will do this once there is enough functionality to execute something big/interesting. It's happening!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8468:86,down,down,86,https://hail.is,https://github.com/hail-is/hail/pull/8468,1,['down'],['down']
Availability,It is currently possible to write a blocked index where the virtual file; offset is exactly ((REAL_FILE_OFFSET << 16) | BLOCK_SIZE). This is a bug; and leads to assertion errors when trying to seek to the appropriate row; because `off == end` for that index.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6633:171,error,errors,171,https://hail.is,https://github.com/hail-is/hail/pull/6633,1,['error'],['errors']
Availability,"It is impossible to submit large batches without this. What happens? The timeout per request is 60s. We have 50 x 8MB = 400MB worth of requests in flight. That means the client needs a reliable sustained MINIMUM bandwidth of ~7MB/s to not time out. This doesn't seem reasonable. Without this change, Konrad wasn't able to submit a large batch (although it probably would have gone through eventually with enough retry/backoff). With this, 136K jobs took 2-3m to submit. FYI @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7971:185,reliab,reliable,185,https://hail.is,https://github.com/hail-is/hail/pull/7971,1,['reliab'],['reliable']
Availability,It is no longer the case that VCF does not support phased haploid calls. Make a note of this in the code. ## Security Assessment; - This change has no security impact. ### Impact Description; Change error messages only.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14742:199,error,error,199,https://hail.is,https://github.com/hail-is/hail/pull/14742,1,['error'],['error']
Availability,"It is not free and we get emails about having too many page views pretty frequently. I suspect this is due to the jobs page having a download icon. The font provided by Google with its material design icons seems to be free to access at any scale. I got the GitHub octocat from GitHub's website. It's a bit bigger. <img width=""1130"" alt=""this PR"" src=""https://github.com/hail-is/hail/assets/106194/31e1cc67-ce9f-4e1f-a6b2-64258a8596c0"">; <img width=""1130"" alt=""main"" src=""https://github.com/hail-is/hail/assets/106194/ce9cc44d-3332-4b88-b733-4ac46a9f8e16"">. I didn't actually dev deploy batch to check the other assets but I suspect they're fine enough. This is what the question mark in a circle looks like: https://fonts.google.com/icons?selected=Material%20Symbols%20Outlined%3Ahelp%3AFILL%400%3Bwght%40400%3BGRAD%400%3Bopsz%4024 And this is what the download icon looks like: https://fonts.google.com/icons?selected=Material%20Symbols%20Outlined%3Adownload%3AFILL%400%3Bwght%40400%3BGRAD%400%3Bopsz%4024",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14302:133,down,download,133,https://hail.is,https://github.com/hail-is/hail/pull/14302,2,['down'],['download']
Availability,"It is possible for socket connect to fail if the shuffle service is down (e.g. https://ci.hail.is/batches/91027/jobs/105).; This change ensure we retry forever, periodically logging that we are retrying",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9378:68,down,down,68,https://hail.is,https://github.com/hail-is/hail/pull/9378,1,['down'],['down']
Availability,"It looks like batch test was an infinite loop for: https://github.com/hail-is/hail/pull/4536. So I bumped off the pod running the test (this was maybe bad behavior on my part, but I was also curious how ci/batch would respond):. ```; $ kubectl delete pod job-17-lz6m5; ```. I thought CI would re-run the test, but it got merged!. Output did get uploaded, here it is: https://storage.googleapis.com/hail-ci-0-1/ci/ee92f64477f68737987fd8f21411b0348a3d3420/e4ae86ea520fbc5d98b84811b2cdb83640163910/index.html. In particular the job log consists of:. ```; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF; ```. I had a `logs -f` running when I did this, so here is the log up to the failure:. [job-17-lz6m5.log](https://github.com/hail-is/hail/files/2474367/job-17-lz6m5.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4541:596,error,error,596,https://hail.is,https://github.com/hail-is/hail/issues/4541,2,"['error', 'failure']","['error', 'failure']"
Availability,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4948:132,error,errors,132,https://hail.is,https://github.com/hail-is/hail/issues/4948,1,['error'],['errors']
Availability,It may be good to have a better error message when users forget to add the file:/// to a unix file path when using hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/381:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/381,1,['error'],['error']
Availability,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752:1784,down,down,1784,https://hail.is,https://github.com/hail-is/hail/pull/10752,1,['down'],['down']
Availability,"It still thought it had `s`, an error message to print as a python traceback, rather than an error id like it has now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10720:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/10720,2,['error'],['error']
Availability,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3288:260,error,errors,260,https://hail.is,https://github.com/hail-is/hail/pull/3288,1,['error'],['errors']
Availability,It was failing with 'Error file not found:' for me. Putting all the; arguments on one line fixed it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6956:21,Error,Error,21,https://hail.is,https://github.com/hail-is/hail/pull/6956,1,['Error'],['Error']
Availability,It was previously hard to retry transient errors from synchronous libraries like; requests because `hailtop` lacked a synchronous retry wrapper. This PR; implements such a function and uses it in every place that hail imports; `requests`. I also finally addressed the 1kg download issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8391:42,error,errors,42,https://hail.is,https://github.com/hail-is/hail/pull/8391,2,"['down', 'error']","['download', 'errors']"
Availability,"It would be great if it was possible to have Hail skip rows that don't have the correct number of fields, and just report them via error messages (without crashing), so that the annotation files (such as EIGEN) can still be used.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/402:131,error,error,131,https://hail.is,https://github.com/hail-is/hail/issues/402,1,['error'],['error']
Availability,"Iterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but ¯\_(ツ)_/¯). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:4493,error,error,4493,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['error'],['error']
Availability,Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.Recur$.apply(Recur.scala:4); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:52); 	at is.hail.expr.ir.ExtractAggregators$.apply(ExtractAggregators.scala:25); 	at is.hail.expr.ir.CompileWithAggregators$.apply(Compile.scala:138); 	at is.hail.expr.ir.CompileWithAggregators$.apply(Compile.scala:217); 	at is.hail.expr.MatrixMapRows.execute(Relational.scala:1272); 	at is.hail.expr.MatrixMapGlobals.execute(Relational.scala:1575); 	at is.hail.expr.MatrixRowsTable.execute(Relational.scala:2275); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:504); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); 	at is.hail.table.Table.write(Table.scala:905); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-258b35d26fb3; Error summary: NullPointerException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3731:11483,Error,Error,11483,https://hail.is,https://github.com/hail-is/hail/issues/3731,1,['Error'],['Error']
Availability,"Iterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3016:2234,alive,alive,2234,https://hail.is,https://github.com/hail-is/hail/pull/3016,1,['alive'],['alive']
Availability,"It’s a random test, and it seems the current tolerance still allows rare sporadic failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14053:45,toler,tolerance,45,https://hail.is,https://github.com/hail-is/hail/pull/14053,2,"['failure', 'toler']","['failures', 'tolerance']"
Availability,Java core dump error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4418:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/4418,1,['error'],['error']
Availability,Java heapspace error in to_pandas,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/12035,1,['error'],['error']
Availability,"Jobs with large logs (>2GiB-ish) can break workers because the current worker code attempts to load the whole log as `bytes` before uploading it to blob storage. This loading into `bytes` also plagues the batch front end when loading logs from blob storage to present to the user.; ; This updates the worker and front end to always stream through logs, never load them into memory. Additionally, in order to make page loads in the UI reasonable, we limit the length of the log that is shown in the UI, with some advice to download the file if it's too large to render on the page. Fixes #13329",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14076:522,down,download,522,https://hail.is,https://github.com/hail-is/hail/pull/14076,1,['down'],['download']
Availability,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8177:199,error,errors,199,https://hail.is,https://github.com/hail-is/hail/pull/8177,2,"['down', 'error']","['down', 'errors']"
Availability,"Joins were not being tested and fail with source mismatch if joins are present in both key and agg expressions. This fix is analogous to that on Table.group_by.aggregate in #3730, processing all joins at once, and not reprocessing later. I don't address here a deeper bug that throws a source error when processing more than one entry-indexed. I've made an issue #3763",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3762:293,error,error,293,https://hail.is,https://github.com/hail-is/hail/pull/3762,1,['error'],['error']
Availability,Just make all the FS streams double close resilient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9001:42,resilien,resilient,42,https://hail.is,https://github.com/hail-is/hail/pull/9001,1,['resilien'],['resilient']
Availability,"K-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:6391,avail,available,6391,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"K-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:6383,avail,available,6383,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,Key Error `va` in `aggregate_rows`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8316:4,Error,Error,4,https://hail.is,https://github.com/hail-is/hail/issues/8316,1,['Error'],['Error']
Availability,"Keys mismatch by type, not number of fields, but error suggests it's the wrong number.; ```; constraint_ht.key; Out[29]: <StructExpression of type struct{gene: str, expressed: str}>; caf_ht.key; Out[30]: <StructExpression of type struct{gene: str, expressed: bool}>; constraint_ht.annotate(**caf_ht[constraint_ht.key]); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-31-6560bd06e877>"", line 1, in <module>; constraint_ht.annotate(**caf_ht[constraint_ht.key]); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 349, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1299, in index; raise ExpressionException(f'Key mismatch: table has {len(self.key)} key fields, '; hail.expr.expressions.base_expression.ExpressionException: Key mismatch: table has 2 key fields, found 1 index expressions.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4951:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/issues/4951,1,['error'],['error']
Availability,"Konrad ran into a problem where we started shutting down the worker because it was idle, but in between checking if there were any jobs still running and shutting down the site, a create job request came in. MJS was sent to the driver, but MJC was never sent because the worker shut down. The driver thought the job failed to schedule because the deactivate request was sent in before create job could return. The end result was Konrad's job still ran, but the database was left with an attempt that has a start time but no end time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10594:52,down,down,52,https://hail.is,https://github.com/hail-is/hail/pull/10594,3,['down'],['down']
Availability,"Kubernetes interprets a pod terminating quickly (e.g. `echo hi`) as pod start up failure. If you submit a batch job for `echo hi`, the batch job will take a very long time to complete. My understanding is that it eventually we get lucky and `sh` takes long enough to `echo hi` that k8s is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4822:55,echo,echo,55,https://hail.is,https://github.com/hail-is/hail/issues/4822,5,"['echo', 'failure']","['echo', 'failure']"
Availability,LD prune throws a requirement error on non-diploid calls,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12971:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/12971,1,['error'],['error']
Availability,"LTRjZjJhNTdhZDkzOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""prPublicId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:5791,avail,available,5791,https://hail.is,https://github.com/hail-is/hail/pull/13718,1,['avail'],['available']
Availability,"LWJmMWY5Mzc1NTVhYyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""prPublicId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13836:5859,avail,available,5859,https://hail.is,https://github.com/hail-is/hail/pull/13836,1,['avail'],['available']
Availability,"LWY3ZGM4YjIwOTVhNiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""prPublicId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13933:5859,avail,available,5859,https://hail.is,https://github.com/hail-is/hail/pull/13933,1,['avail'],['available']
Availability,Left to do:; - error checking on bounds of slices. This could either be done in the emitted code for slice or in the IR with Die and what not. Open to advice on which is better.; - bind shape in the python iR generation so it doesn't get repeated a bunch of times. This would go along with the bounds checking in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6225:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/6225,1,['error'],['error']
Availability,Let me know if you need the context. I don't think it's necessary and will break my interface. I didn't want the job id to be available until the whole batch is submitted and the batch id is available.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6332:126,avail,available,126,https://hail.is,https://github.com/hail-is/hail/pull/6332,2,['avail'],['available']
Availability,Let me know if you think the error message is clear.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9164:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/9164,1,['error'],['error']
Availability,Liftover from UK Biobank bgen(37) to vcf(38) error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['error'],['error']
Availability,Linear Regression in Hail on Broad Cluster Errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:43,Error,Errors,43,https://hail.is,https://github.com/hail-is/hail/issues/5559,1,['Error'],['Errors']
Availability,Load vcf file error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/issues/6747,1,['error'],['error']
Availability,LoadVCF should error on call fields that are not Number=1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3008:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/3008,1,['error'],['error']
Availability,"Loader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1710,error,error,1710,https://hail.is,https://github.com/hail-is/hail/issues/11705,8,['error'],['error']
Availability,LocalBackend Better Errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9569:20,Error,Errors,20,https://hail.is,https://github.com/hail-is/hail/pull/9569,1,['Error'],['Errors']
Availability,"Long awaited, this change prompts the batch driver to only schedule jobs on workers with the most recent instance version, i.e. matches the `INSTANCE_VERSION` global variable. This way we can make backwards incompatible changes between the worker and driver without having to manually kill the whole fleet. This will allow pre-existing workers to finish gracefully, as they will just stop receiving work when the new batch driver is deployed and eventually die off. ### Scheduler changes; Just skips instances where the instance version doesn't match `INSTANCE_VERSION`. ### Autoscaler changes; Cluster stats like free mcpu and live instances are tracked per instance version. The autoscaler now only looks at instances of the latest version when deciding whether it needs more workers. This way we don't get stuck unable to schedule new jobs until the old workers die off because there technically are enough cores available to meet demand but they are from old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13055:916,avail,available,916,https://hail.is,https://github.com/hail-is/hail/pull/13055,1,['avail'],['available']
Availability,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:1729,redundant,redundant,1729,https://hail.is,https://github.com/hail-is/hail/pull/7719,1,['redundant'],['redundant']
Availability,"Looking at the logs, I think these two new states are because we added the log analytics agent based on when the PR merged and the absence of these errors before December 10th. ```; Unknown azure statuses [{'code': 'ProvisioningState/updating', 'level': 'Info', 'displayStatus': 'Updating'}, {'code': 'PowerState/running', 'level': 'Info', 'displayStatus': 'VM running'}] for instance batch-worker-default-standard-166xu; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11160:148,error,errors,148,https://hail.is,https://github.com/hail-is/hail/pull/11160,1,['error'],['errors']
Availability,Looks like they changed their error message. See [here](https://ci.hail.is/batches/3181369/jobs/80) for an example.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11942:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/pull/11942,1,['error'],['error']
Availability,Lots of aggregators breaks down,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516:27,down,down,27,https://hail.is,https://github.com/hail-is/hail/issues/4516,1,['down'],['down']
Availability,"MENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **711/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.5 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570772](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570772) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **701/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.3 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570773](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.clou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:6229,avail,available,6229,https://hail.is,https://github.com/hail-is/hail/pull/13717,3,['avail'],['available']
Availability,"Made changes to clean up/add to the documentation for the datasets API and annotation DB. Changes to documentation:. - Moved raw html in `annotation_database_ui.rst` into `hail/python/hail/docs/_static/annotationdb/annotationdb.html`.; - Added html table to bottom of datasets doc page to show all available datasets, similar to what is currently on annotation DB doc page. Added relevant files to `hail/python/hail/docs/_static/datasets`.; - Moved schemas currently on datasets doc page to their own page. Datasets doc page links to this new page.; - Moved some minor styling from html files to `annotationdb.css`, and cleaned up formatting of html.; - Added doc page for the `DB` class, referenced on the Python API doc page for the experimental module. Only exposes the `available_datasets` attribute and `annotate_rows_db` method, as I didn't think most users need to see any internal methods or the `Dataset` and `DatasetVersion` classes/methods. Most of diff is just formatting changes to `db.py` and `datasets.py` for consistency/readability that did not change functionality. Also added docstrings to methods in `db.py` that were missing them. Added a check in `DB` constructor to make sure the cloud and region combination is valid. To prevent an empty annotation DB instance from being created if user specifies `db = hl.experimental.DB(region='eu', cloud='aws')`, since we don't have an EU bucket on AWS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9663:298,avail,available,298,https://hail.is,https://github.com/hail-is/hail/pull/9663,1,['avail'],['available']
Availability,"Made read back-compatible, with error checking.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/610:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/610,1,['error'],['error']
Availability,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2093:566,error,error,566,https://hail.is,https://github.com/hail-is/hail/pull/2093,1,['error'],['error']
Availability,"Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:6275,failure,failure,6275,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['failure'],['failure']
Availability,"Mainly bug fixes and criu support. I don't like that we're still building from source and hope to replace that soon, but Vedant will need this to start playing with checkpoint/restore.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11695:165,checkpoint,checkpoint,165,https://hail.is,https://github.com/hail-is/hail/pull/11695,1,['checkpoint'],['checkpoint']
Availability,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5054:84,Failure,Failure,84,https://hail.is,https://github.com/hail-is/hail/pull/5054,2,"['Failure', 'alive']","['Failure', 'alive']"
Availability,Make `SemanticHash` Resilient to `FileNotFoundExceptions`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13919:20,Resilien,Resilient,20,https://hail.is,https://github.com/hail-is/hail/pull/13919,1,['Resilien'],['Resilient']
Availability,"Make available [pan-ukb datasets](https://pan.ukbb.broadinstitute.org/docs/hail-format) via datasets API. Includes:; - Summary statistics MatrixTable, meta-analysis MatrixTable (both on GCS and S3); - LD score Table, variant index table, and LD BlockMatrix for each population (AFR, AMR, CSA, EAS, EUR, MID) (S3 only)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10186:5,avail,available,5,https://hail.is,https://github.com/hail-is/hail/pull/10186,1,['avail'],['available']
Availability,Make evaluates the entire recipe before executing it. Consider this:; ```; (base) # cat Makefile; rm -rf file; make foo; foo:; 	echo hello > file; 	echo $(shell cat file); cat: file: No such file or directory; echo hello > file; echo . ```; The file does not exist because Make runs cat before the recipe is executed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9338:128,echo,echo,128,https://hail.is,https://github.com/hail-is/hail/pull/9338,4,['echo'],['echo']
Availability,Make sure that all local alleles are less than the n_total_alleles parameter passed to the function. This error will often occur if a vds is split with regular split_multi rather than vds.split_multi. Closes #13479,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13485:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/pull/13485,1,['error'],['error']
