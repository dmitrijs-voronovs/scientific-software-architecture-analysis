quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:22331,pipeline,pipelines,22331,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['pipeline'],['pipelines']
Deployability,"tring,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing VEP annotations. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:107039,configurat,configuration,107039,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106471,configurat,configuration,106471,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['configurat'],['configuration']
Deployability,"ts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:9186,release,release,9186,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"tter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:9540,pipeline,pipelines,9540,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['pipeline'],['pipelines']
Deployability,"tures. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:45164,pipeline,pipelines,45164,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"tures. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key beh",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:65980,update,update,65980,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to searc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:92129,install,installations,92129,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installations']
Deployability,"type tint32. hail.expr.functions.parse_int32(x)[source]; Parse a string as a 32-bit integer.; Examples; >>> hl.eval(hl.parse_int32('154')); 154. >>> hl.eval(hl.parse_int32('15.4')); None. >>> hl.eval(hl.parse_int32('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint32. hail.expr.functions.parse_int64(x)[source]; Parse a string as a 64-bit integer.; Examples; >>> hl.eval(hl.parse_int64('154')); 154. >>> hl.eval(hl.parse_int64('15.4')); None. >>> hl.eval(hl.parse_int64('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint64. hail.expr.functions.parse_float(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float('1.1')); 1.1. >>> hl.eval(hl.parse_float('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. hail.expr.functions.parse_float32(x)[source]; Parse a string as a 32-bit floating point number.; Examples; >>> hl.eval(hl.parse_float32('1.1')); 1.100000023841858. >>> hl.eval(hl.parse_float32('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat32. hail.expr.functions.parse_float64(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float64('1.1')); 1.1. >>> hl.eval(hl.parse_float64('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/string.html:6999,update,updated,6999,docs/0.2/functions/string.html,https://hail.is,https://hail.is/docs/0.2/functions/string.html,1,['update'],['updated']
Deployability,"ud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:43123,configurat,configuration,43123,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"ul concordance statistics. This value is the number of genotypes ; which were called (homozygous reference, heterozygous, or homozygous variant) in both datasets, ; but where the call did not match between the two.; The column concordance matches the structure of the global summmary, which is detailed above. Once again,; the first index into this array is the state on the left, and the second index is the state on the right.; For example, concordance[1][4] is the number of “no call” genotypes on the left that were called ; homozygous variant on the right. Parameters:right (VariantDataset) – right hand variant dataset for concordance. Returns:The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics. Return type:(list of list of int, KeyTable, KeyTable). count()[source]¶; Returns number of samples and variants in the dataset.; Examples; >>> samples, variants = vds.count(). Notes; This is also the fastest way to force evaluation of a Hail pipeline. Returns:The sample and variant counts. Return type:(int, int). count_variants()[source]¶; Count number of variants in variant dataset. Return type:long. deduplicate()[source]¶; Remove duplicate variants. Returns:Deduplicated variant dataset. Return type:VariantDataset. delete_va_attribute(ann_path, attribute)[source]¶; Removes an attribute from a variant annotation field.; Attributes are key/value pairs that can be attached to a variant annotation field.; The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When expor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:33950,pipeline,pipeline,33950,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['pipeline'],['pipeline']
Deployability,"ult is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str) – The names of the datasets with which to annotate rel. Returns:; MatrixTable or Table – The relational object rel, with the annotations from names; added. property available_datasets; List of names of available annotation datasets. Returns:; list – List of available annotation datasets. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:3617,update,updated,3617,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,1,['update'],['updated']
Deployability,"ult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any]) – Positional arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object.; kwargs (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any]) – Key-word arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object. Return type:; PythonResult. Returns:; resource.PythonResult. image(image); Set the job’s docker image.; Notes; image must already exist and have the same version of Python as what is; being used on the computer submitting the Batch. It also must have the; dill Python package installed. You can use the function docker.build_python_image(); to build a new image containing dill and additional Python packages.; Examples; Set the job’s docker image to hailgenetics/python-dill:3.9-slim:; >>> b = Batch(); >>> j = b.new_python_job(); >>> (j.image('hailgenetics/python-dill:3.9-slim'); ... .call(print, 'hello')); >>> b.run() . Parameters:; image (str) – Docker image to use. Return type:; PythonJob. Returns:; Same job object with docker image set. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:5780,install,installed,5780,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,2,['install'],['installed']
Deployability,"unction or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:; GT: call; AD: array<int32>; DP: ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23562,update,update,23562,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"uration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Trio. View page source. Trio. class hail.genetics.Trio[source]; Class containing information about nuclear family relatedness and sex. Parameters:. s (str) – Sample ID of proband.; fam_id (str or None) – Family ID.; pat_id (str or None) – Sample ID of father.; mat_id (str or None) – Sample ID of mother.; is_female (bool or None) – Sex of proband. Attributes. fam_id; Family ID. is_female; Returns True if the proband is a reported female, False if reported male, and None if no sex is defined. is_male; Returns True if the proband is a reported male, False if reported female, and None if no sex is defined. mat_id; ID of mother in trio, may be missing. pat_id; ID of father in trio, may be missing. s; ID of proband in trio, never missing. Methods. is_complete; Returns True if the trio has a defined mother and father. property fam_id; Family ID. Return type:; str or None. is_complete()[source]; Returns True if the trio has a defined mother and father.; The considered fields are mat_id() and pat_id().; Recall that s may never be missing. The fam_id(); and is_female() fields may be missing in a complete trio. Return type:; bool. property is_female; Returns True if the proband is a reported female,; False if reported male, and None if no sex is defined. Return type:; bool or None. property is_male; Returns True if the proband is a reported male,; False if reported female, and None if no sex is defined. Return type:; bool or None. property mat_id; ID of mother in trio, may be missing. Return type:; str or None. property pat_id; ID of father in trio, may be missing. Return type:; str or None. property s; ID of proband in trio, never missing. Return type:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Trio.html:2389,update,updated,2389,docs/0.2/genetics/hail.genetics.Trio.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Trio.html,1,['update'],['updated']
Deployability,"ureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of float) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the x-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_discrete(name=None, breaks=None, labels=None)[source]; The default discrete x scale. Parameters:. na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:9418,continuous,continuous,9418,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"urns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with all values in the dictionary.; Examples; >>> hl.eval(d.values()) ; [33, 44, 43]. Returns:; ArrayExpression – All values in the dictionary. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:9095,update,updated,9095,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['update'],['updated']
Deployability,"v_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_variant_summary. View page source. clinvar_variant_summary. Versions: 2019-07; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2019-07, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'Type': str; 'Name': str; 'GeneID': int32; 'GeneSymbol': str; 'HGNC_ID': str; 'ClinicalSignificance': str; 'ClinSigSimple': int32; 'LastEvaluated': str; 'RS# (dbSNP)': int32; 'nsv/esv (dbVar)': str; 'RCVaccession': str; 'PhenotypeIDS': str; 'PhenotypeList': str; 'Origin': str; 'OriginSimple': str; 'Assembly': str; 'ChromosomeAccession': str; 'ReferenceAllele': str; 'AlternateAllele': str; 'Cytogenetic': str; 'ReviewStatus': str; 'NumberSubmitters': int32; 'Guidelines': str; 'TestedInGTR': str; 'OtherIDs': str; 'SubmitterCategories': int32; 'VariationID': int32; 'interval': interval<locus<GRCh37>>; 'AlleleID': int32; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/clinvar_variant_summary.html:9926,update,updated,9926,docs/0.2/datasets/schemas/clinvar_variant_summary.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_variant_summary.html,1,['update'],['updated']
Deployability,"val(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >>> hl.eval(s.startswith('The')); True. >>> hl.eval(s.startswith('the')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. strip()[source]; Returns a copy of the string with whitespace removed from the start; and end.; Examples; >>> s2 = hl.str(' once upon a time\n'); >>> hl.eval(s2.strip()); 'once upon a time'. Returns:; StringExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. translate(mapping)[source]; Translates characters of the string using mapping.; Examples; >>> string = hl.literal('ATTTGCA'); >>> hl.eval(string.translate({'T': 'U'})); 'AUUUGCA'. Parameters:; mapping (DictExpression) – Dictionary of character-character translations. Returns:; StringExpression. See also; replace(). upper()[source]; Returns a copy of the string, but with lower case letters converted; to upper case.; Examples; >>> hl.eval(s.upper()); 'THE QUICK BROWN FOX'. Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StringExpression.html:14632,update,updated,14632,docs/0.2/hail.expr.StringExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html,1,['update'],['updated']
Deployability,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html:9760,update,updated,9760,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html:9760,update,updated,9760,docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"verse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of float) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the x-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_discrete(name=None, breaks=None, labels=None)[source]; The default discrete x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of str) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_genomic(reference_genome, name=None)[source]; The default genomic x scale. This is used when the x aesthetic corresponds to a LocusExpression. Parameters:. reference_genome – The reference genome being used.; name (str) – The label to show on y",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:9934,continuous,continuous,9934,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"vided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12206,configurat,configuration,12206,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"w allele index to; the old allele index. Its length is the same as the modified alleles; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out.; Using f; The f argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; Fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23360,update,update,23360,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"w_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12769,release,release,12769,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['release'],['release']
Deployability,"will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:19950,pipeline,pipeline,19950,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['pipeline'],['pipeline']
Deployability,"with kinship lower than min_kinship are excluded; from the results.; statistics (str) – Set of statistics to compute.; If 'kin', only estimate the kinship statistic.; If 'kin2', estimate the above and IBD2.; If 'kin20', estimate the above and IBD0.; If 'all', estimate the above and IBD1.; block_size (int, optional) – Block size of block matrices used in the algorithm.; Default given by BlockMatrix.default_block_size().; include_self_kinship (bool) – If True, include entries for an individual’s estimated kinship with; themselves. Defaults to False. Returns:; Table – A Table mapping pairs of samples to their pair-wise statistics. hail.methods.simulate_random_mating(mt, n_rounds=1, generation_size_multiplier=1.0, keep_founders=True)[source]; Simulate random diploid mating to produce new individuals. Parameters:. mt; n_rounds (int) – Number of rounds of mating.; generation_size_multiplier (float) – Ratio of number of offspring to current population for each round of mating.; keep_founders :obj:`bool` – If true, keep all founders and intermediate generations in the final sample list. If; false, keep only offspring in the last generation. Returns:; MatrixTable. [1]; Purcell, Shaun et al. “PLINK: a tool set for whole-genome association and; population-based linkage analyses.” American journal of human genetics; vol. 81,3 (2007):; 559-75. doi:10.1086/519795. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838/. [2]; Manichaikul, Ani et al. “Robust relationship inference in genome-wide; association studies.” Bioinformatics (Oxford, England) vol. 26,22 (2010):; 2867-73. doi:10.1093/bioinformatics/btq559. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/. [3]; Conomos, Matthew P et al. “Model-free Estimation of Recent Genetic; Relatedness.” American journal of human genetics vol. 98,1 (2016):; 127-48. doi:10.1016/j.ajhg.2015.11.022. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:23253,update,updated,23253,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['update'],['updated']
Deployability,"wnload the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:3366,install,installed,3366,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installed']
Deployability,"wnsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:47829,update,updated,47829,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['update'],['updated']
Deployability,"ws.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.M",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:37312,pipeline,pipelines,37312,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"w¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:1617,patch,patches,1617,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['patch'],['patches']
Deployability,"x_chr_ploidy_from_interval_coverage. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chr_ploidy_from_interval_coverage. View page source. hail.vds.impute_sex_chr_ploidy_from_interval_coverage. hail.vds.impute_sex_chr_ploidy_from_interval_coverage(mt, normalization_contig)[source]; Impute sex chromosome ploidy from a precomputed interval coverage MatrixTable.; The input MatrixTable must have the following row fields:. interval (interval): Genomic interval of interest.; interval_size (int32): Size of interval, in bases. And the following entry fields:. sum_dp (int64): Sum of depth values by base across the interval. Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. mt (MatrixTable) – Interval-by-sample MatrixTable with sum of depth values across the interval.; normalization_contig (str) – Autosomal contig for depth comparison. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html:2002,update,updated,2002,docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html,1,['update'],['updated']
Deployability,"xploded table.; Missing arrays or sets are treated as empty.; Currently, the name argument may not be used if field is not a; top-level field of the table (e.g. name may be used with ht.foo; but not ht.foo.bar). Parameters:. field (str or Expression) – Top-level field name or expression.; name (str or None) – If not None, rename the exploded field to name. Returns:; Table. export(output, types_file=None, header=True, parallel=None, delimiter='\t')[source]; Export to a text file.; Examples; Export to a tab-separated file:; >>> table1.export('output/table1.tsv.bgz'). Note; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python’s gzip.open; and R’s read.table.; Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use flatten() first. Warning; Do not export to a path that is being read from in the same pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:23036,pipeline,pipeline,23036,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability,"xporting missing genotypes without trailing; fields. Bug fixes. (#5306) Fix; ReferenceGenome.add_sequence causing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; opti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100809,pipeline,pipelines,100809,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"y: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. You can count the rows of a table. [6]:. users.count(). [6]:. 943. You can access fields of tables with the Python attribute notation table.field, or with index notation table['field']. The latter is useful when the field names are not valid Python identifiers (if a field name includes a space, for example). [7]:. users.occupation.describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. [8]:. users['occupation'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. users.occupation and users['occupation'] are Hail Expressions; Lets peak at their using show. Notice that the key is shown as well!. [9]:. users.occupation.show(). idoccupationint32str; 1""technician""; 2""other""; 3""writer""; 4""technician""; 5""other""; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. Exercise; The movie dataset has two other tables: movies.ht and ratings.ht. Load these tables and have a quick look around. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:4956,update,updated,4956,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['update'],['updated']
Deployability,"ype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162138,update,updated,162138,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['updated']
Deployability,"zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71938,configurat,configuration,71938,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"| str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. union(s)[source]; Return the union of the set and set s.; Examples; >>> hl.eval(s1.union(s2)); {1, 2, 3, 5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.SetExpression.html:14910,update,updated,14910,docs/0.2/hail.expr.SetExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html,1,['update'],['updated']
Deployability,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:10176,update,updated,10176,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,6,['update'],['updated']
Deployability,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:6621,update,updated,6621,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['update'],['updated']
Deployability,"}]"".format(; missing_sex_count, missing_sex_values; ); ). return Pedigree(trios). @property; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """"""; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father and mother. :rtype: list of :class:`.Trio`; """"""; return list(filter(lambda t: t.is_complete(), self.trios)). [docs] @typecheck_method(samples=sequenceof(nullable(str))); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **Notes**. For any trio, the following steps will be applied:. - If the proband is not in the list of samples provided, the trio is removed.; - If the father is not in the list of samples provided, `pat_id` is set to ``None``.; - If the mother is not in the list of samples provided, `mat_id` is set to ``None``. Parameters; ----------; samples: :obj:`list` [:obj:`str`]; Sample IDs to keep. Returns; -------; :class:`.Pedigree`; """"""; sample_set = set(samples). filtered_trios = []; for trio in self._trios:; restricted_trio = trio._restrict_to(sample_set); if restricted_trio is not None:; filtered_trios.append(restricted_trio). return Pedigree(filtered_trios). [docs] @typecheck_method(path=str); def write(self, path):; """"""Write a .fam file to the given path. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). **Notes**. This method writes a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. .. caution::. Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use :func:`~.import_fam` to; manipulate this information. :param path: output path; :type path: str; """""". lines = [t._to_fam_file_line() for t in self._trios]. with Env.fs().open(path, mode=""w"") as file:; for line in lines:; file.write(line + ""\n""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:8020,update,updated,8020,docs/0.2/_modules/hail/genetics/pedigree.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html,2,['update'],['updated']
Deployability,"’s filter the data to 2007 for our first experiments. [10]:. gp_2007 = gp.filter(gp.year == 2007). If we want to see how many countries from each continent we have, we can use geom_bar, which just takes in an x aesthetic and then implicitly counts how many values of each x there are. [11]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(). [11]:. To make it a little prettier, let’s color per continent as well. We use fill to specify color of shapes (as opposed to color for points and lines. color on a bar chart sets the color of the bar outline.). [12]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent)). [12]:. Maybe we instead want to see not the number of countries per continent, but the number of people living on each continent. We can do this with geom_bar as well by specifying a weight. [13]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent, weight=gp_2007.pop)). [13]:. Histograms are similar to bar plots, except they break a continuous x axis into bins. Let’s import the iris dataset for this. [14]:. iris = hl.Table.from_pandas(plotly.data.iris()); iris.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sepal_length': float64; 'sepal_width': float64; 'petal_length': float64; 'petal_width': float64; 'species': str; 'species_id': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s make a histogram:. [15]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(). [15]:. By default histogram plots groups stacked on top of each other, which is not always easy to interpret. We can specify the position argument to histogram to get different behavior. ""dodge"" puts the bars next to each other:. [16]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""dodge""). [16]:. And ""identity"" plots them over each other. It he",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:5923,continuous,continuous,5923,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['continuous'],['continuous']
Deployability,". class hail.genetics.Pedigree[source]; Class containing a list of trios, with extra functionality. Parameters:; trios (list of Trio) – list of trio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. complete_trios; List of trio objects that have a defined father and mother. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a PLINK .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]; List of trio objects that have a defined father and mother. Return type:; list of Trio. filter_to(samples)[source]; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, pat_id is set to None.; If the mother is not in the list of samples provided, mat_id is set to None. Parameters:; samples (list [str]) – Sample IDs to keep. Returns:; Pedigree. classmethod read(fam_path, delimiter='\\s+')[source]; Read a PLINK .fam file and return a pedigree object.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'). Notes; See PLINK .fam file for; the required format. Parameters:. fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:; Pedigree. property trios; List of trio objects in this pedigree. Return type:; list of Trio. write(path)[source]; Write a .fam file to the given path.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use import_fam() to; manipulate this information. Parameters:; path (str) – output path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html:2658,update,updated,2658,docs/0.2/genetics/hail.genetics.Pedigree.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html,1,['update'],['updated']
Deployability,"﻿. . Annotation Database — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/annotationdb.html:581,pipeline,pipelines,581,docs/0.1/annotationdb.html,https://hail.is,https://hail.is/docs/0.1/annotationdb.html,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:773,continuous,continuous,773,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,"['continuous', 'integrat']","['continuous', 'integration']"
Deployability,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:900,configurat,configuration,900,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['configurat'],['configuration']
Deployability,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:676,install,installed,676,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['install'],['installed']
Deployability,"﻿. Clumping GWAS Results — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Introduction; Hail GWAS Script; Docker Image; Batch Script; Functions; Control Code. Synopsis. Random Forest. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:285,Configurat,Configuration,285,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['Configurat'],['Configuration']
Deployability,"﻿. Hail | ; Amazon Web Services. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Amazon Web Services. View page source. Amazon Web Services; While Hail does not have any built-in tools for working with Amazon EMR, there are two approaches maintained by third parties:. AWS maintains a Hail on AWS quickstart.; The Avillach Lab at Harvard Medical School maintains an open-source tool. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/amazon_web_services.html:891,update,updated,891,docs/0.2/cloud/amazon_web_services.html,https://hail.is,https://hail.is/docs/0.2/cloud/amazon_web_services.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:718,pipeline,pipelines,718,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/annotation.html:1703,update,updated,1703,docs/0.2/guides/annotation.html,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Cheat Sheets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Cheat Sheets. View page source. Cheat Sheets. Note; Hail’s cheat sheets are relatively new. We welcome suggestions; for additional cheatsheets, as well as feedback about our documentation. If; you’d like to add a cheatsheet to the documentation, make a pull request!. Hail Tables Cheat Sheet; Hail MatrixTables Cheat Sheet. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cheatsheets.html:790,update,updated,790,docs/0.2/cheatsheets.html,https://hail.is,https://hail.is/docs/0.2/cheatsheets.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:953,configurat,configuration,953,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['configurat'],['configuration']
Deployability,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:884,configurat,configuration,884,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['configurat'],['configuration']
Deployability,"﻿. Hail | ; Databricks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Databricks. View page source. Databricks; The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_lo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/databricks.html:754,install,installed,754,docs/0.2/cloud/databricks.html,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html,1,['install'],['installed']
Deployability,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets.html:983,pipeline,pipeline,983,docs/0.2/datasets.html,https://hail.is,https://hail.is/docs/0.2/datasets.html,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:806,install,installation,806,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,3,['install'],"['install', 'installation', 'installed']"
Deployability,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:971,pipeline,pipelines,971,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['pipeline'],['pipelines']
Deployability,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:804,configurat,configuration,804,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['configurat'],['configuration']
Deployability,"﻿. Hail | ; Google Cloud Platform. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:906,install,installations,906,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['installations']
Deployability,"﻿. Hail | ; Hadoop Glob Patterns. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources; Hadoop Glob Patterns. View page source. Hadoop Glob Patterns. Pattern; Description. ?. Matches any single character. *. Matches zero or more characters. [abc]. Matches a single character from character set {a,b,c}. [a-b]. Matches a single character from the character range {a…b}. Note that the “^”; character must occur immediately to the right of the opening bracket. [^a]. Matches a single character that is not from character set or range {a}. Note that; the “^”character must occur immediately to the right of the opening bracket. \c. Removes (escapes) any special meaning of character c. {ab,cd}. Matches a string from the string set {ab, cd}. {ab,c{de, fh}}. Matches a string from the string set {ab, cde, cfh}. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hadoop_glob_patterns.html:1232,update,updated,1232,docs/0.2/hadoop_glob_patterns.html,https://hail.is,https://hail.is/docs/0.2/hadoop_glob_patterns.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/index.html:717,install,installation,717,docs/0.2/index.html,https://hail.is,https://hail.is/docs/0.2/index.html,2,['install'],['installation']
Deployability,"﻿. Hail | ; Hail Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview. View page source. Hail Overview; Hail is a library for analyzing structured tabular and matrix data. Hail; contains a collection of primitives for operating on data in parallel, as well; as a suite of functionality for processing genetic data.; This section of Hail’s documentation contains detailed explanations of Hail’s; architecture, primitives, classes, and libraries. Expressions; What is an Expression?; Boolean Logic; Conditional Expressions; Missingness; Functions. Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables; Keys; Referencing Fields; Import; Common Operations; Aggregation; Group-By; Joins; Interacting with Matrix Tables Locally. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/index.html:1256,update,updated,1256,docs/0.2/overview/index.html,https://hail.is,https://hail.is/docs/0.2/overview/index.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials-landing.html:900,install,install,900,docs/0.2/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html,2,"['install', 'update']","['install', 'updated']"
Deployability,"﻿. Hail | ; Hail on the Cloud. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud. View page source. Hail on the Cloud; Public clouds are a natural place to run Hail, offering the ability to run on-demand workloads with; high elasticity. Microsoft Azure, Google Cloud Platform, Databricks and Amazon Web Services make it; possible to rent Spark clusters with thousands of cores on-demand, providing for the elastic compute; requirements of scientific research without an up-front capital investment in hardware. General Advice; Start Small; Estimating time; Estimating cost. Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail_on_the_cloud.html:1421,update,updated,1421,docs/0.2/hail_on_the_cloud.html,https://hail.is,https://hail.is/docs/0.2/hail_on_the_cloud.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Install Hail on GNU/Linux. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on GNU/Linux. View page source. Install Hail on GNU/Linux. Install Java 11.; Install Python 3.9 or later.; Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; Install BLAS and LAPACK.; Install Hail using pip. On a recent Debian-like system, the following should suffice:; apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. Now let’s take Hail for a spin!. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/linux.html:256,install,installation,256,docs/0.2/install/linux.html,https://hail.is,https://hail.is/docs/0.2/install/linux.html,4,"['install', 'update']","['install', 'installation', 'updated']"
Deployability,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/macosx.html:290,install,installation,290,docs/0.2/install/macosx.html,https://hail.is,https://hail.is/docs/0.2/install/macosx.html,8,"['install', 'update']","['install', 'install-completion', 'installation', 'updated']"
Deployability,"﻿. Hail | ; Install Hail on a Spark Cluster. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like sy",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:274,install,installation,274,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['installation']
Deployability,"﻿. Hail | ; Installing Hail. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail. View page source. Installing Hail. Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started.html:246,install,installation,246,docs/0.2/getting_started.html,https://hail.is,https://hail.is/docs/0.2/getting_started.html,3,"['install', 'update']","['installation', 'updated']"
Deployability,"﻿. Hail | ; Libraries. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Libraries. View page source. Libraries; This pages lists any external libraries we are aware of that are built on top of Hail. These libraries are not developed by the Hail team so we cannot necessarily answer; questions about them, but they may provide useful functions not included in base Hail. gnomad (Hail Utilities for gnomAD); This repo contains a number of Hail utility functions and scripts for the gnomAD project and the Translational Genomics Group.; Install with pip install gnomad.; More info can be found in the documentation or on the PyPI project page. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/libraries.html:922,install,install,922,docs/0.2/libraries.html,https://hail.is,https://hail.is/docs/0.2/libraries.html,2,"['install', 'update']","['install', 'updated']"
Deployability,"﻿. Hail | ; LinearMixedModel. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats; LinearMixedModel. View page source. LinearMixedModel. class hail.stats.LinearMixedModel[source]; Class representing a linear mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. Attributes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/stats/hail.stats.LinearMixedModel.html:946,update,updated,946,docs/0.2/stats/hail.stats.LinearMixedModel.html,https://hail.is,https://hail.is/docs/0.2/stats/hail.stats.LinearMixedModel.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Microsoft Azure. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list run",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:676,install,installations,676,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['install'],['installations']
Deployability,"﻿. Hail | ; Other Resources. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources. View page source. Other Resources. Hadoop Glob Patterns. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/other_resources.html:565,update,updated,565,docs/0.2/other_resources.html,https://hail.is,https://hail.is/docs/0.2/other_resources.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API. View page source. Python API; This is the Python API documentation for all Hail Python libraries including Query (hail), a cloud-agnostic; file system implementation (hailtop.fs), and Batch (hailtop.batch). hail; hailtop.fs; hailtop.batch. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/root_api.html:749,update,updated,749,docs/0.2/root_api.html,https://hail.is,https://hail.is/docs/0.2/root_api.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/scans.html:765,rolling,rolling,765,docs/0.2/scans.html,https://hail.is,https://hail.is/docs/0.2/scans.html,1,['rolling'],['rolling']
Deployability,"﻿. Hail | ; Search. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Search. Please activate JavaScript to enable the search functionality.; . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/search.html:516,update,updated,516,docs/0.2/search.html,https://hail.is,https://hail.is/docs/0.2/search.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Use Hail on Azure HDInsight. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/azure.html:270,install,installation,270,docs/0.2/install/azure.html,https://hail.is,https://hail.is/docs/0.2/install/azure.html,2,['install'],"['install', 'installation']"
Deployability,"﻿. Hail | ; Use Hail on Google Dataproc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/dataproc.html:270,install,installation,270,docs/0.2/install/dataproc.html,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html,2,['install'],"['install', 'installation']"
Deployability,"﻿. Hail | ; VDSMetadata. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VDSMetadata. View page source. VDSMetadata. class hail.vds.combiner.VDSMetadata[source]; The path to a Variant Dataset and the number of samples within. Parameters:. path (str) – Path to the variant dataset.; n_samples (int) – Number of samples contained within the Variant Dataset at path. Attributes. n_samples; Alias for field number 1. path; Alias for field number 0. Methods. __add__(value, /); Return self+value. __class_getitem__(); See PEP 585. __contains__(key, /); Return key in self. __eq__(value, /); Return self==value. __ge__(value, /); Return self>=value. __getitem__(key, /); Return self[key]. __getnewargs__(); Return self as a plain tuple. Used by copy and pickle. __gt__(value, /); Return self>value. __iter__(); Implement iter(self). __le__(value, /); Return self<=value. __len__(); Return len(self). __lt__(value, /); Return self<value. __mul__(value, /); Return self*value. __ne__(value, /); Return self!=value. __rmul__(value, /); Return value*self. count(value, /); Return number of occurrences of value. index(value, start=0, stop=9223372036854775807, /); Return first index of value.; Raises ValueError if the value is not present. n_samples; Alias for field number 1. path; Alias for field number 0. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VDSMetadata.html:1965,update,updated,1965,docs/0.2/vds/hail.vds.combiner.VDSMetadata.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VDSMetadata.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/try.html:252,install,installation,252,docs/0.2/install/try.html,https://hail.is,https://hail.is/docs/0.2/install/try.html,2,['install'],"['install', 'installation']"
Deployability,"﻿. Hail | ; genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics. View page source. genetics. Classes. AlleleType; An enumeration for allele type. Call; An object that represents an individual's call at a genomic locus. Locus; An object that represents a location in the genome. Pedigree; Class containing a list of trios, with extra functionality. ReferenceGenome; An object that represents a reference genome. Trio; Class containing information about nuclear family relatedness and sex. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/index.html:1131,update,updated,1131,docs/0.2/genetics/index.html,https://hail.is,https://hail.is/docs/0.2/genetics/index.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.context. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.context. Source code for hail.context; import os; import sys; import warnings; from contextlib import contextmanager; from random import Random; from types import TracebackType; from typing import Dict, List, Optional, Tuple, Type, Union; from urllib.parse import urlparse, urlunparse. from pyspark import SparkContext. import hail; from hail.backend import Backend; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import dictof, enumeration, nullable, oneof, sequenceof, sized_tupleof, typecheck, typecheck_method; from hail.utils import get_env_or_default; from hail.utils.java import BackendType, Env, choose_backend, warning; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:150,Install,Installation,150,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.datasets. Source code for hail.experimental.datasets; from typing import Optional, Union. import hail as hl; from hail.matrixtable import MatrixTable; from hail.table import Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:164,Install,Installation,164,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.db. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.db. Source code for hail.experimental.db; import warnings; from typing import ClassVar, Iterable, List, Optional, Set, Tuple, Union. import hail as hl; from hailtop.utils import external_requests_client_session, retry_response_returning_functions. from ..expr import StructExpression; from ..matrixtable import MatrixTable, matrix_table_type; from ..table import Table, table_type; from ..typecheck import oneof, typecheck_method; from ..utils.java import Env, info; from .datasets_metadata import get_datasets_metadata; from .lens import MatrixRows, TableRows. class DatasetVersion:; """""":class:`DatasetVersion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """""". @staticmethod; def from_json(doc: dict, cloud: str) -> Optional['DatasetVersion']:; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:158,Install,Installation,158,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.export_entries_by_col. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.export_entries_by_col. Source code for hail.experimental.export_entries_by_col; import hail as hl; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck. [docs]@typecheck(; mt=MatrixTable, path=str, batch_size=int, bgzip=bool, header_json_in_file=bool, use_string_key_as_file_name=bool; ); def export_entries_by_col(; mt: MatrixTable,; path: str,; batch_size: int = 256,; bgzip: bool = True,; header_json_in_file: bool = True,; use_string_key_as_file_name: bool = False,; ):; """"""Export entries of the `mt` by column as separate text files. Examples; --------; >>> range_mt = hl.utils.range_matrix_table(10, 10); >>> range_mt = range_mt.annotate_entries(x = hl.rand_unif(0, 1)); >>> hl.experimental.export_entries_by_col(range_mt, 'output/cols_files'). Notes; -----; This function writes a directory with one file per column in `mt`. The; files contain one tab-separated field (with header) for each row field; and entry field in `mt`. The column fields of `mt` are written as JSON; in the first line of each file, prefixed with a ``#``. The above will produce a directory at ``output/cols_files`` with the; following files:. .. code-block:: text. $ ls -l output/cols_files; total 80; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 index.tsv; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-00.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-01.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-02.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-03.tsv.bgz; -rw-r--r-- 1 hail-dev ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html:177,Install,Installation,177,docs/0.2/_modules/hail/experimental/export_entries_by_col.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.expressions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.expressions. Source code for hail.experimental.expressions; import hail as hl; from hail.expr.expressions import analyze, expr_any; from hail.expr.table_type import ttable; from hail.expr.types import hail_type; from hail.typecheck import nullable, typecheck. [docs]@typecheck(expr=expr_any, path=str, overwrite=bool); def write_expression(expr, path, overwrite=False):; """"""Write an Expression. In the same vein as Python's pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False). Example; -------; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.aggregate(hl.agg.mean(ht.x), _localize=False); >>> mean_norm; >>> hl.eval(mean_norm); >>> hl.experimental.write_expression(mean_norm, 'output/expression.he'). Parameters; ----------. expr : :class:`~.Expression`; Expression to write.; path : :class:`str`; Path to which to write expression.; Suggested extension: .he (hail expression).; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination. Returns; -------; None; """"""; source = expr._indices.source; if source is not None:; analyze('write_expression.expr', expr, source._global_indices); source = source.select_globals(__expr=expr); expr = source.index_globals().__expr; hl.utils.range_table(1).filter(False).key_by().drop('idx').annotate_globals(expr=expr).write(; path, overwrite=overwrite; ). [docs]@typecheck(path=str, _assert_type=nullable(hail_type)); def read_expression(path, _assert_type=None):; """"""R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html:167,Install,Installation,167,docs/0.2/_modules/hail/experimental/expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.filtering_allele_frequency. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.filtering_allele_frequency. Source code for hail.experimental.filtering_allele_frequency; from hail.expr.expressions import Float64Expression, expr_float64, expr_int32; from hail.expr.functions import _func; from hail.expr.types import tfloat64; from hail.typecheck import typecheck. [docs]@typecheck(ac=expr_int32, an=expr_int32, ci=expr_float64); def filtering_allele_frequency(ac, an, ci) -> Float64Expression:; """"""; Computes a filtering allele frequency (described below); for `ac` and `an` with confidence `ci`. The filtering allele frequency is the highest true population allele frequency; for which the upper bound of the `ci` (confidence interval) of allele count; under a Poisson distribution is still less than the variant's observed; `ac` (allele count) in the reference sample, given an `an` (allele number). This function defines a ""filtering AF"" that represents; the threshold disease-specific ""maximum credible AF"" at or below which; the disease could not plausibly be caused by that variant. A variant with; a filtering AF >= the maximum credible AF for the disease under consideration; should be filtered, while a variant with a filtering AF below the maximum; credible remains a candidate. This filtering AF is not disease-specific:; it can be applied to any disease of interest by comparing with a; user-defined disease-specific maximum credible AF. For more details, see: `Whiffin et al., 2017 <https://www.nature.com/articles/gim201726>`__. Parameters; ----------; ac : int or :class:`.Expression` of type :p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html:182,Install,Installation,182,docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.full_outer_join_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.full_outer_join_mt. Source code for hail.experimental.full_outer_join_mt; import hail as hl; from hail.matrixtable import MatrixTable. [docs]def full_outer_join_mt(left: MatrixTable, right: MatrixTable) -> MatrixTable:; """"""Performs a full outer join on `left` and `right`. Replaces row, column, and entry fields with the following:. - `left_row` / `right_row`: structs of row fields from left and right.; - `left_col` / `right_col`: structs of column fields from left and right.; - `left_entry` / `right_entry`: structs of entry fields from left and right. Examples; --------. The following creates and joins two random datasets with disjoint sample ids; but non-disjoint variant sets. We use :func:`.or_else` to attempt to find a; non-missing genotype. If neither genotype is non-missing, then the genotype; is set to missing. In particular, note that Samples `2` and `3` have missing; genotypes for loci 1:1 and 1:2 because those loci are not present in `mt2`; and these samples are not present in `mt1`. >>> hl.reset_global_randomness(); >>> mt1 = hl.balding_nichols_model(1, 2, 3); >>> mt2 = hl.balding_nichols_model(1, 2, 3); >>> mt2 = mt2.key_rows_by(locus=hl.locus(mt2.locus.contig,; ... mt2.locus.position+2),; ... alleles=mt2.alleles); >>> mt2 = mt2.key_cols_by(sample_idx=mt2.sample_idx+2); >>> mt1.show(); +---------------+------------+------+------+; | locus | alleles | 0.GT | 1.GT |; +---------------+------------+------+------+; | locus<GRCh37> | array<str> | call | call |; +---------------+------------+------+------+; | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html:174,Install,Installation,174,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.import_gtf. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.import_gtf. Source code for hail.experimental.import_gtf; import functools; import operator. import hail as hl; from hail.genetics.reference_genome import reference_genome_type; from hail.table import Table; from hail.typecheck import nullable, sequenceof, typecheck; from hail.utils import new_temp_file; from hail.utils.java import info. [docs]@typecheck(; path=str,; reference_genome=nullable(reference_genome_type),; skip_invalid_contigs=bool,; min_partitions=nullable(int),; force_bgz=bool,; force=bool,; ); def import_gtf(; path, reference_genome=None, skip_invalid_contigs=False, min_partitions=None, force_bgz=False, force=False; ) -> Table:; """"""Import a GTF file. The GTF file format is identical to the GFF version 2 file format,; and so this function can be used to import GFF version 2 files as; well. See https://www.ensembl.org/info/website/upload/gff.html for more; details on the GTF/GFF2 file format. The :class:`.Table` returned by this function will be keyed by the; ``interval`` row field and will include the following row fields:. .. code-block:: text. 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; ----. This function will return an ``interval`` field of type :class:`.tinterval`; constructed from the ``seqname``, ``start``, and ``end`` fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file. If the ``refer",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:166,Install,Installation,166,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.ld_score_regression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ld_score_regression. Source code for hail.experimental.ld_score_regression; import hail as hl; from hail.expr.expressions import analyze, expr_float64, expr_numeric; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; weight_expr=expr_float64,; ld_score_expr=expr_numeric,; chi_sq_exprs=oneof(expr_float64, sequenceof(expr_float64)),; n_samples_exprs=oneof(expr_numeric, sequenceof(expr_numeric)),; n_blocks=int,; two_step_threshold=int,; n_reference_panel_variants=nullable(int),; ); def ld_score_regression(; weight_expr,; ld_score_expr,; chi_sq_exprs,; n_samples_exprs,; n_blocks=200,; two_step_threshold=30,; n_reference_panel_variants=None,; ) -> Table:; r""""""Estimate SNP-heritability and level of confounding biases from genome-wide association study; (GWAS) summary statistics. Given a set or multiple sets of GWAS summary statistics, :func:`.ld_score_regression` estimates the heritability; of a trait or set of traits and the level of confounding biases present in; the underlying studies by regressing chi-squared statistics on LD scores,; leveraging the model:. .. math::. \mathrm{E}[\chi_j^2] = 1 + Na + \frac{Nh_g^2}{M}l_j. * :math:`\mathrm{E}[\chi_j^2]` is the expected chi-squared statistic; for variant :math:`j` resulting from a test of association between; variant :math:`j` and a trait.; * :math:`l_j = \sum_{k} r_{jk}^2` is the LD score of variant; :math:`j`, calculated as the sum of squared correlat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html:175,Install,Installation,175,docs/0.2/_modules/hail/experimental/ld_score_regression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.ldscore. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ldscore. Source code for hail.experimental.ldscore; import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, expr_numeric; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.bi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:163,Install,Installation,163,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.ldscsim. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ldscsim. Source code for hail.experimental.ldscsim; #!/usr/bin/env python3; # -*- coding: utf-8 -*-; """"""; Simulation framework for testing LDSC. Models for SNP effects:; - Infinitesimal (can simulate n correlated traits); - Spike & slab (can simulate up to 2 correlated traits); - Annotation-informed. Features:; - Field aggregation tools for annotation-informed model and; population stratification with many covariates.; - Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; - Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. @author: nbaya; """""". import numpy as np; import pandas as pd; from scipy import stats. import hail as hl; from hail.expr.expressions import expr_array, expr_call, expr_float64, expr_int32; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils.java import Env. [docs]@typecheck(; mt=MatrixTable,; genotype=oneof(expr_int32, expr_float64, expr_call),; h2=(oneof(float, int, list, np.ndarray)),; pi=nullable(oneof(float, int, list, np.ndarray)),; rg=nullable(oneof(float, int, list, np.ndarray)),; annot=nullable(oneof(expr_float64, expr_int32)),; popstrat=nullable(oneof(expr_int32, expr_float64)),; popstrat_var=nullable(oneof(float, int)),; exact_h2=bool,; ); def simulate_phenotypes(; mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:163,Install,Installation,163,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.loop. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.loop. Source code for hail.experimental.loop; from typing import Callable. from hail import ir; from hail.expr.expressions import construct_expr, construct_variable, expr_any, to_expr, unify_all; from hail.expr.types import hail_type; from hail.typecheck import anytype, typecheck; from hail.utils.java import Env. [docs]@typecheck(f=anytype, typ=hail_type, args=expr_any); def loop(f: Callable, typ, *args):; r""""""Define and call a tail-recursive function with given arguments. Notes; -----; The argument `f` must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. .. math::. f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}. we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)). Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called. This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let's; consider two different recursive definitions for the triangle function; :math:`f(x) = 0 + 1 + \dots + x`:. >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1). >>> def triangle2(x, total):; ... if x == 0:; ... return total; ... return triangle2(x - 1, total + x). The first function definition, `triangle1`, will call itself and then add x.; This is an example of a n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:160,Install,Installation,160,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.pca. Source code for hail.experimental.pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:159,Install,Installation,159,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.phase_by_transmission. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.phase_by_transmission. Source code for hail.experimental.phase_by_transmission; from typing import List. import hail as hl; from hail.expr.expressions import expr_array, expr_call, expr_locus, expr_str; from hail.matrixtable import MatrixTable; from hail.typecheck import sequenceof, typecheck. [docs]@typecheck(; locus=expr_locus(),; alleles=expr_array(expr_str),; proband_call=expr_call,; father_call=expr_call,; mother_call=expr_call,; ); def phase_by_transmission(; locus: hl.expr.LocusExpression,; alleles: hl.expr.ArrayExpression,; proband_call: hl.expr.CallExpression,; father_call: hl.expr.CallExpression,; mother_call: hl.expr.CallExpression,; ) -> hl.expr.ArrayExpression:; """"""Phases genotype calls in a trio based allele transmission. Notes; -----; In the phased calls returned, the order is as follows:; - Proband: father_allele | mother_allele; - Parents: transmitted_allele | untransmitted_allele. Phasing of sex chromosomes:; - Sex chromosomes of male individuals should be haploid to be phased correctly.; - If `proband_call` is diploid on non-par regions of the sex chromosomes, it is assumed to be female. Returns `NA` when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html:177,Install,Installation,177,docs/0.2/_modules/hail/experimental/phase_by_transmission.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.plots. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.plots. Source code for hail.experimental.plots; import json. import numpy as np; import pandas as pd; from bokeh.layouts import gridplot; from bokeh.models import ColumnDataSource, Div, HoverTool, TabPanel, Tabs, Title; from bokeh.palettes import Spectral8; from bokeh.plotting import figure; from bokeh.transform import factor_cmap. import hail as hl; from hail.typecheck import typecheck; from hail.utils.hadoop_utils import hadoop_ls, hadoop_open; from hail.utils.java import warning. [docs]def plot_roc_curve(ht, scores, tp_label='tp', fp_label='fp', colors=None, title='ROC Curve', hover_mode='mouse'):; """"""Create ROC curve from Hail Table. One or more `score` fields must be provided, which are assessed against `tp_label` and `fp_label` as truth data. High scores should correspond to true positives. Parameters; ----------; ht : :class:`.Table`; Table with required data; scores : :class:`str` or :obj:`list` of :obj:`.str`; Top-level location of scores in ht against which to generate PR curves.; tp_label : :class:`str`; Top-level location of true positives in ht.; fp_label : :class:`str`; Top-level location of false positives in ht.; colors : :obj:`dict` of :class:`str`; Optional colors to use (score -> desired color).; title : :class:`str`; Title of plot.; hover_mode : :class:`str`; Hover mode; one of 'mouse' (default), 'vline' or 'hline'. Returns; -------; :obj:`tuple` of :class:`bokeh.plotting.figure` and :obj:`list` of :class:`str`; Figure, and list of AUCs corresponding to scores.; """"""; if colors is None:; # Get a palette aut",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html:161,Install,Installation,161,docs/0.2/_modules/hail/experimental/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.tidyr. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.tidyr. Source code for hail.experimental.tidyr; import hail as hl; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(ht=Table, key=str, value=str, fields=str); def gather(ht, key, value, *fields) -> Table:; """"""Collapse fields into key-value pairs. :func:`.gather` mimics the functionality of the `gather()` function found in R's; ``tidyr`` package. This is a way to turn ""wide"" format data into ""long""; format data. Parameters; ----------; ht : :class:`.Table`; A Hail table.; key : :class:`str`; The name of the key field in the gathered table.; value : :class:`str`; The name of the value field in the gathered table.; fields : variable-length args of obj:`str`; Names of fields to gather in ``ht``. Returns; -------; :class:`.Table`; Table with original ``fields`` gathered into ``key`` and ``value`` fields."""""". ht = ht.annotate(_col_val=hl.array([hl.struct(field_name=field, value=ht[field]) for field in fields])); ht = ht.drop(*fields); ht = ht.explode(ht['_col_val']); ht = ht.annotate(**{key: ht['_col_val'][0], value: ht['_col_val'][1]}); ht = ht.drop('_col_val'). ht_tmp = new_temp_file(); ht.write(ht_tmp). return hl.read_table(ht_tmp). [docs]@typecheck(ht=Table, field=str, value=str, key=nullable(oneof(str, sequenceof(str)))); def spread(ht, field, value, key=None) -> Table:; """"""Spread a key-value pair of fields across multiple fields. :func:`.spread` mimics the functionality of the `spread()` function in R's; `tidyr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html:161,Install,Installation,161,docs/0.2/_modules/hail/experimental/tidyr.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.experimental.time. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.time. Source code for hail.experimental.time; import hail as hl; from hail.expr.expressions import expr_int64, expr_str; from hail.expr.functions import _func; from hail.typecheck import typecheck. [docs]@typecheck(format=expr_str, time=expr_int64, zone_id=expr_str); def strftime(format, time, zone_id):; """"""; Convert Unix timestamp to a formatted datetime string. Examples; --------. >>> hl.eval(hl.experimental.strftime(""%Y.%m.%d %H:%M:%S %z"", 1562569201, ""America/New_York"")); '2019.07.08 03:00:01 -04:00'. >>> hl.eval(hl.experimental.strftime(""%A, %B %e, %Y. %r"", 876541523, ""GMT+2"")); 'Saturday, October 11, 1997. 05:45:23 AM'. >>> hl.eval(hl.experimental.strftime(""%A, %B %e, %Y. %r"", 876541523, ""+08:00"")); 'Saturday, October 11, 1997. 11:45:23 AM'. Notes; -----; The following formatting characters are supported in format strings: A a B b D d e F H I j k l M m n p R r S s T t U u V v W Y y z; See documentation here: https://linux.die.net/man/3/strftime. A zone id can take one of three forms. It can be an explicit offset, like ""+01:00"", a relative offset, like ""GMT+2"",; or a IANA timezone database (TZDB) identifier, like ""America/New_York"". Wikipedia maintains a list of TZDB identifiers here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. Currently, the formatter implicitly uses the ""en_US"" locale. Parameters; ----------; format : str or :class:`.Expression` of type :py:data:`.tstr`; The format string describing how to render the time.; time : int of :class:`.Expression` of type :py:data:`.tint64`; A long represent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/time.html:160,Install,Installation,160,docs/0.2/_modules/hail/experimental/time.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/time.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.expr.aggregators.aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.aggregators.aggregators. Source code for hail.expr.aggregators.aggregators; import difflib; from functools import update_wrapper, wraps. import hail as hl; from hail import ir; from hail.expr import (; Aggregation,; ArrayExpression,; BooleanExpression,; DictExpression,; Expression,; ExpressionException,; Float64Expression,; Indices,; Int64Expression,; NDArrayNumericExpression,; NumericExpression,; SetExpression,; StringExpression,; StructExpression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; to_expr,; unify_all,; unify_types,; ); from hail.expr.expressions.typed_expressions import construct_variable; from hail.expr.functions import _quantile_from_cdf, _result_from_raw_cdf, float32, rbind; from hail.expr.types import (; hail_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import TypeChecker, func_spec, identity, nullable, oneof, sequenceof, typecheck, typecheck_method; from hail.utils import wrap_to_list; from hail.utils.java import Env. class AggregableChecker(TypeChecker):; def __init__(self, coercer):; self.coercer = coercer; super(AggregableChecker, self).__init__(). def expects(self):; return self.coercer.expects(). def format(self, arg):; return self.coercer.format(arg). def check(self, x, caller, param):; x = self.coercer.check(x, caller, param); if len(x._ir.search(lambda node: isinstance(nod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:171,Install,Installation,171,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.expr.builders. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.builders. Source code for hail.expr.builders; import hail as hl; from hail import ir; from hail.expr.expressions import (; ExpressionException,; construct_expr,; expr_any,; expr_bool,; expr_str,; unify_types,; unify_types_limited,; ); from hail.typecheck import typecheck_method. class ConditionalBuilder(object):; def __init__(self):; self._ret_type = None; self._cases = []. def _unify_type(self, t):; if self._ret_type is None:; self._ret_type = t; else:; r = unify_types_limited(self._ret_type, t); if not r:; raise TypeError(""'then' expressions must have same type, found '{}' and '{}'"".format(self._ret_type, t)). [docs]class SwitchBuilder(ConditionalBuilder):; """"""Class for generating conditional trees based on value of an expression. Examples; --------. >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.SwitchBuilder.when` or; :meth:`~hail.expr.builders.SwitchBuilder.default` method calls must be the; same type. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.switch`. Parameters; ----------; expr : :class:`.Expression`; Value to match against.; """""". @typecheck_method(base=expr_any); def __init__(self, base):; self._base = base; self._when_missing_case = None; super(SwitchBuilder, self).__init__(). def _finish(self, default):; a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:156,Install,Installation,156,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.expr.expressions.base_expression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.base_expression. Source code for hail.expr.expressions.base_expression; from typing import Any, List, Mapping, Tuple, overload. import numpy as np; import pandas as pd. import hail; import hail as hl; from hail import ir; from hail.expr import expressions; from hail.expr.types import (; HailType,; from_numpy,; is_compound,; is_numeric,; is_setlike,; summary_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import anyfunc, linked_list, nullable, typecheck_method; from hail.utils.java import Env; from hail.utils.linkedlist import LinkedList. from .indices import Aggregation, Indices. class Summary(object):; def __init__(self, type, count, summ_fields, nested, header=None):; self.count = count; self.summ_fields = summ_fields; self.nested = nested; self.type = type; self.header = header. @staticmethod; def pct(x):; return f'{x*100:.2f}%'. @staticmethod; def format(x):; if isinstance(x, float):; return f'{x:.2f}'; else:; return str(x). def __str__(self):; return self._ascii_string(depth=0, prefix=None). def __repr__(self):; return self.__str__(). def _repr_html_(self):; return self._html_string(prefix=None). def _ascii_string(self, depth, prefix):; spacing = ' ' * depth. summary = ''; if self.header:; summary += f'\n{spacing}{self.header}'. if prefix is not None:; summary += f'\n\n{spacing}- {prefix} ({summary_type(self.type)}):'. if len(self.summ_fields) > 0:; max_n_len = max(len(n) for n in self",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:175,Install,Installation,175,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.expr.expressions.expression_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:176,Install,Installation,176,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.expr.expressions.typed_expressions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.typed_expressions. Source code for hail.expr.expressions.typed_expressions; from typing import Dict, Mapping, Optional, Sequence, Union. import numpy as np; from deprecated import deprecated. import hail as hl; from hail import ir; from hail.expr.types import (; HailType,; is_numeric,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; tset,; tstr,; tstream,; tstruct,; ttuple,; ); from hail.typecheck import (; anyfunc,; dictof,; func_spec,; identity,; nullable,; oneof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils.java import Env, warning; from hail.utils.linkedlist import LinkedList; from hail.utils.misc import get_nice_attr_error, get_nice_field_error, wrap_to_list, wrap_to_tuple. from .base_expression import Expression, ExpressionException, to_expr, unify_all, unify_types; from .expression_typecheck import (; coercer_from_dtype,; expr_any,; expr_array,; expr_bool,; expr_dict,; expr_int32,; expr_int64,; expr_interval,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; expr_tuple,; ); from .indices import Aggregation, Indices. [docs]class CollectionExpression(Expression):; """"""Expression of type :class:`.tarray` or :class:`.tset`. >>> a = hl.literal([1, 2, 3, 4, 5]). >>> s3 = hl.literal({'Alice', 'Bob', 'Charlie'}); """""". def _filter_missing_method(self, filter_missing: bool, name: str, ret_type: HailType, *args):; collection = self; if filter_missing:; collection = self.filter(hl.is_defined); return collection._me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:177,Install,Installation,177,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.expr.functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.functions. Source code for hail.expr.functions; import builtins; import functools; import itertools; import operator; from typing import Any, Callable, Iterable, Optional, TypeVar, Union. import numpy as np; import pandas as pd; from deprecated import deprecated. import hail; import hail as hl; from hail import ir; from hail.expr.expressions import (; ArrayExpression,; ArrayNumericExpression,; BooleanExpression,; CallExpression,; DictExpression,; Expression,; ExpressionException,; Float32Expression,; Float64Expression,; Int32Expression,; Int64Expression,; IntervalExpression,; LocusExpression,; NumericExpression,; SetExpression,; StreamExpression,; StringExpression,; StructExpression,; TupleExpression,; apply_expr,; cast_expr,; coercer_from_dtype,; construct_expr,; construct_variable,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_dict,; expr_float32,; expr_float64,; expr_int32,; expr_int64,; expr_interval,; expr_locus,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; expr_stream,; expr_struct,; expr_tuple,; impute_type,; to_expr,; unify_all,; unify_exprs,; unify_types_limited,; ); from hail.expr.types import (; HailType,; hail_type,; is_float32,; is_float64,; is_int32,; is_int64,; is_numeric,; is_primitive,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; trngstate,; tset,; tstr,; tstream,; tstruct,; ttuple,; ); from hail.genetics.allele_type import AlleleType; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typec",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:157,Install,Installation,157,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:153,Install,Installation,153,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:163,Install,Installation,163,docs/0.2/_modules/hail/genetics/allele_type.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:156,Install,Installation,156,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:157,Install,Installation,157,docs/0.2/_modules/hail/genetics/locus.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:160,Install,Installation,160,docs/0.2/_modules/hail/genetics/pedigree.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:168,Install,Installation,168,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:1477,update,updated,1477,docs/0.2/_modules/hail/ggplot/aes.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html,4,"['Configurat', 'Install', 'update']","['Configuration', 'Installation', 'updated']"
Deployability,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:1291,update,updated,1291,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html,4,"['Configurat', 'Install', 'update']","['Configuration', 'Installation', 'updated']"
Deployability,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:156,Install,Installation,156,docs/0.2/_modules/hail/ggplot/facets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:155,Install,Installation,155,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:156,Install,Installation,156,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:156,Install,Installation,156,docs/0.2/_modules/hail/ggplot/labels.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:155,Install,Installation,155,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:161,Install,Installation,161,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.linalg.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.utils.misc. Source code for hail.linalg.utils.misc; import numpy as np. import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, raise_unless_row_indexed; from hail.typecheck import nullable, oneof, typecheck; from hail.utils.java import Env. [docs]@typecheck(a=np.ndarray, radius=oneof(int, float)); def array_windows(a, radius):; """"""Returns start and stop indices for window around each array value. Examples; --------. >>> hl.linalg.utils.array_windows(np.array([1, 2, 4, 4, 6, 8]), 2); (array([0, 0, 1, 1, 2, 4]), array([2, 4, 5, 5, 6, 6])). >>> hl.linalg.utils.array_windows(np.array([-10.0, -2.5, 0.0, 0.0, 1.2, 2.3, 3.0]), 2.5); (array([0, 1, 1, 1, 2, 2, 4]), array([1, 4, 6, 6, 7, 7, 7])). Notes; -----; For an array ``a`` in ascending order, the resulting ``starts`` and ``stops``; arrays have the same length as ``a`` and the property that, for all indices; ``i``, ``[starts[i], stops[i])`` is the maximal range of indices ``j`` such; that ``a[i] - radius <= a[j] <= a[i] + radius``. Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; :meth:`.BlockMatrix.sparsify_row_intervals`. Parameters; ----------; a: :obj:`numpy.ndarray` of signed integer or float values; 1-dimensional array of values, non-decreasing with respect to index.; radius: :obj:`float`; Non-negative radius of window for values. Returns; -------; (:class:`numpy.ndarray` of :obj:`int`, :class:`numpy.ndarray` of :obj:`int`); Tuple of start indices array and stop indices array.; """"""; if radius < 0:;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html:160,Install,Installation,160,docs/0.2/_modules/hail/linalg/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.matrixtable. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.matrixtable. Source code for hail.matrixtable; import itertools; import warnings; from collections import Counter; from typing import Any, Dict, Iterable, List, Optional, Tuple. from deprecated import deprecated. import hail as hl; from hail import ir; from hail.expr.expressions import (; Expression,; ExpressionException,; Indices,; StructExpression,; TupleExpression,; analyze,; construct_expr,; construct_reference,; expr_any,; expr_bool,; expr_struct,; extract_refs_by_indices,; unify_all,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.types import tarray, tset, types_match; from hail.table import ExprContainer, Table, TableIndexKeyError; from hail.typecheck import (; anyfunc,; anytype,; dictof,; enumeration,; lazy,; nullable,; numeric,; oneof,; sequenceof,; typecheck,; typecheck_method,; ); from hail.utils import deduplicate, default_handler, storage_level; from hail.utils.java import Env, info, warning; from hail.utils.misc import check_annotate_exprs, get_key_by_exprs, get_select_exprs, process_joins, wrap_to_tuple. [docs]class GroupedMatrixTable(ExprContainer):; """"""Matrix table grouped by row or column that can be aggregated into a new matrix table."""""". def __init__(; self,; parent: 'MatrixTable',; row_keys=None,; computed_row_key=None,; col_keys=None,; computed_col_key=None,; entry_fields=None,; row_fields=None,; col_fields=None,; partitions=None,; ):; super(GroupedMatrixTable, self).__init__(); self._parent = parent; self._copy_fields_from(parent); self._row_keys = row_keys; self._computed_row_key = computed_row_key; self._c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:154,Install,Installation,154,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:165,Install,Installation,165,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:156,Install,Installation,156,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:155,Install,Installation,155,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:154,Install,Installation,154,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:153,Install,Installation,153,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.relatedness.identity_by_descent. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.identity_by_descent. Source code for hail.methods.relatedness.identity_by_descent; import hail as hl; from hail import ir; from hail.backend.spark_backend import SparkBackend; from hail.expr import analyze; from hail.expr.expressions import expr_float64; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str; from hail.table import Table; from hail.typecheck import nullable, numeric, typecheck; from hail.utils.java import Env. [docs]@typecheck(dataset=MatrixTable, maf=nullable(expr_float64), bounded=bool, min=nullable(numeric), max=nullable(numeric)); def identity_by_descent(dataset, maf=None, bounded=True, min=None, max=None) -> Table:; """"""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:182,Install,Installation,182,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.relatedness.king. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.king. Source code for hail.methods.relatedness.king; import hail as hl; from hail.expr.expressions import expr_call, matrix_table_source; from hail.typecheck import nullable, typecheck; from hail.utils import deduplicate; from hail.utils.java import Env. [docs]@typecheck(call_expr=expr_call, block_size=nullable(int)); def king(call_expr, *, block_size=None):; r""""""Compute relatedness estimates between individuals using a KING variant. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate the kinship coefficient for every pair of samples. >>> kinship = hl.king(dataset.GT). Notes; -----. The following presentation summarizes the methods section of `Manichaikul,; et. al. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/>`__, but; adopts a more consistent notation for matrices. Let. - :math:`i` and :math:`j` be two individuals in the dataset. - :math:`N^{Aa}_{i}` be the number of heterozygote genotypes for individual; :math:`i`. - :math:`N^{Aa,Aa}_{i,j}` be the number of variants at which a pair of; individuals both have heterozygote genotypes. - :math:`N^{AA,aa}_{i,j}` be the number of variants at which a pair of; individuals have opposing homozygote genotypes. - :math:`S_{i,j}` be the set of single-nucleotide variants for which both; individuals :math:`i` and :math:`j` have a non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; hetero",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:167,Install,Installation,167,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.relatedness.mating_simulation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.mating_simulation. Source code for hail.methods.relatedness.mating_simulation; import hail as hl; from hail.matrixtable import MatrixTable; from hail.typecheck import numeric, typecheck. [docs]@typecheck(mt=MatrixTable, n_rounds=int, generation_size_multiplier=numeric, keep_founders=bool); def simulate_random_mating(mt, n_rounds=1, generation_size_multiplier=1.0, keep_founders=True):; """"""Simulate random diploid mating to produce new individuals. Parameters; ----------; mt; n_rounds : :obj:`int`; Number of rounds of mating.; generation_size_multiplier : :obj:`float`; Ratio of number of offspring to current population for each round of mating.; keep_founders :obj:`bool`; If true, keep all founders and intermediate generations in the final sample list. If; false, keep only offspring in the last generation. Returns; -------; :class:`.MatrixTable`; """"""; if generation_size_multiplier <= 0:; raise ValueError(; f""simulate_random_mating: 'generation_size_multiplier' must be greater than zero: got {generation_size_multiplier}""; ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html:180,Install,Installation,180,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.relatedness.pc_relate. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.pc_relate. Source code for hail.methods.relatedness.pc_relate; from typing import Optional. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.backend.spark_backend import SparkBackend; from hail.expr import (; ArrayNumericExpression,; BooleanExpression,; CallExpression,; Float64Expression,; analyze,; expr_array,; expr_call,; expr_float64,; matrix_table_source,; ); from hail.expr.types import tarray; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import enumeration, nullable, numeric, typecheck; from hail.utils import new_temp_file; from hail.utils.java import Env. from ..pca import _hwe_normalized_blanczos, hwe_normalized_pca. [docs]@typecheck(; call_expr=expr_call,; min_individual_maf=numeric,; k=nullable(int),; scores_expr=nullable(expr_array(expr_float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def pc_relate(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:172,Install,Installation,172,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.methods.statgen. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.statgen. Source code for hail.methods.statgen; import builtins; import itertools; import math; from typing import Callable, Dict, List, Optional, Tuple, Union. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.expr import (; Expression,; ExpressionException,; NDArrayNumericExpression,; StructExpression,; analyze,; expr_any,; expr_call,; expr_float64,; expr_locus,; expr_numeric,; matrix_table_source,; raise_unless_column_indexed,; raise_unless_entry_indexed,; raise_unless_row_indexed,; table_source,; ); from hail.expr.functions import expit; from hail.expr.types import tarray, tbool, tfloat64, tint32, tndarray, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_row_key_variant; from hail.stats import LinearMixedModel; from hail.table import Table; from hail.typecheck import anytype, enumeration, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils import FatalError, new_temp_file, wrap_to_list; from hail.utils.java import Env, info, warning. from ..backend.spark_backend import SparkBackend; from . import pca, relatedness. pc_relate = relatedness.pc_relate; identity_by_descent = relatedness.identity_by_descent; _blanczos_pca = pca._blanczos_pca; _hwe_normalized_blanczos = pca._hwe_normalized_blanczos; _spectral_moments = pca._spectral_moments; _pca_and_moments = pca._pca_and_moments; hwe_normalized_pca = pca.hwe_nor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:158,Install,Installation,158,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.nd.nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.nd.nd. Source code for hail.nd.nd; from functools import reduce. import hail as hl; from hail.expr.expressions import (; Int64Expression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_tuple,; unify_all,; ); from hail.expr.expressions.typed_expressions import NDArrayNumericExpression; from hail.expr.functions import _ndarray; from hail.expr.functions import array as aarray; from hail.expr.types import HailType, tfloat32, tfloat64, tndarray, ttuple; from hail.ir import Apply, NDArrayConcat, NDArrayEigh, NDArrayInv, NDArrayQR, NDArraySVD; from hail.typecheck import nullable, oneof, sequenceof, tupleof, typecheck. tsequenceof_nd = oneof(sequenceof(expr_ndarray()), expr_array(expr_ndarray())); shape_type = oneof(expr_int64, tupleof(expr_int64), expr_tuple()). [docs]def array(input_array, dtype=None):; """"""Construct an :class:`.NDArrayExpression`. Examples; --------. >>> hl.eval(hl.nd.array([1, 2, 3, 4])); array([1, 2, 3, 4], dtype=int32). >>> hl.eval(hl.nd.array([[1, 2, 3], [4, 5, 6]])); array([[1, 2, 3],; [4, 5, 6]], dtype=int32). >>> hl.eval(hl.nd.array(np.identity(3))); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]). >>> hl.eval(hl.nd.array(hl.range(10, 20))); array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=int32). Parameters; ----------; input_array : :class:`.ArrayExpression`, numpy ndarray, or nested python lists/tuples; The array to convert to a Hail ndarray.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. Returns; -------; :class:`.NDArray",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:148,Install,Installation,148,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.plot.plots. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.plot.plots. Source code for hail.plot.plots; import collections; import math; import warnings; from typing import Any, Callable, Dict, List, Optional, Sequence, Set, Tuple, Union. import bokeh; import bokeh.io; import bokeh.models; import bokeh.palettes; import bokeh.plotting; import numpy as np; import pandas as pd; from bokeh.layouts import gridplot; from bokeh.models import (; BasicTicker,; CategoricalColorMapper,; CDSView,; ColorBar,; ColorMapper,; Column,; ColumnDataSource,; CustomJS,; DataRange1d,; GridPlot,; GroupFilter,; HoverTool,; IntersectionFilter,; Label,; Legend,; LegendItem,; LinearColorMapper,; LogColorMapper,; LogTicker,; Plot,; Renderer,; Select,; Slope,; Span,; ); from bokeh.plotting import figure; from bokeh.transform import transform. import hail; from hail.expr import aggregators; from hail.expr.expressions import (; Expression,; Float32Expression,; Float64Expression,; Int32Expression,; Int64Expression,; LocusExpression,; NumericExpression,; StringExpression,; expr_any,; expr_float64,; expr_locus,; expr_numeric,; expr_str,; raise_unless_row_indexed,; ); from hail.expr.functions import _error_from_cdf_python; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import dictof, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils.java import warning; from hail.utils.struct import Struct. palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']. [docs]def output_notebook():; """"""Configure the Bokeh out",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:153,Install,Installation,153,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.stats.linear_mixed_model. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.stats.linear_mixed_model. Source code for hail.stats.linear_mixed_model; [docs]class LinearMixedModel(object):; r""""""Class representing a linear mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94. """""". def __init__(self, py, px, s, y=None, x=None, p_path=None):; raise NotImplementedError(""LinearMixedModel is no longer implemented/supported as of Hail 0.2.94""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html:893,update,updated,893,docs/0.2/_modules/hail/stats/linear_mixed_model.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html,4,"['Configurat', 'Install', 'update']","['Configuration', 'Installation', 'updated']"
Deployability,"﻿. Hail | ; hail.table. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.table. Source code for hail.table; import collections; import itertools; import pprint; import shutil; from typing import Callable, ClassVar, Dict, List, Optional, Sequence, Union, overload. import numpy as np; import pandas; import pyspark. import hail as hl; from hail import ir; from hail.expr.expressions import (; ArrayExpression,; BooleanExpression,; CallExpression,; CollectionExpression,; DictExpression,; Expression,; ExpressionException,; Indices,; IntervalExpression,; LocusExpression,; NDArrayExpression,; NumericExpression,; StringExpression,; StructExpression,; TupleExpression,; analyze,; construct_expr,; construct_reference,; expr_any,; expr_array,; expr_bool,; expr_stream,; expr_struct,; extract_refs_by_indices,; to_expr,; unify_all,; ); from hail.expr.table_type import ttable; from hail.expr.types import dtypes_from_pandas, hail_type, tarray, tset, tstruct, types_match; from hail.typecheck import (; anyfunc,; anytype,; dictof,; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; table_key_type,; typecheck,; typecheck_method,; ); from hail.utils import deduplicate; from hail.utils.interval import Interval; from hail.utils.java import Env, info, warning; from hail.utils.misc import (; check_annotate_exprs,; check_collisions,; check_keys,; get_key_by_exprs,; get_nice_attr_error,; get_nice_field_error,; get_select_exprs,; plural,; process_joins,; storage_level,; wrap_to_tuple,; ); from hail.utils.placement_tree import PlacementTree. table_type = lazy(). class TableIndexKeyError(Exception):; def __init__(self, key_type, in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:148,Install,Installation,148,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.utils.hadoop_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.hadoop_utils. Source code for hail.utils.hadoop_utils; import gzip; import io; import os.path; import sys; from typing import Any, Dict, List. from hail.fs.hadoop_fs import HadoopFS; from hail.typecheck import enumeration, typecheck; from hail.utils.java import Env, info. [docs]@typecheck(path=str, mode=enumeration('r', 'w', 'x', 'rb', 'wb', 'xb'), buffer_size=int); def hadoop_open(path: str, mode: str = 'r', buffer_size: int = 8192):; """"""Open a file through the Hadoop filesystem API. Supports distributed; file systems like hdfs, gs, and s3. Warning; -------; Due to an implementation limitation, :func:`hadoop_open` may be quite; slow for large data sets (anything larger than 50 MB). Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported mo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:161,Install,Installation,161,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.utils.interval. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.interval. Source code for hail.utils.interval; import hail as hl; from hail.typecheck import anytype, lazy, nullable, typecheck_method. interval_type = lazy(). [docs]class Interval(object):; """"""; An object representing a range of values between `start` and `end`. >>> interval2 = hl.Interval(3, 6). Parameters; ----------; start : any type; Object with type `point_type`.; end : any type; Object with type `point_type`.; includes_start : :obj:`bool`; Interval includes start.; includes_end : :obj:`bool`; Interval includes end. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.interval.take(5)``. This is rare; it is much; more common to manipulate the :class:`.IntervalExpression` object, which is; constructed using the following functions:. - :func:`.interval`; - :func:`.locus_interval`; - :func:`.parse_locus_interval`; """""". @typecheck_method(; start=anytype,; end=anytype,; includes_start=bool,; includes_end=bool,; point_type=nullable(lambda: hl.expr.types.hail_type),; ); def __init__(self, start, end, includes_start=True, includes_end=False, point_type=None):; if point_type is None:; from hail.expr.expressions import impute_type, unify_types_limited. start_type = impute_type(start); end_type = impute_type(end); point_type = unify_types_limited(start_type, end_type); if point_type is None:; raise TypeError(""'start' and 'end' have incompatible types: '{}', '{}'."".format(start_type, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:157,Install,Installation,157,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.misc. Source code for hail.utils.misc; import atexit; import datetime; import difflib; import json; import os; import re; import secrets; import shutil; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:153,Install,Installation,153,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.utils.struct. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.struct. Source code for hail.utils.struct; import pprint; from collections import OrderedDict; from collections.abc import Mapping; from typing import Any, Dict. from hail.typecheck import anytype, typecheck, typecheck_method; from hail.utils.misc import get_nice_attr_error, get_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:155,Install,Installation,155,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.utils.tutorial. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.tutorial. Source code for hail.utils.tutorial; import os; import zipfile; from urllib.request import urlretrieve. import hail as hl; from hailtop.utils import sync_retry_transient_errors. from .java import Env, info; from .misc import local_path_uri, new_local_temp_dir, new_temp_file. __all__ = ['get_1kg', 'get_hgdp', 'get_movie_lens']. resources = {; '1kg_annotations': 'https://storage.googleapis.com/hail-tutorial/1kg_annotations.txt',; '1kg_matrix_table': 'https://storage.googleapis.com/hail-tutorial/1kg.vcf.bgz',; '1kg_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/ensembl_gene_annotations.txt',; 'HGDP_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_pop_and_sex_annotations.tsv',; 'HGDP_matrix_table': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_subset.vcf.bgz',; 'HGDP_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_gene_annotations.tsv',; 'movie_lens_100k': 'https://files.grouplens.org/datasets/movielens/ml-100k.zip',; }. tmp_dir: str = None. def init_temp_dir():; global tmp_dir; if tmp_dir is None:; tmp_dir = new_local_temp_dir(). def _dir_exists(fs, path):; return fs.exists(path) and fs.is_dir(path). def _file_exists(fs, path):; return fs.exists(path) and fs.is_file(path). def _copy_to_tmp(fs, src, extension=None):; dst = new_temp_file(extension=extension); fs.copy(src, dst); return dst. [docs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:157,Install,Installation,157,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.vds.combiner.load_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.load_combiner. View page source. hail.vds.combiner.load_combiner. hail.vds.combiner.load_combiner(path)[source]; Load a VariantDatasetCombiner from path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.load_combiner.html:911,update,updated,911,docs/0.2/vds/hail.vds.combiner.load_combiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.load_combiner.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.combiner.new_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.new_combiner. View page source. hail.vds.combiner.new_combiner. hail.vds.combiner.new_combiner(*, output_path, temp_path, save_path=None, gvcf_paths=None, vds_paths=None, vds_sample_counts=None, intervals=None, import_interval_size=None, use_genome_default_intervals=False, use_exome_default_intervals=False, gvcf_external_header=None, gvcf_sample_names=None, gvcf_info_to_keep=None, gvcf_reference_entry_fields_to_keep=None, call_fields=['PGT'], branch_factor=100, target_records=24000, gvcf_batch_size=None, batch_size=None, reference_genome='default', contig_recoding=None, force=False)[source]; Create a new VariantDatasetCombiner or load one from save_path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html:1419,update,updated,1419,docs/0.2/vds/hail.vds.combiner.new_combiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.combiner.variant_dataset_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.combiner.variant_dataset_combiner. Source code for hail.vds.combiner.variant_dataset_combiner; import collections; import hashlib; import json; import os; import sys; import uuid; from itertools import chain; from math import floor, log; from typing import ClassVar, Collection, Dict, List, NamedTuple, Optional, Union. import hail as hl; from hail.expr import HailType, tmatrix; from hail.genetics.reference_genome import ReferenceGenome; from hail.utils import FatalError, Interval; from hail.utils.java import info, warning. from ..variant_dataset import VariantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instanc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:180,Install,Installation,180,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.vds.filter_chromosomes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_chromosomes. View page source. hail.vds.filter_chromosomes. hail.vds.filter_chromosomes(vds, *, keep=None, remove=None, keep_autosomes=False)[source]; Filter chromosomes of a VariantDataset in several possible modes.; Notes; There are three modes for filter_chromosomes(), based on which argument is passed; to the function. Exactly one of the below arguments must be passed by keyword. keep: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with only those; chromosomes.; remove: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with those chromosomes; removed.; keep_autosomes: This argument expects the value True, and returns a dataset without; sex and mitochondrial chromosomes. Parameters:. vds (VariantDataset) – Dataset.; keep – Keep a specified list of contigs.; remove – Remove a specified list of contigs; keep_autosomes – If true, keep only autosomal chromosomes. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_chromosomes.html:1821,update,updated,1821,docs/0.2/vds/hail.vds.filter_chromosomes.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_chromosomes.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.filter_intervals. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_intervals. View page source. hail.vds.filter_intervals. hail.vds.filter_intervals(vds, intervals, *, split_reference_blocks=False, keep=True)[source]; Filter intervals in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; intervals (Table or ArrayExpression of type tinterval) – Intervals to filter on.; split_reference_blocks (bool) – If true, remove reference data outside the given intervals by segmenting reference; blocks at interval boundaries. Results in a smaller result, but this filter mode; is more computationally expensive to evaluate.; keep (bool) – Whether to keep, or filter out (default) rows that fall within any; interval in intervals. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_intervals.html:1476,update,updated,1476,docs/0.2/vds/hail.vds.filter_intervals.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_intervals.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.filter_samples. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_samples. View page source. hail.vds.filter_samples. hail.vds.filter_samples(vds, samples, *, keep=True, remove_dead_alleles=False)[source]; Filter samples in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; samples (Table or list of str) – Samples to keep or remove.; keep (bool) – Whether to keep (default), or filter out the samples from samples_table.; remove_dead_alleles (bool) – If true, remove alleles observed in no samples. Alleles with AC == 0 will be; removed, and LA values recalculated. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_samples.html:1321,update,updated,1321,docs/0.2/vds/hail.vds.filter_samples.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_samples.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.filter_variants. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_variants. View page source. hail.vds.filter_variants. hail.vds.filter_variants(vds, variants_table, *, keep=True)[source]; Filter variants in a VariantDataset, without removing reference; data. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; variants_table (Table) – Variants to filter on.; keep (bool) – Whether to keep (default), or filter out the variants from variants_table. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_variants.html:1185,update,updated,1185,docs/0.2/vds/hail.vds.filter_variants.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_variants.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.functions. Source code for hail.vds.functions; import hail as hl; from hail.expr.expressions import expr_any, expr_array, expr_call, expr_int32; from hail.expr.functions import _func; from hail.typecheck import enumeration, typecheck. [docs]@typecheck(lgt=expr_call, la=expr_array(expr_int32)); def lgt_to_gt(lgt, la):; """"""Transform LGT into GT using local alleles array. Parameters; ----------; lgt : :class:`.CallExpression`; LGT value.; la : :class:`.ArrayExpression`; Local alleles array. Returns; -------; :class:`.CallExpression`; """"""; return hl.rbind(lgt, lambda lgt: hl.if_else(lgt.is_non_ref(), _func(""lgt_to_gt"", hl.tcall, lgt, la), lgt)). [docs]@typecheck(; array=expr_array(),; local_alleles=expr_array(expr_int32),; n_alleles=expr_int32,; fill_value=expr_any,; number=enumeration('A', 'R', 'G'),; ); def local_to_global(array, local_alleles, n_alleles, fill_value, number):; """"""Reindex a locally-indexed array to globally-indexed. Examples; --------; >>> local_alleles = hl.array([0, 2]); >>> local_ad = hl.array([9, 10]); >>> local_pl = hl.array([94, 0, 123]). >>> hl.eval(local_to_global(local_ad, local_alleles, n_alleles=3, fill_value=0, number='R')); [9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; -----; The `number` parameter matches the `VCF specification <https://samtools.github.io/hts-specs/VCFv4.3.pdf>`__; number definitions:. - ``A`` indicates one value per allele, excluding the reference.; - ``R`` indicates one value per allele, including",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/functions.html:156,Install,Installation,156,docs/0.2/_modules/hail/vds/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/functions.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.vds.impute_sex_chromosome_ploidy. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chromosome_ploidy. View page source. hail.vds.impute_sex_chromosome_ploidy. hail.vds.impute_sex_chromosome_ploidy(vds, calling_intervals, normalization_contig, use_variant_dataset=False)[source]; Impute sex chromosome ploidy from depth of reference or variant data within calling intervals.; Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. vds (vds: VariantDataset) – Dataset.; calling_intervals (Table or ArrayExpression) – Calling intervals with consistent read coverage (for exomes, trim the capture intervals).; normalization_contig (str) – Autosomal contig for depth comparison.; use_variant_dataset (bool) – Whether to use depth of variant data within calling intervals instead of reference data. Default will use reference data. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html:1972,update,updated,1972,docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.lgt_to_gt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.lgt_to_gt. View page source. hail.vds.lgt_to_gt. hail.vds.lgt_to_gt(lgt, la)[source]; Transform LGT into GT using local alleles array. Parameters:. lgt (CallExpression) – LGT value.; la (ArrayExpression) – Local alleles array. Returns:; CallExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.lgt_to_gt.html:988,update,updated,988,docs/0.2/vds/hail.vds.lgt_to_gt.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.lgt_to_gt.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.merge_reference_blocks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.merge_reference_blocks. View page source. hail.vds.merge_reference_blocks. hail.vds.merge_reference_blocks(ds, equivalence_function, merge_functions=None)[source]; Merge adjacent reference blocks according to user equivalence criteria.; Examples; Coarsen GQ granularity into bins of 10 and merges blocks with the same GQ in order to; compress reference data.; >>> rd = vds.reference_data ; >>> vds.reference_data = rd.annotate_entries(GQ = rd.GQ - rd.GQ % 10) ; >>> vds2 = hl.vds.merge_reference_blocks(vds,; ... equivalence_function=lambda block1, block2: block1.GQ == block2.GQ),; ... merge_functions={'MIN_DP': 'min'}) . Notes; The equivalence_function argument expects a function from two reference blocks to a; boolean value indicating whether they should be combined. Adjacency checks are builtin; to the method (two reference blocks are ‘adjacent’ if the END of one block is one base; before the beginning of the next).; The merge_functions. Parameters:; ds (VariantDataset or MatrixTable) – Variant dataset or reference block matrix table. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.merge_reference_blocks.html:1837,update,updated,1837,docs/0.2/vds/hail.vds.merge_reference_blocks.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.merge_reference_blocks.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.methods. Source code for hail.vds.methods; import hail as hl; from hail import ir; from hail.expr import expr_any, expr_array, expr_bool, expr_interval, expr_locus, expr_str; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import dictof, enumeration, func_spec, nullable, oneof, sequenceof, typecheck; from hail.utils.java import Env, info, warning; from hail.utils.misc import new_temp_file, wrap_to_list; from hail.vds.variant_dataset import VariantDataset. def write_variant_datasets(vdss, paths, *, overwrite=False, stage_locally=False, codec_spec=None):; """"""Write many `vdses` to their corresponding path in `paths`.""""""; ref_writer = ir.MatrixNativeMultiWriter(; [f""{p}/reference_data"" for p in paths], overwrite, stage_locally, codec_spec; ); var_writer = ir.MatrixNativeMultiWriter([f""{p}/variant_data"" for p in paths], overwrite, stage_locally, codec_spec); Env.backend().execute(ir.MatrixMultiWrite([vds.reference_data._mir for vds in vdss], ref_writer)); Env.backend().execute(ir.MatrixMultiWrite([vds.variant_data._mir for vds in vdss], var_writer)). [docs]@typecheck(vds=VariantDataset); def to_dense_mt(vds: 'VariantDataset') -> 'MatrixTable':; """"""Creates a single, dense :class:`.MatrixTable` from the split; :class:`.VariantDataset` representation. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation. Returns; -------; :class:`.MatrixTable`; Dataset in dense MatrixTable representation.; """"""; ref = vds.reference_data; # FIXME(chrisvittal) consider changing END ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:154,Install,Installation,154,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.vds.read_vds. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.read_vds. View page source. hail.vds.read_vds. hail.vds.read_vds(path, *, intervals=None, n_partitions=None, _assert_reference_type=None, _assert_variant_type=None, _warn_no_ref_block_max_length=True)[source]; Read in a VariantDataset written with VariantDataset.write(). Parameters:; path (str). Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.read_vds.html:1057,update,updated,1057,docs/0.2/vds/hail.vds.read_vds.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.read_vds.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.sample_qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.sample_qc. Source code for hail.vds.sample_qc; from collections.abc import Sequence; from typing import Optional. import hail as hl; from hail.expr.expressions import Expression; from hail.expr.expressions.typed_expressions import (; ArrayExpression,; CallExpression,; LocusExpression,; NumericExpression,; StructExpression,; ); from hail.genetics.allele_type import AlleleType; from hail.methods.misc import require_first_key_field_locus; from hail.methods.qc import _qc_allele_type; from hail.table import Table; from hail.typecheck import nullable, sequenceof, typecheck; from hail.utils.java import Env; from hail.utils.misc import divide_null; from hail.vds.variant_dataset import VariantDataset. @typecheck(global_gt=Expression, alleles=ArrayExpression); def vmt_sample_qc_variant_annotations(; *,; global_gt: 'Expression',; alleles: 'ArrayExpression',; ) -> tuple['Expression', 'Expression']:; """"""Compute the necessary variant annotations for :func:`.vmt_sample_qc`, that is,; allele count (AC) and an integer representation of allele type. Parameters; ----------; global_gt : :class:`.Expression`; Call expression of the global GT of a variants matrix table usually generated; by :func:`..lgt_to_gt`; alleles : :class:`.ArrayExpression`; Array expression of the alleles of a variants matrix table; (generally ``vds.variant_data.alleles``). Returns; -------; :class:`tuple`; Tuple of expressions representing the AC (first element) and allele type; (second element).; """""". return (hl.agg.call_stats(global_gt, alleles).AC, alleles[1:].map(lambda alt: _qc_all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html:156,Install,Installation,156,docs/0.2/_modules/hail/vds/sample_qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hail.vds.sample_qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.sample_qc. View page source. hail.vds.sample_qc. hail.vds.sample_qc(vds, *, gq_bins=(0, 20, 60), dp_bins=(0, 1, 10, 20, 30), dp_field=None)[source]; Compute sample quality metrics about a VariantDataset.; If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; gq_bins (tuple of int) – Tuple containing cutoffs for genotype quality (GQ) scores.; dp_bins (tuple of int) – Tuple containing cutoffs for depth (DP) scores.; dp_field (str) – Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns:; Table – Hail Table of results, keyed by sample. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.sample_qc.html:1546,update,updated,1546,docs/0.2/vds/hail.vds.sample_qc.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.sample_qc.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html:1144,update,updated,1144,docs/0.2/vds/hail.vds.split_multi.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html:1059,patch,patch,1059,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html,2,"['patch', 'update']","['patch', 'updated']"
Deployability,"﻿. Hail | ; hail.vds.to_dense_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_dense_mt. View page source. hail.vds.to_dense_mt. hail.vds.to_dense_mt(vds)[source]; Creates a single, dense MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in dense MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.to_dense_mt.html:1055,update,updated,1055,docs/0.2/vds/hail.vds.to_dense_mt.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_dense_mt.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.to_merged_sparse_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_merged_sparse_mt. View page source. hail.vds.to_merged_sparse_mt. hail.vds.to_merged_sparse_mt(vds, *, ref_allele_function=None)[source]; Creates a single, merged sparse MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in the merged sparse MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html:1136,update,updated,1136,docs/0.2/vds/hail.vds.to_merged_sparse_mt.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.variant_dataset. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.variant_dataset. Source code for hail.vds.variant_dataset; import json; import os. import hail as hl; from hail.genetics import ReferenceGenome; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck_method; from hail.utils.java import info, warning. extra_ref_globals_file = 'extra_reference_globals.json'. [docs]def read_vds(; path,; *,; intervals=None,; n_partitions=None,; _assert_reference_type=None,; _assert_variant_type=None,; _warn_no_ref_block_max_length=True,; ) -> 'VariantDataset':; """"""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:162,Install,Installation,162,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/batch_api.html:873,deploy,deploy,873,docs/0.2/batch_api.html,https://hail.is,https://hail.is/docs/0.2/batch_api.html,2,"['deploy', 'update']","['deploy', 'updated']"
Deployability,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html:1631,update,updated,1631,docs/0.2/_modules/hailtop/frozendict.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html,4,"['Configurat', 'Install', 'update']","['Configuration', 'Installation', 'updated']"
Deployability,"﻿. Hail | ; hailtop.fs.fs_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.fs.fs_utils. Source code for hailtop.fs.fs_utils; import io; from typing import List, Optional. from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration; from hailtop.utils.gcs_requester_pays import GCSRequesterPaysFSCache. from .router_fs import RouterFS; from .stat_result import FileListEntry. _fses = GCSRequesterPaysFSCache(fs_constructor=RouterFS). [docs]def open(; path: str,; mode: str = 'r',; buffer_size: int = 8192,; *,; requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None,; ) -> io.IOBase:; """"""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Writ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:157,Install,Installation,157,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/index.html:792,pipeline,pipelines,792,docs/0.2/linalg/index.html,https://hail.is,https://hail.is/docs/0.2/linalg/index.html,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"﻿. Hail | ; linalg/utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; linalg/utils. View page source. linalg/utils. array_windows(a, radius); Returns start and stop indices for window around each array value. locus_windows(locus_expr, radius[, ...]); Returns start and stop indices for window around each locus. hail.linalg.utils.array_windows(a, radius)[source]; Returns start and stop indices for window around each array value.; Examples; >>> hl.linalg.utils.array_windows(np.array([1, 2, 4, 4, 6, 8]), 2); (array([0, 0, 1, 1, 2, 4]), array([2, 4, 5, 5, 6, 6])). >>> hl.linalg.utils.array_windows(np.array([-10.0, -2.5, 0.0, 0.0, 1.2, 2.3, 3.0]), 2.5); (array([0, 1, 1, 1, 2, 2, 4]), array([1, 4, 6, 6, 7, 7, 7])). Notes; For an array a in ascending order, the resulting starts and stops; arrays have the same length as a and the property that, for all indices; i, [starts[i], stops[i]) is the maximal range of indices j such; that a[i] - radius <= a[j] <= a[i] + radius.; Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; BlockMatrix.sparsify_row_intervals(). Parameters:. a (numpy.ndarray of signed integer or float values) – 1-dimensional array of values, non-decreasing with respect to index.; radius (float) – Non-negative radius of window for values. Returns:; (numpy.ndarray of int, numpy.ndarray of int) – Tuple of start indices array and stop in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/utils/index.html:150,Install,Installation,150,docs/0.2/linalg/utils/index.html,https://hail.is,https://hail.is/docs/0.2/linalg/utils/index.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability,"﻿. Hail | ; stats. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats. View page source. stats. Classes. LinearMixedModel; Class representing a linear mixed model. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/stats/index.html:795,update,updated,795,docs/0.2/stats/index.html,https://hail.is,https://hail.is/docs/0.2/stats/index.html,1,['update'],['updated']
Deployability,"﻿. Hail | Index . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scala",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:787,install,install,787,index.html,https://hail.is,https://hail.is/index.html,2,['install'],"['install', 'installed']"
Deployability,"﻿. Hail | References . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 2",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:308,install,installed,308,references.html,https://hail.is,https://hail.is/references.html,2,"['install', 'update']","['installed', 'updated']"
Energy Efficiency," 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34323,efficient,efficient,34323,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['efficient'],['efficient']
Energy Efficiency," : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""; if gene_symbols is None and gene_ids is None and transcript_ids is None:; raise ValueError('get_gene_intervals requires at least one of gene_symbols, gene_ids, or transcript_ids'); ht = _load_gencode_gtf(gtf_file, reference_genome); criteria = []; if gene_symbols:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_name == y), gene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval))); if verbose:; info(; f'get_gene_intervals found {len(gene_info)} entries:\n'; + ""\n"".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == ""gene"" else x[3]})', gene_info)); ); intervals = list(map(lambda x: x[-1], gene_info)); return intervals. def _load_gencode_gtf(gtf_file=None, reference_genome=None):; """"""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""; GTFS = {; 'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',; 'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annota",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:8694,reduce,reduce,8694,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['reduce'],['reduce']
Energy Efficiency," >>> hl.eval(hl.any([False, True, False])); True. >>> hl.eval(hl.any([False, False, False])); False. The third form:. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.any(lambda x: x[-1] == 'x', a)); True. >>> hl.eval(hl.any(lambda x: x % 4 == 0, s)); False. Notes; -----; :func:`~.any` returns ``False`` when given an empty array or empty argument list.; """"""; base = hl.literal(False); if builtins.len(args) == 0:; return base; if builtins.len(args) == 1:; arg = arg_check(args[0], 'any', 'collection', oneof(collection_type, expr_bool)); if arg.dtype == hl.tbool:; return arg; return arg.any(lambda x: x); if builtins.len(args) == 2:; if callable(args[0]):; f = arg_check(args[0], 'any', 'f', any_to_bool_type); collection = arg_check(args[1], 'any', 'collection', collection_type); return collection.any(f); n_args = builtins.len(args); args = [args_check(x, 'any', 'exprs', i, n_args, expr_bool) for i, x in builtins.enumerate(args)]; return functools.reduce(operator.ior, args, base). [docs]def all(*args) -> BooleanExpression:; """"""Check for all ``True`` in boolean expressions or collections of booleans. :func:`~.all` comes in three forms:. 1. ``hl.all(boolean, ...)``. Are all arguments ``True``?. 2. ``hl.all(collection)``. Are all elements of the collection ``True``?. 3. ``hl.all(function, collection)``. Does ``function`` return ``True`` for; all values in this collection?. Examples; --------. The first form:. >>> hl.eval(hl.all()); True. >>> hl.eval(hl.all(True)); True. >>> hl.eval(hl.all(False)); False. >>> hl.eval(hl.all(True, True, True)); True. >>> hl.eval(hl.all(False, False, True, False)); False. The second form:. >>> hl.eval(hl.all([False, True, False])); False. >>> hl.eval(hl.all([True, True, True])); True. The third form:. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.all(lambda x: hl.len(x) > 3, a)); False. >>> hl.eval(hl.all(lambda x: x < 10, s)); True. Notes; -----; :func:`~.all` returns `",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:109361,reduce,reduce,109361,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['reduce'],['reduce']
Energy Efficiency," >>> hl.eval(y // 2); 2.0. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; The floor of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59413,power,power,59413,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['power'],['power']
Energy Efficiency," Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:6224,allocate,allocated,6224,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['allocate'],['allocated']
Energy Efficiency," Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from a diagonal band. By default,; all elements outside the band but inside blocks that overlap the; band are set to zero as well.; The band is defined in terms of inclusive lower and upper indices; relative to the diagonal. For example, the indices -1, 0, and 1; correspond to the sub-diagonal, diagonal, and super-diagonal,; respectively. The diagonal band contains the elements at positions; \((i, j)\) such that. \[\mathrm{lower} \leq j - i \leq \mathrm{upper}.\]; lower must be less than or equal to upper, but their values may; exceed the dimensions of the matrix, the band need not include the; diagonal, and the matrix need not be square. Parameters:. lower (int) – Index of lowest band relative to the diagonal.; upper (int) – Index of highest band relative to the diagonal.; blocks_only (bool) – If False, set all elements outside the band to zero.; If True, only set all blocks outside the band to blocks; of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sparsify_rectangles(rectangles)[source]; Filter to blocks overlapping the union of rectangular regions.; Examples; Consider the following block matrix:; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to blocks covering three rectangles and collect to NumPy:; >>> bm.sparsify_rectangles([[0, 1, 0, 1], [0, 3, 0, 2], [1, 2, 0, 4]]).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 9., 10., 0., 0.],; [13., 14., 0., 0.]]). Notes; This method creates a block-sparse matrix by zeroing out (dropping); all blocks which are disjoint from the union of a set of rectangular; regions. Partially overlapping blocks are not modified.; Each rectangle is encoded as a list of length four of; the form [row_start, row_stop, col_start, col_s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:29944,efficient,efficient,29944,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency," Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:2299,efficient,efficient,2299,index.html,https://hail.is,https://hail.is/index.html,1,['efficient'],['efficient']
Energy Efficiency," Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). [docs]@typecheck(nd=expr_ndarray(), full_matrices=bool, compute_uv=bool); def svd(nd, full_matrices=True, compute_uv=True):; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv : :class:`.bool`; If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns; -------; - u: :class:`.NDArrayNumericExpression`; The left singular vectors.; - s: :class:`.NDArrayNumericExpression`; The singular values.; - vt: :class:`.NDArrayNumericExpression`; The right singular vectors.; """"""; float_nd = nd.ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:11084,reduce,reduced,11084,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduced']
Energy Efficiency," This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general, once you know the wall clock time of your job, you can enter your cluster parameters into the; Google Cloud Pricing Calculator. and get a precise estimate; of cost using the latest prices. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:3479,charge,charges,3479,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['charge'],['charges']
Energy Efficiency," already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_fro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:26929,reduce,reduce,26929,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reduce'],['reduce']
Energy Efficiency," alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate al",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:7140,reduce,reduced,7140,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['reduce'],['reduced']
Energy Efficiency," by zeroing out all blocks; which are disjoint from a diagonal band. By default,; all elements outside the band but inside blocks that overlap the; band are set to zero as well. The band is defined in terms of inclusive `lower` and `upper` indices; relative to the diagonal. For example, the indices -1, 0, and 1; correspond to the sub-diagonal, diagonal, and super-diagonal,; respectively. The diagonal band contains the elements at positions; :math:`(i, j)` such that. .. math::. \mathrm{lower} \leq j - i \leq \mathrm{upper}. `lower` must be less than or equal to `upper`, but their values may; exceed the dimensions of the matrix, the band need not include the; diagonal, and the matrix need not be square. Parameters; ----------; lower: :obj:`int`; Index of lowest band relative to the diagonal.; upper: :obj:`int`; Index of highest band relative to the diagonal.; blocks_only: :obj:`bool`; If ``False``, set all elements outside the band to zero.; If ``True``, only set all blocks outside the band to blocks; of zeros; this is more efficient. Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if lower > upper:; raise ValueError(f'sparsify_band: lower={lower} is greater than upper={upper}'). bounds = hl.literal((lower, upper), hl.ttuple(hl.tint64, hl.tint64)); return BlockMatrix(BlockMatrixSparsify(self._bmir, bounds._ir, BandSparsifier(blocks_only))). [docs] @typecheck_method(lower=bool, blocks_only=bool); def sparsify_triangle(self, lower=False, blocks_only=False):; """"""Filter to the upper or lower triangle. Examples; --------; Consider the following block matrix:. >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:. >>> bm.sparsify_triangle().to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:32202,efficient,efficient,32202,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['efficient'],['efficient']
Energy Efficiency," each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; BlockMatrix.write_from_entry_expr().; The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; ld_matrix().; To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:22403,efficient,efficiently,22403,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['efficient'],['efficiently']
Energy Efficiency," fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Expl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:3568,efficient,efficient,3568,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['efficient'],['efficient']
Energy Efficiency," importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:1751,efficient,efficient,1751,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['efficient'],['efficient']
Energy Efficiency," locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic varia",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117823,efficient,efficient,117823,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['efficient'],['efficient']
Energy Efficiency," not the genotypes. Use; split_multi_hts() if possible, or split the genotypes yourself using; one of the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2.; old_locus (locus) – The original, unsplit locus.; old_alleles (array<str>) – The original, unsplit alleles. All other fields are left unchanged. Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:84444,efficient,efficient,84444,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['efficient'],['efficient']
Energy Efficiency," num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. num_mismatch()[source]¶; Returns the number of mismatched bases in this alternate allele.; Fails if the ref and alt alleles are not the same length. Return type:int. ref¶; Reference allele. Return type:str. stripped_snp()[source]¶; Returns the one-character reduced SNP.; Fails if called on an alternate allele that is not a SNP. Return type:str, str. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:3413,reduce,reduced,3413,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,1,['reduce'],['reduced']
Energy Efficiency," simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:2697,charge,charges,2697,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['charge'],['charges']
Energy Efficiency," the entire Batch and are not outputs of a BashJob.; vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). We use the gwas function defined above to create a new job on the batch to; perform a GWAS that outputs a binary PLINK file and association results:; g = gwas(batch, vcf, phenotypes). We call the clump function once per chromosome and aggregate a list of the; clumping results files passing the outputs from the g job defined above; as inputs to the clump function:; results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). Finally, we use the merge function to concatenate the results into a single file; and then write this output to a permanent location using Batch.write_output().; The inputs to the merge function are the clumped output files from each of the clump; jobs.; m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). The last thing we do is submit the Batch to the service and then close the Backend:; batch.run(open=True, wait=False) # doctest: +SKIP; backend.close(). Synopsis; We provide the code used above in one place for your reference:. run_gwas.py; import argparse. import hail as hl. def run_gwas(vcf_file, phenotypes_file, output_file):; table = hl.import_table(phenotypes_file, impute=True).key_by('Sample'). hl.import_vcf(vcf_file).write('tmp.mt'); mt = hl.read_matrix_table('tmp.mt'). mt = mt.annotate_cols(pheno=table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = (; (mt.GT.is_hom_ref() & (ab <= 0.1)); | (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)); | (mt.GT.is_hom_var() & (ab >= 0.9)); ); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). eigenvalues, pcs",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:11572,consumption,consumption,11572,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['consumption'],['consumption']
Energy Efficiency," with height 0, those cannot be log transformed and were left as 0s.""; ). changes = {; ""bin_freq"": bin_freq,; ""n_larger"": math.log10(data.n_larger) if data.n_larger > 0.0 else data.n_larger,; ""n_smaller"": math.log10(data.n_smaller) if data.n_smaller > 0.0 else data.n_smaller,; }; data = data.annotate(**changes); y_axis_label = 'log10 Frequency'; else:; y_axis_label = 'Frequency'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='black',; ); if data.n_larger > 0:; p.quad(; bottom=0,; top=data.n_larger,; left=data.bin_edges[-1],; right=(data.bin_edges[-1] + (data.bin_edges[1] - data.bin_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_inte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:12776,green,green,12776,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['green'],['green']
Energy Efficiency," with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z^T Z\). In the expressions below, we elide transpositions; of symmetric matrices:. \[\begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}\]; Before substituting the definition of \(P_0\), simplify it using the reduced QR; decomposition:. \[\begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:70529,reduce,reduced,70529,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['reduce'],['reduced']
Energy Efficiency," x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:3867,power,power,3867,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,2,['power'],['power']
Energy Efficiency," yourself using; one of the entry modification methods: :meth:`.MatrixTable.annotate_entries`,; :meth:`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:112204,efficient,efficient,112204,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['efficient'],['efficient']
Energy Efficiency,"""""""; return _func(""dnorm"", tfloat64, x, mu, sigma, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, log_p=expr_bool); def dpois(x, lamb, log_p=False) -> Float64Expression:; """"""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""; return _func(""dpois"", tfloat64, x, lamb, log_p). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def exp(x) -> Float64Expression:; """"""Computes `e` raised to the power `x`. Examples; --------. >>> hl.eval(hl.exp(2)); 7.38905609893065. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""exp"", tfloat64, x). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def fisher_exact_test(c1, c2, c3, c4) -> StructExpression:; """"""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:30544,power,power,30544,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['power'],['power']
Energy Efficiency,"',; u'HG02282',; u'HG02477']},; {u'at': u'8:95863909:A:T', u'homvars': []},; {u'at': u'8:97172671:C:T', u'homvars': []}]. takeBy¶; takeBy is an aggregator that takes elements of an aggregable ordered; by a lambda function (smallest to largest). We can easily select the; variants with the lowest p-values after regression:. In [42]:. top_5_pvals = (vds.linreg('sa.metadata.CaffeineConsumption'); .query_variants('variants.map(v => {at: str(v), pval: va.linreg.pval}).takeBy(x => x.pval, 5)')); pprint(top_5_pvals). 2018-10-18 01:26:07 Hail: INFO: Running linear regression on 1000 samples with 1 covariate including intercept... [{u'at': u'10:56025604:A:C', u'pval': 5.595049078641033e-05},; {u'at': u'20:55431571:A:C', u'pval': 0.00010899661736561121},; {u'at': u'10:91099630:T:C', u'pval': 0.00013497679316886596},; {u'at': u'4:149350527:T:C', u'pval': 0.00017786066989195366},; {u'at': u'7:152600817:G:A', u'pval': 0.0002252314501866726}]. Aggregating by key¶; The; aggregate_by_key; method is likely the most powerful piece of query functionality in Hail.; It’s a method on KeyTable.; You can produce key tables from a; VariantDataset with; three methods:. variants_table():; a key table with the variant and variant annotations as columns.; There is one row per variant.; samples_table():; a key table with the sample and sample annotations as columns. There; is one row per sample.; genotypes_table():; a key table that is the coordinate representation of the genetic; matrix. The columns are the variant, variant annotations, sample,; sample annotations, and genotype. There is one row per variant/sample; combination: (N * M) total rows!. Using; aggregate_by_key; with; genotypes_table; can produce counts of loss of function variants in cases and controls; per gene, compute the mean depth per sample per exon, and much more. You; define the aggregation keys, and you define how to combine the rows.; This method produces another; KeyTable.; We use it here to compute the mean depth and quali",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:19041,power,powerful,19041,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['power'],['powerful']
Energy Efficiency,"(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by an array or a scalar using floor division. Examples; --------. >>> hl.eval(a1 // 2); [0, 0, 1, 1, 2, 2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Positionally compute the left modulo the right. Examples; --------. >>> hl.eval(a1 % 2); [0, 1, 0, 1, 0, 1]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, other):; """"""Positionally raise to the power of an array or a scalar. Examples; --------. >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('**', other, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class SetExpression(CollectionExpression):; """"""Expression of type :class:`.tset`. >>> s1 = hl.literal({1, 2, 3}); >>> s2 = hl.literal({1, 3, 5}). See Also; --------; :class:`.CollectionExpression`; """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(SetExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tset); self._ec = ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:25273,power,power,25273,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['power'],['power']
Energy Efficiency,"(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable) – The function to execute.; iterables (Iterable[Any]) – The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None]) – This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator’s; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int) – The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the default_image argument to BatchPoolExecutor; This function does not return the function’s output, it returns a; BatchPoolFuture whose BatchPoolFuture.result() method; can be used to access ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5933,reduce,reduce,5933,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['reduce'],['reduce']
Energy Efficiency,"(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float32Expression.html:4073,power,power,4073,docs/0.2/hail.expr.Float32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html,4,['power'],['power']
Energy Efficiency,") – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covaria",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:82594,efficient,efficient,82594,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficient']
Energy Efficiency,")). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(threshold=numeric,; tiebreaking_expr=nullable(strlike),; maf=nullable(strlike),; bounded=bool); def ibd_prune(self, threshold, tiebreaking_expr=None, maf=None, bounded=True):; """"""; Prune samples from the :py:class:`.VariantDataset` based on :py:meth:`~hail.VariantDataset.ibd` PI_HAT measures of relatedness. .. include:: requireTGenotype.rst. **Examples**; ; Prune samples so that no two have a PI_HAT value greater than or equal to 0.6.; ; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:. >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). **Notes**. The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted. ; ; The tiebreaking_expr namespace has the following variables available:; ; - ``s1``: The first sample id.; - ``sa1``: The annotations associated with s1.; - ``s2``: The second sample id. ; - ``sa2``: The annotations associated with s2. ; ; The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping ``s1``. Any positive integer expresses a preference for keeping ``s2``. A zero expresses no preference. This function must induce a `preorder <https://en.wikipedia.org/wiki/Preorder>`__ on the samples, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:84977,efficient,efficient,84977,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficient']
Energy Efficiency,"); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9344,reduce,reduced,9344,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduced']
Energy Efficiency,"); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= max_size, ht.G_take)).T,; ); ht = ht.annotate(Q=((ht.y_residual @ ht.G).map(lambda x: x**2) * ht.weight).sum(0)). # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:83030,reduce,reduced,83030,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['reduce'],['reduced']
Energy Efficiency,", and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1851,consumption,consumption,1851,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['consumption'],['consumption']
Energy Efficiency,",; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) # doctest: +SKIP. Only compute the kinship statistic. This is more efficient than; computing all statistics. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') # doctest: +SKIP. Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using :meth:`.Table.filter`. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) # doctest: +SKIP. One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:2506,efficient,efficient,2506,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['efficient'],['efficient']
Energy Efficiency,"----------+------------+---------------+; | 20:10019093 | [""A"",""G""] | 1 |; | 20:10019093 | [""A"",""G""] | 2 |; | 20:10026348 | [""A"",""G""] | 1 |; | 20:10026348 | [""A"",""G""] | 2 |; | 20:10026357 | [""T"",""C""] | 1 |; | 20:10026357 | [""T"",""C""] | 2 |; | 20:10030188 | [""T"",""A""] | 1 |; | 20:10030188 | [""T"",""A""] | 2 |; | 20:10030452 | [""G"",""A""] | 1 |; | 20:10030452 | [""G"",""A""] | 2 |; +---------------+------------+---------------+; showing top 10 rows. Aggregation; MatrixTable has three methods to compute aggregate statistics. MatrixTable.aggregate_rows(); MatrixTable.aggregate_cols(); MatrixTable.aggregate_entries(). These methods take an aggregated expression and evaluate it, returning; a Python value.; An example of querying entries is to compute the global mean of field GQ:; >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) ; 67.73196915777027. It is possible to compute multiple values simultaneously by; creating a tuple or struct. This is encouraged, because grouping two; computations together is far more efficient by traversing the dataset only once; rather than twice.; >>> mt.aggregate_entries((hl.agg.stats(mt.DP), hl.agg.stats(mt.GQ))) ; (Struct(mean=41.83915800445897, stdev=41.93057654787303, min=0.0, max=450.0, n=34537, sum=1444998.9999999995),; Struct(mean=67.73196915777027, stdev=29.80840934057741, min=0.0, max=99.0, n=33720, sum=2283922.0000000135)). See the Aggregators page for the complete list of aggregator; functions. Group-By; Matrix tables can be aggregated along the row or column axis to produce a new; matrix table. MatrixTable.group_rows_by(); MatrixTable.group_cols_by(). First let’s add a random phenotype as a new column field case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:10347,efficient,efficient,10347,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['efficient'],['efficient']
Energy Efficiency,"------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.; The pca function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca function does the same, using HWE-normalized genotypes for the PCA. [41]:. eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). [Stage 158:> (0 + 1) / 1]. [42]:. pprint(eigenvalues). [18.084111467840707,; 9.9840764",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:20336,consumption,consumption,20336,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['consumption'],['consumption']
Energy Efficiency,"-------; :class:`numpy.ndarray`; """""". def parse_rects(fname):; rect_idx_and_bounds = [int(i) for i in re.findall(r'\d+', fname)]; if len(rect_idx_and_bounds) != 5:; raise ValueError(f'Invalid rectangle file name: {fname}'); return rect_idx_and_bounds. rect_files = [file['path'] for file in hl.utils.hadoop_ls(path) if not re.match(r'.*\.crc', file['path'])]; rects = [parse_rects(os.path.basename(file_path)) for file_path in rect_files]. n_rows = max(rects, key=lambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \time",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72554,reduce,reduced,72554,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduced']
Energy Efficiency,". RunningBatchType — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; RunningBatchType. Backend; LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; RunningBatchType. View page source. RunningBatchType. class hailtop.batch.backend.RunningBatchType; The type of value returned by Backend._run(). The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; alias of TypeVar(‘RunningBatchType’). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html:606,monitor,monitor,606,docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,2,['monitor'],['monitor']
Energy Efficiency,".01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:21734,consumption,consumption,21734,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['consumption'],['consumption']
Energy Efficiency,"0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:; >>> bm.sparsify_triangle().to_numpy() ; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:; >>> bm.sparsify_triangle(blocks_only=True).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triangle. By; default, all elements outside the triangle but inside blocks that; overlap the triangle are set to zero as well. Parameters:. lower (bool) – If False, keep the upper triangle.; If True, keep the lower triangle.; blocks_only (bool) – If False, set all elements outside the triangle to zero.; If True, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sqrt()[source]; Element-wise square root. Returns:; BlockMatrix. sum(axis=None)[source]; Sums array elements over one or both axes.; Examples; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:34646,efficient,efficient,34646,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"01822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including intercept... In [53]:. qqplot(pvals, 5, 6). In [54]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:25716,green,green,25716,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['green'],['green']
Energy Efficiency,"1, 0, 2, 2],; ... stops= [2, 0, 3, 4],; ... blocks_only=True); ... .to_numpy()) ; array([[ 1., 2., 0., 0.],; [ 5., 6., 0., 0.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from all row intervals. By default, all elements; outside the row intervals but inside blocks that overlap the row; intervals are set to zero as well.; starts and stops must both have length equal to the number of; rows. The interval for row i is [starts[i], stops[i]). In; particular, 0 <= starts[i] <= stops[i] <= n_cols is required; for all i.; This method requires the number of rows to be less than \(2^{31}\). Parameters:. starts (list of int, or numpy.ndarray of int) – Start indices for each row (inclusive).; stops (list of int, or numpy.ndarray of int) – Stop indices for each row (exclusive).; blocks_only (bool) – If False, set all elements outside row intervals to zero.; If True, only set all blocks outside row intervals to blocks; of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sparsify_triangle(lower=False, blocks_only=False)[source]; Filter to the upper or lower triangle.; Examples; Consider the following block matrix:; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:; >>> bm.sparsify_triangle().to_numpy() ; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:; >>> bm.sparsify_triangle(blocks_only=True).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triang",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:33301,efficient,efficient,33301,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"6-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 3699–3707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated population; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders wit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:9398,power,power,9398,references.html,https://hail.is,https://hail.is/references.html,1,['power'],['power']
Energy Efficiency,": {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""; t = self.to_table_row_major(n_partitions, maximum_cache_memory_in_bytes); t = t.transmute(entries=t.entries.map(lambda i: hl.struct(element=i))); t = t.annotate_globals(cols=hl.range(self.n_cols).map(lambda i: hl.struct(col_idx=hl.int64(i)))); return t._unlocalize_entries('entries', 'cols', ['col_idx']). [docs] @staticmethod; @typecheck(; path_in=str,; path_out=str,; delimiter=str,; header=nullable(str),; add_index=bool,; parallel=nullable(ExportType.checker),; partition_size=nullable(int),; entries=enumeration('full', 'lower', 'strict_lower', 'upper', 'strict_upper'),; ); def export(; path_in,; path_out,; delimiter='\t',; header=None,; add_index=False,; paral",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:57213,reduce,reduce,57213,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduce']
Energy Efficiency,":`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:75214,reduce,reduced,75214,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduced']
Energy Efficiency,"; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistic",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:7515,efficient,efficient,7515,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['efficient'],['efficient']
Energy Efficiency,"; >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in descending order,; \(V^T\) is an \(r \times m\) matrix whose rows are; (orthonormal) right singular vectors. If the singular values in \(\Sigma\) are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:35878,reduce,reduced,35878,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"; >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5064,power,power,5064,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,2,['power'],['power']
Energy Efficiency,"; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:6882,reduce,reduced,6882,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"; [ 5., 6., 0., 0.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from all row intervals. By default, all elements; outside the row intervals but inside blocks that overlap the row; intervals are set to zero as well. `starts` and `stops` must both have length equal to the number of; rows. The interval for row ``i`` is ``[starts[i], stops[i])``. In; particular, ``0 <= starts[i] <= stops[i] <= n_cols`` is required; for all ``i``. This method requires the number of rows to be less than :math:`2^{31}`. Parameters; ----------; starts: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Start indices for each row (inclusive).; stops: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Stop indices for each row (exclusive).; blocks_only: :obj:`bool`; If ``False``, set all elements outside row intervals to zero.; If ``True``, only set all blocks outside row intervals to blocks; of zeros; this is more efficient.; Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if isinstance(starts, np.ndarray):; if starts.dtype not in (np.int32, np.int64):; raise ValueError(""sparsify_row_intervals: starts ndarray must have dtype 'int32' or 'int64'""); starts = [int(s) for s in starts]; if isinstance(stops, np.ndarray):; if stops.dtype not in (np.int32, np.int64):; raise ValueError(""sparsify_row_intervals: stops ndarray must have dtype 'int32' or 'int64'""); stops = [int(s) for s in stops]. n_rows = self.n_rows; n_cols = self.n_cols; if n_rows >= (1 << 31):; raise ValueError(f'n_rows must be less than 2^31, found {n_rows}'); if len(starts) != n_rows or len(stops) != n_rows:; raise ValueError(f'starts and stops must both have length {n_rows} (the number of rows)'); if any([start < 0 for start in starts]):; raise ValueError('all start values must be non-negative'); if any([stop > self.n_cols for stop in stops]):; raise ValueError(f'all stop valu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:36875,efficient,efficient,36875,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['efficient'],['efficient']
Energy Efficiency,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statisti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33668,reduce,reduces,33668,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['reduce'],['reduces']
Energy Efficiency,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54124,reduce,reduces,54124,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['reduce'],['reduces']
Energy Efficiency,"AT above this value will; not be included in the output. Must be in [0,1]. Returns:A KeyTable mapping pairs of samples to their IBD; statistics. Return type:KeyTable. ibd_prune(threshold, tiebreaking_expr=None, maf=None, bounded=True)[source]¶; Prune samples from the VariantDataset based on ibd() PI_HAT measures of relatedness. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Prune samples so that no two have a PI_HAT value greater than or equal to 0.6.; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:; >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). Notes; The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted.; The tiebreaking_expr namespace has the following variables available:. s1: The first sample id.; sa1: The annotations associated with s1.; s2: The second sample id.; sa2: The annotations associated with s2. The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping s1. Any positive integer expresses a preference for keeping s2. A zero expresses no preference. This function must induce a preorder on the samples, in particular:. tiebreaking_expr(sample1, sample2) must equal -1 * tie breaking_expr(sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:68980,efficient,efficient,68980,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficient']
Energy Efficiency,"Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis plat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1009,power,powerful,1009,index.html,https://hail.is,https://hail.is/index.html,1,['power'],['powerful']
Energy Efficiency,"If True, keep the lower triangle.; blocks_only (bool) – If False, set all elements outside the triangle to zero.; If True, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sqrt()[source]; Element-wise square root. Returns:; BlockMatrix. sum(axis=None)[source]; Sums array elements over one or both axes.; Examples; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in desce",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:35464,reduce,reduced,35464,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"Local SSD; Average number of days per month = 365.25 / 12 = 30.4375. Cost per GB per month = $0.048. Cost per core per hour = $0.048 * 375 / 30.4375 / 24 / 16. = $0.001685 per core per hour. Storage; Average number of days per month = 365.25 / 12 = 30.4375. Cost per GB per month = $0.17. Cost per GB per hour = $0.17 / 30.4375 / 24. IP network cost= $0.0003125 per core per hour for nonpreemptible worker types; = $0.00015625 per core per hour for spot worker types. Service cost= $0.01 per core per hour. Logs, Specs, and Firewall Fee= $0.005 per core per hour. The sum of these costs is $0.02684125 per core/hour for standard spot workers, $0.02929425 per core/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:6250,schedul,scheduled,6250,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['schedul'],['scheduled']
Energy Efficiency,"Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size.; To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to float64. One-dimensional ndarrays; of shape (n) are promoted to two-dimensional ndarrays of shape (1,; n), i.e. a single row.; Block matrices support broadcasting of +, -, *, and /; between matrices of different shapes, consistent with the NumPy; broadcasting rules.; There is one exception: block matrices do not currently support element-wise; “outer product” of a single row and a single column",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:1701,power,power,1701,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['power'],['power']
Energy Efficiency,"STER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2709,charge,charges,2709,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['charge'],['charges']
Energy Efficiency,"The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21176,power,power,21176,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['power'],['power']
Energy Efficiency,"True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource) – Resource to be written to a file.; dest (str) – Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11979,charge,charges,11979,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['charge'],['charges']
Energy Efficiency,"])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable) – The function to execute.; iterables (Iterable[Any]) – The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None]) – This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator’s; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int) – The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the under",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5722,schedul,schedule,5722,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['schedul'],['schedule']
Energy Efficiency,"a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2525,charge,charges,2525,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['charge'],['charges']
Energy Efficiency,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedTable.html:2386,reduce,reduces,2386,docs/0.2/hail.GroupedTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html,1,['reduce'],['reduces']
Energy Efficiency,"achine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2732,allocate,allocate,2732,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['allocate'],['allocate']
Energy Efficiency,"ackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10178,charge,charges,10178,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['charge'],['charges']
Energy Efficiency,"am bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linregMultiPheno(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac,; min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; variant_block_size=integral); def linreg3(self, ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes; more efficiently than looping over :py:meth:`.linreg`. This; method is more efficient than :py:meth:`.linreg_multi_pheno`; but doesn't implicitly filter on allele count or allele; frequency. .. warning::. :py:meth:`.linreg3` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of ``y``. - **va.linreg.nCompleteSamples** (*Int*) -- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:112499,efficient,efficient,112499,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficient']
Energy Efficiency,"ambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \times r` matrix whose columns are; (orthonormal) left singular vectors,. - :math:`\Sigma` is an :math:`r \times r` diagonal matrix of non-negative; singular values in descending order,. - :math:`V^T` is an :math:`r \times m` matrix whose rows are; (orthonormal) right singular vectors. If the singular values in :math:`\Sigma` are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computationa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72984,reduce,reduced,72984,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduced']
Energy Efficiency,"ant annotations are added. va.linreg.beta (Double) – fit genotype coefficient, \(\hat\beta_1\); va.linreg.se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:82533,efficient,efficiently,82533,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"ar matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. hail.nd.svd(nd, full_matrices=True, compute_uv=True)[source]; Performs a singular value decomposition. Parameters:. nd (NDArrayNumericExpression) – A 2 dimensional ndarray, shape(M, N).; full_matrices (bool) – If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv (bool) – If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns:. - u (NDArrayNumericExpression) – The left singular vectors.; - s (NDArrayNumericExpression) – The singular values.; - vt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:7619,reduce,reduced,7619,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"arameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:4200,power,power,4200,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,1,['power'],['power']
Energy Efficiency,"array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7319,efficient,efficient,7319,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['efficient'],['efficient']
Energy Efficiency,"atus (both split or both multi-allelic). Parameters:right (VariantDataset) – right-hand variant dataset. Returns:Joined variant dataset. Return type:VariantDataset. ld_matrix(force_local=False)[source]¶; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you’ll want to reduce the number of variants with methods like; sample_variants(), filter_variants_expr(), or ld_prune() before; calling this unless your dataset is very small. Parameters:force_local (bool) – If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds \(5000^2\) and locally otherwise. Returns:Matrix of r values between pairs of variants. Return type:LDMatrix. ld_prune(r2=0.2, window=1000000, memory_per_core=256, num_cores=1)[source]¶; Prune variants in linkage disequilibrium (LD). Important; The genotype_schema() must be of type TGenotype in order to use this method. Requires was_split equals True.; Examples; Export the set of common LD pruned variants to a file:; >>> vds_result = (vds.variant_qc(); ... .filter_variants_expr(""va.qc.AF >= 0.05 && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:74848,reduce,reduce,74848,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['reduce'],['reduce']
Energy Efficiency,"ble spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75966,charge,charges,75966,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['charge'],['charges']
Energy Efficiency,"ble(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symme",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:29746,schedul,schedule,29746,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['schedul'],['schedule']
Energy Efficiency,"butes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1736,reduce,reduced,1736,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,1,['reduce'],['reduced']
Energy Efficiency,"calize=True) -> int:; """"""Count the number of columns in the matrix. Examples; --------. Count the number of columns:. >>> n_cols = dataset.count_cols(). Returns; -------; :obj:`int`; Number of columns in the matrix.; """"""; count_ir = ir.TableCount(ir.MatrixColsTable(self._mir)); if _localize:; return Env.backend().execute(count_ir); else:; return construct_expr(ir.LiftMeOut(count_ir), hl.tint64). [docs] def count(self) -> Tuple[int, int]:; """"""Count the number of rows and columns in the matrix. Examples; --------. >>> dataset.count(). Returns; -------; :obj:`int`, :obj:`int`; Number of rows, number of cols.; """"""; count_ir = ir.MatrixCount(self._mir); return Env.backend().execute(count_ir). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; _drop_cols=bool,; _drop_rows=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:80522,efficient,efficient,80522,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['efficient'],['efficient']
Energy Efficiency,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:12094,schedul,scheduled,12094,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,2,['schedul'],['scheduled']
Energy Efficiency,"cs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the MatrixTable.entries method to convert our matrix table to a table (with one row for each sample for each variant). In this representation, it is easy to aggregate over any fields we like, which is often ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:23335,consumption,consumption,23335,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['consumption'],['consumption']
Energy Efficiency,"ct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on which; scipy.linalg.svd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) the gramian matrix \(X^T X\),; Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition \(X^T X = V S V^T\) with; numpy.linalg.eigh() or scipy.linalg.eigh(),; Compute the singular values as \(\Sigma = S^\frac{1}{2}\) and the; the left singular vectors as the block matrix; \(U = X V \Sigma^{-1}\). In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice \(U\) (e.g. based on the singular values), or; discard \(U\) entirely.; If \(n \leq m\), the three stages instead use the gramian; \(X X^T = U S U^T\) and return \(V^T\) as the; block matrix \(\Sigma^{-1} U^T X\). Warning; Computing reduced SVD via the gramian presents an added wrinkle when; \(X\) is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:37611,efficient,efficient,37611,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"d form:. >>> hl.eval(hl.all([False, True, False])); False. >>> hl.eval(hl.all([True, True, True])); True. The third form:. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.all(lambda x: hl.len(x) > 3, a)); False. >>> hl.eval(hl.all(lambda x: x < 10, s)); True. Notes; -----; :func:`~.all` returns ``True`` when given an empty array or empty argument list.; """"""; base = hl.literal(True); if builtins.len(args) == 0:; return base; if builtins.len(args) == 1:; arg = arg_check(args[0], 'any', 'collection', oneof(collection_type, expr_bool)); if arg.dtype == hl.tbool:; return arg; return arg.all(lambda x: x); if builtins.len(args) == 2:; if callable(args[0]):; f = arg_check(args[0], 'all', 'f', any_to_bool_type); collection = arg_check(args[1], 'all', 'collection', collection_type); return collection.all(f); n_args = builtins.len(args); args = [args_check(x, 'all', 'exprs', i, n_args, expr_bool) for i, x in builtins.enumerate(args)]; return functools.reduce(operator.iand, args, base). [docs]@typecheck(f=func_spec(1, expr_bool), collection=expr_oneof(expr_set(), expr_array())); def find(f: Callable, collection):; """"""Returns the first element where `f` returns ``True``. Examples; --------. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.find(lambda x: x[-1] == 'x', a)); 'fox'. >>> hl.eval(hl.find(lambda x: x % 4 == 0, s)); None. Notes; -----; If `f` returns ``False`` for every element, then the result is missing. Sets are unordered. If `collection` is of type :class:`.tset`, then the; element returned comes from no guaranteed ordering. Parameters; ----------; f : function ( (arg) -> :class:`.BooleanExpression`); Function to evaluate for each element of the collection. Must return a; :class:`.BooleanExpression`.; collection : :class:`.ArrayExpression` or :class:`.SetExpression`; Collection expression. Returns; -------; :class:`.Expression`; Expression whose type is the element type of the collection.; """""";",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:111027,reduce,reduce,111027,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['reduce'],['reduce']
Energy Efficiency,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5186,efficient,efficient,5186,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['efficient'],['efficient']
Energy Efficiency,"dTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5582,reduce,reduces,5582,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['reduce'],['reduces']
Energy Efficiency,"d_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10194,reduce,reduced,10194,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduced']
Energy Efficiency,"ding left and; right singular vectors by -1. The computational complexity of SVD is; roughly :math:`nmr`. We now describe the implementation in more detail.; If :math:`\sqrt[3]{nmr}` is less than or equal to `complexity_bound`,; then :math:`X` is localized to an ndarray on which; :func:`scipy.linalg.svd` is called. In this case, all components are; returned as ndarrays. If :math:`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:74916,efficient,efficient,74916,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['efficient'],['efficient']
Energy Efficiency,"e UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantile",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90114,reduce,reduces,90114,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reduce'],['reduces']
Energy Efficiency,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:11568,efficient,efficient,11568,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['efficient'],['efficient']
Energy Efficiency,"e; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:23720,consumption,consumption,23720,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['consumption'],['consumption']
Energy Efficiency,"e=2). Filter to the upper triangle and collect to NumPy:. >>> bm.sparsify_triangle().to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:. >>> bm.sparsify_triangle(blocks_only=True).to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triangle. By; default, all elements outside the triangle but inside blocks that; overlap the triangle are set to zero as well. Parameters; ----------; lower: :obj:`bool`; If ``False``, keep the upper triangle.; If ``True``, keep the lower triangle.; blocks_only: :obj:`bool`; If ``False``, set all elements outside the triangle to zero.; If ``True``, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if lower:; lower_band = 1 - self.n_rows; upper_band = 0; else:; lower_band = 0; upper_band = self.n_cols - 1. return self.sparsify_band(lower_band, upper_band, blocks_only). @typecheck_method(intervals=expr_tuple([expr_array(expr_int64), expr_array(expr_int64)]), blocks_only=bool); def _sparsify_row_intervals_expr(self, intervals, blocks_only=False):; return BlockMatrix(BlockMatrixSparsify(self._bmir, intervals._ir, RowIntervalSparsifier(blocks_only))). @typecheck_method(indices=expr_array(expr_int32)); def _sparsify_blocks(self, indices):; return BlockMatrix(BlockMatrixSparsify(self._bmir, indices._ir, PerBlockSparsifier())). [docs] @typecheck_method(; starts=oneof(sequenceof(int), np.ndarray), stops=oneof(sequenceof(int), np.ndarray), blocks_only=bool; ); def sparsify_row_intervals(self, starts, stops, blocks_only=False):; """"""Creates a block-sparse matrix by filterin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:34006,efficient,efficient,34006,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['efficient'],['efficient']
Energy Efficiency,"eBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and it’s index (file extensions matter!):; >>> fasta = b.read_input_group(**{'fasta': 'data/example.fasta',; ... 'fasta.idx': 'data/example.fasta.idx'}). Create a resource group where the identifiers don’t match the file extensions:; >>> rg = b.read_input_group(foo='data/foo.txt',; ... bar='data/bar.txt'). rg.foo and rg.bar will not have the .txt file extension and; instead will be {root}.foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8643,charge,charges,8643,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['charge'],['charges']
Energy Efficiency,"e_level, with_local_temp_file; from hail.utils.java import Env. block_matrix_type = lazy(). [docs]class BlockMatrix(object):; """"""Hail's block-distributed matrix of :py:data:`.tfloat64` elements. .. include:: ../_templates/experimental.rst. A block matrix is a distributed analogue of a two-dimensional; `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html>`__ with; shape ``(n_rows, n_cols)`` and NumPy dtype ``float64``.; Import the class with:. >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; :meth:`default_block_size`. **Operations and broadcasting**. The core operations are consistent with NumPy: ``+``, ``-``, ``*``, and; ``/`` for element-wise addition, subtraction, multiplication, and division;; ``@`` for matrix multiplication; ``T`` for transpose; and ``**`` for; element-wise exponentiation to a scalar power. For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (:obj:`int` or :obj:`float`). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size. To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to ``float64``. One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exceptio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:3127,power,power,3127,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['power'],['power']
Energy Efficiency,"ean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – lower bound for 95% confidence interval; ci95Upper (Double) – upper bound for 95% confidence interval. Calculates the p-value, odds ratio, and 95% confidence interval with Fisher’s exact test (FET) for 2x2 tables.; Examples; Annotate each variant with Fisher’s exact test association results (assumes minor/major allele count variant annotations have been computed):; >>> (vds.annotate_variants_expr(; ... 'va.fet = let macCase = gs.filter(g => sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'macControl = gs.filter(g => !sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'majCase = gs.filter(g => sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() and '; ... 'majControl = gs.filter(g =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:4451,power,power,4451,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['power'],['power']
Energy Efficiency,"ear regression key table is:. gene; beta; se; tstat; pval. geneA; -0.084; 0.368; -0.227; 0.841. geneB; -0.542; 0.335; -1.617; 0.247. geneC; 0.075; 0.515; 0.145; 0.898. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of linear regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). linreg_multi_pheno(ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes more efficiently; than looping over linreg(). Warning; linreg_multi_pheno() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of y. va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; min_ac (int)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:90181,efficient,efficiently,90181,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"ed 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66134,monitor,monitoring,66134,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['monitor'],['monitoring']
Energy Efficiency,"ed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:130248,efficient,efficient,130248,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficient']
Energy Efficiency,"equires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:8288,efficient,efficient,8288,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['efficient'],['efficient']
Energy Efficiency,"eric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The ``&`` and ``|`` operators have higher priority than comparison; operators like ``==``, ``<``, or ``>``. Parentheses are often; necessary:. >>> x = hl.literal(5). >>> hl.eval((x < 10) & (x > 2)); True. Para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59632,power,power,59632,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['power'],['power']
Energy Efficiency,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3748,power,power,3748,docs/0.2/hail.expr.Float32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html,6,['power'],['power']
Energy Efficiency,"ession. __mul__(other)[source]; Positionally multiply by an array or a scalar.; Examples; >>> hl.eval(a2 * 5); [5, -5, 5, -5, 5, -5]. >>> hl.eval(a1 * a2); [0, -1, 2, -3, 4, -5]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to multiply by. Returns:; ArrayNumericExpression – Array of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the array.; Examples; >>> hl.eval(-a1); [0, -1, -2, -3, -4, -5]. Returns:; ArrayNumericExpression – Array expression of the same type. __pow__(other)[source]; Positionally raise to the power of an array or a scalar.; Examples; >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positional differences. __truediv__(other)[source]; Positionally divide by an array or a scalar.; Examples; >>> hl.eval(a1 / 10) ; [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. >>> hl.eval(a2 / a1) ; [inf, -1.0, 0.5, -0.3333333333333333, 0.25, -0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to divide by. Returns:; ArrayNumericExpression – Array of positional quotients. aggregate(f); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:4125,power,power,4125,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['power'],['power']
Energy Efficiency,"est')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:3720,power,power,3720,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['power'],['power']
Energy Efficiency,"et = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column order:; >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:; >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters:; indices (list of int) – List of old column indices. Returns:; MatrixTable. property col; Returns a struct expression of all column-indexed fields, including keys.; Examples; Get all column field names:; >>> list(dataset.col) ; ['s', 'sample_qc', 'is_case', 'pheno', 'cov', 'cov1', 'cov2', 'cohorts', 'pop']. Returns:; StructExpression – Struct of all column fields. property col_key; Column key struct.; Examples; Get the column key field names:; >>> list(dataset.col_key)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:19003,efficient,efficient,19003,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['efficient'],['efficient']
Energy Efficiency,"etric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^T` and :math:`Z^T Z` are the squared singular values of :math:`Z`;; therefore, we instead focus on :math:`Z^T Z`. In the expressions below, we elide transpositions; of symmetric matrices:. .. math::. \begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}. Before substituting the definition of :math:`P_0`, simplify it using the reduced QR; decomposition:. .. math::. \begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:89697,reduce,reduced,89697,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['reduce'],['reduced']
Energy Efficiency,"eturns; -------; :class:`.NDArrayNumericExpression`; A 1-dimensional ndarray from `start` to `stop` by `step`.; """"""; return _ndarray(hl.range(start, stop, step)). [docs]@typecheck(shape=shape_type, value=expr_any, dtype=nullable(HailType)); def full(shape, value, dtype=None):; """"""Creates a hail :class:`.NDArrayNumericExpression` full of the specified value. Examples; --------. Create a 5 by 7 NDArray of type :py:data:`.tfloat64` 9s. >>> hl.nd.full((5, 7), 9). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.full((5, 7), 9, dtype=hl.tint32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; value : :class:`.Expression` or python value; Value to fill ndarray with.; dtype : :class:`.HailType`; Desired hail type. Returns; -------; :class:`.NDArrayNumericExpression`; An ndarray of the specified shape filled with the specified value.; """"""; if isinstance(shape, Int64Expression):; shape_product = shape; else:; shape_product = reduce(lambda a, b: a * b, shape); return arange(hl.int32(shape_product)).map(lambda x: cast_expr(value, dtype)).reshape(shape). [docs]@typecheck(shape=shape_type, dtype=HailType); def zeros(shape, dtype=tfloat64):; """"""Creates a hail :class:`.NDArrayNumericExpression` full of zeros. Examples; --------. Create a 5 by 7 NDArray of type :py:data:`.tfloat64` zeros. >>> hl.nd.zeros((5, 7)). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.zeros((5, 7), dtype=hl.tfloat32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. See Also; --------; :func:`.full`. Returns; -------; :class:`.NDArrayNumericExpression`; ndarray of the specified size full of zeros.; """"""; return full(shape, 0, dtype). [docs]@typecheck(shape=shape_type, dtype=HailType); def ones(shape, dtype=tfloat64):; """"""Creates a hail :class:`.NDArrayNumericExpre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:4234,reduce,reduce,4234,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduce']
Energy Efficiency,"f quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper fu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:114640,reduce,reduces,114640,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['reduce'],['reduces']
Energy Efficiency,"float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def pc_relate(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) # doctest: +SKIP. Only compute the kinship statistic. This is more efficient than; computing all statistics. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') # doctest: +SKIP. Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using :meth:`.Table.filter`. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) # doctest: +SKIP. One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:2292,efficient,efficient,2292,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['efficient'],['efficient']
Energy Efficiency,"g.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9159,power,power,9159,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['power'],['power']
Energy Efficiency,"hape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (nd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:6807,reduce,reduced,6807,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"he block matrix must be stored in row-major format, as results; from :meth:`.BlockMatrix.write` with ``force_row_major=True`` and from; :meth:`.BlockMatrix.write_from_entry_expr`. Otherwise,; :meth:`export` will fail. Notes; -----; The five options for `entries` are illustrated below. Full:. .. code-block:: text. 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:. .. code-block:: text. 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:. .. code-block:: text. 0.8; 0.7 0.3. Upper triangle:. .. code-block:: text. 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:. .. code-block:: text. 0.8 0.7; 0.3. The number of columns must be less than :math:`2^{31}`. The number of partitions (file shards) exported equals the ceiling; of ``n_rows / partition_size``. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data. If `parallel` is ``None``, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below. It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with Python's ``gzip.open`` and R's ``read.table``. Parameters; ----------; path_in: :class:`str`; Path to input block matrix, stored row-major on disk.; path_out: :class:`str`; Path for export.; Use extension ``.gz`` for gzip or ``.bgz`` for block gzip.; delimiter: :class:`str`; Column delimiter.; header: :class:`str`, optional; If provided, `header` is prepended before the first row of data.; add_index: :obj:`bool`; If ``True``, add an initial column with the absolute row index.; parallel: :class:`str`, optional; If ``'header_per_shard'``, create a folder with one file per; partition, each with a header if provid",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:60535,reduce,reduces,60535,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduces']
Energy Efficiency,"hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Int32Expression.html:3746,power,power,3746,docs/0.2/hail.expr.Int32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html,2,['power'],['power']
Energy Efficiency,"iants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-allelic sites. Important; The genotype_schema() must be of type TGenotype in order to use this method. This method is much less computationally expensive than; split_multi(), and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. Returns:Dataset with no multiallelic sites, which can; be used for biallelic-only methods. Return type:VariantDataset. filter_samples_expr(expr, keep=True)[source]¶; Filter samples with the expression language.; Examples; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:54956,efficient,efficiently,54956,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"iants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:75142,efficient,efficiently,75142,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:55708,reduce,reduce,55708,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduce']
Energy Efficiency,"indexed call expression.; block_size (int, optional) – Block size of block matrices used in the algorithm.; Default given by BlockMatrix.default_block_size(). Returns:; MatrixTable – A MatrixTable whose rows and columns are keys are taken from; call-expr’s column keys. It has one entry field, phi. hail.methods.pc_relate(call_expr, min_individual_maf, *, k=None, scores_expr=None, min_kinship=None, statistics='all', block_size=None, include_self_kinship=False)[source]; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Note; Requires the dataset to contain only diploid genotype calls. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) . Only compute the kinship statistic. This is more efficient than; computing all statistics.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') . Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\rig",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:12472,efficient,efficient,12472,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['efficient'],['efficient']
Energy Efficiency,"ion Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:1291,charge,charges,1291,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['charge'],['charges']
Energy Efficiency,"ition_size=2). This produces two compressed files which uncompress to:; idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. idx A B C; 2 0.7 0.3 1.0. Warning; The block matrix must be stored in row-major format, as results; from BlockMatrix.write() with force_row_major=True and from; BlockMatrix.write_from_entry_expr(). Otherwise,; export() will fail. Notes; The five options for entries are illustrated below.; Full:; 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:; 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:; 0.8; 0.7 0.3. Upper triangle:; 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:; 0.8 0.7; 0.3. The number of columns must be less than \(2^{31}\).; The number of partitions (file shards) exported equals the ceiling; of n_rows / partition_size. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data.; If parallel is None, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below.; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with Python’s gzip.open and R’s read.table. Parameters:. path_in (str) – Path to input block matrix, stored row-major on disk.; path_out (str) – Path for export.; Use extension .gz for gzip or .bgz for block gzip.; delimiter (str) – Column delimiter.; header (str, optional) – If provided, header is prepended before the first row of data.; add_index (bool) – If True, add an initial column with the absolute row index.; parallel (str, optional) – If 'header_per_shard', create a folder with one file per; partition, each with a header if provided.; If 'separate_header', create a folder with one file per; partition without a hea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:14884,reduce,reduces,14884,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduces']
Energy Efficiency,"k:: text. idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. .. code-block:: text. idx A B C; 2 0.7 0.3 1.0. Warning; -------; The block matrix must be stored in row-major format, as results; from :meth:`.BlockMatrix.write` with ``force_row_major=True`` and from; :meth:`.BlockMatrix.write_from_entry_expr`. Otherwise,; :meth:`export` will fail. Notes; -----; The five options for `entries` are illustrated below. Full:. .. code-block:: text. 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:. .. code-block:: text. 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:. .. code-block:: text. 0.8; 0.7 0.3. Upper triangle:. .. code-block:: text. 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:. .. code-block:: text. 0.8 0.7; 0.3. The number of columns must be less than :math:`2^{31}`. The number of partitions (file shards) exported equals the ceiling; of ``n_rows / partition_size``. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data. If `parallel` is ``None``, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below. It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with Python's ``gzip.open`` and R's ``read.table``. Parameters; ----------; path_in: :class:`str`; Path to input block matrix, stored row-major on disk.; path_out: :class:`str`; Path for export.; Use extension ``.gz`` for gzip or ``.bgz`` for block gzip.; delimiter: :class:`str`; Column delimiter.; header: :class:`str`, optional; If provided, `header` is prepended before the first row of data.; add_index: :obj:`bool`; If ``True``, add an initial column with the absolute row index.; parallel: :class",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:60407,efficient,efficient,60407,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['efficient'],['efficient']
Energy Efficiency,"lds:; row_idx (:py:data.`tint64`, key field) – Row index; entries (tarray of tfloat64) – Entries for the row. Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters:. n_partitions (int or None) – Number of partitions of the table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by functions such as numpy.fromfile; (if a local file) and BlockMatrix.fromfile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:42522,reduce,reduce,42522,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduce']
Energy Efficiency,"le must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:18438,efficient,efficient,18438,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['efficient'],['efficient']
Energy Efficiency,"le.gz',; ... header=' '.join(['idx', 'A', 'B', 'C']),; ... add_index=True,; ... parallel='header_per_shard',; ... partition_size=2). This produces two compressed files which uncompress to:; idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. idx A B C; 2 0.7 0.3 1.0. Warning; The block matrix must be stored in row-major format, as results; from BlockMatrix.write() with force_row_major=True and from; BlockMatrix.write_from_entry_expr(). Otherwise,; export() will fail. Notes; The five options for entries are illustrated below.; Full:; 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:; 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:; 0.8; 0.7 0.3. Upper triangle:; 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:; 0.8 0.7; 0.3. The number of columns must be less than \(2^{31}\).; The number of partitions (file shards) exported equals the ceiling; of n_rows / partition_size. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data.; If parallel is None, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below.; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with Python’s gzip.open and R’s read.table. Parameters:. path_in (str) – Path to input block matrix, stored row-major on disk.; path_out (str) – Path for export.; Use extension .gz for gzip or .bgz for block gzip.; delimiter (str) – Column delimiter.; header (str, optional) – If provided, header is prepended before the first row of data.; add_index (bool) – If True, add an initial column with the absolute row index.; parallel (str, optional) – If 'header_per_shard', create a folder with one file per; parti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:14756,efficient,efficient,14756,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"les(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:13796,power,power,13796,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['power'],['power']
Energy Efficiency,"lled packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl datap",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:85603,allocate,allocated,85603,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['allocate'],['allocated']
Energy Efficiency,"ls=bool,; _drop_cols=bool,; _drop_rows=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_matrix_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _drop_cols=_drop_cols,; _drop_rows=_drop_rows,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(; output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str), _partitions=nullable(expr_any); ); def write(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _partitions=None,; ):; """"""Write to disk. Examples; --------. >>> data",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:81075,efficient,efficient,81075,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['efficient'],['efficient']
Energy Efficiency,"m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in descending order,; \(V^T\) is an \(r \times m\) matrix whose rows are; (orthonormal) right singular vectors. If the singular values in \(\Sigma\) are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on which; scipy.linalg.svd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) the gramian matrix \(X^T X\),; Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition \(X^T X = V S V^T\) with; numpy.linalg.eigh() or scipy.linalg.eigh(),; Compute the singular values as \(\Sigma = S^\frac{1}{2}\) and the; the left singular vectors as the block matrix; \(U = X V \Sigma^{-1}\). In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice \(U\) (e.g. based on the singular values), or; discard \(U\) entirely.; If \(n \leq m\), the three stages instead use the gramian; \(X X^T = U S U^T\) and return \(V^T\) as the; block matrix \(\Sigma^{-1} U^T X\). Warning; Computing reduced SVD via the gramian presents an added wrinkle when; \(X\) is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:37091,reduce,reduced,37091,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \times r` matrix whose columns are; (orthonormal) left singular vectors,. - :math:`\Sigma` is an :math:`r \times r` diagonal matrix of non-negative; singular values in descending order,. - :math:`V^T` is an :math:`r \times m` matrix whose rows are; (orthonormal) right singular vectors. If the singular values in :math:`\Sigma` are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly :math:`nmr`. We now describe the implementation in more detail.; If :math:`\sqrt[3]{nmr}` is less than or equal to `complexity_bound`,; then :math:`X` is localized to an ndarray on which; :func:`scipy.linalg.svd` is called. In this case, all components are; returned as ndarrays. If :math:`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:74351,reduce,reduced,74351,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduced']
Energy Efficiency,"mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1638,power,powerful,1638,index.html,https://hail.is,https://hail.is/index.html,1,['power'],['powerful']
Energy Efficiency,"n \times n\) diagonal matrix of eigenvalues of \(K\) in descending order. \(S_{ii}\) is the eigenvalue of eigenvector \(U_{:,i}\); \(U^T = n \times n\) orthonormal matrix, the transpose (and inverse) of \(U\). A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\mathbb{R}^n\) by \(U^T\) transforms the model to one with independent residuals. For any particular value of \(\delta\), the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:101256,efficient,efficient,101256,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficient']
Energy Efficiency,"n open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Pla",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10151,charge,charges,10151,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['charge'],['charges']
Energy Efficiency,"nal” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $ impala-shell -q ""COMPUTE STATS variants"". Before running any queries it’s worth understanding the table schema, which is easily; done by calling DESCRIBE on the table:; $ impala-shell -q ""DESCRIBE variants"". +-------------+----------------------------------+-----------------------------+; | name | type | comment |; +-------------+----------------------------------+-----------------------------+; | variant | struct< | Inferred from Parquet file. |; | | contig:string, | |; | | start:int, | |; | | ref:string, | |; | | altalleles:array<struct< | |; | | ref:string, | |; | | alt:string | |; | | >> | |; | | > | |; | annotations | struct< | Inferred from Parquet file. |; | | rsid:string, | |; | | qual:double, | |; | | filters:array<string>, | |; | | pass:boolean, | |; | | info:struct< | |; | | negative_train_site:boolean, | |; | | hwp:double, | |; | | ac:array<int>, | |; ...; | | > | |; | | > | |; | gs | array<struct< | Infer",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/sql.html:1986,efficient,efficiently,1986,docs/0.1/sql.html,https://hail.is,https://hail.is/docs/0.1/sql.html,1,['efficient'],['efficiently']
Energy Efficiency,"nce(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggreg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9179,power,power,9179,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['power'],['power']
Energy Efficiency,"nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \times r` matrix whose columns are; (orthonormal) left singular vectors,. - :math:`\Sigma` is an :math:`r \times r` diagonal matrix of non-negative; singular values in descending order,. - :math:`V^T` is an :math:`r \times m` matrix whose rows are; (orthonormal) right singular vectors. If the singular values in :math:`\Sigma` are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly :math:`nmr`. We now describe the implementation in more detail.; If :math:`\sqrt[3]{nmr}` is less than or equal to `complexity_bound`,; then :math:`X` is localized to an ndarray on which; :func:`scipy.linalg.svd` is called. In this case, all components are; returned as ndarrays. If :math:`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:73395,reduce,reduced,73395,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['reduce'],['reduced']
Energy Efficiency,"ndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9271,reduce,reduced,9271,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduced']
Energy Efficiency,"ndent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. hail.nd.svd(nd, full_matrices=True, compute_uv=True)[source]; Performs a singular value decomposition. Parameters:. nd (NDArrayNumericExpression) – A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:7100,reduce,reduced,7100,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"near_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9582,reduce,reduced,9582,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduced']
Energy Efficiency,"ning example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker contain",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:11322,charge,charges,11322,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['charge'],['charges']
Energy Efficiency,"nning pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:33600,reduce,reduce,33600,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reduce'],['reduce']
Energy Efficiency,"numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:4066,power,power,4066,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,1,['power'],['power']
Energy Efficiency,"of :obj:`int`; Indices of rows to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""; BlockMatrix._check_indices(rows_to_keep, self.n_rows); return BlockMatrix(BlockMatrixFilter(self._bmir, [rows_to_keep, []])). [docs] @typecheck_method(cols_to_keep=sequenceof(int)); def filter_cols(self, cols_to_keep):; """"""Filters matrix columns. Parameters; ----------; cols_to_keep: :obj:`list` of :obj:`int`; Indices of columns to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""; BlockMatrix._check_indices(cols_to_keep, self.n_cols); return BlockMatrix(BlockMatrixFilter(self._bmir, [[], cols_to_keep])). [docs] @typecheck_method(rows_to_keep=sequenceof(int), cols_to_keep=sequenceof(int)); def filter(self, rows_to_keep, cols_to_keep):; """"""Filters matrix rows and columns. Notes; -----; This method has the same effect as :meth:`BlockMatrix.filter_cols`; followed by :meth:`BlockMatrix.filter_rows` (or vice versa), but; filters the block matrix in a single pass which may be more efficient. Parameters; ----------; rows_to_keep: :obj:`list` of :obj:`int`; Indices of rows to keep. Must be non-empty and increasing.; cols_to_keep: :obj:`list` of :obj:`int`; Indices of columns to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""; BlockMatrix._check_indices(rows_to_keep, self.n_rows); BlockMatrix._check_indices(cols_to_keep, self.n_cols); return BlockMatrix(BlockMatrixFilter(self._bmir, [rows_to_keep, cols_to_keep])). @staticmethod; def _pos_index(i, size, name, allow_size=False):; if 0 <= i < size or (i == size and allow_size):; return i; elif 0 <= i + size < size:; return i + size; else:; raise ValueError(f'invalid {name} {i} for axis of size {size}'). @staticmethod; def _range_to_keep(idx, size):; if isinstance(idx, int):; pos_idx = BlockMatrix._pos_index(idx, size, 'index'); return slice(pos_idx, pos_idx + 1, 1). assert isinstance(idx, slice); if idx.step and idx.step <= 0:; raise ValueE",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:27991,efficient,efficient,27991,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['efficient'],['efficient']
Energy Efficiency,"of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / block_size)^2`` if all blocks are computed. Warning; -------; See all warnings on :meth:`.BlockMatrix.from_entry_expr`. In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; :meth:`.BlockMatrix.write_from_entry_expr`. The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; :meth:`ld_matrix`. To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; block_size : :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.BlockMatrix`; Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index.; """"""; bm = BlockMatrix.from_entry_expr(entry_expr, mean_impute=True, center=True, normalize=True, block_size=block_size); return bm @ bm.T. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; block_size=nullable(int),; ); def ld_matrix(entry_expr, locus_expr, radius, coord_expr=None, block_size=None) -> BlockMatrix:; """"""Com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:134975,efficient,efficiently,134975,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['efficient'],['efficiently']
Energy Efficiency,"of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The ``&`` and ``|`` operators have higher priority than comparison; operators like ``==``, ``<``, or ``>``. Parentheses are often; necessar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59585,power,power,59585,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['power'],['power']
Energy Efficiency,"ollection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of linear regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.linregBurden(key_name, variant_keys, single_key, agg_expr, y,; jarray(Env.jvm().java.lang.String, covariates)); linreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return linreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg_multi_pheno(self, ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes more efficiently; than looping over :py:meth:`.linreg`. .. warning::. :py:meth:`.linreg_multi_pheno` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of ``y``. - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:110464,efficient,efficiently,110464,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"omplete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Hei",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:12807,reduce,reduces,12807,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['reduce'],['reduces']
Energy Efficiency,"on(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise Not",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8377,reduce,reduces,8377,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['reduce'],['reduces']
Energy Efficiency,"one),; ""tooltip"": (""hovertext"", None),; ""fill_legend"": (""name"", None),; ""alpha"": (""marker_opacity"", None),; }. def __init__(self, aes, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False):; super().__init__(aes); self.k = k; self.smoothing = smoothing; self.fill = fill; self.color = color; self.alpha = alpha; self.smoothed = smoothed. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; from hail.expr.functions import _error_from_cdf_python. def plot_group(df, idx):; data = df.attrs['data']. if self.smoothed:; n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev):; inv_scale = (np.sqrt(n * slope) / self.smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (; (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); ); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). trace_args = {; ""x"": x_d,; ""y"": final,; ""mode"": ""lines"",; ""fill"": ""tozeroy"",; ""row"": facet_row,; ""col"": facet_col,; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_scatter(**trace_args); else:; confidence = 5. y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]). left = np.concatenate([[min_x], x[keep]]); right = np.concatenate([x[keep], [max_x]]); widths = right - left. trace_args = {; ""x"": [min_x, *x[keep]],; ""y"": slopes,; ""row"": facet_row,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:18612,power,power,18612,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['power'],['power']
Energy Efficiency,"or Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27096,reduce,reduce,27096,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reduce'],['reduce']
Energy Efficiency,"order. :math:`S_{ii}` is the eigenvalue of eigenvector :math:`U_{:,i}`; - :math:`U^T = n \\times n` orthonormal matrix, the transpose (and inverse) of :math:`U`. A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^TX\\beta, \sigma_g^2 (S + \delta I)\\right). for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (:math:`y`) and covariate vectors (columns of :math:`X`) in :math:`\mathbb{R}^n` by :math:`U^T` transforms the model to one with independent residuals. For any particular value of :math:`\delta`, the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:130258,efficient,efficient,130258,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficient']
Energy Efficiency,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:6949,power,power,6949,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['power'],['power']
Energy Efficiency,"ot explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.count()), _localize=False; ); mt = mt.annotate_globals(yvec=hl.nd.array(yvec), covmat=hl.nd.array(covmat), n_complete_samples=n); # Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #; covmat_Q, _ = hl.nd.qr(mt.covmat); mt = mt.annotate_globals(covmat_Q=covmat_Q); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:81671,reduce,reduced,81671,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['reduce'],['reduced']
Energy Efficiency,"p(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tnd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10144,reduce,reduced,10144,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduced']
Energy Efficiency,"parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:11613,charge,charges,11613,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['charge'],['charges']
Energy Efficiency,"passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5471,reduce,reduces,5471,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['reduce'],['reduces']
Energy Efficiency,"patible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1630,power,powerful,1630,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['power'],['powerful']
Energy Efficiency,"r. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linregMultiPheno(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac,; min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; variant_block_size=integral); def linreg3(self, ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes; more efficiently than looping over :py:meth:`.linreg`. This; method is more efficient than :py:meth:`.linreg_multi_pheno`; but doesn't implicitly filter on allele count or allele; frequency. .. warning::. :py:meth:`.linreg3` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of ``y``. - **va.linreg.nCompleteSamples** (*Int*) -- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.li",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:112428,efficient,efficiently,112428,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"riantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles ar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:169478,efficient,efficient,169478,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficient']
Energy Efficiency,"rn construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13998,efficient,efficient,13998,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['efficient'],['efficient']
Energy Efficiency,"rns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. hail.nd.svd(nd, full_matrices=True, compute_uv=True)[source]; Performs a singular value decomposition. Parameters:. nd (NDArrayNumericExpression) – A 2 dimensional ndarray, shape(M, N).; full_matrices (bool) – If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv (bool) – If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns:. - u (NDArrayNumericExpression) – The left singular vectors.; - s (NDArrayNumericExpression) – The singular values.; - vt (NDArrayNumericExpression) – The right singular vectors. hail.nd.inv(nd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:7669,reduce,reduced,7669,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"ry('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid arguments. ‘lowmem’ corresponds to; approximately 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6817,charge,charges,6817,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['charge'],['charges']
Energy Efficiency,"r} requires at least one argument""); if (; builtins.len(exprs) == 1; and (isinstance(exprs[0].dtype, (tarray, tset))); and is_numeric(exprs[0].dtype.element_type); ):; [e] = exprs; if filter_nan and e.dtype.element_type in (tfloat32, tfloat64):; name = 'nan' + name; return array(e)._filter_missing_method(filter_missing, name, exprs[0].dtype.element_type); else:; if not builtins.all(is_numeric(e.dtype) for e in exprs):; expr_types = ', '.join(""'{}'"".format(e.dtype) for e in exprs); raise TypeError(; f""{name!r} expects a single numeric array expression or multiple numeric expressions\n""; f"" Found {builtins.len(exprs)} arguments with types {expr_types}""; ); unified_typ = unify_types_limited(*(e.dtype for e in exprs)); ec = coercer_from_dtype(unified_typ); indices, aggs = unify_all(*exprs). func_name = name; if filter_missing:; func_name += '_ignore_missing'; if filter_nan and unified_typ in (tfloat32, tfloat64):; func_name = 'nan' + func_name; return construct_expr(; functools.reduce(lambda l, r: ir.Apply(func_name, unified_typ, l, r), [ec.coerce(e)._ir for e in exprs]),; unified_typ,; indices,; aggs,; ). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filter_missing=builtins.bool; ); def nanmax(*exprs, filter_missing: builtins.bool = True) -> NumericExpression:; """"""Returns the maximum value of a collection or of given arguments, excluding NaN. Examples; --------. Compute the maximum value of an array:. >>> hl.eval(hl.nanmax([1.1, 50.1, float('nan')])); 50.1. Take the maximum value of arguments:. >>> hl.eval(hl.nanmax(1.1, 50.1, float('nan'))); 50.1. Notes; -----; Like the Python builtin ``max`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the maximum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:124815,reduce,reduce,124815,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['reduce'],['reduce']
Energy Efficiency,"s 'f' to return an expression of type '{}', found '{}'"".format(s, t)); return t. return collection.flatmap(f). [docs]@typecheck(f=func_spec(1, expr_any), collection=expr_oneof(expr_set(), expr_array())); def group_by(f: Callable, collection) -> DictExpression:; """"""Group collection elements into a dict according to a lambda function. Examples; --------. >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.group_by(lambda x: hl.len(x), a)); {3: ['The', 'fox'], 5: ['quick', 'brown']}. Parameters; ----------; f : function ( (arg) -> :class:`.Expression`); Function to evaluate for each element of the collection to produce a key for the; resulting dictionary.; collection : :class:`.ArrayExpression` or :class:`.SetExpression`; Collection expression. Returns; -------; :class:`.DictExpression`.; Dictionary keyed by results of `f`.; """"""; return collection.group_by(f). [docs]@typecheck(f=func_spec(2, expr_any), zero=expr_any, collection=expr_oneof(expr_set(), expr_array())); def fold(f: Callable, zero, collection) -> Expression:; """"""Reduces a collection with the given function `f`, provided the initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`.; collection : :class:`.ArrayExpression` or :class:`.SetExpression`. Returns; -------; :class:`.Expression`; """"""; return collection.fold(lambda x, y: f(x, y), zero). [docs]@typecheck(f=func_spec(2, expr_any), zero=expr_any, a=expr_array()); def array_scan(f: Callable, zero, a) -> ArrayExpression:; """"""Map each element of `a` to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Paramet",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:114047,Reduce,Reduces,114047,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Reduce'],['Reduces']
Energy Efficiency,"s well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the Matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:23208,power,powered,23208,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['power'],['powered']
Energy Efficiency,"s. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5255,power,power,5255,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,1,['power'],['power']
Energy Efficiency,"s/hail:0.2.37. COPY run_gwas.py /. batch_clumping.py; import hailtop.batch as hb. def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(; ofile={'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam', 'assoc': '{root}.assoc'}; ); g.command(f""""""; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; """"""); return g. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f""""""; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; """"""); return c. def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f""""""; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; """"""); return merger. if __name__ == '__main__':; backend = hb.ServiceBackend(); batch = hb.Batch(backend=backend, name='clumping'). vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). g = gwas(batch, vcf, phenotypes). results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). batch.run(open=True, wait=False); backend.close(). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:15309,consumption,consumption,15309,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['consumption'],['consumption']
Energy Efficiency,"se, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object wit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:7801,schedul,scheduled,7801,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['schedul'],['scheduled']
Energy Efficiency,"set(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(force_local=bool); def ld_matrix(self, force_local=False):; """"""Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. **Examples**. >>> ld_mat = vds.ld_matrix(). **Notes**. Each entry (i, j) in the LD matrix gives the :math:`r` value between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::. \\rho_{x_i,x_j} = \\frac{\\mathrm{Cov}(X_i,X_j)}{\\sigma_{X_i} \\sigma_{X_j}}. Also note that variants with zero variance (:math:`\\sigma = 0`) will be dropped from the matrix. .. caution::. The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you'll want to reduce the number of variants with methods like; :py:meth:`.sample_variants`, :py:meth:`.filter_variants_expr`, or :py:meth:`.ld_prune` before; calling this unless your dataset is very small. :param bool force_local: If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds :math:`5000^2` and locally otherwise. :return: Matrix of r values between pairs of variants.; :rtype: :py:class:`LDMatrix`; """""". jldm = self._jvdf.ldMatrix(force_local); return LDMatrix(jldm). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg(self, y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association us",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:96704,reduce,reduce,96704,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['reduce'],['reduce']
Energy Efficiency,"son. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5389,power,power,5389,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,1,['power'],['power']
Energy Efficiency,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:14084,charge,charges,14084,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['charge'],['charges']
Energy Efficiency,"stance, `flatmap` on a ``set<str>`` should take; a ``str`` and return a ``set``. Returns; -------; :class:`.CollectionExpression`; """"""; expected_type, s = (tarray, 'array') if isinstance(self._type, tarray) else (tset, 'set'); value_type = f(construct_variable(Env.get_uid(), self.dtype.element_type)).dtype. if not isinstance(value_type, expected_type):; raise TypeError(; ""'flatmap' expects 'f' to return an expression of type '{}', found '{}'"".format(s, value_type); ). def f2(x):; return hl.array(f(x)) if isinstance(value_type, tset) else f(x). def transform_ir(array, name, body):; return ir.toArray(ir.StreamFlatMap(ir.toStream(array), name, ir.ToStream(body))). array_flatmap = hl.array(self)._ir_lambda_method(transform_ir, f2, self.dtype.element_type, identity). if isinstance(self.dtype, tset):; return hl.set(array_flatmap); assert isinstance(self.dtype, tarray), self.dtype; return array_flatmap. [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def fold(self, f, zero):; """"""Reduces the collection with the given function `f`, provided the initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`. Returns; -------; :class:`.Expression`.; """"""; collection = self; if not isinstance(collection, ArrayExpression):; collection = hl.array(collection); return collection._to_stream().fold(lambda x, y: f(x, y), zero). [docs] @typecheck_method(f=func_spec(1, expr_bool)); def all(self, f):; """"""Returns ``True`` if `f` returns ``True`` for every element. Examples; --------. >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; -----; This method returns ``True`` if the collection is empty. Parameters; ----------; f : function ( (arg) -> :class",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:6493,Reduce,Reduces,6493,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['Reduce'],['Reduces']
Energy Efficiency,"sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in descending order,; \(V^T\) is an \(r \times m\) matrix whose rows are; (orthonormal) right singular vectors. If the singular values in \(\Sigma\) are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on which; scipy.linalg.svd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:36204,reduce,reduced,36204,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"t aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:87224,efficient,efficient,87224,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['efficient'],['efficient']
Energy Efficiency,"t to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2574,charge,charges,2574,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,2,['charge'],['charges']
Energy Efficiency,"t.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the uns",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:82258,monitor,monitor,82258,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['monitor'],['monitor']
Energy Efficiency,"t:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <ht",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:145672,reduce,reduces,145672,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['reduce'],['reduces']
Energy Efficiency,"t[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; MatrixTable – Matrix table where each entry corresponds to an entry in the block matrix. to_ndarray()[source]; Collects a BlockMatrix into a local hail ndarray expression on driver. This should not; be done for large matrices. Returns:; NDArrayExpression. to_numpy(_force_blocking=False)[source]; Collects the block matrix into a NumPy ndarray.; Examples; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; The resulting ndarray will have the same shape as the block matrix. Returns:; numpy.ndarray. to_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; row_idx (:py:data.`tint64`, key field) – Row index; entries (tarray o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:40596,reduce,reduce,40596,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduce']
Energy Efficiency,"t_type),; key='interval',; ); else:; key_dtype = calling_intervals.key.dtype; if (; len(key_dtype) != 1; or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.referen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:14916,efficient,efficient,14916,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['efficient'],['efficient']
Energy Efficiency,"tch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:1048,schedul,scheduler,1048,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['schedul'],['scheduler']
Energy Efficiency,"th::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). [docs]@typecheck(nd=expr_ndarray(), full_matrices=bool, compute_uv=bool); def svd(nd, full_matrices=True, compute_uv=True):; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv : :class:`.b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10624,reduce,reduced,10624,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduced']
Energy Efficiency,"tion:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:9549,efficient,efficient,9549,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['efficient'],['efficient']
Energy Efficiency,"tistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check wheth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:19082,consumption,consumption,19082,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['consumption'],['consumption']
Energy Efficiency,"turn self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Divide two numbers with floor division. Examples; --------. >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; The floor of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(ot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59199,power,power,59199,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,4,['power'],['power']
Energy Efficiency,"t{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\beta_v, \sigma_{g,v}^2 (K + \hat{\delta} I)\right)\]; Note that the only new rotation to compute here is \(U^T v\).; To test the null hypothesis that the genotype coefficient \(\beta^0_v\) is zero, we consider the restricted model with parameters \(((0, \beta^1_v, \ldots, \beta^c_v), \sigma_{g,v}^2)\) within the full model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lmmreg() can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter n_eigs to use only the top n_eigs eigenvectors. Alternatively, specify dropped_variance_fraction to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, dropped_varian",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:106290,reduce,reduces,106290,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['reduce'],['reduces']
Energy Efficiency,"ult_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and it’s index (file extensions matter!):; >>> fasta =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8139,charge,charges,8139,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['charge'],['charges']
Energy Efficiency,"umns are keys are taken from; call-expr’s column keys. It has one entry field, phi. hail.methods.pc_relate(call_expr, min_individual_maf, *, k=None, scores_expr=None, min_kinship=None, statistics='all', block_size=None, include_self_kinship=False)[source]; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Note; Requires the dataset to contain only diploid genotype calls. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) . Only compute the kinship statistic. This is more efficient than; computing all statistics.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') . Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:12671,efficient,efficient,12671,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['efficient'],['efficient']
Energy Efficiency,"unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:80381,efficient,efficient,80381,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['efficient'],['efficient']
Energy Efficiency,"up even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... pr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:13343,efficient,efficient,13343,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['efficient'],['efficient']
Energy Efficiency,"vd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) the gramian matrix \(X^T X\),; Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition \(X^T X = V S V^T\) with; numpy.linalg.eigh() or scipy.linalg.eigh(),; Compute the singular values as \(\Sigma = S^\frac{1}{2}\) and the; the left singular vectors as the block matrix; \(U = X V \Sigma^{-1}\). In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice \(U\) (e.g. based on the singular values), or; discard \(U\) entirely.; If \(n \leq m\), the three stages instead use the gramian; \(X X^T = U S U^T\) and return \(V^T\) as the; block matrix \(\Sigma^{-1} U^T X\). Warning; Computing reduced SVD via the gramian presents an added wrinkle when; \(X\) is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:37877,reduce,reduced,37877,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"x rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; Creates a block matrix from a binary file. log; Element-wise natural logarithm. persist; Persists this block matrix in memory or on disk. random; Creates a block matrix with standard normal or uniform random entries. read; Reads a block matrix. rectangles_to_numpy; Instantiates a NumPy ndarray from files of rectangles written out using export_rectangles() or export_blocks(). sparsify_band; Filter to a diagonal band. sparsify_rectangles; Filter to blocks overlapping the union of rectangular regions. sparsify_row_intervals; Creates a block-sparse matrix by filtering to an interval for each row. sparsify_triangle; Filter to the upper or lower triangle. sqrt; Element-wise square root. sum; Sums array elements over one or both axes. svd; Computes the reduced singular value decomposition. to_matrix_table_row_major; Returns a matrix table with row key of row_idx and col key col_idx, whose entries are structs of a single field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:9621,reduce,reduced,9621,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"xt"", None),; ""fill_legend"": (""name"", None),; ""alpha"": (""marker_opacity"", None),; }. def __init__(self, aes, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False):; super().__init__(aes); self.k = k; self.smoothing = smoothing; self.fill = fill; self.color = color; self.alpha = alpha; self.smoothed = smoothed. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; from hail.expr.functions import _error_from_cdf_python. def plot_group(df, idx):; data = df.attrs['data']. if self.smoothed:; n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev):; inv_scale = (np.sqrt(n * slope) / self.smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (; (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); ); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). trace_args = {; ""x"": x_d,; ""y"": final,; ""mode"": ""lines"",; ""fill"": ""tozeroy"",; ""row"": facet_row,; ""col"": facet_col,; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_scatter(**trace_args); else:; confidence = 5. y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]). left = np.concatenate([[min_x], x[keep]]); right = np.concatenate([x[keep], [max_x]]); widths = right - left. trace_args = {; ""x"": [min_x, *x[keep]],; ""y"": slopes,; ""row"": facet_row,; ""col"": facet_col,; ""width""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:18632,power,power,18632,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['power'],['power']
Energy Efficiency,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:1697,efficient,efficiently,1697,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['efficient'],['efficiently']
Energy Efficiency,"{29}\). Parameters:. path_out (str) – Path for folder of exported files.; rectangles (list of list of int) – List of rectangles of the form; [row_start, row_stop, col_start, col_stop].; delimiter (str) – Column delimiter.; binary (bool) – If true, export elements as raw bytes in row major order. classmethod fill(n_rows, n_cols, value, block_size=None)[source]; Creates a block matrix with all elements the same value.; Examples; Create a block matrix with 10 rows, 20 columns, and all elements equal to 1.0:; >>> bm = BlockMatrix.fill(10, 20, 1.0). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; value (float) – Value of all elements.; block_size (int, optional) – Block size. Default given by default_block_size(). Returns:; BlockMatrix. filter(rows_to_keep, cols_to_keep)[source]; Filters matrix rows and columns.; Notes; This method has the same effect as BlockMatrix.filter_cols(); followed by BlockMatrix.filter_rows() (or vice versa), but; filters the block matrix in a single pass which may be more efficient. Parameters:. rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing.; cols_to_keep (list of int) – Indices of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_cols(cols_to_keep)[source]; Filters matrix columns. Parameters:; cols_to_keep (list of int) – Indices of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_rows(rows_to_keep)[source]; Filters matrix rows. Parameters:; rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:20461,efficient,efficient,20461,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"{g,v}^2)` via rotation of the model. .. math::. y \\sim \\mathrm{N}\\left(X_v\\beta_v, \sigma_{g,v}^2 (K + \\hat{\delta} I)\\right). Note that the only new rotation to compute here is :math:`U^T v`. To test the null hypothesis that the genotype coefficient :math:`\\beta^0_v` is zero, we consider the restricted model with parameters :math:`((0, \\beta^1_v, \ldots, \\beta^c_v), \sigma_{g,v}^2)` within the full model with parameters :math:`(\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter ``n_eigs`` to use only the top ``n_eigs`` eigenvectors. Alternatively, specify ``dropped_variance_fraction`` to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:135828,reduce,reduces,135828,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['reduce'],['reduces']
Energy Efficiency,"; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3939,power,power,3939,docs/0.2/hail.expr.Float32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html,4,['power'],['power']
Energy Efficiency,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets.html:1170,charge,charges,1170,docs/0.2/datasets.html,https://hail.is,https://hail.is/docs/0.2/datasets.html,1,['charge'],['charges']
Energy Efficiency,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/try.html:667,power,powered,667,docs/0.2/install/try.html,https://hail.is,https://hail.is/docs/0.2/install/try.html,1,['power'],['powered']
Energy Efficiency,"﻿. Hail | ; hail.nd.nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.nd.nd. Source code for hail.nd.nd; from functools import reduce. import hail as hl; from hail.expr.expressions import (; Int64Expression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_tuple,; unify_all,; ); from hail.expr.expressions.typed_expressions import NDArrayNumericExpression; from hail.expr.functions import _ndarray; from hail.expr.functions import array as aarray; from hail.expr.types import HailType, tfloat32, tfloat64, tndarray, ttuple; from hail.ir import Apply, NDArrayConcat, NDArrayEigh, NDArrayInv, NDArrayQR, NDArraySVD; from hail.typecheck import nullable, oneof, sequenceof, tupleof, typecheck. tsequenceof_nd = oneof(sequenceof(expr_ndarray()), expr_array(expr_ndarray())); shape_type = oneof(expr_int64, tupleof(expr_int64), expr_tuple()). [docs]def array(input_array, dtype=None):; """"""Construct an :class:`.NDArrayExpression`. Examples; --------. >>> hl.eval(hl.nd.array([1, 2, 3, 4])); array([1, 2, 3, 4], dtype=int32). >>> hl.eval(hl.nd.array([[1, 2, 3], [4, 5, 6]])); array([[1, 2, 3],; [4, 5, 6]], dtype=int32). >>> hl.eval(hl.nd.array(np.identity(3))); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]). >>> hl.eval(hl.nd.array(hl.range(10, 20))); array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=int32). Parameters; ----------; input_array : :class:`.ArrayExpression`, numpy ndarray, or nested python lists/tuples; The array to convert to a Hail ndarray.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. Returns; -------; :class:`.NDArray",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:481,reduce,reduce,481,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['reduce'],['reduce']
Energy Efficiency,"﻿. Hail | Get Help . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/gethelp.html:159,efficient,efficient,159,gethelp.html,https://hail.is,https://hail.is/gethelp.html,1,['efficient'],['efficient']
Integrability," (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating infor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:103575,message,message,103575,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability," (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_type, indices, aggregations). if b_ndim_orig == 1:; if no_crash:; result = hl.struct(solution=result.solution.reshape((-1)), failed=result.failed); else:; result = result.reshape((-1)); return result. [docs]@typecheck(A=expr_ndarray(), b=expr_ndarray(), lower=expr_bool, no_crash=bool); def solve_triangular(A, b, lower=False, no_crash=False):; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpression`, (N, N); Triangular coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables.; lower : `bool`:; If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the triangular system Ax = B. Shape is same as shape of B. """"""; nd_dep_ndim_orig = b.ndim; A, b = solve_helper(A, b, nd_dep_ndim_orig). indices, aggregations = unify_all(A, b). if no_crash:; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:7797,Depend,Dependent,7797,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['Depend'],['Dependent']
Integrability," (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be included in the; calculation.; In the case that there are no non-missing pairs, the result will be missing. See also; linreg(). Parameters:. x (Expression of type tfloat64); y (Expression of type tfloat64). Returns:; Float64Expression. hail.expr.aggregators.group_by(group, agg_expr)[source]; Compute aggregation statistics stratified ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:29666,depend,dependent,29666,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['depend'],['dependent']
Integrability," 0. In the above conditional, the condition is x > 0, the consequent is 1,; and the alternate is 0.; Here is the Hail expression equivalent with if_else():; >>> hl.if_else(x > 0, 1, 0); <Int32Expression of type int32>. This example returns an Int32Expression which can be used in more; computations. We can add the conditional expression to our array a from; earlier:; >>> a + hl.if_else(x > 0, 1, 0); <ArrayNumericExpression of type array<int32>>. Case Statements; More complicated conditional statements can be constructed with case().; For example, we might want to return 1 if x < -1, 2 if; -1 <= x <= 2 and 3 if x > 2.; >>> (hl.case(); ... .when(x < -1, 1); ... .when((x >= -1) & (x <= 2), 2); ... .when(x > 2, 3); ... .or_missing()); <Int32Expression of type int32>. Notice that this expression ends with a call to or_missing(),; which means that if none of the conditions are met, a missing value is returned.; Cases started with case() can end with a call to; or_missing(), default(), or; or_error(), depending on what you want to happen if none; of the when clauses are met.; It’s important to note that missingness propagates up in Hail, so if the value; of the discriminant in a case statement is missing, then the result will be; missing as well.; >>> y = hl.missing(hl.tint32); >>> result = hl.case().when(y > 0, 1).default(-1); >>> hl.eval(result). The value of result will be missing, not 1 or -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ...",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/expressions.html:6676,depend,depending,6676,docs/0.2/overview/expressions.html,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html,1,['depend'],['depending']
Integrability," 2) compute the eigendecomposition :math:`K = USU^T` of the kinship matrix; 3) fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (`REML <https://en.wikipedia.org/wiki/Restricted_maximum_likelihood>`__), storing results in global annotations under ``global.lmmreg``; 4) test each variant for association, storing results under ``va.lmmreg`` in variant annotations. This plan can be modified as follows:. - Set ``run_assoc=False`` to not test any variants for association, i.e. skip Step 5.; - Set ``use_ml=True`` to use maximum likelihood instead of REML in Steps 4 and 5.; - Set the ``delta`` argument to manually set the value of :math:`\delta` rather that fitting :math:`\delta` in Step 4.; - Set the ``global_root`` argument to change the global annotation root in Step 4.; - Set the ``va_root`` argument to change the variant annotation root in Step 5. :py:meth:`.lmmreg` adds 9 or 13 global annotations in Step 4, depending on whether :math:`\delta` is set or fit. +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | Annotation | Type | Value |; +==============================================+======================+======================================================================================================================================================+; | ``global.lmmreg.useML`` | Boolean | true if fit by ML, false if fit by REML |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.beta`` | Dict[String, Double] | map from *intercept* and the given ``covariates`` expressions to the corresponding fit :math:`\\beta` coefficients ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:116805,depend,depending,116805,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['depending']
Integrability," =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:31677,depend,dependent,31677,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['depend'],['dependent']
Integrability," @author: nbaya; """""". import numpy as np; import pandas as pd; from scipy import stats. import hail as hl; from hail.expr.expressions import expr_array, expr_call, expr_float64, expr_int32; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils.java import Env. [docs]@typecheck(; mt=MatrixTable,; genotype=oneof(expr_int32, expr_float64, expr_call),; h2=(oneof(float, int, list, np.ndarray)),; pi=nullable(oneof(float, int, list, np.ndarray)),; rg=nullable(oneof(float, int, list, np.ndarray)),; annot=nullable(oneof(expr_float64, expr_int32)),; popstrat=nullable(oneof(expr_int32, expr_float64)),; popstrat_var=nullable(oneof(float, int)),; exact_h2=bool,; ); def simulate_phenotypes(; mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact_h2=False; ):; r""""""Simulate phenotypes for testing LD score regression. Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Optionally adds; population stratification. Parameters; ----------; mt : :class:`.MatrixTable`; :class:`.MatrixTable` containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; genotype : :class:`.Expression` or :class:`.CallExpression`; Entry field containing genotypes of individuals to be used for the; simulation.; h2 : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`; SNP-based heritability of simulated trait.; pi : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`, optional; Probability of SNP being causal when simulating under the spike & slab; model.; rg : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`, optional; Genetic correlation between traits.; annot : :class:`.Expression`, optional; Row field to use as our aggrega",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:2174,depend,depending,2174,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,2,['depend'],['depending']
Integrability," HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Aggregable; Aggregable[Array[Double]]; Aggregable[Array[Float]]; Aggregable[Array[Int]]; Aggregable[Array[Long]]; Aggregable[Double]; Aggregable[Float]; Aggregable[Genotype]; Aggregable[Int]; Aggregable[Long]; Aggregable[T]. AltAllele; Array; Array[Array[T]]; Array[Boolean]; Array[Double]; Array[Float]; Array[Int]; Array[Long]; Array[String]; Array[T]. Boolean; Call; Dict; Double; Float; Genotype; Int; Interval; Locus; Long; Set; Set[Double]; Set[Float]; Set[Int]; Set[Long]; Set[Set[T]]; Set[String]; Set[T]. String; Struct; Variant. Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Types. View page source. Types¶. Aggregable¶; An Aggregable is a Hail data type representing a distributed row or column of a matrix. Hail exposes a number of methods to compute on aggregables depending on the data type. Aggregable[Array[Double]]¶. sum(): Array[Double] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Float]]¶. sum(): Array[Float] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Int]]¶. sum(): Array[Int]. Compute the sum by index. All elements in the aggregable must have the same length.; Examples; Count the total number of occurrences of each allele across samples, per variant:; >>> vds_result = vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/types.html:1054,depend,depending,1054,docs/0.1/types.html,https://hail.is,https://hail.is/docs/0.1/types.html,1,['depend'],['depending']
Integrability," How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up the two reference genomes (source and destination):; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') ; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Then we can liftover ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:1228,depend,dependencies,1228,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability," If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs. :param bool keep: If true, keep variants matching expr. :param bool filter_altered_genotypes: If true, genotypes that contain filtered-out alleles are set to missing. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :param bool keepStar: If true, keep variants where the only allele left is a ``*`` allele. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.filterAlleles(expr, annotation, filter_altered_genotypes, keep, subset, max_shift,; keep_star); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_genotypes(self, expr, keep=True):; """"""Filter genotypes based on expression. **Examples**. Filter genotypes by allele balance dependent on genotype call:. >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). **Notes**. ``expr`` is in genotype context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``v`` (*Variant*): :ref:`variant`; - ``sa``: sample annotations; - ``va``: variant annotations; - ``global``: global annotations. For more information, see the documentation on `data representation, annotations <overview.html#>`__, and; the `expression language <exprlang.html>`__. .. caution::; When ``expr`` evaluates to missing, the genotype will be removed regardless of whether ``keep=True`` or ``keep=False``. :param str expr: Boolean filter expression.; ; :param bool keep: Keep genotypes where ``expr`` evaluates to true. :return: Filtered variant dataset.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.filterGenotypes(expr, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:66131,depend,dependent,66131,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['dependent']
Integrability," Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5439,depend,dependencies,5439,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['depend'],['dependencies']
Integrability," Sex chromosomes of male individuals should be haploid to be phased correctly.; - If `proband_call` is diploid on non-par regions of the sex chromosomes, it is assumed to be female. Returns `NA` when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid on non-PAR region of Y. In addition, individual phased genotype calls are returned as missing in the following situations:; 1. All mother genotype calls non-PAR region of Y; 2. Diploid father genotype calls on non-PAR region of X for a male proband (proband and mother are still phased as father doesn't participate in allele transmission). Note; ----; :func:`~.phase_trio_matrix_by_transmission` provides a convenience wrapper for phasing a trio matrix. Parameters; ----------; locus : :class:`.LocusExpression`; Expression for the locus in the trio matrix; alleles : :class:`.ArrayExpression`; Expression for the alleles in the trio matrix; proband_call : :class:`.CallExpression`; Expression for the proband call in the trio matrix; father_call : :class:`.CallExpression`; Expression for the father call in the trio matrix; mother_call : :class:`.CallExpression`; Expression for the mother call in the trio matrix. Returns; -------; :class:`.ArrayExpression`; Array containing: [phased proband call, phased father call, phased mother call]"""""". def call_to_one_hot_alleles_array(; call: hl.expr.CallExpression, alleles: hl.expr.ArrayExpression; ) -> hl.expr.ArrayExpression:; """"""; Get the set of all different one-hot-encoded allele-vectors in a genotype call.; It is returned as an ordered array where the first vector corresponds to the first allele,; and the second vector (only present if het) the seco",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html:2420,wrap,wrapper,2420,docs/0.2/_modules/hail/experimental/phase_by_transmission.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html,2,['wrap'],['wrapper']
Integrability," The `f` argument should be a function taking one argument, an expression of; the element type of `array`, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; :func:`array_agg` is an array of elements of the return type of `f`. Parameters; ----------; f :; Aggregation function to apply to each element of the exploded array.; array : :class:`.ArrayExpression`; Array to aggregate. Returns; -------; :class:`.ArrayExpression`; """"""; return _agg_func.array_agg(array, f). @typecheck(expr=expr_str); def _impute_type(expr):; ret_type = hl.dtype(; 'struct{anyNonMissing: bool,'; 'allDefined: bool,'; 'supportsBool: bool,'; 'supportsInt32: bool,'; 'supportsInt64: bool,'; 'supportsFloat64: bool}'; ). return _agg_func('ImputeType', [expr], ret_type, []). class ScanFunctions(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:60205,wrap,wraps,60205,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,4,['wrap'],"['wrapper', 'wraps']"
Integrability," aesthetics specific to this geom.; min_val (int or float) – Minimum value to include in histogram; max_val (int or float) – Maximum value to include in histogram; bins (int) – Number of bins to plot. 30 by default.; fill – A single fill color for all bars of histogram, overrides fill aesthetic.; color – A single outline color for all bars of histogram, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; position (str) – Tells how to deal with different groups of data at same point. Options are “stack” and “dodge”. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_density(mapping={}, *, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False)[source]; Creates a smoothed density plot.; This method uses the hl.agg.approx_cdf aggregator to compute a sketch; of the distribution of the values of x. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf.; Note: this function currently does not support same interface as R’s ggplot.; Supported aesthetics: x, color, fill. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; k (int) – Passed to the approx_cdf aggregator. The size of the aggregator scales; linearly with k. The default value of 1000 is likely sufficient for; most uses.; smoothing (float) – Controls the amount of smoothing applied.; fill – A single fill color for all density plots, overrides fill aesthetic.; color – A single line color for all density plots, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; smoothed (boolean) – If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_hline(yintercept, *, linetype='solid', color=None)[source]; Plots a horizontal line at yintercept. Parameters:. yintercept (float) – Location to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:5848,interface,interface,5848,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['interface'],['interface']
Integrability," allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77668,integrat,integration,77668,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Integrability," based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; hl.range(0, mt.alleles.length()).map(lambda newi: mt.AD[mt.new_to_old[newi]])",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162549,depend,depend,162549,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['depend'],['depend']
Integrability," be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:76956,message,message,76956,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability," each pair of samples in the accompanying sample list.; The output formats are consistent with PLINK formats as created by the make-rel and make-grm commands and used by GCTA.; Attributes. key_schema; Returns the signature of the key indexing this matrix. Methods. __init__. export_gcta_grm; Export kinship matrix as .grm file. export_gcta_grm_bin; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. export_id_file; Export samples as .id file. export_rel; Export kinship matrix as .rel file. export_tsv; Export kinship matrix to tab-delimited text file with sample list as header. matrix; Gets the matrix backing this kinship matrix. sample_list; Gets the list of samples. export_gcta_grm(output)[source]¶; Export kinship matrix as .grm file. See PLINK formats. Parameters:output (str) – File path for output. export_gcta_grm_bin(output, opt_n_file=None)[source]¶; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See PLINK formats. Parameters:; output (str) – File path for output.; opt_n_file (str or None) – The file path to the N file. export_id_file(output)[source]¶; Export samples as .id file. See PLINK formats. Parameters:output (str) – File path for output. export_rel(output)[source]¶; Export kinship matrix as .rel file. See PLINK formats. Parameters:output (str) – File path for output. export_tsv(output)[source]¶; Export kinship matrix to tab-delimited text file with sample list as header. Parameters:output (str) – File path for output. key_schema¶; Returns the signature of the key indexing this matrix. Return type:Type. matrix()[source]¶; Gets the matrix backing this kinship matrix. Returns:Matrix of kinship values. Return type:IndexedRowMatrix. sample_list()[source]¶; Gets the list of samples. The (i, j) entry of the matrix encodes the relatedness of the ith and jth samples. Returns:List of samples. Return type:list of str. Next ; Previous. © Copyright 2016, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KinshipMatrix.html:1493,depend,depending,1493,docs/0.1/hail.KinshipMatrix.html,https://hail.is,https://hail.is/docs/0.1/hail.KinshipMatrix.html,1,['depend'],['depending']
Integrability," effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) New",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:51964,depend,dependency,51964,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['depend'],"['dependency', 'depends']"
Integrability," evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is eith",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70416,integrat,integration,70416,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integration']
Integrability," if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(substr)[source]; Returns whether substr is contained in the string.; Examples; >>> hl.eval(s.contains('fox')); True. >>> hl.eval(s.contains('dog')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. endswith(substr)[source]; Returns whether substr is a suffix of the string.; Examples; >>> hl.eval(s.endswith('fox')); True. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StringExpression.html:4272,depend,dependencies,4272,docs/0.2/hail.expr.StringExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html,1,['depend'],['dependencies']
Integrability," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:8249,depend,dependencies,8249,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,['depend'],"['dependencies', 'depends']"
Integrability," int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits.; annot (Expression, optional) – Row field to use as our aggregated annotations.; popstrat (Expression, optional) – Column field to use as our aggregated covariates for adding population; stratification.; exact_h2 (bool, optional) – Whether to exactly simulate ratio of variance of genetic component of; phenotype to variance of phenotype to be h2. If False, ratio will be; h2 in expectation. Observed h2 in the simulation will be close to; expected h2 for large-scale simulations. Returns:; MatrixTable – MatrixTable with simulated betas and phenotypes, simulated according; to specified model. hail.experimental.ldscsim.make_betas(mt, h2, pi=None, annot=None, rg=None)[source]; Generates betas under different models.; Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Parameters:. mt (MatrixTable) – MatrixTable containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; h2 (float or int or list or numpy.ndarray) – SNP-based heritability of simulated trait(s).; pi (float or int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model. If doing two-trait spike & slab pi is a list of probabilities for; overlapping causal SNPs (see docstring of multitrait_ss()); annot (Expression, optional) – Row field of aggregated annotations for annotation-informed model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits. Returns:. mt (MatrixTable) – MatrixTable with betas as a row field, simulated according to specified model.; pi (list) – Probability of a SNP bein",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/ldscsim.html:4619,depend,depending,4619,docs/0.2/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html,1,['depend'],['depending']
Integrability," is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. mem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4685,depend,dependency,4685,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['dependency']
Integrability," lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:19174,integrat,integrate,19174,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integrate']
Integrability," more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. Compute cost. Caution; The prices shown below are approximate prices based on us-central1. Actual prices are; based on the current spot prices for a given worker type and the region in which the worker is running in.; You can use Job.regions() to specify which regions to run a job in. = $0.01 per core per hour for spot standard worker types; = $0.012453 per core per hour for spot highmem worker types; = $0.0074578 per core per hour for spot highcpu worker types; = $0.04749975 per core per hour for nonpreemptible standard worker types; = $0.0591515 per core per hour for nonpreemptible highmem worker types; = $0.0354243 per core per hour for nonpreemptible highcpu worker types. Disk cost; Boot Disk; Average number of days per month = 365.25 / 12 = 30.4",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:4136,depend,depends,4136,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['depend'],['depends']
Integrability," order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:1946,depend,dependency,1946,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['depend'],['dependency']
Integrability," order, ; the table's key types. Note that using ``vds_key`` is slower than annotation with a standard ; key type. Each expression in the list ``vds_key`` has the following symbols in; scope:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; ; **The** ``root`` **and** ``expr`` **arguments**; ; .. note::; ; One of ``root`` or ``expr`` is required, but not both. ; ; The ``expr`` parameter expects an annotation assignment involving ``va`` (the existing ; variant annotations in the dataset) and ``table`` (the values(s) in the table),; like ``va.col1 = table.col1, va.col2 = table.col2`` or ``va = merge(va, table)``.; The ``root`` parameter expects an annotation path beginning in ``va``, like ``va.annotations``.; Passing ``root='va.annotations'`` is the same as passing ``expr='va.annotations = table'``. ``expr`` has the following symbols in scope:. - ``va``: variant annotations; - ``table``: See note. .. note:: ; ; The value of ``table`` inside root/expr depends on the number of values in the key table, ; as well as the ``product`` argument. There are three behaviors based on the number of values; and one branch for ``product`` being true and false, for a total of six modes:; ; +-------------------------+-------------+--------------------+-----------------------------------------------+; | Number of value columns | ``product`` | Type of ``table`` | Value of ``table`` |; +=========================+=============+====================+===============================================+; | More than 2 | False | ``Struct`` | Struct with an element for each column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 1 | False | ``T`` | The value column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 0 | False | ``Boolean`` | Existence of any matching key. |; +-------------------------+-------------+--------------------+-----------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:27037,depend,depends,27037,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['depends']
Integrability," reference expressions.; named_exprs : keyword args of type :class:`.Expression`; Field names and expressions to compute them. Returns; -------; :class:`.GroupedTable`; Grouped table; use :meth:`.GroupedTable.aggregate` to complete the aggregation.; """"""; key, computed_key = get_key_by_exprs(; 'Table.group_by', exprs, named_exprs, self._row_indices, override_protected_indices={self._global_indices}; ); return GroupedTable(self, self.row.annotate(**computed_key).select(*key)). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate(self, expr, _localize=True):; """"""Aggregate over rows into a local value. Examples; --------; Aggregate over rows:. >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; expr = to_expr(expr); base, _ = self._process_joins(expr); analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}). agg_ir = ir.TableAggregate(base._tir, expr._ir). if _localize:; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]. return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copie",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:58186,depend,dependent,58186,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['depend'],['dependent']
Integrability," row index field. Returns:; MatrixTable – Dataset with new field. aggregate_cols(expr, _localize=True)[source]; Aggregate over columns to a local value.; Examples; Aggregate over columns:; >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; Unlike most MatrixTable methods, this method does not support; meaningful references to fields that are not global or indexed by column.; This method should be thought of as a more convenient alternative to; the following:; >>> cols_table = dataset.cols(); >>> cols_table.aggregate(; ... hl.struct(fraction_female=hl.agg.fraction(cols_table.pheno.is_female),; ... case_ratio=hl.agg.count_where(cols_table.is_case) / hl.agg.count())). Note; This method supports (and expects!) aggregation over columns. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. aggregate_entries(expr, _localize=True)[source]; Aggregate over entries to a local value.; Examples; Aggregate over entries:; >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; This method should be thought of as a more convenient alternative to; the following:; >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; This method supports (and expects!) aggregation over entries. Parameters:; expr (Expression) – Aggregation expressions. Returns:; any – Aggregated value dependent on expr. aggregate_rows(expr, _localize=True)[source]; Aggregate over rows to a local value.; Examples; Aggregate over rows:; >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:9328,depend,dependent,9328,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['depend'],['dependent']
Integrability," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1709,message,message,1709,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html,3,['message'],['message']
Integrability," summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:143711,depend,dependent,143711,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['dependent']
Integrability," that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:54593,interface,interface,54593,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['interface'],['interface']
Integrability," the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fixed the documentation for job memory and storage requests to have default units in bytes. Previous. © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:6734,message,message,6734,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['message'],['message']
Integrability," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:10554,interface,interface,10554,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['interface'],['interface']
Integrability," ts):; return t0. return None. def unify_exprs(*exprs: 'Expression') -> Tuple:; assert len(exprs) > 0; types = {e.dtype for e in exprs}. # all types are the same; if len(types) == 1:; return (*exprs, True). for t in types:; c = expressions.coercer_from_dtype(t); if all(c.can_coerce(e.dtype) for e in exprs):; return (*tuple([c.coerce(e) for e in exprs]), True). # cannot coerce all types to the same type; return (*exprs, False). [docs]class Expression(object):; """"""Base class for Hail expressions."""""". __array_ufunc__ = None # disable NumPy coercions, so Hail coercions take priority. @typecheck_method(x=ir.IR, type=nullable(HailType), indices=Indices, aggregations=linked_list(Aggregation)); def __init__(; self, x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; self._ir: ir.IR = x; self._type = type; self._indices = indices; self._aggregations = aggregations; self._summary = None. [docs] def describe(self, handler=print):; """"""Print information about type, index, and dependencies.""""""; if self._aggregations:; agg_indices = set(); for a in self._aggregations:; agg_indices = agg_indices.union(a.indices.axes); agg_tag = ' (aggregated)'; agg_str = (; f'Includes aggregation with index {list(agg_indices)}\n'; f' (Aggregation index may be promoted based on context)'; ); else:; agg_tag = ''; agg_str = ''. bar = '--------------------------------------------------------'; s = (; '{bar}\n'; 'Type:\n'; ' {t}\n'; '{bar}\n'; 'Source:\n'; ' {src}\n'; 'Index:\n'; ' {inds}{agg_tag}{maybe_bar}{agg}\n'; '{bar}'.format(; bar=bar,; t=self.dtype.pretty(indent=4),; src=self._indices.source,; inds=list(self._indices.axes),; maybe_bar='\n' + bar + '\n' if agg_str else '',; agg_tag=agg_tag,; agg=agg_str,; ); ); handler(s). [docs] def __lt__(self, other):; return self._compare_op(""<"", other). [docs] def __le__(self, other):; return self._compare_op(""<="", other). [docs] def __gt__(self, other):; return self._compare_op("">"", other). [docs] d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:16254,depend,dependencies,16254,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,2,['depend'],['dependencies']
Integrability," users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gclo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:2269,depend,dependent,2269,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['depend'],['dependent']
Integrability," was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% over",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:74011,message,message,74011,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability," x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0]). Run the GWAS with linear_regression_rows() using variant dosages as covariates:; >>> gwas = hl.linear_regression_rows( ; ... x=mt_annot.GT.n_alt_alleles(),; ... y=mt_annot.pheno.blood_pressure,; ... covariates=[1, mt_annot.pheno.age, mt_annot.snp1, mt_annot.snp2]). dependencies:; linear_regression_rows(), aggregators.collect(), parse_variant(), variant_str(). Stratified by Group. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:10343,rout,routines,10343,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['rout'],['routines']
Integrability," | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:75663,depend,dependent,75663,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['depend'],['dependent']
Integrability," — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; Job. BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to alwa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1006,depend,dependencies,1006,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['dependencies']
Integrability,"""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOU",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38475,depend,depending,38475,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['depend'],['depending']
Integrability,"(arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f)[source]; Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:3894,depend,dependencies,3894,docs/0.2/hail.expr.CollectionExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html,1,['depend'],['dependencies']
Integrability,"(str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4180,depend,dependencies,4180,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['dependencies']
Integrability,")); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). dependencies:; MatrixTable.aggregate_entries(), aggregators.mean(), aggregators.fraction(), StructExpression. Aggregate Per Column Group. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, G",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:5349,depend,dependencies,5349,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:6210,depend,dependencies,6210,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,5,['depend'],['dependencies']
Integrability,", MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:7095,depend,dependencies,7095,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,", statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing “linkage disequilibrium” between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra.; To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a block-sparse matrix. Otherwise, we say the matrix; is block-dense. The property is_sparse() encodes this state.; Dropped blocks are not stored in memory or on write(). In fact,; blocks that are dropped prior to an action like export() or; to_numpy() are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes.; Block-sparse matrices may be created with; sparsify_band(),; sparsify_rectangles(),; sparsify_row_intervals(),; and sparsify_triangle().; The following methods naturally propagate block-sparsity:. Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify()",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:6237,depend,depend,6237,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['depend'],['depend']
Integrability,",; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:3302,depend,dependent,3302,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['depend'],['dependent']
Integrability,"-+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper inclu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:73985,integrat,integration,73985,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Integrability,"--+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Retu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94956,depend,dependent,94956,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['depend'],['dependent']
Integrability,"---+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93214,integrat,integration,93214,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Integrability,"-----+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pape",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77492,integrat,integration,77492,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Integrability,"------+-------+; | 0 | 1 | 2 |; | 1 | 3 | 4 |; +---------+-------+-------+. Notes. Matrix dimensions are inferred from input data.; You must provide row and column dimensions by specifying rows or; entries (inclusive) and cols or entries (inclusive).; The respective dimensions of rows, cols and entries must match should; you provide rows and entries or cols and entries (inclusive). Parameters:. globals (dict from str to any) – Global fields by name.; rows (dict from str to list of any) – Row fields by name.; cols (dict from str to list of any) – Column fields by name.; entries (dict from str to list of list of any) – Matrix entries by name in the form entry[row_idx][col_idx]. Returns:; MatrixTable – A MatrixTable assembled from inputs whose rows are keyed by row_idx; and columns are keyed by col_idx. classmethod from_rows_table(table)[source]; Construct matrix table with no columns from a table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Examples; Import a text table and construct a rows-only matrix table:; >>> table = hl.import_table('data/variant-lof.tsv'); >>> table = table.transmute(**hl.parse_variant(table['v'])).key_by('locus', 'alleles'); >>> sites_mt = hl.MatrixTable.from_rows_table(table). Notes; All fields in the table become row-indexed fields in the; result. Parameters:; table (Table) – The table to be converted. Returns:; MatrixTable. property globals; Returns a struct expression including all global fields. Returns:; StructExpression. globals_table()[source]; Returns a table with a single row with the globals of the matrix table.; Examples; Extract the globals table:; >>> globals_table = dataset.globals_table(). Returns:; Table – Table with the globals from the matrix, with a single row. group_cols_by(*exprs, **named_exprs)[source]; Group columns, used with GroupedMatrixTable.aggregate().; Examples; Aggregate to a matrix with cohort as column keys, comput",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:38181,interface,interface,38181,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['interface'],['interface']
Integrability,"-------+-------+-------+. Notes; This method returns a table with a new field whose name is given by; the name parameter, with type tint64. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like Table.take() or Table.export()) will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields may be defined in several ways:. In terms of constant values. Every row will have the same value.; In terms of other fields in the table.; In terms of fields in other tables, this is called “joining”. Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:8133,depend,dependent,8133,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['depend'],['dependent']
Integrability,"-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.show(handler=lambda x: logging.info(x)) . Parameters:. n or n_rows (int) – Maximum number of rows to show, or negative to show all rows.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(handler=None)[source]; Compute and print summary information about the fields in the table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. tail(n)[source]; Subset table to last n rows.; Examples; Subset to the last three rows:; >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; The number of partitions in the new table is equal to the number of; partitions containing the last n rows. Parameters:; n (int) – Number of rows to include. Returns:; Table – Table including the last n rows. take(n, _localize=True)[source]; Collect the first n rows of the table into a local list.; Examples; Take the first three rows:; >>> first3 = table1.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Notes; This method does not need to look at all the data in the table, and; allows for fast queries of the start of the table.; This method is equivalent to Table.head() followed by; Table.collect(). Parameters:; n (int) – Numbe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:67438,interface,interface,67438,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['interface'],['interface']
Integrability,"-------; bed : :class:`str`; PLINK BED file. bim : :class:`str`; PLINK BIM file. fam : :class:`str`; PLINK FAM file. min_partitions : :obj:`int`, optional; Minimum number of partitions. Useful in conjunction with `block_size`. missing : :class:`str`; String used to denote missing values **only** for the phenotype field.; This is in addition to ""-9"", ""0"", and ""N/A"" for case-control; phenotypes. delimiter : :class:`str`; FAM file field delimiter regex. quant_pheno : :obj:`bool`; If ``True``, FAM phenotype is interpreted as quantitative. a2_reference : :obj:`bool`; If ``True``, A2 is treated as the reference allele. If False, A1 is treated; as the reference allele. reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use. contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by ``reference_genome``. If ``None``, the; default is dependent on the ``reference_genome``. For ""GRCh37"", the default; is ``{'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}``. For ""GRCh38"", the; default is ``{'1': 'chr1', ..., '22': 'chr22', '23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'}``. skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. n_partitions : :obj:`int`, optional; Number of partitions. If both `n_partitions` and `block_size`; are specified, `n_partitions` will be used. block_size : :obj:`int`, optional; Block size, in MB. Default: 128MB blocks. Returns; -------; :class:`.MatrixTable`. """""". if contig_recoding is None:; if reference_genome is None:; contig_recoding = {}; elif reference_genome.name == ""GRCh37"":; contig_recoding = {'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}; elif reference_genome.name == ""GRCh38"":; contig_recoding = {; **{str(i): f'chr{i}' for i in range(1, 23)},; **{'23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'},; }; else:; contig_recoding = {}.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:89395,depend,dependent,89395,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['depend'],['dependent']
Integrability,"-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66781,message,messages,66781,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['messages']
Integrability,"-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound**3:; return _svd(self.to_numpy(), full_matrices=False, compute_uv=compute_uv, overwrite_a=True); else:; return self._svd_gr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76700,depend,dependent,76700,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['depend'],['dependent']
Integrability,". Backend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; Backend. LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Backend. View page source. Backend. class hailtop.batch.backend.Backend(requester_pays_fses); Bases: ABC, Generic[RunningBatchType]; Abstract class for backends.; Methods. _async_run; Execute a batch. _run; See _async_run(). async_close. close; Close a Hail Batch Backend. requester_pays_fs. rtype:; RouterAsyncFS. validate_file. rtype:; None. abstract async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch.; :rtype: Optional[TypeVar(RunningBatchType)]. Warning; This method should not be called directly. Instead, use batch.Batch.run(). _run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); See _async_run().; :rtype: Optional[TypeVar(RunningBatchType)]. Warning; This method should not be called directly. Instead, use batch.Batch.run(). async async_close(). close(); Close a Hail Batch Backend.; Notes; This method should be called after executing your batches at the; end of your script. requester_pays_fs(requester_pays_config). Return type:; RouterAsyncFS. async validate_file(uri, requester_pays_config=None). Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.Backend.html:685,Rout,RouterAsyncFS,685,docs/batch/api/backend/hailtop.batch.backend.Backend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.Backend.html,2,['Rout'],['RouterAsyncFS']
Integrability,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch. View page source. Batch; Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the Batch Service. Contents. Getting Started; Installation. Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Clumping GWAS Results; Random Forest. Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Exact Match Expression; Partial Match Expression; Keyword Expression; Predefined Keyword Expression; Combining Multiple Statements. Python Version Compatibility Policy; Change Log. Indices and tables. Index; Search Page. Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/index.html:544,depend,dependencies,544,docs/batch/index.html,https://hail.is,https://hail.is/docs/batch/index.html,1,['depend'],['dependencies']
Integrability,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/getting_started.html:555,depend,depends,555,docs/batch/getting_started.html,https://hail.is,https://hail.is/docs/batch/getting_started.html,1,['depend'],['depends']
Integrability,". Job — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; Job. BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:823,depend,dependencies,823,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['dependencies']
Integrability,". One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exception: block matrices do not currently support element-wise; ""outer product"" of a single row and a single column, although the same; effect can be achieved for ``*`` by using ``@``. Warning; -------. For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:4650,depend,dependency,4650,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['depend'],['dependency']
Integrability,". Python API — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob. Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture. Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend. Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API. View page source. Python API; This is the API documentation for Batch, and provides detailed information; on the Python programming interface.; Use import hailtop.batch to access this functionality. Batches; A Batch is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:774,interface,interface,774,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,2,"['depend', 'interface']","['dependencies', 'interface']"
Integrability,".; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:3990,message,message,3990,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['message'],['message']
Integrability,".; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. property contig; Returns the chromosome.; Examples; >>> hl.eval(locus.contig); '1'. Returns:; StringExpression – The chromosome for this locus. property contig_idx; Returns the chromosome.; Examples; >>> hl.eval(locus.contig_idx); 0. Returns:; StringExpression – The index of the chromosome for this locus. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:3493,depend,dependencies,3493,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['depend'],['dependencies']
Integrability,".isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:5040,depend,dependencies,5040,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,4,['depend'],['dependencies']
Integrability,"1,; min=-1.0,; max=10.0,; n=3500,; sum=13943.0). However, these metrics aren’t perfectly representative of the samples in our dataset. Here’s why:. [18]:. table.count(). [18]:. 3500. [19]:. mt.count_cols(). [19]:. 284. Since there are fewer samples in our dataset than in the full thousand genomes cohort, we need to look at annotations on the dataset. We can use aggregate_cols to get the metrics for only the samples in our dataset. [20]:. mt.aggregate_cols(hl.agg.counter(mt.pheno.SuperPopulation)). [20]:. {'AFR': 76, 'AMR': 34, 'EAS': 72, 'EUR': 47, 'SAS': 55}. [21]:. pprint(mt.aggregate_cols(hl.agg.stats(mt.pheno.CaffeineConsumption))). Struct(mean=4.415492957746479,; stdev=1.577763427465917,; min=0.0,; max=9.0,; n=284,; sum=1254.0). The functionality demonstrated in the last few cells isn’t anything especially new: it’s certainly not difficult to ask these questions with Pandas or R dataframes, or even Unix tools like awk. But Hail can use the same interfaces and query language to analyze collections that are much larger, like the set of variants.; Here we calculate the counts of each of the 12 possible unique SNPs (4 choices for the reference base * 3 choices for the alternate base).; To do this, we need to get the alternate allele of each variant and then count the occurences of each unique ref/alt pair. This can be done with Hail’s counter function. [22]:. snp_counts = mt.aggregate_rows(hl.agg.counter(hl.Struct(ref=mt.alleles[0], alt=mt.alleles[1]))); pprint(snp_counts). {Struct(ref='G', alt='A'): 2367,; Struct(ref='C', alt='T'): 2418,; Struct(ref='T', alt='A'): 77,; Struct(ref='C', alt='G'): 150,; Struct(ref='G', alt='C'): 111,; Struct(ref='G', alt='T'): 477,; Struct(ref='T', alt='G'): 466,; Struct(ref='T', alt='C'): 1864,; Struct(ref='A', alt='G'): 1929,; Struct(ref='C', alt='A'): 494,; Struct(ref='A', alt='T'): 75,; Struct(ref='A', alt='C'): 451}. We can list the counts in descending order using Python’s Counter class. [23]:. from collections import Counter; c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:9843,interface,interfaces,9843,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['interface'],['interfaces']
Integrability,"10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :cl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:3472,depend,depends,3472,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['depend'],['depends']
Integrability,"1111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:5037,depend,dependencies,5037,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,1,['depend'],['dependencies']
Integrability,"32 | str | int32 | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | 9 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | 9 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | 10 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | 10 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; This method is used to specify all the fields of a new row key. The old; key fields may be overwritten by newly-assigned fields, as described in; Table.annotate(). If not overwritten, they are preserved as non-key; fields.; See Table.select() for more information about how to define new; key fields. Parameters:; keys (varargs of type str) – Field(s) to key by. Returns:; Table – Table with a new key. static multi_way_zip_join(tables, data_field_name, global_field_name)[source]; Combine many tables in a zip join. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The row type of the returned table is a struct with the key fields, and; one extra field, data_field_name, which is an array of structs with; the non key fields, one per input. The array elements are missing if; their corresponding input had no row with that key or possibly if there; is another input with more rows with that key than the corresponding; input.; The global type of the returned table is an array of structs of the; global type of all of the inputs.; The types for every input must be identical, not merely compatible,; including the keys.; A zip join is similar to an outer join however rows are not duplicated; to create the full Cartesian product of duplicate keys. Instead, there; is exactly one entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:45529,interface,interface,45529,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['interface'],['interface']
Integrability,"4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:81472,integrat,integration,81472,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Integrability,"6.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4771,depend,depend,4771,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['depend']
Integrability,":; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). [docs]class CaseBuilder(ConditionalBuilder):; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBuilder.default` method calls must be the; same type. Parameters; ----------; missing_false: :obj:`.bool`; Treat missing predicates as ``False``. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.switch`; """""". def __init__(self, missing_false=False):; super(CaseBuilder, self).__init__(); self._missing_false = missing_false. def _finish(self, default):; assert len(self._cases) > 0. from hail.expr.functions import if_else. expr = default; for conditi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:5258,message,message,5258,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,2,['message'],['message']
Integrability,"; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22384,message,message,22384,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Rel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:67034,depend,dependency,67034,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependency']
Integrability,"; raise TypeError(f""'literal': type mismatch: expected '{t}', found '{x.dtype}'""); elif x._indices.source is not None:; if x._indices.axes:; raise ExpressionException(; f""'literal' can only accept scalar or global expression arguments,""; f"" found indices {x._indices.axes}""; ); return False; elif x is None or x is pd.NA:; return False; else:; t._typecheck_one_level(x); return True. if dtype is None:; dtype = impute_type(x). # Special handling of numpy. Have to extract from numpy scalars, do nothing on numpy arrays; if isinstance(x, np.generic):; x = x.item(); elif isinstance(x, np.ndarray):; pass; else:; try:; dtype._traverse(x, typecheck_expr); except TypeError as e:; raise TypeError(""'literal': object did not match the passed type '{}'"".format(dtype)) from e. if wrapper['has_free_vars']:; raise ValueError(; ""'literal' cannot be used with hail expressions that depend ""; ""on other expressions. Use expression 'x' directly ""; ""instead of passing it to 'literal'.""; ). if wrapper['has_expr']:; return literal(hl.eval(to_expr(x, dtype)), dtype). if x is None or x is pd.NA:; return hl.missing(dtype); elif is_primitive(dtype):; if dtype == tint32:; assert is_int32(x); assert tint32.min_value <= x <= tint32.max_value; return construct_expr(ir.I32(x), tint32); elif dtype == tint64:; assert is_int64(x); assert tint64.min_value <= x <= tint64.max_value; return construct_expr(ir.I64(x), tint64); elif dtype == tfloat32:; assert is_float32(x); return construct_expr(ir.F32(x), tfloat32); elif dtype == tfloat64:; assert is_float64(x); return construct_expr(ir.F64(x), tfloat64); elif dtype == tbool:; assert isinstance(x, builtins.bool); return construct_expr(ir.TrueIR() if x else ir.FalseIR(), tbool); else:; assert dtype == tstr; assert isinstance(x, builtins.str); return construct_expr(ir.Str(x), tstr); else:; return construct_expr(ir.EncodedLiteral(dtype, x), dtype). [docs]@deprecated(version=""0.2.59"", reason=""Replaced by hl.if_else""); @typecheck(condition=expr_bool, consequent=expr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:11419,wrap,wrapper,11419,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['wrap'],['wrapper']
Integrability,"; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:9179,message,messages,9179,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['message'],['messages']
Integrability,"= ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, outp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:51602,interface,interface,51602,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['interface'],['interface']
Integrability,"== 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:2339,depend,dependencies,2339,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:7679,interface,interface,7679,docs/0.2/hail.expr.NDArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html,1,['interface'],['interface']
Integrability,"=[(local_output_file, f'{vep_output_path}/annotations/{part_name}.tsv.gz')],; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, cancelling the batch...""); b.cancel(); backend._batch = None; raise. if status['n_succeeded'] != status['n_jobs']:; failing_job = next(iter(b.jobs('!success'))); failing_job = b.get_job(failing_job['job_id']); message = {'batch_status': status, 'job_status': failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; )",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:35898,message,message,35898,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['message'],['message']
Integrability,"=[0.03249975499062629],; multiple_standard_error=4.949747468305833,; multiple_r_squared=0.9973961101073441,; adjusted_r_squared=0.9947922202146882,; f_stat=383.0408163265306,; multiple_p_value=0.03249975499062629,; n=2); }. Compute call statistics stratified by population group and case status:. >>> ann = ds.annotate_rows(call_stats=hl.agg.group_by(hl.struct(pop=ds.pop, is_case=ds.is_case),; ... hl.agg.call_stats(ds.GT, ds.alleles))). Parameters; ----------; group : :class:`.Expression` or :obj:`list` of :class:`.Expression`; Group to stratify the result by.; agg_expr : :class:`.Expression`; Aggregation or scan expression to compute per grouping. Returns; -------; :class:`.DictExpression`; Dictionary where the keys are `group` and the values are the result of computing; `agg_expr` for each unique value of `group`.; """""". return _agg_func.group_by(group, agg_expr). @typecheck(expr=expr_any); def _prev_nonnull(expr) -> ArrayExpression:; wrap = expr.dtype in {tint32, tint64, tfloat32, tfloat64, tbool, tcall}; if wrap:; expr = hl.or_missing(hl.is_defined(expr), hl.tuple([expr])); r = _agg_func('PrevNonnull', [expr], expr.dtype, []); if wrap:; r = r[0]; return r. [docs]@typecheck(f=func_spec(1, expr_any), array=expr_array()); def array_agg(f, array):; """"""Aggregate an array element-wise using a user-specified aggregation function. Examples; --------; Start with a range table with an array of random boolean values:. >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction ``True`` per element:. >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) # doctest: +SKIP_OUTPUT_CHECK; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; -----; This function requires that all values of `array` have the same length. If; two values have different lengths, then an exception will be thrown. The `f` argument should be a function taking one argument, an expression of; the element typ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:58347,wrap,wrap,58347,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['wrap'],['wrap']
Integrability,">>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. descri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:2760,depend,dependencies,2760,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,">>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is present in the dictionary.; Examples; >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters:; item (Expression) – Key to test for inclusion. Returns:; BooleanExpression – True if item is a key of the dictionary, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:3453,depend,dependencies,3453,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['depend'],['dependencies']
Integrability,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.TupleExpression.html:6594,interface,interface,6594,docs/0.2/hail.expr.TupleExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html,1,['interface'],['interface']
Integrability,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:3078,message,message,3078,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,2,['message'],['message']
Integrability,"Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:4280,depend,dependencies,4280,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependencies']
Integrability,"Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_fl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70886,integrat,integration,70886,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integration']
Integrability,"GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:14602,contract,contract,14602,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['contract'],['contract']
Integrability,"New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:103168,message,message,103168,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_nonce = False; else:; backend.set_flags(rng_nonce=hex(global_seed)); Env._hc = self. def initialize_references(self, default_reference):; assert self._backend; self._backend.initialize_references(); if default_reference in BUILTIN_REFERENCES:; self._default_ref = self._backend.get_reference(default_reference); else:; self._default_ref = ReferenceGenome.read(default_reference). @property; def default_reference(self) -> ReferenceGenome:; assert self._default_ref is not None, '_default_ref should have been initialized in HailContext.create'; return self._default_ref. @default_reference.setter; def default_reference(self, value):; if not i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:3555,Interface,Interfaces,3555,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['Interface'],['Interfaces']
Integrability,"Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop*",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:38723,depend,dependency,38723,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependency']
Integrability,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106025,integrat,integration,106025,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Integrability,"Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries muc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:45936,message,message,45936,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20078,integrat,integration,20078,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Integrability,"Struct(beta=[34.25],; standard_error=[1.75],; t_stat=[19.571428571428573],; p_value=[0.03249975499062629],; multiple_standard_error=4.949747468305833,; multiple_r_squared=0.9973961101073441,; adjusted_r_squared=0.9947922202146882,; f_stat=383.0408163265306,; multiple_p_value=0.03249975499062629,; n=2); }. Compute call statistics stratified by population group and case status:. >>> ann = ds.annotate_rows(call_stats=hl.agg.group_by(hl.struct(pop=ds.pop, is_case=ds.is_case),; ... hl.agg.call_stats(ds.GT, ds.alleles))). Parameters; ----------; group : :class:`.Expression` or :obj:`list` of :class:`.Expression`; Group to stratify the result by.; agg_expr : :class:`.Expression`; Aggregation or scan expression to compute per grouping. Returns; -------; :class:`.DictExpression`; Dictionary where the keys are `group` and the values are the result of computing; `agg_expr` for each unique value of `group`.; """""". return _agg_func.group_by(group, agg_expr). @typecheck(expr=expr_any); def _prev_nonnull(expr) -> ArrayExpression:; wrap = expr.dtype in {tint32, tint64, tfloat32, tfloat64, tbool, tcall}; if wrap:; expr = hl.or_missing(hl.is_defined(expr), hl.tuple([expr])); r = _agg_func('PrevNonnull', [expr], expr.dtype, []); if wrap:; r = r[0]; return r. [docs]@typecheck(f=func_spec(1, expr_any), array=expr_array()); def array_agg(f, array):; """"""Aggregate an array element-wise using a user-specified aggregation function. Examples; --------; Start with a range table with an array of random boolean values:. >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction ``True`` per element:. >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) # doctest: +SKIP_OUTPUT_CHECK; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; -----; This function requires that all values of `array` have the same length. If; two values have different lengths, then an exception will be thrown. The `f` a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:58271,wrap,wrap,58271,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['wrap'],['wrap']
Integrability,"['has_free_vars'] |= (; builtins.len(x._ir.free_vars) > 0; or builtins.len(x._ir.free_agg_vars) > 0; or builtins.len(x._ir.free_scan_vars) > 0; ). if x.dtype != t:; raise TypeError(f""'literal': type mismatch: expected '{t}', found '{x.dtype}'""); elif x._indices.source is not None:; if x._indices.axes:; raise ExpressionException(; f""'literal' can only accept scalar or global expression arguments,""; f"" found indices {x._indices.axes}""; ); return False; elif x is None or x is pd.NA:; return False; else:; t._typecheck_one_level(x); return True. if dtype is None:; dtype = impute_type(x). # Special handling of numpy. Have to extract from numpy scalars, do nothing on numpy arrays; if isinstance(x, np.generic):; x = x.item(); elif isinstance(x, np.ndarray):; pass; else:; try:; dtype._traverse(x, typecheck_expr); except TypeError as e:; raise TypeError(""'literal': object did not match the passed type '{}'"".format(dtype)) from e. if wrapper['has_free_vars']:; raise ValueError(; ""'literal' cannot be used with hail expressions that depend ""; ""on other expressions. Use expression 'x' directly ""; ""instead of passing it to 'literal'.""; ). if wrapper['has_expr']:; return literal(hl.eval(to_expr(x, dtype)), dtype). if x is None or x is pd.NA:; return hl.missing(dtype); elif is_primitive(dtype):; if dtype == tint32:; assert is_int32(x); assert tint32.min_value <= x <= tint32.max_value; return construct_expr(ir.I32(x), tint32); elif dtype == tint64:; assert is_int64(x); assert tint64.min_value <= x <= tint64.max_value; return construct_expr(ir.I64(x), tint64); elif dtype == tfloat32:; assert is_float32(x); return construct_expr(ir.F32(x), tfloat32); elif dtype == tfloat64:; assert is_float64(x); return construct_expr(ir.F64(x), tfloat64); elif dtype == tbool:; assert isinstance(x, builtins.bool); return construct_expr(ir.TrueIR() if x else ir.FalseIR(), tbool); else:; assert dtype == tstr; assert isinstance(x, builtins.str); return construct_expr(ir.Str(x), tstr); else:; return constru",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:11211,wrap,wrapper,11211,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,4,"['depend', 'wrap']","['depend', 'wrapper']"
Integrability,"_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: use_exome_default_intervals=True and; use_genome_default_intervals=True.; The combiner serializes itself to save_path so that it can be restarted after failure. Parameters:. save_path (str) – The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path (str) – The location to store the new VariantDataset.; temp_path (str) – The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome (ReferenceGenome) – The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor (int) – The number of Variant Datasets to combine at once.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:2236,depend,depends,2236,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['depend'],['depends']
Integrability,"_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each vari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:141432,depend,depend,141432,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['depend']
Integrability,"_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). dependencies:; MatrixTable.aggregate_entries(), aggregators.mean(), aggregators.fraction(), StructExpression. Aggregate Per Column Group. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.agg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:4422,depend,dependencies,4422,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:6635,depend,depends,6635,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['depends']
Integrability,"_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nest",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:53090,depend,dependent,53090,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['depend'],['dependent']
Integrability,"_set:; if ((v1.position - v2.position) <= window and correlation(v1, v2) >= r2):; keep = False; if keep:; pruned_set.append(v1). The parameter window defines the maximum distance in base pairs between two variants to check whether; the variants are independent (\(R^2\) < r2) where r2 is the maximum \(R^2\) allowed.; \(R^2\) is defined as the square of Pearson’s correlation coefficient; \({\rho}_{x,y}\) between the two genotype vectors \({\mathbf{x}}\) and \({\mathbf{y}}\). \[{\rho}_{x,y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}\]; ld_prune() with default arguments is equivalent to plink --indep-pairwise 1000kb 1 0.2.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:77099,depend,depend,77099,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['depend']
Integrability,"a combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:2498,depend,dependencies,2498,index.html,https://hail.is,https://hail.is/index.html,1,['depend'],['dependencies']
Integrability,"a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface and defaults; may change in future versions.; """"""; hl.current_backend().validate_file(output). _, ext = os.path.splitext(output); if ext == '.gz':; warning(; 'VCF export with standard gzip compression requested. This is almost *never* desired and will '; 'cause issues with other tools that consume VCF files. The compression format used for VCF '; 'files is traditionally *block* gzip compression. To use block gzip compression with hail VCF '; 'export, use a path ending in `.bgz`.'; ). if isinstance(dataset, Table):; mt = MatrixTable.from_rows_table(dataset); dataset = mt.key_cols_by(sample=""""). require_col_key_str(dataset, 'export_vcf'); require_row_key_variant(dataset, 'export_vcf'). if 'filters' in dataset.row and dataset.filters.dtype != hl.tset(hl.tstr):; raise ValueError(; f""'export_vcf': expect the 'filters' field to be set<str>, found {dataset.filters.dtype}""; f""\n Either transform this field to set<str> to export as VCF FILTERS field, or drop it from the dataset.""; ). in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:20905,interface,interface,20905,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['interface'],['interface']
Integrability,"a'],; h2=h2,; popstrat=None if popstrat is None else mt['popstrat_' + uid],; popstrat_var=popstrat_var,; exact_h2=exact_h2,; ); mt = annotate_all(; mt=mt,; global_exprs={; 'ldscsim': hl.struct(**{; 'h2': h2[0] if len(h2) == 1 else h2,; **({} if pi == [None] else {'pi': pi}),; **({} if rg == [None] else {'rg': rg[0] if len(rg) == 1 else rg}),; **({} if annot is None else {'is_annot_inf': True}),; **({} if popstrat is None else {'is_popstrat_inf': True}),; **({} if popstrat_var is None else {'popstrat_var': popstrat_var}),; 'exact_h2': exact_h2,; }); },; ); mt = _clean_fields(mt, uid); return mt. [docs]@typecheck(; mt=MatrixTable,; h2=(oneof(float, int, list, np.ndarray)),; pi=nullable(oneof(float, int, list, np.ndarray)),; annot=nullable(oneof(expr_float64, expr_int32)),; rg=nullable(oneof(float, int, list, np.ndarray)),; ); def make_betas(mt, h2, pi=None, annot=None, rg=None):; r""""""Generates betas under different models. Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Parameters; ----------; mt : :class:`.MatrixTable`; MatrixTable containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; h2 : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`; SNP-based heritability of simulated trait(s).; pi : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`, optional; Probability of SNP being causal when simulating under the spike & slab; model. If doing two-trait spike & slab `pi` is a list of probabilities for; overlapping causal SNPs (see docstring of :func:`.multitrait_ss`); annot : :class:`.Expression`, optional; Row field of aggregated annotations for annotation-informed model.; rg : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`, optional; Genetic correlation between traits. Returns; -------; mt :",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:5385,depend,depending,5385,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,2,['depend'],['depending']
Integrability,"able, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; The returned interval is inclusive of both the start and end; endpoints. Parameters:. before (Expression of type tint32) – Number of bases to include before the locus. Truncates at 1.; after (Expression of type tint32) – Number of bases to i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:10758,interface,interface,10758,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['interface'],['interface']
Integrability,"able.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable way",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:17718,message,message,17718,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:91223,message,message,91223,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:125310,rout,routines,125310,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['rout'],['routines']
Integrability,"alent of _annotate_all, but checks source MatrixTable of exprs. ascertainment_bias(mt, y, P); Adds ascertainment bias to a binary phenotype to give it a sample prevalence of P = cases/(cases+controls). binarize(mt, y, K[, exact]); Binarize phenotype y such that it has prevalence K = cases/(cases+controls) Uses inverse CDF of Gaussian to set binarization threshold when exact = False, otherwise uses ranking to determine threshold. agg_fields(tb[, coef_dict, str_expr, axis]); Aggregates by linear combination fields matching either keys in coef_dict or str_expr. get_coef_dict(tb[, str_expr, ref_coef_dict, ...]); Gets either col or row fields matching str_expr and take intersection with keys in coefficient reference dict. hail.experimental.ldscsim.simulate_phenotypes(mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact_h2=False)[source]; Simulate phenotypes for testing LD score regression.; Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Optionally adds; population stratification. Parameters:. mt (MatrixTable) – MatrixTable containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; genotype (Expression or CallExpression) – Entry field containing genotypes of individuals to be used for the; simulation.; h2 (float or int or list or numpy.ndarray) – SNP-based heritability of simulated trait.; pi (float or int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits.; annot (Expression, optional) – Row field to use as our aggregated annotations.; popstrat (Expression, optional) – Column field to use as our aggregated covariates for adding population; stratification.; exa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/ldscsim.html:3040,depend,depending,3040,docs/0.2/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html,1,['depend'],['depending']
Integrability,"ant_qc.AF[1] > 0.01). eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). mt = mt.annotate_cols(scores=pcs[mt.s].scores). gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The followin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:4832,depend,dependencies,4832,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['depend'],['dependencies']
Integrability,"are all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that \(g_{is}\) is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first.; The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the statistics parameter:. i (col_key.dtype) – First sample. (key field); j (col_key.dtype) – Second sample. (key field); kin (tfloat64) – Kinship estimate, \(\widehat{\phi_{ij}}\).; ibd2 (tfloat64) – IBD2 estimate, \(\widehat{k^{(2)}_{ij}}\).; ibd0 (tfloat64) – IBD0 estimate, \(\widehat{k^{(0)}_{ij}}\).; ibd1 (tfloat64) – IBD1 estimate, \(\widehat{k^{(1)}_{ij}}\). Here col_key refers to the column key of the source matrix table,; and col_key.dtype is a struct containing the column key fields.; There is one row for each pair of distinct samples (columns), where i; corresponds to the column of smaller column index. In particular, if the; same column key value exists for \(n\) columns, then the resulting; table will have \(\binom{n-1}{2}\) rows with both key fields equal to; that column key value. This may result in unexpected behavior in downstream; processing. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; min_individual_maf (float) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:19525,depend,depending,19525,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['depend'],['depending']
Integrability,"ariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:58794,depend,dependent,58794,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['depend'],['dependent']
Integrability,"artial match, keyword, or predefined keyword) listed below. When the query is run, each; statement will be joined to the next with the AND operator. Exact Match Expression; A single word enclosed with double quotes that is an exact match for either the name or; value of an attribute.; Example: ""pca_pipeline"". Partial Match Expression; A single word without any quotes that is a partial match for either the name or the value; of an attribute.; Example: pipe. Keyword Expression; The left hand side of the statement is the name of the attribute and the right hand side; is the value to search against. Allowed operators are =, ==, !=, =~, and; !~ where the operators with tildes are looking for partial matches.; Example: name = pca_pipeline; Example: name =~ pca. Predefined Keyword Expression; The left hand side of the statement is a special Batch-specific keyword which can be one of the values; listed in the tables below. Allowed operators are dependent on the type of the value expected for each; keyword, but can be one of =, ==, !=, >, >=, <, <=, =~, !~.; The right hand side is the value to search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/advanced_search_help.html:1602,depend,dependent,1602,docs/batch/advanced_search_help.html,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html,1,['depend'],['dependent']
Integrability,"asses; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental. View page source. Experimental; This module serves two functions: as a staging area for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:1172,depend,depend,1172,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['depend'],['depend']
Integrability,"at prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previously internal _num_allele_type as numeric_allele_type; and deprecated it. Add new AlleleType enumeration for users to be; able to easily use the values returned by numeric_allele_type.; (#14297); vds.sample_gc now uses independent aggregators. Users may now; import these functions and use them directly.; (#14405); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by loc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:13356,depend,depends,13356,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['depends']
Integrability,"ata.locus.position and; ending at vds.reference_data.END (inclusive!). There is no GT call field; because all calls in the reference data are implicitly homozygous reference (in; the future, a table of ploidy by interval may be included to allow for proper; representation of structural variation, but there is no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:7486,depend,depending,7486,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['depend'],['depending']
Integrability,"atch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type Jo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:3567,depend,dependency,3567,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependency']
Integrability,"ated for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:7412,depend,depends,7412,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,3,['depend'],"['dependencies', 'depends']"
Integrability,"ation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plotting With hail.ggplot Overview. View page source. Plotting With hail.ggplot Overview. Warning; Plotting functionality is in early stages and is experimental. The hl.ggplot module is designed based on R’s tidyverse ggplot2 library. This module provides a subset of ggplot2’s; functionality to allow users to generate plots in much the same way they would in ggplot2.; This module is intended to be a new, more flexible way of plotting compared to the hl.plot module. This module; currently uses plotly to generate plots, as opposed to hl.plot, which uses bokeh. Core functions. ggplot; Create the initial plot object. aes; Create an aesthetic mapping. coord_cartesian; Set the boundaries of the plot. hail.ggplot.ggplot(table, mapping={})[source]; Create the initial plot object.; This function is the beginning of all plots using the hail.ggplot interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result.; Examples; Create a y = x^2 scatter plot; >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters:. table – The table containing the data to plot.; mapping – Default list of aesthetic mappings from table data to plot attributes. Returns:; GGPlot. hail.ggplot.aes(**kwargs)[source]; Create an aesthetic mapping. Parameters:; kwargs – Map aesthetic names to hail expressions based on table’s plot. Returns:; Aesthetic – The aesthetic mapping to be applied. hail.ggplot.coord_cartesian(xlim=None, ylim=None)[source]; Set the boundaries of the plot. Parameters:. xlim (tuple with two int) – The minimum and maximum x value to show on the plot.; ylim (tuple with two int) – The minimum and maximum y value to show on the plot. Returns:; FigureAttribu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:1518,interface,interface,1518,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['interface'],['interface']
Integrability,"ation function to apply to each element of the exploded array.; array : :class:`.ArrayExpression`; Array to aggregate. Returns; -------; :class:`.ArrayExpression`; """"""; return _agg_func.array_agg(array, f). @typecheck(expr=expr_str); def _impute_type(expr):; ret_type = hl.dtype(; 'struct{anyNonMissing: bool,'; 'allDefined: bool,'; 'supportsBool: bool,'; 'supportsInt32: bool,'; 'supportsInt64: bool,'; 'supportsFloat64: bool}'; ). return _agg_func('ImputeType', [expr], ret_type, []). class ScanFunctions(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:60542,wrap,wrapper,60542,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,4,['wrap'],['wrapper']
Integrability,"because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:1964,depend,dependency,1964,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,2,['depend'],"['dependency', 'dependent']"
Integrability,"bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:8651,interface,interface,8651,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['interface'],['interface']
Integrability,"called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:110933,depend,depend,110933,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['depend']
Integrability,"ch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4833,depend,dependencies,4833,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['dependencies']
Integrability,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:8146,depend,depend,8146,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['depend'],['depend']
Integrability,"color=None, size=None, alpha=None)[source]; Create a scatter plot where each point is text from the text aesthetic.; Supported aesthetics: x, y, label, color, tooltip. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_bar(mapping={}, *, fill=None, color=None, alpha=None, position='stack', size=None)[source]; Create a bar chart that counts occurrences of the various values of the x aesthetic.; Supported aesthetics: x, color, fill, weight. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_col(mapping={}, *, fill=None, color=None, alpha=None, position='stack', size=None)[source]; Create a bar chart that uses bar heights specified in y aesthetic.; Supported aesthetics: x, y, color, fill. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_histogram(mapping={}, *, min_val=None, max_val=None, bins=None, fill=None, color=None, alpha=None, position='stack', size=None)[source]; Creates a histogram.; Note: this function currently does not support same interface as R’s ggplot.; Supported aesthetics: x, color, fill. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; min_val (int or float) – Minimum value to include in histogram; max_val (int or float) – Maximum value to include in histogram; bins (int) – Number of bins to plot. 30 by default.; fill – A single fill color for all bars of histogram, overrides fill aesthetic.; color – A single outline color for all bars of histogram, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; position (str) – Tells how to deal with different groups of data at same point. Options are “stack” and “dodge”. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_density(mapping={}, *, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False)[source]; Creates a smoothed density plot.; This method uses the hl.agg.approx_cdf aggregator to compute a sketch; of the distribution of the values of x. It th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:4731,interface,interface,4731,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['interface'],['interface']
Integrability,"column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:3907,depend,dependencies,3907,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:11991,depend,dependencies,11991,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,2,['depend'],['dependencies']
Integrability,"ctive protein with PTSD, traumatic events, and social support. Neuropsychopharmacol. (2020). https://doi.org/10.1038/s41386-020-0655-6 https://www.nature.com/articles/s41386-020-0655-6#citeas. 2019. Farhan, Sali MK, et al. “Exome sequencing in amyotrophic lateral sclerosis implicates a novel gene, DNAJC7, encoding a heat-shock protein” Nature Neuroscience (2019): 307835. https://www.nature.com/articles/s41593-019-0530-0; Gay, Nicole R. et al. “Impact of admixture and ancestry on eQTL analysis and GWAS colocalization in GTEx” bioRxiv (2019) 836825; https://www.biorxiv.org/content/10.1101/836825v1; Sakaue, Saori et al. “Trans-biobank analysis with 676,000 individuals elucidates the association of polygenic risk scores of complex traits with human lifespan” bioRxiv (2019): 856351 https://www.biorxiv.org/content/10.1101/856351v1; Polimanti, Renato et al. “Leveraging genome-wide data to investigate differences between opioid use vs. opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium” bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al. “Meta-analysis of Scandinavian Schizophrenia Exomes” bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al. “Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals” bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al. “Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses.” Hum Genet 138, 1331–1340 (2019). https://www.ncbi.nlm.nih.gov/pubmed/31659433; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” The Lancet Respiratory Medicine 7.6 (2019): 509-522. https",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:12210,depend,dependence,12210,references.html,https://hail.is,https://hail.is/references.html,1,['depend'],['dependence']
Integrability,"d #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0]). Run the GWAS with linear_regression_rows() using variant dosages as covariates:; >>> gwas = hl.linear_regression_rows( ; ... x=mt_annot.GT.n_alt_alleles(),; ... y=mt_annot.pheno.blood_pressure,; ... covariates=[1, mt_annot.pheno.age, mt_annot.snp1, mt_annot.snp2]). dependencies:; linear_regression_rows(), aggregators.collect(), parse_variant(), variant_str(). Stratified by Group. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: Use the linear_regression_rows() method for each group; >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.group_by() and aggregators.linreg() aggregators; >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). dependencies:; linear_regression_rows(), aggregato",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:11042,depend,dependencies,11042,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"d in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71038,integrat,integration,71038,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integration']
Integrability,"d isinstance(weights, list):; raise ValueError(""When y is a single list, weights should be a single expression.""); elif not y_is_list and isinstance(weights, list):; raise ValueError(""When y is a single expression, weights should be a single expression.""). weights = wrap_to_list(weights) if weights is not None else None. for e in itertools.chain.from_iterable(y) if is_chained else y:; analyze('linear_regression_rows_nd/y', e, mt._col_indices). for e in covariates:; analyze('linear_regression_rows_nd/covariates', e, mt._col_indices). _warn_if_no_intercept('linear_regression_rows_nd', covariates). x_field_name = Env.get_uid(); if is_chained:; y_field_name_groups = [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_name_groups), itertools.chain.from_iterable(y))); if weights is not None and len(weights) != len(y):; raise ValueError(""Must specify same number of weights as groups of phenotypes""); else:; y_field_name_groups = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_name_groups, y)); # Wrapping in a list since the code is written for the more general chained case.; y_field_name_groups = [y_field_name_groups]; if weights is not None and len(weights) != 1:; raise ValueError(""Must specify same number of weights as groups of phenotypes""). cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); weight_field_names = list(f'__weight_for_group_{i}' for i in range(len(weights))) if weights is not None else None; weight_dict = dict(zip(weight_field_names, weights)) if weights is not None else {}. row_field_names = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows_nd'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **weight_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_field_names,; col_key=[],; entry_exprs={x_field_name: x},; ). entries_field_name = 'ent'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:17244,Wrap,Wrapping,17244,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['Wrap'],['Wrapping']
Integrability,"d returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combini",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:9426,depend,dependencies,9426,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39004,depend,depends,39004,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['depend'],['depends']
Integrability,"d standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.sta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:52719,depend,dependent,52719,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['depend'],['dependent']
Integrability,"dditional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8412,message,message,8412,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['message'],['message']
Integrability,"defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_bed(), MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:6670,depend,dependencies,6670,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"d” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:5394,depend,depend,5394,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,2,['depend'],"['depend', 'dependency']"
Integrability,"e all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:8811,depend,dependencies,8811,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependencies']
Integrability,"e; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:1172,interface,interface,1172,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['interface'],['interface']
Integrability,"e; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95121,depend,dependency,95121,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependency']
Integrability,"ect is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:2608,depend,dependencies,2608,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['depend'],['dependencies']
Integrability,"ed types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to pro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:1476,wrap,wraps,1476,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['wrap'],['wraps']
Integrability,"een iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:61527,message,message,61527,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"ef aggregate_cols(self, expr, _localize=True) -> Any:; """"""Aggregate over columns to a local value. Examples; --------; Aggregate over columns:. >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by column. This method should be thought of as a more convenient alternative to; the following:. >>> cols_table = dataset.cols(); >>> cols_table.aggregate(; ... hl.struct(fraction_female=hl.agg.fraction(cols_table.pheno.is_female),; ... case_ratio=hl.agg.count_where(cols_table.is_case) / hl.agg.count())). Note; ----; This method supports (and expects!) aggregation over columns. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; base, _ = self._process_joins(expr); analyze('MatrixTable.aggregate_cols', expr, self._global_indices, {self._col_axis}). cols_field = Env.get_uid(); globals = base.localize_entries(columns_array_field_name=cols_field).index_globals(); if len(self._col_key) == 0:; cols = globals[cols_field]; else:; if Env.hc()._warn_cols_order:; warning(; ""aggregate_cols(): Aggregates over cols ordered by 'col_key'.""; ""\n To preserve matrix table column order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_cols_order = False; cols = hl.sorted(globals[cols_field], key=lambda x: x.select(*self._col_key.keys())). agg_ir = ir.Let('global', globals.drop(cols_field)._ir, ir.StreamAgg(ir.ToStream(cols._ir), 'sa', expr._ir)). if _localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(agg_ir, expr.dtype). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_entries(self, expr, _localize=True):; """"""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:67617,depend,dependent,67617,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['depend'],['dependent']
Integrability,"ef literal(x: Any, dtype: Optional[Union[HailType, str]] = None):; """"""Captures and broadcasts a Python variable or object as an expression. Examples; --------. >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""; wrapper = {'has_expr': False, 'has_free_vars': False}. def typecheck_expr(t, x):; if isinstance(x, Expression):; wrapper['has_expr'] = True; wrapper['has_free_vars'] |= (; builtins.len(x._ir.free_vars) > 0; or builtins.len(x._ir.free_agg_vars) > 0; or builtins.len(x._ir.free_scan_vars) > 0; ). if x.dtype != t:; raise TypeError(f""'literal': type mismatch: expected '{t}', found '{x.dtype}'""); elif x._indices.source is not None:; if x._indices.axes:; raise ExpressionException(; f""'literal' can only accept scalar or global expression arguments,""; f"" found indices {x._indices.axes}""; ); return False; elif x is None or x is pd.NA:; return False; else:; t._typecheck_one_level(x); return True. if dtype is None:; dtype = impute_type(x). # Special handling of numpy. Have to extract from numpy scalars, do nothing on numpy arrays; if isinstance(x, np.generic):; x = x.item(); elif isinstance(x, np.ndarray):; pass; else:; try:; dtype._traverse(x, typecheck_expr); except TypeError as e:; raise TypeError(""'literal': ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:10126,wrap,wrapper,10126,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['wrap'],['wrapper']
Integrability,"ele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotatio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:59156,interface,interface,59156,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['interface'],['interface']
Integrability,"ele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:74161,integrat,integration,74161,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Integrability,"ence (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; Aggregate Row Values Into a Local Value; Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Aggregation. View page source. Aggregation; For a full list of aggregators, see the aggregators; section of the API reference. Table Aggregations. Aggregate Over Rows Into A Local Value. One aggregation. description:; Compute the fraction of rows where SEX == 'M' in a table. code:; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. dependencies:; Table.aggregate(), aggregators.fraction(). Multiple aggregations. description:; Compute two aggregation statistics, the fraction of rows where; SEX == 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and Ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:1188,depend,dependencies,1188,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"ences usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Miss",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21412,message,message,21412,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"enotype, which; differs from default PLINK behavior. Use missing='-9' to interpret this; value as missing. Parameters:. bed (str) – PLINK BED file.; bim (str) – PLINK BIM file.; fam (str) – PLINK FAM file.; min_partitions (int, optional) – Minimum number of partitions. Useful in conjunction with block_size.; missing (str) – String used to denote missing values only for the phenotype field.; This is in addition to “-9”, “0”, and “N/A” for case-control; phenotypes.; delimiter (str) – FAM file field delimiter regex.; quant_pheno (bool) – If True, FAM phenotype is interpreted as quantitative.; a2_reference (bool) – If True, A2 is treated as the reference allele. If False, A1 is treated; as the reference allele.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome. If None, the; default is dependent on the reference_genome. For “GRCh37”, the default; is {'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}. For “GRCh38”, the; default is {'1': 'chr1', ..., '22': 'chr22', '23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'}.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; n_partitions (int, optional) – Number of partitions. If both n_partitions and block_size; are specified, n_partitions will be used.; block_size (int, optional) – Block size, in MB. Default: 128MB blocks. Returns:; MatrixTable. hail.methods.import_table(paths, key=None, min_partitions=None, impute=False, no_header=False, comment=(), delimiter='\t', missing='NA', types={}, quote=None, skip_blank_lines=False, force_bgz=False, filter=None, find_replace=None, force=False, source_file_field=None)[source]; Import delimited text file (text table) as Table.; The resulting Table will have no key fields. Use; Table.key_by() to specify keys. See also:; import_matrix_table().; Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:30158,depend,dependent,30158,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['depend'],['dependent']
Integrability,"er than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.n_alt_alleles(),; ... mt.GT.n_alt_alleles()), mt.prior))). dependencies:; import_plink(), variant_qc(), import_table(),; coalesce(), case(), cond(), Call.n_alt_alleles(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:14579,depend,dependencies,14579,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"er, these metrics aren’t perfectly representative of the samples in; our dataset. Here’s why:. In [21]:. table.count(). Out[21]:. 3500L. In [22]:. vds.num_samples. Out[22]:. 1000. Since there are fewer samples in our dataset than in the full thousand; genomes cohort, we need to look at annotations on the dataset. We can do; this with; query_samples. In [23]:. vds.query_samples('samples.map(s => sa.SuperPopulation).counter()'). Out[23]:. {u'AFR': 101L, u'AMR': 285L, u'EAS': 308L, u'EUR': 298L, u'SAS': 8L}. In [24]:. pprint(vds.query_samples('samples.map(s => sa.CaffeineConsumption).stats()')). {u'max': 10.0,; u'mean': 6.783000000000003,; u'min': 3.0,; u'nNotMissing': 1000L,; u'stdev': 1.624780292839619,; u'sum': 6783.000000000003}. The functionality demonstrated in the last few cells isn’t anything; especially new: it’s certainly not difficult to ask these questions with; Pandas or R dataframes, or even Unix tools like awk. But Hail can; use the same interfaces and query language to analyze collections that; are much larger, like the set of variants.; Here we calculate the counts of each of the 12 possible unique SNPs (4; choices for the reference base * 3 choices for the alternate base). To; do this, we need to map the variants to their alternate allele, filter; to SNPs, and count by unique ref/alt pair:. In [25]:. snp_counts = vds.query_variants('variants.map(v => v.altAllele()).filter(aa => aa.isSNP()).counter()'); pprint(Counter(snp_counts).most_common()). [(AltAllele(ref=C, alt=T), 2436L),; (AltAllele(ref=G, alt=A), 2387L),; (AltAllele(ref=A, alt=G), 1944L),; (AltAllele(ref=T, alt=C), 1879L),; (AltAllele(ref=C, alt=A), 496L),; (AltAllele(ref=G, alt=T), 480L),; (AltAllele(ref=T, alt=G), 468L),; (AltAllele(ref=A, alt=C), 454L),; (AltAllele(ref=C, alt=G), 150L),; (AltAllele(ref=G, alt=C), 112L),; (AltAllele(ref=T, alt=A), 79L),; (AltAllele(ref=A, alt=T), 76L)]. It’s nice to see that we can actually uncover something biological from; this small dataset: we see that t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:10837,interface,interfaces,10837,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['interface'],['interfaces']
Integrability,"eral(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(value)[source]; Tests whether a value is contained in the interval.; Examples; >>> hl.eval(interval.contains(3)); True. >>> hl.eval(interval.contains(11)); False. Parameters:; value – Object with type matching the interval point type. Returns:; BooleanExpression – True if value is contained in the interval, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. property end; Returns the end point.; Examples; >>> hl.eval(interval.end); 11. Returns:; Expression. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:2994,depend,dependencies,2994,docs/0.2/hail.expr.IntervalExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html,1,['depend'],['dependencies']
Integrability,"erate on because the work is not evenly distributed across partitions. Parameters:max_partitions (int) – Desired number of partitions. If the current number of partitions is less than max_partitions, do nothing. Returns:Variant dataset with the number of partitions equal to at most max_partitions. Return type:VariantDataset. num_partitions()[source]¶; Number of partitions.; Notes; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:129622,interface,interface,129622,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['interface'],['interface']
Integrability,"erence_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_table(self.url)._maybe_flexindex_table_by_expr(indexer_key_expr, all_matches=all_matches). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configurat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:4363,message,message,4363,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['message'],['message']
Integrability,"ers:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:46610,interface,interface,46610,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['interface'],['interface']
Integrability,"erty.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. Warning; In order to support a wide variety of data types, this function splits only; the variants on a MatrixTable, but not the genotypes. Use; split_multi_hts() if possible, or split the genotypes yourself using; one of the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:82925,interface,interface,82925,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['interface'],['interface']
Integrability,"es the representation of missing data in the table. Note; The comment and missing parameters are NOT regexes. The no_header option indicates that the file has no header line. If this option is passed, ; then the column names will be f0, f1, … fN (0-indexed).; The types option allows the user to pass the types of columns in the table. This is a ; dict keyed by str, with Type values. See the examples above for; a standard usage. Additionally, this option can be used to override type imputation. For example,; if a column in a file refers to chromosome and does not contain any sex chromosomes, it will be; imputed as an integer, while most Hail methods expect chromosome to be passed as a string. Using; the impute=True mode and passing types={'Chromosome': TString()} will solve this problem.; The min_partitions option can be used to increase the number of partitions (level of sharding); of an imported table. The default partition size depends on file system and a number of other ; factors (including the min_block_size of the hail context), but usually is between 32M and 128M. Parameters:; paths (str or list of str) – Files to import.; key (str or list of str) – Key column(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – File has no header and the N columns are named f0, f1, … fN (0-indexed); impute (bool) – Impute column types from the file; comment (str or None) – Skip lines beginning with the given pattern; delimiter (str) – Field delimiter regex; missing (str) – Specify identifier to be treated as missing; types (dict with str keys and Type values) – Define types of fields in annotations files; quote (str or None) – Quote character. Returns:Key table constructed from text table. Return type:KeyTable. import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False, call_fields=[])[source]¶; Import VCF file(s) as variant dataset.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:18146,depend,depends,18146,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['depend'],['depends']
Integrability,"esent values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:9135,depend,depends,9135,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['depend'],['depends']
Integrability,"esourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:2687,interface,interface,2687,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['interface'],['interface']
Integrability,"ession involving v (variant), va (variant annotations), ; and aIndex (allele index); annotation (str) – Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices); subset (bool) – If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs.; keep (bool) – If true, keep variants matching expr; filter_altered_genotypes (bool) – If true, genotypes that contain filtered-out alleles are set to missing.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered.; keepStar (bool) – If true, keep variants where the only allele left is a * allele. Returns:Filtered variant dataset. Return type:VariantDataset. filter_genotypes(expr, keep=True)[source]¶; Filter genotypes based on expression.; Examples; Filter genotypes by allele balance dependent on genotype call:; >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). Notes; expr is in genotype context so the following symbols are in scope:. s (Sample): sample; v (Variant): Variant; sa: sample annotations; va: variant annotations; global: global annotations. For more information, see the documentation on data representation, annotations, and; the expression language. Caution; When expr evaluates to missing, the genotype will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep genotypes where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_intervals(intervals, keep=True)[source]¶; Filter variants with an interval or list of intervals.; Examples; Filter to one interval:; >>> vds_result = vds.filter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:52303,depend,dependent,52303,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['dependent']
Integrability,"ession refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:11291,interface,interface,11291,docs/0.2/hail.expr.CollectionExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html,1,['interface'],['interface']
Integrability,"etic variance component coefficients; \(h^2 = \frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \frac{1}{1 + \delta} =\) genetic proportion of residual phenotypic variance. Under a linear mixed model, \(y\) is sampled from the \(n\)-dimensional multivariate normal distribution with mean \(X \beta\) and variance components that are scalar multiples of \(K\) and \(I\):. \[y \sim \mathrm{N}\left(X\beta, \sigma_g^2 K + \sigma_e^2 I\right)\]; Thus the model posits that the residuals \(y_i - X_{i,:}\beta\) and \(y_j - X_{j,:}\beta\) have covariance \(\sigma_g^2 K_{ij}\) and approximate correlation \(h^2 K_{ij}\). Informally: phenotype residuals are correlated as the product of overall heritability and pairwise kinship. By contrast, standard (unmixed) linear regression is equivalent to fixing \(\sigma_2\) (equivalently, \(h^2\)) at 0 above, so that all phenotype residuals are independent.; Caution: while it is tempting to interpret \(h^2\) as the narrow-sense heritability of the phenotype alone, note that its value depends not only the phenotype and genetic data, but also on the choice of sample covariates.; Fitting the global model; The core algorithm is essentially a distributed implementation of the spectral approach taken in FastLMM. Let \(K = USU^T\) be the eigendecomposition of the real symmetric matrix \(K\). That is:. \(U = n \times n\) orthonormal matrix whose columns are the eigenvectors of \(K\); \(S = n \times n\) diagonal matrix of eigenvalues of \(K\) in descending order. \(S_{ii}\) is the eigenvalue of eigenvector \(U_{:,i}\); \(U^T = n \times n\) orthonormal matrix, the transpose (and inverse) of \(U\). A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:99950,depend,depends,99950,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['depends']
Integrability,"eturn isinstance(other, tdict) and self.key_type == other.key_type and self.value_type == other.value_type. def _pretty(self, b, indent, increment):; b.append('dict<'); self.key_type._pretty(b, indent, increment); b.append(', '); self.value_type._pretty(b, indent, increment); b.append('>'). def _parsable_string(self):; return ""Dict[{},{}]"".format(self.key_type._parsable_string(), self.value_type._parsable_string()). def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[dict, frozendict]:; d = {; self.key_type._convert_from_json_na(elt['key'], _should_freeze=True): self.value_type._convert_from_json_na(; elt['value'], _should_freeze=_should_freeze; ); for elt in x; }; if _should_freeze:; return frozendict(d); return d. def _convert_to_json(self, x):; return [; {'key': self.key_type._convert_to_json(k), 'value': self.value_type._convert_to_json(v)}; for k, v in x.items(); ]. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> Union[dict, frozendict]:; # NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.; d = {}; length = byte_reader.read_int32(); for _ in range(length):; element = self._array_repr.element_type._convert_from_encoding(byte_reader, _should_freeze); d[element.key] = element.value. if _should_freeze:; return frozendict(d); return d. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); for k, v in value.items():; self._array_repr.element_type._convert_to_encoding(byte_writer, {'key': k, 'value': v}). def _propagate_jtypes(self, jtype):; self._key_type._add_jtype(jtype.keyType()); self._value_type._add_jtype(jtype.valueType()). def unify(self, t):; return isinstance(t, tdict) and self.key_type.unify(t.key_type) and self.value_type.unify(t.value_type). def subst(self):; return tdict(self._key_type.subst(), self._value_type.subst()). def clear(self):; self.key_type.clear(); self.value_type.clear(). def _get_context",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:28604,wrap,wrapper,28604,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['wrap'],['wrapper']
Integrability,"ew rows of the matrix table to the console. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The output can be passed piped to another output source using the handler argument:; >>> mt.show(handler=lambda x: logging.info(x)) . Parameters:. n_rows (int) – Maximum number of rows to show.; n_cols (int) – Maximum number of columns to show.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(*, rows=True, cols=True, entries=True, handler=None)[source]; Compute and print summary information about the fields in the matrix table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. rows (bool) – Compute summary for the row fields.; cols (bool) – Compute summary for the column fields.; entries (bool) – Compute summary for the entry fields. tail(n_rows, n_cols=None, *, n=None)[source]; Subset matrix to last n rows.; Examples; >>> mt_range = hl.utils.range_matrix_table(100, 100). Passing only one argument will take the last n rows:; >>> mt_range.tail(10).count(); (10, 100). Passing two arguments refers to rows and columns, respectively:; >>> mt_range.tail(10, 20).count(); (10, 20). Either argument may be None to indicate no filter.; Last 10 rows, all columns:; >>> mt_range.tail(10, None).count(); (10, 100). All rows, last 10 columns:; >>> mt_range.tail(None, 10).count(); (100, 10). Notes; For backwards compatibility, the n parameter is not named n_rows,; but the parameter refers to the number of rows to keep.; The number of partitions in the new matrix is equal to the number of; partitions containing the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:61086,interface,interface,61086,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['interface'],['interface']
Integrability,"ew” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipyt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2852,depend,dependencies,2852,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['depend'],['dependencies']
Integrability,"expr_any); def default(self, then):; """"""Finish the switch statement by adding a default case. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0 and self._when_missing_case is None:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). [docs]class CaseBuilder(ConditionalBuilder):; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBui",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:4754,message,message,4754,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,6,['message'],['message']
Integrability,"ey.values()); ):; raise ValueError(; 'anti_join: cannot join: table must have a key of the same type(s) and be the same length or shorter:'; f'\n Left key: {"", "".join(str(x.dtype) for x in self.key.values())}'; f'\n Right key: {"", "".join(str(x.dtype) for x in other.key.values())}'; ). return self.filter(hl.is_missing(other.index(*(self.key[i] for i in range(len(other.key)))))). [docs] @typecheck_method(; right=table_type, how=enumeration('inner', 'outer', 'left', 'right'), _mangle=anyfunc, _join_key=nullable(int); ); def join(; self,; right: 'Table',; how='inner',; _mangle: Callable[[str, int], str] = lambda s, i: f'{s}_{i}',; _join_key: Optional[int] = None,; ) -> 'Table':; """"""Join two tables together. Examples; --------; Join `table1` to `table2` to produce `table_joined`:. >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; -----; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing. Both tables must have the same number of keys and the corresponding; types of each key must be the same (order matters), but the key",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:100474,depend,depends,100474,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['depend'],['depends']
Integrability,"f sex chromosomes:; - Sex chromosomes of male individuals should be haploid to be phased correctly.; - If proband_call is diploid on non-par regions of the sex chromosomes, it is assumed to be female.; Returns NA when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid on non-PAR region of Y; In addition, individual phased genotype calls are returned as missing in the following situations:; 1. All mother genotype calls non-PAR region of Y; 2. Diploid father genotype calls on non-PAR region of X for a male proband (proband and mother are still phased as father doesn’t participate in allele transmission). Note; phase_trio_matrix_by_transmission() provides a convenience wrapper for phasing a trio matrix. Parameters:. locus (LocusExpression) – Expression for the locus in the trio matrix; alleles (ArrayExpression) – Expression for the alleles in the trio matrix; proband_call (CallExpression) – Expression for the proband call in the trio matrix; father_call (CallExpression) – Expression for the father call in the trio matrix; mother_call (CallExpression) – Expression for the mother call in the trio matrix. Returns:; ArrayExpression – Array containing: [phased proband call, phased father call, phased mother call]. hail.experimental.phase_trio_matrix_by_transmission(tm, call_field='GT', phased_call_field='PBT_GT')[source]; Adds a phased genoype entry to a trio MatrixTable based allele transmission in the trio.; Example; >>> # Create a trio matrix; >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). >>> # Phase trios by transmission; >>> phased_trio_datas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:19870,wrap,wrapper,19870,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['wrap'],['wrapper']
Integrability,"f. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4367,depend,depends,4367,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['depends']
Integrability,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76373,depend,depends,76373,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['depend'],['depends']
Integrability,"fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52133,depend,dependent,52133,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['depend'],['dependent']
Integrability,"for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:32728,message,messages,32728,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['message'],['messages']
Integrability,"g; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendere",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:39793,message,message,39793,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:96828,rout,routine,96828,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['rout'],"['routine', 'routines']"
Integrability,"group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:69515,depend,dependencies,69515,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependencies']
Integrability,"h and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:3658,depend,depends,3658,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['depends']
Integrability,"h more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""; if Env.hc()._warn_entries_order and len(self.col_key) > 0:; warning(; ""entries(): Resulting entries table is sorted by '(row_key, col_key)'.""; ""\n To preserve row-major matrix table order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_entries_order = False. return Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:89769,depend,depending,89769,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['depend'],['depending']
Integrability,"h was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of typ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:69202,integrat,integrate,69202,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integrate']
Integrability,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2043,message,message,2043,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['message'],['message']
Integrability,"he effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be include",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:29388,depend,dependent,29388,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['depend'],['dependent']
Integrability,"her (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the matrix table; it is; filtering the matrix table to row keys present in another table.; To discard rows whose key is present in other, use; anti_join_rows().; Examples; >>> ds_result = ds.semi_join_rows(rows_to_keep). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement a semi-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_defined(rows_to_keep.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), semi_join_cols(). show(n_rows=None, n_cols=None, include_row_fields=False, width=None, truncate=None, types=True, handler=None)[source]; Print the first few rows of the matrix table to the console. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The output can be passed piped to another output source using the handler argument:; >>> mt.show(handler=lambda x: logging.info(x)) . Parameters:. n_rows (int) – Maximum number of rows to show.; n_cols (int) – Maximum number of columns to show.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(*, rows=True, cols=True, entries=True, handler=None)[source]; Compute and print summary information about the fields in the matrix table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. rows (bool) – Compute summary for the row fields.; cols (bool) – Compute ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:60220,interface,interface,60220,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['interface'],['interface']
Integrability,"hon variable or object as an expression. Examples; --------. >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""; wrapper = {'has_expr': False, 'has_free_vars': False}. def typecheck_expr(t, x):; if isinstance(x, Expression):; wrapper['has_expr'] = True; wrapper['has_free_vars'] |= (; builtins.len(x._ir.free_vars) > 0; or builtins.len(x._ir.free_agg_vars) > 0; or builtins.len(x._ir.free_scan_vars) > 0; ). if x.dtype != t:; raise TypeError(f""'literal': type mismatch: expected '{t}', found '{x.dtype}'""); elif x._indices.source is not None:; if x._indices.axes:; raise ExpressionException(; f""'literal' can only accept scalar or global expression arguments,""; f"" found indices {x._indices.axes}""; ); return False; elif x is None or x is pd.NA:; return False; else:; t._typecheck_one_level(x); return True. if dtype is None:; dtype = impute_type(x). # Special handling of numpy. Have to extract from numpy scalars, do nothing on numpy arrays; if isinstance(x, np.generic):; x = x.item(); elif isinstance(x, np.ndarray):; pass; else:; try:; dtype._traverse(x, typecheck_expr); except TypeError as e:; raise TypeError(""'literal': object did not match the passed type '{}'"".format(dtype)) from e. if wrapper['has_free_vars']:; raise ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:10239,wrap,wrapper,10239,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,4,['wrap'],['wrapper']
Integrability,"hrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. **Performance**. Hail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:125103,rout,routine,125103,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['rout'],['routine']
Integrability,"ibutions to variation in human stature in prehistoric Europe.” bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al. “Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction.” Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al. “Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder.” Science (2018): 1. https://science.sciencemag.org/content/362/6420/eaat6576.full; Molnos, Sophie Claudia. Metabolites: implications in type 2 diabetes and the effect of epigenome-wide interaction with genetic variation. Diss. Technische Universität München, 2018. https://mediatum.ub.tum.de/1372795f; Bis, Joshua C., et al. “Whole exome sequencing study identifies novel rare and common Alzheimer’s-associated variants involved in immune response and transcriptional regulation.” Molecular Psychiatry (2018): 1. https://www.nature.com/articles/s41380-018-0112-7; Gormley, Padhraig, et al. “Common variant burden contributes to the familial aggregation of migraine in 1,589 families.” Neuron 98.4 (2018): 743-753. https://www.ncbi.nlm.nih.gov/pubmed/30189203; Rivas, Manuel A., et al. “Insights into the genetic epidemiology of Crohn’s and rare diseases in the Ashkenazi Jewish population.” PLoS Genetics 14.5 (2018): e1007329. https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007329; Satterstrom, F. Kyle, et al. “ASD and ADHD have a similar burden of rare protein-truncating variants.” bioRxiv (2018): 277707. https://www.biorxiv.org/content/10.1101/277707v1; Zekavat, Seyedeh M., et al. “Deep coverage whole genome sequences and plasma lipoprotein",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:18158,mediat,mediatum,18158,references.html,https://hail.is,https://hail.is/references.html,1,['mediat'],['mediatum']
Integrability,"ich we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:3335,depend,dependencies,3335,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependencies']
Integrability,"ields_referenced)). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_rows(self, expr, _localize=True) -> Any:; """"""Aggregate over rows to a local value. Examples; --------; Aggregate over rows:. >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by row. This method should be thought of as a more convenient alternative to; the following:. >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; base, _ = self._process_joins(expr); analyze('MatrixTable.aggregate_rows', expr, self._global_indices, {self._row_axis}); rows_table = ir.MatrixRowsTable(base._mir); subst_query = ir.subst(expr._ir, {}, {'va': ir.Ref('row', rows_table.typ.row_type)}). agg_ir = ir.TableAggregate(rows_table, subst_query); if _localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_cols(self, expr, _localize=True) -> Any:; """"""Aggregate over columns to a local value. Examples; --------; Aggregate over columns:. >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful ref",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:66080,depend,dependent,66080,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['depend'],['dependent']
Integrability,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:103042,integrat,integrate,103042,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['integrat'],['integrate']
Integrability,"in their corresponding arrays. Note; The order of the columns is not guaranteed. Returns:; MatrixTable. cols()[source]; Returns a table with all column fields in the matrix.; Examples; Extract the column table:; >>> cols_table = dataset.cols(). Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the column key (which becomes the table key). To preserve the original; column order as the table row order, first unkey the columns using; key_cols_by() with no arguments. Returns:; Table – Table with all column fields from the matrix, with one row per column of the matrix. compute_entry_filter_stats(row_field='entry_stats_row', col_field='entry_stats_col')[source]; Compute statistics about the number and fraction of filtered entries. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. row_field (str) – Name for computed row field (default: entry_stats_row.; col_field (str) – Name for computed column field (default: entry_stats_col. Returns:; MatrixTable. Notes; Adds a new row field, row_field, and a new column field, col_field,; each of which are structs with the following fields:. n_filtered (tint64) - Number of filtered entries per row; or column.; n_remaining (tint64) - Number of entries not filtered per; row or column.; fraction_filtered (tfloat32) - Number of filtered entries; divided by the total number of filtered and remaining entries. See also; filter_entries(), unfilter_entries(). count()[source]; Count the number of rows and columns in the matrix.; Examples; >>> dataset.count(). Returns:; int, int – Number of rows, number of cols. count_cols(_localize=True)[source]; Count the number of columns in the matrix.; Examples; Count the number of columns:; >>> n_cols = dataset.count_cols(). Returns:; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:23647,interface,interface,23647,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['interface'],['interface']
Integrability,"ingular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39286,depend,dependent,39286,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['depend'],['dependent']
Integrability,"ion_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:103401,message,messages,103401,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['messages']
Integrability,"ion` that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a :class:`.CaseBuilder`. Parameters; ----------; condition: :class:`.BooleanExpression`; then : :class:`.Expression`. Returns; -------; :class:`.CaseBuilder`; Mutates and returns `self`.; """"""; self._unify_type(then.dtype); self._cases.append((condition, then)); return self. [docs] @typecheck_method(then=expr_any); def default(self, then):; """"""Finish the case statement by adding a default case. Notes; -----; If no condition from a :meth:`~.CaseBuilder.when` call is ``True``,; then `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the case statement by returning missing. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:8033,message,message,8033,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,10,['message'],['message']
Integrability,"is; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:61175,message,message,61175,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"it verify; that the allele has not changed strand. We can keep the old one for; reference and filter out any liftover that changed strands using:; >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38', include_strand=True),; ... old_locus=ht.locus) ; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) ; >>> ht = ht.key_by(locus=ht.new_locus.result) . dependencies:; liftover(), add_liftover(), get_reference(). Filtering and Pruning. Remove related individuals from a dataset. tags:; kinship. description:; Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. code:; >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). dependencies:; pc_relate(), maximal_independent_set(). understanding:. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using pc_relate(). We filter this; result based on a kinship threshold, which gives us a table of related pairs.; From this table of pairs, we can compute the complement of the maximal; independent set using maximal_independent_set(). The parameter; keep=False in maximal_independent_set specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It’s important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals.; Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals. From a table of intervals. tags:; genomic region, genomic range. description:; Import a text file of locus i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:3548,depend,dependencies,3548,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"kedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.dtype.items()):; if isinstance(self._ir, ir.MakeStruct):; expr = construct_expr(self._ir.fields[i][1], t, self._indices, self._aggregations); elif isinstance(self._ir, ir.SelectedTopLevelReference):; expr = construct_expr(; ir.ProjectedTopLevelReference(self._ir.ref.name, f, t), t, self._indices, self._aggregations; ); elif isinstance(self._ir, ir.SelectFields):; expr = construct_expr(ir.GetField(self._ir.old, f), t, self._indices, self._aggregations); else:; expr = construct_expr(ir.GetField(self._ir, f), t, self._indices, self._aggregations); self._set_field(f, expr). def _set_field(self, key, value):; if key not in self._fields:; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; if key in self.__dict__ or hasattr(super(), key):; self._warn_on_shadowed_name.add(key); else:; self.__dict__[key] = value; self._fields[key] = value. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:44559,message,message,44559,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['message'],['message']
Integrability,"l and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailct",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:80430,wrap,wrapping,80430,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['wrap'],['wrapping']
Integrability,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29304,depend,depends,29304,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['depend'],['depends']
Integrability,"l.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; >>> hl.eval(call.unphased_diploid_gt_index()); 1. Returns:; Expression of type tint32. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CallExpression.html:11166,interface,interface,11166,docs/0.2/hail.expr.CallExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html,1,['interface'],['interface']
Integrability,"le aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). dependencies:; MatrixTable.aggregate_entries(), aggregators.mean(), aggregators.fraction(), StructExpression. Aggregate Per Column Group. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:4965,depend,dependencies,4965,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:15075,interface,interface,15075,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,2,['interface'],['interface']
Integrability,"linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:4987,depend,depends,4987,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['depend'],['depends']
Integrability,"llele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93390,integrat,integration,93390,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Integrability,"llowing:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28477,depend,depend,28477,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['depend'],['depend']
Integrability,"localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(agg_ir, expr.dtype). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_entries(self, expr, _localize=True):; """"""Aggregate over entries to a local value. Examples; --------; Aggregate over entries:. >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; -----; This method should be thought of as a more convenient alternative to; the following:. >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; ----; This method supports (and expects!) aggregation over entries. Parameters; ----------; expr : :class:`.Expression`; Aggregation expressions. Returns; -------; any; Aggregated value dependent on `expr`.; """""". base, _ = self._process_joins(expr); analyze('MatrixTable.aggregate_entries', expr, self._global_indices, {self._row_axis, self._col_axis}); agg_ir = ir.MatrixAggregate(base._mir, expr._ir); if _localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(field_expr=oneof(str, Expression)); def explode_rows(self, field_expr) -> 'MatrixTable':; """"""Explodes a row field of type array or set, copying the entire row for each element. Examples; --------; Explode rows by annotated genes:. >>> dataset_result = dataset.explode_rows(dataset.gene). Notes; -----; The new matrix table will have `N` copies of each row, where `N` is the number; of elements that row contains for the field denoted by `field_expr`. The field; referenced in `field_expr` is replaced in the sequence of duplicated rows by the; sequence of elements in the array or set. All other fields remain t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:69388,depend,dependent,69388,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['depend'],['dependent']
Integrability,"locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed from the PL (genotype likelihood) fields using these; factorizations:. \[\mathrm{P}(x = (AA, AA, AB) \mid d) = \left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:55250,depend,depends,55250,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['depend'],['depends']
Integrability,"lue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:7508,interface,interface,7508,docs/0.2/hail.expr.IntervalExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html,1,['interface'],['interface']
Integrability,"mean of 5 trillion numbers?; That’s a lot of data, and turns out to be the rough number of genotypes; in the preprocessed gnomAD VCF,; which contained about 20 thousand samples and 250 million variants. Hail; is designed to handle datasets of this size and larger, and does so by; computing in parallel on many computers using Apache; Spark.; But we still want a simple programming model that allows us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 33064602",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:11021,interface,interface,11021,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['interface'],['interface']
Integrability,"ment expects a list of Hail expressions whose types match, in order, the ; table's key types.; ; Each expression in the list ``vds_key`` has the following symbols in; scope:. - ``s`` (*String*): sample ID; - ``sa``: sample annotations; ; **The** ``root`` **and** ``expr`` **arguments**; ; .. note::; ; One of ``root`` or ``expr`` is required, but not both. ; ; The ``expr`` parameter expects an annotation expression involving ``sa`` (the existing ; sample annotations in the dataset) and ``table`` (a struct containing the columns in ; the table), like ``sa.col1 = table.col1, sa.col2 = table.col2`` or ``sa = merge(sa, table)``.; The ``root`` parameter expects an annotation path beginning in ``sa``, like ``sa.annotations``.; Passing ``root='sa.annotations'`` is exactly the same as passing ``expr='sa.annotations = table'``. ``expr`` has the following symbols in scope:. - ``sa``: sample annotations; - ``table``: See note. .. note:: ; ; The value of ``table`` inside root/expr depends on the number of values in the key table, ; as well as the ``product`` argument. There are three behaviors based on the number of values; and one branch for ``product`` being true and false, for a total of six modes:; ; +-------------------------+-------------+--------------------+-----------------------------------------------+; | Number of value columns | ``product`` | Type of ``table`` | Value of ``table`` |; +=========================+=============+====================+===============================================+; | More than 2 | False | ``Struct`` | Struct with an element for each column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 1 | False | ``T`` | The value column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 0 | False | ``Boolean`` | Existence of any matching key. |; +-------------------------+-------------+--------------------+-----------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:18975,depend,depends,18975,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['depends']
Integrability,"missing`` parameters are **NOT** regexes. The ``no_header`` option indicates that the file has no header line. If this option is passed, ; then the column names will be ``f0``, ``f1``, ... ``fN`` (0-indexed). ; ; The ``types`` option allows the user to pass the types of columns in the table. This is a ; dict keyed by ``str``, with :py:class:`~hail.expr.Type` values. See the examples above for; a standard usage. Additionally, this option can be used to override type imputation. For example,; if a column in a file refers to chromosome and does not contain any sex chromosomes, it will be; imputed as an integer, while most Hail methods expect chromosome to be passed as a string. Using; the ``impute=True`` mode and passing ``types={'Chromosome': TString()}`` will solve this problem.; ; The ``min_partitions`` option can be used to increase the number of partitions (level of sharding); of an imported table. The default partition size depends on file system and a number of other ; factors (including the ``min_block_size`` of the hail context), but usually is between 32M and 128M.; ; :param paths: Files to import.; :type paths: str or list of str. :param key: Key column(s).; :type key: str or list of str. :param min_partitions: Minimum number of partitions.; :type min_partitions: int or None. :param bool no_header: File has no header and the N columns are named ``f0``, ``f1``, ... ``fN`` (0-indexed); ; :param bool impute: Impute column types from the file; ; :param comment: Skip lines beginning with the given pattern; :type comment: str or None; ; :param str delimiter: Field delimiter regex; ; :param str missing: Specify identifier to be treated as missing; ; :param types: Define types of fields in annotations files ; :type types: dict with str keys and :py:class:`.Type` values; ; :return: Key table constructed from text table.; :rtype: :class:`.KeyTable`. :param quote: Quote character; :type quote: str or None; """""". key = wrap_to_list(key); paths = wrap_to_list(paths); jtype",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:14843,depend,depends,14843,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['depend'],['depends']
Integrability,"mit // len(self._gvcf_import_intervals); warning(f'gvcf_batch_size of {old_value} would produce too many tasks ' f'using {value} instead'); self._gvcf_batch_size = value. [docs] def __eq__(self, other):; if other.__class__ != VariantDatasetCombiner:; return False; for slot in self.__serialized_slots__:; if getattr(self, slot) != getattr(other, slot):; return False; return True. @property; def finished(self) -> bool:; """"""Have all GVCFs and input Variant Datasets been combined?""""""; return not self._gvcfs and not self._vdses. [docs] def save(self):; """"""Save a :class:`.VariantDatasetCombiner` to its `save_path`.""""""; fs = hl.current_backend().fs; try:; backup_path = self._save_path + '.bak'; if fs.exists(self._save_path):; fs.copy(self._save_path, backup_path); with fs.open(self._save_path, 'w') as out:; json.dump(self, out, indent=2, cls=Encoder); if fs.exists(backup_path):; fs.remove(backup_path); except OSError as e:; # these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail; print(f'Failed saving {self.__class__.__name__} state at {self._save_path}'); print(f'An attempt was made to copy {self._save_path} to {backup_path}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF mer",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:11651,message,messages,11651,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['message'],['messages']
Integrability,"mum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:112844,depend,dependent,112844,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['dependent']
Integrability,"n `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed). In [13]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; metadata: Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int; }; }. We can apply conditional filters on things like population with; if/else:. In [14]:. vds.filter_samples_expr('if (sa.metadata.Population == ""EAS"") sa.qc.dpMean > 8 else sa.qc.dpMean > 4').num_samples. Out[14]:. 897. Filtering variants and genotypes¶; One of the advantages of Hail’s filtering interface is that it’s equally; easy to filter samples, variants, or genotypes. If one is handed a fresh; VCF text file, it’s pretty easy to write a program to filter variants,; but much harder to filter samples or genotypes. Other data; representations may lend themselves to a different operation being easy,; and the others hard. In Hail, we’ve abstracted away all of this – it’s; easy to filter anything!. In [15]:. vds.count_variants(). Out[15]:. 10961L. In [16]:. # Filter on allele frequency; vds.filter_variants_expr('va.qc.AF > 0.1', keep=True).count_variants(). Out[16]:. 7993L. In [17]:. # Filter on allele frequency and GQ mean; vds.filter_variants_expr('va.qc.AF > 0.1 && va.qc.gqMean > 20').count_variants(). Out[17]:. 7879L. In [18]:. # Genotype call rate across the entire dataset; vds.summarize().call_rate. Out[18]:. 0.9831634887327798. As we can see in the previous cell, the overall call rate of this; dataset is 98.7%. In [19]:. vds.filter_genotypes('g.gq >= 20', keep=True).summa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:6572,interface,interface,6572,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['interface'],['interface']
Integrability,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:8810,interface,interface,8810,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['interface'],['interface']
Integrability,"nctional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer joins on; MatrixTables. Version 0.2.11; Released 2019-03-06. New features. (#5374) Add default; arguments to hl.add_sequence for running on GCP.; (#5481) Added; sample_cols method to MatrixTable.; (#5501) Exposed; MatrixTable.unfilter_entries. See filter_entries; documentation for more information.; (#5480) Added; n_cols argument to MatrixTable.head.; (#5529) Added; Table.{semi_join, anti_join} and; MatrixTable.{semi_join_rows, semi_join_cols, anti_join_rows, anti_join_cols}.; (#5528) Added; {MatrixTable, Table}.checkpoint methods as wrappers around; write / read_{matrix_table, table}. Bug fixes. (#5416) Resolved; issue wherein VEP and certain regressions were recomputed on each; use, rather than once.; (#5419) Resolved; issue with import_vcf force_bgz and file size checks.; (#5427) Resolved; issue with Table.show and dictionary field types.; (#5468) Resolved; ordering problem with Expression.show on key fields that are not; the first key.; (#5492) Fixed; hl.agg.collect crashing when collecting float32 values.; (#5525) Fixed; hl.trio_matrix crashing when complete_trios is False. Version 0.2.10; Released 2019-02-15. New features. (#5272) Added a new; ‘delimiter’ option to Table.export.; (#5251) Add utility; aliases to hl.plot for output_notebook and show.; (#5249) Add; histogram2d function to hl.plot module.; (#5247) Expose; MatrixTable.localize_entries method for converting to a Table; with an entries array.; (#5300) Add new; filter and find_replace arguments to hl.import_table and; hl.import_vcf to apply regex and substitutio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:98638,wrap,wrappers,98638,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['wrap'],['wrappers']
Integrability,"nd BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#916",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66558,interface,interface,66558,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['interface'],['interface']
Integrability,"nds have the same block size.; To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to float64. One-dimensional ndarrays; of shape (n) are promoted to two-dimensional ndarrays of shape (1,; n), i.e. a single row.; Block matrices support broadcasting of +, -, *, and /; between matrices of different shapes, consistent with the NumPy; broadcasting rules.; There is one exception: block matrices do not currently support element-wise; “outer product” of a single row and a single column, although the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3062,depend,dependency,3062,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['depend'],['dependency']
Integrability,"ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains_allele(allele)[source]; Returns true if the call has one or more called alleles of the given index.; >>> c = hl.call(0, 3). >>> hl.eval(c.contains_allele(3)); True. >>> hl.eval(c.contains_allele(1)); False. Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CallExpression.html:3807,depend,dependencies,3807,docs/0.2/hail.expr.CallExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html,1,['depend'],['dependencies']
Integrability,"netic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that :math:`g_{is}` is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first. The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the `statistics` parameter:. - `i` (``col_key.dtype``) -- First sample. (key field); - `j` (``col_key.dtype``) -- Second sample. (key field); - `kin` (:py:data:`.tfloat64`) -- Kinship estimate, :math:`\widehat{\phi_{ij}}`.; - `ibd2` (:py:data:`.tfloat64`) -- IBD2 estimate, :math:`\widehat{k^{(2)}_{ij}}`.; - `ibd0` (:py:data:`.tfloat64`) -- IBD0 estimate, :math:`\widehat{k^{(0)}_{ij}}`.; - `ibd1` (:py:data:`.tfloat64`) -- IBD1 estimate, :math:`\widehat{k^{(1)}_{ij}}`. Here ``col_key`` refers to the column key of the source matrix table,; and ``col_key.dtype`` is a struct containing the column key fields. There is one row for each pair of distinct samples (columns), where `i`; corresponds to the column of smaller column index. In particular, if the; same column key value exists for :math:`n` columns, then the resulting; table will have :math:`\binom{n-1}{2}` rows with both key fields equal to; that column key value. This may result in unexpected behavior in downst",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:9684,depend,depending,9684,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['depend'],['depending']
Integrability,"ng item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7572,depend,dependencies,7572,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['depend'],['dependencies']
Integrability,"nitializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:2683,interface,interface,2683,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['interface'],['interface']
Integrability,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:132234,integrat,integrate,132234,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['integrat'],['integrate']
Integrability,"ns to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102560,depend,depending,102560,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['depend'],['depending']
Integrability,"ns:; Expression – Struct field. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other)[source]; Return self!=value. annotate(**named_exprs)[source]; Add new fields or recompute existing fields.; Examples; >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; If an expression in named_exprs shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters:; named_exprs (keyword args of Expression) – Fields to add. Returns:; StructExpression – Struct with new or updated fields. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. drop(*fields)[source]; Drop fields from the struct.; Examples; >>> hl.eval(struct.drop('b')); Struct(a=5). Parameters:; fields (varargs of str) – Fields to drop. Returns:; StructExpression – Struct without certain fields. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:3438,depend,dependencies,3438,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['depend'],['dependencies']
Integrability,"nt_where(cols_table.is_case) / hl.agg.count())). Note; This method supports (and expects!) aggregation over columns. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. aggregate_entries(expr, _localize=True)[source]; Aggregate over entries to a local value.; Examples; Aggregate over entries:; >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; This method should be thought of as a more convenient alternative to; the following:; >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; This method supports (and expects!) aggregation over entries. Parameters:; expr (Expression) – Aggregation expressions. Returns:; any – Aggregated value dependent on expr. aggregate_rows(expr, _localize=True)[source]; Aggregate over rows to a local value.; Examples; Aggregate over rows:; >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; Unlike most MatrixTable methods, this method does not support; meaningful references to fields that are not global or indexed by row.; This method should be thought of as a more convenient alternative to; the following:; >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. annotate_cols(**named_exprs)[source]; Create new column-indexed fields by name.; Examples",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:10129,depend,dependent,10129,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['depend'],['dependent']
Integrability,"o Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1494,message,message,1494,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['message'],['message']
Integrability,"of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:3094,depend,dependencies,3094,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"ollowing four steps in order:. filter to samples in given kinship matrix to those for which sa.pheno, sa.cov, and sa.cov2 are all defined; compute the eigendecomposition \(K = USU^T\) of the kinship matrix; fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (REML), storing results in global annotations under global.lmmreg; test each variant for association, storing results under va.lmmreg in variant annotations. This plan can be modified as follows:. Set run_assoc=False to not test any variants for association, i.e. skip Step 5.; Set use_ml=True to use maximum likelihood instead of REML in Steps 4 and 5.; Set the delta argument to manually set the value of \(\delta\) rather that fitting \(\delta\) in Step 4.; Set the global_root argument to change the global annotation root in Step 4.; Set the va_root argument to change the variant annotation root in Step 5. lmmreg() adds 9 or 13 global annotations in Step 4, depending on whether \(\delta\) is set or fit. Annotation; Type; Value. global.lmmreg.useML; Boolean; true if fit by ML, false if fit by REML. global.lmmreg.beta; Dict[String, Double]; map from intercept and the given covariates expressions to the corresponding fit \(\beta\) coefficients. global.lmmreg.sigmaG2; Double; fit coefficient of genetic variance, \(\hat{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient of environmental variance \(\hat{\sigma}_e^2\). global.lmmreg.delta; Double; fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:93302,depend,depending,93302,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['depending']
Integrability,"on. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the ndarray. Returns:; NDArrayNumericExpression – Array expression of the same type. __sub__(other)[source]; Positionally subtract a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to subtract. Returns:; NDArrayNumericExpression – NDArray of positional differences. __truediv__(other)[source]; Positionally divide by a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to divide by. Returns:; NDArrayNumericExpression – NDArray of positional quotients. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:5565,depend,dependencies,5565,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['depend'],['dependencies']
Integrability,"on. View page source. Aggregation; For a full list of aggregators, see the aggregators; section of the API reference. Table Aggregations. Aggregate Over Rows Into A Local Value. One aggregation. description:; Compute the fraction of rows where SEX == 'M' in a table. code:; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. dependencies:; Table.aggregate(), aggregators.fraction(). Multiple aggregations. description:; Compute two aggregation statistics, the fraction of rows where; SEX == 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:1845,depend,dependencies,1845,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"onding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc describe. Deprecations. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:64625,depend,dependencies,64625,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependencies']
Integrability,"only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K and V here mean any type, including other compound; types. Hail’s Array[T] objects are similar to Python’s lists, except; they must be homogenous: that is, each element must be of the same type.; Arrays are 0-indexed. Here are some examples of simple array; expressions.; Array literals are constructed with square brackets. In [28]:. hc.eval_expr_typed('[1, 2, 3, 4, 5]'). Out[28]:. ([1, 2, 3, 4, 5], Array[Int]). Arrays are indexed with square brackets and support Python’s slice; syntax. In [29]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[0]'). Out[29]:. (1, Int). In [30]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:3]'). Out[30]:. ([2, 3], Array[Int]). In [31]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:]'). Out[31]:. ([2, 3, 4, 5], Array[Int]). In [32]:. hc.eval_expr_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:7167,message,message,7167,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['message'],['message']
Integrability,"ons (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84164,depend,dependencies,84164,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependencies']
Integrability,"onsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75544,message,message,75544,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"opy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]). left = np.concatenate([[min_x], x[keep]]); right = np.concatenate([x[keep], [max_x]]); widths = right - left. trace_args = {; ""x"": [min_x, *x[keep]],; ""y"": slopes,; ""row"": facet_row,; ""col"": facet_col,; ""width"": widths,; ""offset"": 0,; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_bar(**trace_args). for idx, group_df in enumerate(grouped_data):; plot_group(group_df, idx). def get_stat(self):; return StatCDF(self.k). [docs]def geom_density(mapping=aes(), *, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False):; """"""Creates a smoothed density plot. This method uses the `hl.agg.approx_cdf` aggregator to compute a sketch; of the distribution of the values of `x`. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; k: `int`; Passed to the `approx_cdf` aggregator. The size of the aggregator scales; linearly with `k`. The default value of `1000` is likely sufficient for; most uses.; smoothing: `float`; Controls the amount of smoothing applied.; fill:; A single fill color for all density plots, overrides ``fill`` aesthetic.; color:; A single line color for all density plots, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; smoothed: `boolean`; If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomDensity(mapping, k, smoothing, fill, color, alpha, smoothed). class GeomHLine(Geom):; d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:20345,interface,interface,20345,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['interface'],['interface']
Integrability,"or any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns True if item is in the set.; Examples; >>> hl.eval(s1.contains(1)); True. >>> hl.eval(s1.contains(10)); False. Parameters:; item (Expression) – Value for inclusion test. Returns:; BooleanExpression – True if item is in the set. describe(handler=<built-in function print>); Print information about type, index, and dependencies. difference(s)[source]; Return the set of elements in the set that are not present in set s.; Examples; >>> hl.eval(s1.difference(s2)); {2}. >>> hl.eval(s2.difference(s1)); {5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements not in s. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""]",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.SetExpression.html:5953,depend,dependencies,5953,docs/0.2/hail.expr.SetExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html,1,['depend'],['dependencies']
Integrability,"osted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are maki",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:2186,depend,dependencies,2186,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['depend'],['dependencies']
Integrability,"p-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; CaseBuilder. View page source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1405,message,message,1405,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html,1,['message'],['message']
Integrability,"pects!) aggregation over entries. Parameters:; expr (Expression) – Aggregation expressions. Returns:; any – Aggregated value dependent on expr. aggregate_rows(expr, _localize=True)[source]; Aggregate over rows to a local value.; Examples; Aggregate over rows:; >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; Unlike most MatrixTable methods, this method does not support; meaningful references to fields that are not global or indexed by row.; This method should be thought of as a more convenient alternative to; the following:; >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. annotate_cols(**named_exprs)[source]; Create new column-indexed fields by name.; Examples; Compute statistics about the GQ distribution per sample:; >>> dataset_result = dataset.annotate_cols(sample_gq_stats = hl.agg.stats(dataset.GQ)). Add sample metadata from a hail.Table.; >>> dataset_result = dataset.annotate_cols(population = s_metadata[dataset.s].pop). Note; This method supports aggregation over rows. For instance, the usage:; >>> dataset_result = dataset.annotate_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Notes; This method creates new column fields, but can also overwrite existing fields. Only; same-scope fields can be overwritten: for example, it is not possible to annotate a; global field foo and later create an column field foo. However, it would be possible; to create an column field foo and later create another column field foo, overwriting; the first.; The arguments to the method should either be Expression; obje",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:11012,depend,dependent,11012,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['depend'],['dependent']
Integrability,"ped to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that constru",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:10117,depend,depend,10117,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['depend']
Integrability,"pendencies:; linear_regression_rows(), aggregators.collect(), parse_variant(), variant_str(). Stratified by Group. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: Use the linear_regression_rows() method for each group; >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.group_by() and aggregators.linreg() aggregators; >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). dependencies:; linear_regression_rows(), aggregators.group_by(), aggregators.linreg(). understanding:. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the linear_regression_rows() method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:12018,depend,dependencies,12018,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"pression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>)[source]; Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True)[source]; Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:2623,depend,dependencies,2623,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['depend'],['dependencies']
Integrability,"pulation or cohort, then the vds_key argument must be passed to describe the key in the dataset ; to use for the join. This argument expects a list of Hail expressions whose types match, in order, the ; table’s key types.; Each expression in the list vds_key has the following symbols in; scope:. s (String): sample ID; sa: sample annotations. The root and expr arguments. Note; One of root or expr is required, but not both. The expr parameter expects an annotation expression involving sa (the existing ; sample annotations in the dataset) and table (a struct containing the columns in ; the table), like sa.col1 = table.col1, sa.col2 = table.col2 or sa = merge(sa, table).; The root parameter expects an annotation path beginning in sa, like sa.annotations.; Passing root='sa.annotations' is exactly the same as passing expr='sa.annotations = table'.; expr has the following symbols in scope:. sa: sample annotations; table: See note. Note; The value of table inside root/expr depends on the number of values in the key table, ; as well as the product argument. There are three behaviors based on the number of values; and one branch for product being true and false, for a total of six modes:. Number of value columns; product; Type of table; Value of table. More than 2; False; Struct; Struct with an element for each column. 1; False; T; The value column. 0; False; Boolean; Existence of any matching key. More than 2; True; Array[Struct]; An array with a struct for each matching key. 1; True; Array[T]; An array with a value for each matching key. 0; True; Int; The number of matching keys. Common uses for the expr argument; Put annotations on the top level under sa; expr='sa = merge(sa, table)'. Annotate only specific annotations from the table; expr='sa.annotations = select(table, toKeep1, toKeep2, toKeep3)'. The above is equivalent to; expr='''sa.annotations.toKeep1 = table.toKeep1,; sa.annotations.toKeep2 = table.toKeep2,; sa.annotations.toKeep3 = table.toKeep3'''. Finally, for mor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:17903,depend,depends,17903,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['depends']
Integrability,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:9778,integrat,integrate,9778,references.html,https://hail.is,https://hail.is/references.html,1,['integrat'],['integrate']
Integrability,"p. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the rows of the matrix table by the row-indexed field gene; using MatrixTable.group_rows_by(), which returns a; GroupedMatrixTable. Then use GroupedMatrixTable.aggregate(); to compute an aggregation per grouped row.; The result is a matrix table with an entry field n_non_ref that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to group_rows_by, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to aggregate. Other row fields and entry fields; are dropped. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:6297,depend,dependencies,6297,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"ral chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20495,integrat,integration,20495,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Integrability,"rameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0 and self._when_missing_case is None:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). [docs]class CaseBuilder(ConditionalBuilder):; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBuilder.default` method calls must be the; same type. Parameters; ----------; missing_false: :obj:`.bool`; Treat missing predicates as ``False``. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:5007,message,message,5007,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,2,['message'],['message']
Integrability,"rays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:55701,protocol,protocols,55701,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['protocol'],['protocols']
Integrability,"re. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1962,depend,dependencies,1962,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['depend'],['dependencies']
Integrability,"reate more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:6813,depend,dependent,6813,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependent']
Integrability,"regate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns:; Table – Table with all non-global fields from the matrix, with one row per entry of the matrix. property entry; Returns a struct expression including all row-and-column-indexed fields.; Examples; Get all entry field names:; >>> list(dataset.entry); ['GT', 'AD', 'DP', 'GQ', 'PL']. Returns:; StructExpression – Struct of all entry fields. explode_cols(field_expr)[source]; Explodes a column field of type array or set, copying the entire column for each element.; Examples; Explode columns by annotated cohorts:; >>> dataset_result = dataset.explode_cols(dataset.cohorts). Notes; The new matrix table will have N copies of each column, where N is the; number of elements that column contains for the field denoted by field_expr.; The field referenced in field_expr is replaced in the sequence of duplicated; columns by the sequence of elements in the array or set. All other fields remain; the same, includ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:28648,depend,depending,28648,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['depend'],['depending']
Integrability,"rents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid d)` and :math:`\mathrm{P}(x \mid m)`; are computed from the PL (genotype likelihood) fields using these; factorizations:. .. math::; \mathrm{P}(x = (AA, AA, AB) \mid d) = \left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:23274,depend,depends,23274,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['depend'],['depends']
Integrability,"ression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:2849,depend,dependencies,2849,docs/0.2/hail.expr.NDArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html,1,['depend'],['dependencies']
Integrability,"rk 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95981,message,message,95981,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"rmatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:2791,depend,dependencies,2791,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,2,['depend'],['dependencies']
Integrability,"rom the :math:`n`-dimensional `multivariate normal distribution <https://en.wikipedia.org/wiki/Multivariate_normal_distribution>`__ with mean :math:`X \\beta` and variance components that are scalar multiples of :math:`K` and :math:`I`:. .. math::. y \sim \mathrm{N}\\left(X\\beta, \sigma_g^2 K + \sigma_e^2 I\\right). Thus the model posits that the residuals :math:`y_i - X_{i,:}\\beta` and :math:`y_j - X_{j,:}\\beta` have covariance :math:`\sigma_g^2 K_{ij}` and approximate correlation :math:`h^2 K_{ij}`. Informally: phenotype residuals are correlated as the product of overall heritability and pairwise kinship. By contrast, standard (unmixed) linear regression is equivalent to fixing :math:`\sigma_2` (equivalently, :math:`h^2`) at 0 above, so that all phenotype residuals are independent. **Caution:** while it is tempting to interpret :math:`h^2` as the `narrow-sense heritability <https://en.wikipedia.org/wiki/Heritability#Definition>`__ of the phenotype alone, note that its value depends not only the phenotype and genetic data, but also on the choice of sample covariates. **Fitting the global model**. The core algorithm is essentially a distributed implementation of the spectral approach taken in `FastLMM <https://www.microsoft.com/en-us/research/project/fastlmm/>`__. Let :math:`K = USU^T` be the `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices>`__ of the real symmetric matrix :math:`K`. That is:. - :math:`U = n \\times n` orthonormal matrix whose columns are the eigenvectors of :math:`K`; - :math:`S = n \\times n` diagonal matrix of eigenvalues of :math:`K` in descending order. :math:`S_{ii}` is the eigenvalue of eigenvector :math:`U_{:,i}`; - :math:`U^T = n \\times n` orthonormal matrix, the transpose (and inverse) of :math:`U`. A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:128710,depend,depends,128710,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['depends']
Integrability,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20633,integrat,integration,20633,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Integrability,"rrayNumericExpression`; ndarray of the specified size full of ones.; """"""; return full(shape, 1, dtype). [docs]@typecheck(nd=expr_ndarray()); def diagonal(nd):; """"""Gets the diagonal of a 2 dimensional NDArray. Examples; --------. >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional NDArray, shape(M, N). Returns; -------; :class:`.NDArrayExpression`; A 1 dimension NDArray of length min(M, N), containing the diagonal of `nd`.; """"""; assert nd.ndim == 2, ""diagonal requires 2 dimensional ndarray""; shape_min = hl.min(nd.shape[0], nd.shape[1]); return hl.nd.array(hl.range(hl.int32(shape_min)).map(lambda i: nd[i, i])). [docs]@typecheck(a=expr_ndarray(), b=expr_ndarray(), no_crash=bool); def solve(a, b, no_crash=False):; """"""Solve a linear system. Parameters; ----------; a : :class:`.NDArrayNumericExpression`, (N, N); Coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_type, indices, aggregations). if b_ndim_orig == 1:; if no_crash:; result = hl.struct(solution=result.solution.reshape((-1)), failed=result.failed); else:; result = result.reshape((-1)); return result. [docs]@typecheck(A=expr_ndarray(), b=expr_ndarray(), lower=expr_bool, no_crash=bool); def solve_triangular(A, b, lower=False, no_crash=False):; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:6703,Depend,Dependent,6703,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['Depend'],['Dependent']
Integrability,"rrent variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two job",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:1947,depend,dependencies,1947,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependencies']
Integrability,"rt construct_variable; from hail.expr.functions import _quantile_from_cdf, _result_from_raw_cdf, float32, rbind; from hail.expr.types import (; hail_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import TypeChecker, func_spec, identity, nullable, oneof, sequenceof, typecheck, typecheck_method; from hail.utils import wrap_to_list; from hail.utils.java import Env. class AggregableChecker(TypeChecker):; def __init__(self, coercer):; self.coercer = coercer; super(AggregableChecker, self).__init__(). def expects(self):; return self.coercer.expects(). def format(self, arg):; return self.coercer.format(arg). def check(self, x, caller, param):; x = self.coercer.check(x, caller, param); if len(x._ir.search(lambda node: isinstance(node, ir.BaseApplyAggOp))) == 0:; raise ExpressionException(; f""{caller} must be placed outside of an aggregation. See ""; ""https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701""; ); return x. agg_expr = AggregableChecker. class AggFunc(object):; def __init__(self):; self._as_scan = False; self._agg_bindings = set(). def correct_prefix(self):; return ""scan"" if self._as_scan else ""agg"". def incorrect_prefix(self):; return ""agg"" if self._as_scan else ""scan"". def correct_plural(self):; return ""scans"" if self._as_scan else ""aggregations"". def incorrect_plural(self):; return ""aggregations"" if self._as_scan else ""scans"". def check_scan_agg_compatibility(self, caller, node):; if self._as_scan != isinstance(node, ir.ApplyScanOp):; raise ExpressionException(; ""'{correct}.{caller}' cannot contain {incorrect}"".format(; correct=self.correct_prefix(), caller=caller, incorrect=self.incorrect_plural(); ); ). @typecheck_method(; agg_op=str, seq_op_args=sequenceof(expr_any), ret_type=hail_type, init_op_args=sequenceof(expr_any); ); def __call__(self, agg_op, seq_op_args, ret_type, init_op_args=()):; indices, aggregations = unify_all(*seq_op_args, *init_op",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:2186,interface,interface,2186,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['interface'],['interface']
Integrability,"ry for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmetho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:2798,rout,routed,2798,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['rout'],['routed']
Integrability,"s of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1847,message,message,1847,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,2,['message'],['message']
Integrability,"s()]. [docs] def matrix(self):; """"""; Gets the matrix backing this kinship matrix. :return: Matrix of kinship values.; :rtype: `IndexedRowMatrix <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix>`__; """"""; from pyspark.mllib.linalg.distributed import IndexedRowMatrix. return IndexedRowMatrix(self._jkm.matrix()). [docs] @typecheck_method(output=strlike); def export_tsv(self, output):; """"""; Export kinship matrix to tab-delimited text file with sample list as header.; ; :param str output: File path for output. ; """"""; self._jkm.exportTSV(output). [docs] @typecheck_method(output=strlike); def export_rel(self, output):; """"""; Export kinship matrix as .rel file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output. ; """"""; self._jkm.exportRel(output). [docs] @typecheck_method(output=strlike); def export_gcta_grm(self, output):; """"""; Export kinship matrix as .grm file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output.; """"""; self._jkm.exportGctaGrm(output). [docs] @typecheck_method(output=strlike,; opt_n_file=nullable(strlike)); def export_gcta_grm_bin(self, output, opt_n_file=None):; """"""; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output. ; ; :param opt_n_file: The file path to the N file. ; :type opt_n_file: str or None; """"""; self._jkm.exportGctaGrmBin(output, joption(opt_n_file)). [docs] @typecheck_method(output=strlike); def export_id_file(self, output):; """"""; Export samples as .id file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output.; """"""; self._jkm.exportIdFile(output). © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/kinshipMatrix.html:2821,depend,depending,2821,docs/0.1/_modules/hail/kinshipMatrix.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/kinshipMatrix.html,1,['depend'],['depending']
Integrability,"s, we can compute the complement of the maximal; independent set using maximal_independent_set(). The parameter; keep=False in maximal_independent_set specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It’s important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals.; Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals. From a table of intervals. tags:; genomic region, genomic range. description:; Import a text file of locus intervals as a table, then use; this table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_locus_intervals('data/gene.interval_list', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_locus_intervals(), MatrixTable.filter_rows(). understanding:. We have a matrix table mt containing the loci we would like to filter, and a; list of locus intervals stored in a file. We can import the intervals into a; table with import_locus_intervals().; Hail supports implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table.; interval_table[mt.locus] joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals.; To do our filtering, we can filter to the rows of our matrix table where the; struct expression interval_table[mt.locus] is defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then us",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:4839,depend,dependencies,4839,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1981,integrat,integrated,1981,index.html,https://hail.is,https://hail.is/index.html,1,['integrat'],['integrated']
Integrability,"select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:3593,depend,dependencies,3593,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"set up the two reference genomes (source and destination):; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') ; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Then we can liftover the locus coordinates in a Table or MatrixTable (here, ht); from reference genome 'GRCh37' to 'GRCh38':; >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38')) ; >>> ht = ht.filter(hl.is_defined(ht.new_locus)) ; >>> ht = ht.key_by(locus=ht.new_locus) . Note that this approach does not retain the old locus, nor does it verify; that the allele has not changed strand. We can keep the old one for; reference and filter out any liftover that changed strands using:; >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38', include_strand=True),; ... old_locus=ht.locus) ; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) ; >>> ht = ht.key_by(locus=ht.new_locus.result) . dependencies:; liftover(), add_liftover(), get_reference(). Filtering and Pruning. Remove related individuals from a dataset. tags:; kinship. description:; Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. code:; >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). dependencies:; pc_relate(), maximal_independent_set(). understanding:. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using pc_relate(). We filter this; result based on a kinship threshold, which gives us a table of related pairs.; From this table of pairs, we can compute the complement of the maximal; independent set using maximal_independent_set(). The parameter; keep=False in max",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:2966,depend,dependencies,2966,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"sions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. inner – Only rows with a matching key in the opposite table are included; in the resulting table.; left – All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; right – All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; outer – All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing. Both tables must have the same number of keys and the corresponding; types of each key must be the same (order matters), but the key names; can be different. Fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:37111,depend,depends,37111,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['depend'],['depends']
Integrability,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:13461,depend,depend,13461,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['depend'],['depend']
Integrability,"ssions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset. View page source. Variant Dataset; The VariantDataset is an extra layer of abstraction of the Hail Matrix Table for working; with large sequencing datasets. It was initially developed in response to the gnomAD project’s need; to combine, represent, and analyze 150,000 whole genomes. It has since been used on datasets as; large as 955,000 whole exomes. The VariantDatasetCombiner produces a; VariantDataset by combining any number of GVCF and/or VariantDataset files. Warning; Hail 0.1 also had a Variant Dataset class. Although pieces of the interfaces are similar, they should not; be considered interchangeable and do not represent the same data. Variant Dataset. VariantDataset; Class for representing cohort-level genomic data. read_vds(path, *[, intervals, n_partitions, ...]); Read in a VariantDataset written with VariantDataset.write(). filter_samples(vds, samples, *[, keep, ...]); Filter samples in a VariantDataset. filter_variants(vds, variants_table, *[, keep]); Filter variants in a VariantDataset, without removing reference data. filter_intervals(vds, intervals, *[, ...]); Filter intervals in a VariantDataset. filter_chromosomes(vds, *[, keep, remove, ...]); Filter chromosomes of a VariantDataset in several possible modes. sample_qc(vds, *[, gq_bins, dp_bins, dp_field]); Compute sample quality metrics about a VariantDataset. split_multi(vds, *[, filter_changed_loci]); Split the multiallelic variants in a VariantDataset. interval_coverage(vds, intervals[, ...]); Compute statistics about base coverage by interval. impute_sex_chromosome_ploidy(vds, ...[, ...])",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:1209,interface,interfaces,1209,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['interface'],['interfaces']
Integrability,"stance) for these types, then the vds_key argument ; should be passed. This argument expects a list of expressions whose types match, in order, ; the table’s key types. Note that using vds_key is slower than annotation with a standard ; key type.; Each expression in the list vds_key has the following symbols in; scope:. v (Variant): Variant; va: variant annotations. The root and expr arguments. Note; One of root or expr is required, but not both. The expr parameter expects an annotation assignment involving va (the existing ; variant annotations in the dataset) and table (the values(s) in the table),; like va.col1 = table.col1, va.col2 = table.col2 or va = merge(va, table).; The root parameter expects an annotation path beginning in va, like va.annotations.; Passing root='va.annotations' is the same as passing expr='va.annotations = table'.; expr has the following symbols in scope:. va: variant annotations; table: See note. Note; The value of table inside root/expr depends on the number of values in the key table, ; as well as the product argument. There are three behaviors based on the number of values; and one branch for product being true and false, for a total of six modes:. Number of value columns; product; Type of table; Value of table. More than 2; False; Struct; Struct with an element for each column. 1; False; T; The value column. 0; False; Boolean; Existence of any matching key. More than 2; True; Array[Struct]; An array with a struct for each matching key. 1; True; Array[T]; An array with a value for each matching key. 0; True; Int; The number of matching keys. Common uses for the expr argument; Put annotations on the top level under va:; expr='va = merge(va, table)'. Annotate only specific annotations from the table:; expr='va.annotations = select(table, toKeep1, toKeep2, toKeep3)'. The above is roughly equivalent to:; expr='''va.annotations.toKeep1 = table.toKeep1,; va.annotations.toKeep2 = table.toKeep2,; va.annotations.toKeep3 = table.toKeep3'''. Final",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:25935,depend,depends,25935,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['depend'],['depends']
Integrability,"t was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:5838,depend,dependency,5838,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependency']
Integrability,"t(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2218,integrat,integrate,2218,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['integrat'],['integrate']
Integrability,"t, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:4034,depend,dependencies,4034,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependencies']
Integrability,"t; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referenc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:33393,message,message,33393,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['message']
Integrability,"t=383.0408163265306,; multiple_p_value=0.03249975499062629,; n=2); }. Compute call statistics stratified by population group and case status:. >>> ann = ds.annotate_rows(call_stats=hl.agg.group_by(hl.struct(pop=ds.pop, is_case=ds.is_case),; ... hl.agg.call_stats(ds.GT, ds.alleles))). Parameters; ----------; group : :class:`.Expression` or :obj:`list` of :class:`.Expression`; Group to stratify the result by.; agg_expr : :class:`.Expression`; Aggregation or scan expression to compute per grouping. Returns; -------; :class:`.DictExpression`; Dictionary where the keys are `group` and the values are the result of computing; `agg_expr` for each unique value of `group`.; """""". return _agg_func.group_by(group, agg_expr). @typecheck(expr=expr_any); def _prev_nonnull(expr) -> ArrayExpression:; wrap = expr.dtype in {tint32, tint64, tfloat32, tfloat64, tbool, tcall}; if wrap:; expr = hl.or_missing(hl.is_defined(expr), hl.tuple([expr])); r = _agg_func('PrevNonnull', [expr], expr.dtype, []); if wrap:; r = r[0]; return r. [docs]@typecheck(f=func_spec(1, expr_any), array=expr_array()); def array_agg(f, array):; """"""Aggregate an array element-wise using a user-specified aggregation function. Examples; --------; Start with a range table with an array of random boolean values:. >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction ``True`` per element:. >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) # doctest: +SKIP_OUTPUT_CHECK; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; -----; This function requires that all values of `array` have the same length. If; two values have different lengths, then an exception will be thrown. The `f` argument should be a function taking one argument, an expression of; the element type of `array`, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; :func:`array_agg` is an array of e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:58472,wrap,wrap,58472,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['wrap'],['wrap']
Integrability,"table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals.; To do our filtering, we can filter to the rows of our matrix table where the; struct expression interval_table[mt.locus] is defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_bed(), MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to bia",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:6339,depend,dependencies,6339,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"tarting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) I",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:58431,message,messages,58431,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['message'],['messages']
Integrability,"te: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1857,depend,dependencies,1857,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['depend'],['dependencies']
Integrability,"the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4501,Depend,Dependencies,4501,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['Depend'],['Dependencies']
Integrability,"the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tm",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8489,message,messages,8489,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['message'],['messages']
Integrability,"the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:4596,message,messages,4596,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['message'],['messages']
Integrability,"ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""; assert isinstance(mapping, Aesthetic); return GGPlot(table, mapping). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:11023,interface,interface,11023,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,2,['interface'],['interface']
Integrability,"tter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:9471,depend,dependency,9471,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['depend'],['dependency']
Integrability,"type = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44720,depend,depending,44720,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['depend'],['depending']
Integrability,"ues Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Aggregation. View page source. Aggregation; For a full list of aggregators, see the aggregators; section of the API reference. Table Aggregations. Aggregate Over Rows Into A Local Value. One aggregation. description:; Compute the fraction of rows where SEX == 'M' in a table. code:; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. dependencies:; Table.aggregate(), aggregators.fraction(). Multiple aggregations. description:; Compute two aggregation statistics, the fraction of rows where; SEX == 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:1566,depend,dependencies,1566,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['depend'],['dependencies']
Integrability,"ug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21499,depend,dependent,21499,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependent']
Integrability,"um likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:10992,depend,dependent,10992,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['depend'],['dependent']
Integrability,"ur images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:12627,message,messages,12627,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['message'],['messages']
Integrability,"urns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:8165,depend,dependencies,8165,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"urns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with all values in the dictionary.; Examples; >>> hl.eval(d.values()) ; [33, 44, 43]. Returns:; ArrayExpression – All values in the dictionary. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:8582,interface,interface,8582,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['interface'],['interface']
Integrability,"us_intervals().; Hail supports implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table.; interval_table[mt.locus] joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals.; To do our filtering, we can filter to the rows of our matrix table where the; struct expression interval_table[mt.locus] is defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_bed(), MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). depen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:6080,depend,dependencies,6080,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['depend'],['dependencies']
Integrability,"utations into native code, and running them in parallel.; The result of the expression is computed only when it is needed. So z is; an expression representing the computation of x + y, but not the actual; value.; To peek at the value of this computation, there are two options:; eval(), which returns a Python value, and Expression.show(),; which prints a human-readable representation of an expression.; >>> hl.eval(z); 11; >>> z.show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 11 |; +--------+. Hail’s expressions are especially important for interacting with fields in; tables and matrix tables. Throughout Hail documentation and tutorials, you will; see code like this:; >>> ht2 = ht.annotate(C4 = ht.C3 + 3 * ht.C2 ** 2). This snippet of code is adding a field, C4, to a table, ht, and; returning the result as a new table, ht2. The code passed to the; Table.annotate() method is an expression that references the fields; C3 and C2 in ht.; Notice that 3 and 2 are not wrapped in constructor functions like; hl.int32(3). In the same way that Hail expressions can be combined together; via operations like addition and multiplication, they can also be combined with; Python objects.; For example, we can add a Python int to an Int32Expression.; >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an Int32Expression to an; int.; >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like numpy; or scipy.; Hail has many subclasses of Expression – one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail ArrayNumericExpression with; array():; >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. Expression objects keep track of their data type, which is; why we can see that a is of type array<int32> in the output above. An; expression’s type can also be ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/expressions.html:3151,wrap,wrapped,3151,docs/0.2/overview/expressions.html,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html,1,['wrap'],['wrapped']
Integrability,"utations, but is probably the most complex part of Hail. See the; pair of tutorials on the expression language to learn more!; Here, we can use query_variants to pull out 5 variants to see what; they look like. In [7]:. vds.query_variants('variants.take(5)'). Out[7]:. [Variant(contig=1, start=904165, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=909917, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=986963, ref=C, alts=[AltAllele(ref=C, alt=T)]),; Variant(contig=1, start=1563691, ref=T, alts=[AltAllele(ref=T, alt=G)]),; Variant(contig=1, start=1707740, ref=T, alts=[AltAllele(ref=T, alt=G)])]. There are often several ways to do something in Hail. Here are two ways; to peek at the first few sample IDs:. In [8]:. vds.query_samples('samples.take(5)'). Out[8]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. In [9]:. vds.sample_ids[:5]. Out[9]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. There’s a similar interface for looking at the genotypes in a dataset.; We use; query_genotypes; to look at the first few genotype calls. In [10]:. vds.query_genotypes('gs.take(5)'). Out[10]:. [Genotype(GT=0, AD=[4, 0], DP=4, GQ=12, PL=[0, 12, 194]),; Genotype(GT=1, AD=[4, 3], DP=7, GQ=85, PL=[85, 0, 109]),; Genotype(GT=0, AD=[1, 0], DP=1, GQ=3, PL=[0, 3, 42]),; Genotype(GT=0, AD=[14, 0], DP=14, GQ=42, PL=[0, 42, 533]),; Genotype(GT=0, AD=[12, 0], DP=12, GQ=36, PL=[0, 36, 420])]. Integrate sample annotations¶; Hail treats variant and sample annotations as first-class citizens.; Annotations are usually a critical part of any genetic study. Sample; annotations are where you’ll store information about sample phenotypes,; ancestry, sex, and covariates. Variant annotations can be used to store; information like gene membership and functional impact for use in QC or; analysis.; In this tutorial, we demonstrate how to take a text file and use it to; annotate the samples in a VDS.; iPython supports various cell “magics”. The %%",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:4868,interface,interface,4868,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['interface'],['interface']
Integrability,"val(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >>> hl.eval(s.startswith('The')); True. >>> hl.eval(s.startswith('the')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. strip()[source]; Returns a copy of the string with whitespace removed from the start; and end.; Examples; >>> s2 = hl.str(' once upon a time\n'); >>> hl.eval(s2.strip()); 'once upon a time'. Returns:; StringExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. translate(mapping)[source]; Translates characters of the string using mapping.; Examples; >>> string = hl.literal('ATTTGCA'); >>> hl.eval(string.translate({'T': 'U'})); 'AUUUGCA'. Parameters:; mapping (DictExpression) – Dictionary of character-character translations. Returns:; StringExpression. See also; replace(). upper()[source]; Returns a copy of the string, but with lower case letters converted; to upper case.; Examples; >>> hl.eval(s.upper()); 'THE QUICK BROWN FOX'. Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StringExpression.html:13793,interface,interface,13793,docs/0.2/hail.expr.StringExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html,1,['interface'],['interface']
Integrability,"vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, cancelling the batch...""); b.cancel(); backend._batch = None; raise. if status['n_succeeded'] != status['n_jobs']:; failing_job = next(iter(b.jobs('!success'))); failing_job = b.get_job(failing_job['job_id']); message = {'batch_status': status, 'job_status': failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:36024,message,message,36024,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['message'],['message']
Integrability,"whether; the variants are independent (:math:`R^2` < ``r2``) where ``r2`` is the maximum :math:`R^2` allowed.; :math:`R^2` is defined as the square of `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`{\\rho}_{x,y}` between the two genotype vectors :math:`{\\mathbf{x}}` and :math:`{\\mathbf{y}}`. .. math::. {\\rho}_{x,y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}. :py:meth:`.ld_prune` with default arguments is equivalent to ``plink --indep-pairwise 1000kb 1 0.2``.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK. Be sure to provide enough disk space per worker because :py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values. :param int memory_per_core: Total amount of memory available for each core in MB. If unsure, use the default value. :param int num_cores: The number of cores available. Equivalent to the total number of workers times the number of cores per worker. :return: Variant dataset filtered to those variants which remain after LD pruning.; :rtype: :py:class:`.VariantDataset`; """""". jvd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:94602,depend,depend,94602,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['depend'],['depend']
Integrability,"which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93489,depend,dependencies,93489,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['depend'],['dependencies']
Integrability,"width / 2; bar_width = bin_width; else:; raise ValueError(f""Histogram does not support position = {self.position}""). right_xs = left_xs + bin_width. trace_args = {; ""x"": x,; ""y"": df.y,; ""row"": facet_row,; ""col"": facet_col,; ""customdata"": list(zip(left_xs, right_xs)),; ""width"": bar_width,; ""hovertemplate"": ""Range: [%{customdata[0]:.3f}-%{customdata[1]:.3f})<br>""; ""Count: %{y}<br>""; ""<extra></extra>"",; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_bar(**trace_args). for idx, group_df in enumerate(grouped_data):; plot_group(group_df, idx). fig_so_far.update_layout(barmode=bar_position_plotly_to_gg(self.position)). def get_stat(self):; return StatBin(self.min_val, self.max_val, self.bins). [docs]def geom_histogram(; mapping=aes(),; *,; min_val=None,; max_val=None,; bins=None,; fill=None,; color=None,; alpha=None,; position='stack',; size=None,; ):; """"""Creates a histogram. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; min_val: `int` or `float`; Minimum value to include in histogram; max_val: `int` or `float`; Maximum value to include in histogram; bins: `int`; Number of bins to plot. 30 by default.; fill:; A single fill color for all bars of histogram, overrides ``fill`` aesthetic.; color:; A single outline color for all bars of histogram, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:12686,interface,interface,12686,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['interface'],['interface']
Integrability,"xporting missing genotypes without trailing; fields. Bug fixes. (#5306) Fix; ReferenceGenome.add_sequence causing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; opti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100696,interface,interface,100696,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['interface'],['interface']
Integrability,"xpression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1330,wrap,wraps,1330,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['wrap'],['wraps']
Integrability,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:1729,interface,interfaces,1729,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['interface'],['interfaces']
Integrability,"| NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79251,depend,dependent,79251,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['depend'],['dependent']
Integrability,"| str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. union(s)[source]; Return the union of the set and set s.; Examples; >>> hl.eval(s1.union(s2)); {1, 2, 3, 5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.SetExpression.html:14336,interface,interface,14336,docs/0.2/hail.expr.SetExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html,1,['interface'],['interface']
Integrability,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:9843,interface,interface,9843,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,6,['interface'],['interface']
Integrability,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:6280,interface,interface,6280,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['interface'],['interface']
Integrability,"; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. count(value)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.TupleExpression.html:2811,depend,dependencies,2811,docs/0.2/hail.expr.TupleExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html,1,['depend'],['dependencies']
Integrability,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:784,integrat,integration,784,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['integrat'],['integration']
Integrability,"﻿. . KinshipMatrix — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; KinshipMatrix. View page source. KinshipMatrix¶. class hail.KinshipMatrix(jkm)[source]¶; Represents a symmetric matrix encoding the relatedness of each pair of samples in the accompanying sample list.; The output formats are consistent with PLINK formats as created by the make-rel and make-grm commands and used by GCTA.; Attributes. key_schema; Returns the signature of the key indexing this matrix. Methods. __init__. export_gcta_grm; Export kinship matrix as .grm file. export_gcta_grm_bin; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. export_id_file; Export samples as .id file. export_rel; Export kinship matrix as .rel file. export_tsv; Export kinship matrix to tab-delimited text file with sample list as header. matrix; Gets the matrix backing this kinship matrix. sample_list; Gets the list of samples. export_gcta_grm(output)[source]¶; Export kinship matrix as .grm file. See PLINK formats. Parameters:output (str) – File path for output. export_gcta_grm_bin(output, opt_n_file=None)[source]¶; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See PLINK formats. Parameters:; output (str) – File path for output.; opt_n_file (str or None) – The file path to the N file. export_id_file(output)[source]¶; Export samples as .id file. See PLINK formats. Parameters:output (str) – File path for output. export_rel(output)[source]¶; Export kinship matrix as .rel file. See PLINK formats. Parameters:output (str) – File path for output. export_tsv(output)[source]¶; Export kinship matrix to tab-delimited text file",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KinshipMatrix.html:921,depend,depending,921,docs/0.1/hail.KinshipMatrix.html,https://hail.is,https://hail.is/docs/0.1/hail.KinshipMatrix.html,1,['depend'],['depending']
Integrability,"﻿. . Language Constructs — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Language Constructs. View page source. Language Constructs¶. va.foo = 5 + va.bar. Annotation expression. Bind variable va.foo to the result of evaluating 5 + va.bar. if (p) a else b. The value of the conditional is the value of a or b depending on p. If p is missing, the value of the conditional is missing.; if (5 % 2 == 0) 5 else 7; 7. if (5 > NA: Int) 5 else 7; NA: Int. let v1 = e1 and v2 = e2 and … and vn = en in b. Bind variables v1 through vn to result of evaluating the ei. The value of the let is the value of b. v1 is visible in e2 through en, etc.; let v1 = 5 and v2 = 7 and v3 = 2 in v1 * v2 * v3; 70. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/language_constructs.html:571,depend,depending,571,docs/0.1/language_constructs.html,https://hail.is,https://hail.is/docs/0.1/language_constructs.html,1,['depend'],['depending']
Integrability,"﻿. . Python API — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API. View page source. Python API¶; This is the API documentation for Hail, and provides detailed information; on the Python programming interface.; Classes. hail.HailContext; The main entry point for Hail functionality. hail.VariantDataset; Hail’s primary representation of genomic data, a matrix keyed by sample and variant. hail.KeyTable; Hail’s version of a SQL table where columns can be designated as keys. hail.KinshipMatrix; Represents a symmetric matrix encoding the relatedness of each pair of samples in the accompanying sample list. hail.LDMatrix; Represents a symmetric matrix encoding the Pearson correlation between each pair of variants in the accompanying variant list. Modules. representation; expr; utils. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/api.html:491,interface,interface,491,docs/0.1/api.html,https://hail.is,https://hail.is/docs/0.1/api.html,1,['interface'],['interface']
Integrability,"﻿. . hail.expr — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % cla",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/expr.html:577,message,message,577,docs/0.1/_modules/hail/expr.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html,4,['message'],['message']
Integrability,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/annotation.html:840,depend,dependencies,840,docs/0.2/guides/annotation.html,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html,2,['depend'],['dependencies']
Integrability,"﻿. Hail | ; BlockMatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:895,interface,interface,895,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['interface'],['interface']
Integrability,"﻿. Hail | ; Expression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:946,depend,dependencies,946,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['depend'],['dependencies']
Integrability,"﻿. Hail | ; Hail Query Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions; init(); asc(); desc(); stop(); spark_context(); tmp_dir(); default_reference(); get_reference(); set_global_seed(); reset_global_randomness(); citation(); version(). hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API. View page source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:1022,interface,interface,1022,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['interface'],['interface']
Integrability,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/macosx.html:1298,message,message,1298,docs/0.2/install/macosx.html,https://hail.is,https://hail.is/docs/0.2/install/macosx.html,1,['message'],['message']
Integrability,"﻿. Hail | ; hail.expr.aggregators.aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.aggregators.aggregators. Source code for hail.expr.aggregators.aggregators; import difflib; from functools import update_wrapper, wraps. import hail as hl; from hail import ir; from hail.expr import (; Aggregation,; ArrayExpression,; BooleanExpression,; DictExpression,; Expression,; ExpressionException,; Float64Expression,; Indices,; Int64Expression,; NDArrayNumericExpression,; NumericExpression,; SetExpression,; StringExpression,; StructExpression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; to_expr,; unify_all,; unify_types,; ); from hail.expr.expressions.typed_expressions import construct_variable; from hail.expr.functions import _quantile_from_cdf, _result_from_raw_cdf, float32, rbind; from hail.expr.types import (; hail_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import TypeChecker, func_spec, identity, nullable, oneof, sequenceof, typecheck, typecheck_method; from hail.utils import wrap_to_list; from hail.utils.java import Env. class AggregableChecker(TypeChecker):; def __init__(self, coercer):; self.coercer = coercer; super(AggregableChecker, self).__init__(). def expects(self):; return self.coercer.expects(). def format(self, arg):; return self.coercer.format(arg). def check(self, x, caller, param):; x = self.coercer.check(x, caller, param); if len(x._ir.search(lambda node: isinstance(nod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:582,wrap,wraps,582,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['wrap'],['wraps']
Integrability,"﻿. Hail | ; hailtop.fs.fs_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.fs.fs_utils. Source code for hailtop.fs.fs_utils; import io; from typing import List, Optional. from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration; from hailtop.utils.gcs_requester_pays import GCSRequesterPaysFSCache. from .router_fs import RouterFS; from .stat_result import FileListEntry. _fses = GCSRequesterPaysFSCache(fs_constructor=RouterFS). [docs]def open(; path: str,; mode: str = 'r',; buffer_size: int = 8192,; *,; requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None,; ) -> io.IOBase:; """"""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Writ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:695,Rout,RouterFS,695,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,2,['Rout'],['RouterFS']
Integrability,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/index.html:690,interface,interface,690,docs/0.2/linalg/index.html,https://hail.is,https://hail.is/docs/0.2/linalg/index.html,1,['interface'],['interface']
Integrability,"﻿. Hail | ; nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; nd. View page source. nd. NDArray Functions. Notes; This is a recently added, experimental module. We would love to hear what use cases you have for this as we expand this functionality.; As much as possible, we try to mimic the numpy array interface. array(input_array[, dtype]); Construct an NDArrayExpression. arange(start[, stop, step]); Returns a 1-dimensions ndarray of integers from start to stop by step. full(shape, value[, dtype]); Creates a hail NDArrayNumericExpression full of the specified value. zeros(shape[, dtype]); Creates a hail NDArrayNumericExpression full of zeros. ones(shape[, dtype]); Creates a hail NDArrayNumericExpression full of ones. diagonal(nd); Gets the diagonal of a 2 dimensional NDArray. solve(a, b[, no_crash]); Solve a linear system. solve_triangular(A, b[, lower, no_crash]); Solve a triangular linear system Ax = b for x. qr(nd[, mode]); Performs a QR decomposition. svd(nd[, full_matrices, compute_uv]); Performs a singular value decomposition. inv(nd); Performs a matrix inversion. concatenate(nds[, axis]); Join a sequence of arrays along an existing axis. hstack(arrs); Stack arrays in sequence horizontally (column wise). vstack(arrs); Stack arrays in sequence vertically (row wise). eye(N[, M, dtype]); Construct a 2-D NDArrayExpression with ones on the main diagonal and zeros elsewhere. identity(N[, dtype]); Constru",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:877,interface,interface,877,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['interface'],['interface']
Integrability,"﻿. Hail | Tutorial . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Import, prototype, scale; ; Perform analyses with distributed; dataframe-like; collections. import hail as hl; mt = hl.import_vcf('gs://bucket/path/myVCF.vcf.bgz'); mt.write('gs://bucket/path/dataset.mt', overwrite=True); # read matrix into env; mt = hl.read_matrix_table('gs://bucket/path/dataset.mt'); mt1 = hl.import_vcf('/path/to/my.vcf.bgz'); mt2 = hl.import_bgen('/path/to/my.bgen'); mt3 = hl.import_plink(bed='/path/to/my.bed',; bim='/path/to/my.bim',; fam='/path/to/my.fam'). Input Unification; ; Import formats such as bed, bgen, plink, or vcf, and manipulate them using a common dataframe-like interface. Genomic Dataframes; For large and dense structured matrices, like sequencing data, coordinate representations are; both; hard to work with and computationally inefficient. A core piece of Hail functionality is the; MatrixTable, a 2-dimensional generalization of Table. The MatrixTable makes it possible to; filter,; annotate, and aggregate symmetrically over rows and columns. # What is a MatrixTable?; mt.describe(widget=True). # filter to rare, loss-of-function variants; mt = mt.filter_rows(mt.variant_qc.AF[1] < 0.005); mt = mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's declarative syntax. imputed_sex =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/tutorial.html:715,interface,interface,715,tutorial.html,https://hail.is,https://hail.is/tutorial.html,1,['interface'],['interface']
Modifiability," 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:1003,variab,variable,1003,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],"['variable', 'variables']"
Modifiability," # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84346,variab,variables,84346,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability," (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:13195,variab,variable,13195,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability," (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_type, indices, aggregations). if b_ndim_orig == 1:; if no_crash:; result = hl.struct(solution=result.solution.reshape((-1)), failed=result.failed); else:; result = result.reshape((-1)); return result. [docs]@typecheck(A=expr_ndarray(), b=expr_ndarray(), lower=expr_bool, no_crash=bool); def solve_triangular(A, b, lower=False, no_crash=False):; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpression`, (N, N); Triangular coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables.; lower : `bool`:; If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the triangular system Ax = B. Shape is same as shape of B. """"""; nd_dep_ndim_orig = b.ndim; A, b = solve_helper(A, b, nd_dep_ndim_orig). indices, aggregations = unify_all(A, b). if no_crash:; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:7807,variab,variables,7807,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['variab'],['variables']
Modifiability," (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:19211,config,config,19211,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['config'],['config']
Modifiability," (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=tab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:55652,variab,variable-length,55652,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability," (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be included in the; calculation.; In the case that there are no non-missing pairs, the result will be missing. See also; linreg(). Parameters:. x (Expression of type tfloat64); y (Expression of type tfloat64). Returns:; Float64Expression. hail.expr.aggregators.group_by(group, agg_expr)[source]; Compute aggregation statistics stratified ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:29676,variab,variable,29676,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['variab'],['variable']
Modifiability," ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_generation_tup, idx):; prev_size = prev_generation_tup[1]; n_new = hl.int32(hl.floor(prev_size * generation_size_multiplier)); new_generation = hl.range(n_new).map(; lambda i: hl.struct(; s=hl.str('generation_') + hl.str(idx + 1) + hl.str('_idx_') + hl.str(i),; original=hl.missing('str'),; mother=hl.rand_int32(0, prev_size),; father=hl.rand_int32(0, prev_size),; ); ); return (new_generation, (prev_size + n_new) if keep_founders else n_new). ht = ht.annotate_globals(; generations=hl.range(n_rounds).scan(; lambda prev, idx: make_new_generation(prev, idx), (ht.generation_0, hl.len(ht.generation_0)); ); ). def simulate_mating_calls(prev_generation_calls, new_generation):; new_samples = new_generation.map(; lambda samp: hl.call(; prev_generation_calls[samp.mother][hl.rand_int32(0, 2)],; prev_generation_calls[samp.father][hl.rand_int32(0, 2)],; ); ); if keep_founders:; return prev_generation_calls.extend(new_samples); else:; return new_samples. ht = ht.annotate(; __new_entries=hl.fold(; lambda prev_calls, generation_metadata: simulate_mating_calls(prev_calls, generation_metadata[0]),; ht.__entries.GT,; ht.generations[1:],; ).map(lambda gt: hl.struct(GT=gt)); ); ht = ht.annotate_globals(; __new_cols=ht.generations.flatmap(lambda x: x[0]) if keep_founders else ht.generations[-1][0]; ); ht = ht.drop('__entries', '__cols', 'generation_0', 'generations'); return ht._unlocalize_entries('__new_entries', '__new_cols', list('s')). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html:2906,extend,extend,2906,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,2,['extend'],['extend']
Modifiability," - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+=========+========+============+===============+; | 1 | HomVar | HomVar | Het | Auto | Dad, Mom, Kid |; +------+---------+---------+--------+------------+---------------+; | 2 | HomRef | HomRef | Het | Auto | Dad",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7374,extend,extending,7374,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['extend'],['extending']
Modifiability," ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cannot be interpreted as an expression. Parameters:; t (str or HailType) – Type of the missing expression. Returns:; Expression – A missing expression of type t. hail.expr.functions.null(t)[source]; Deprecated in favor of missing().; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.null(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.null('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cann",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:6723,variab,variable-length,6723,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable-length']
Modifiability," 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5419,polymorphi,polymorphism,5419,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability," 1], mu=-3, sigma=0).value); 0.516439358616939; >>> hl.eval(hl.pgenchisq(10 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0; >>> hl.eval(hl.pgenchisq(40 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; -----. We follow Wikipedia's notational conventions. Some texts refer to the weight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Param",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:68320,variab,variables,68320,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variables']
Modifiability," :class:`.Expression` of type :py:data:`.tfloat64`; df1 : float or :class:`.Expression` of type :py:data:`.tfloat64`; Parameter of the F-distribution; df2 : float or :class:`.Expression` of type :py:data:`.tfloat64`; Parameter of the F-distribution; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pF"", tfloat64, x, df1, df2, lower_tail, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def ppois(x, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a Poisson distribution. Examples; --------. >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is a; Poisson random variable with rate parameter `lamb`. If `lower_tail` is false,; returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; lamb : float or :class:`.Expression` of type :py:data:`.tfloat64`; Rate parameter of Poisson distribution.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""ppois"", tfloat64, x, lamb, lower_tail, log_p). [docs]@typecheck(p=expr_float64, df=expr_float64, ncp=nullable(expr_float64), lower_tail=expr_bool, log_p=expr_bool); def qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False) -> Float64Expression:; """"""The quantile function of a chi-squared distribution with `df` degrees of; freedom, inver",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:76365,variab,variable,76365,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability," Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Relatedness. View page source. Relatedness; The relatedness of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:1010,inherit,inherited,1010,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,2,['inherit'],['inherited']
Modifiability," Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, step: Int): Array[Int]. Generate an Array with values in the interval [start, stop) in increments of step.; let r = range(0, 5, 2) in r; result: [0, 2, 4]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but not including this number.; step (Int) – Difference between each number in the sequence. range(start: Int, stop: Int): Array[Int]. Generate an Array with values in the interval [start, stop).; let r = range(5, 8) in r; result: [5, 6, 7]. Arguments. start (Int) – Starting number of the sequence.; stop (Int) – Generate numbers up to, but not including this number. range(stop: Int): Array[Int]. Generate an Array with values in the interval [0, stop).; let r = range(3) in r; result",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:16059,variab,variable,16059,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1071,config,configuration,1071,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,4,"['config', 'variab']","['configuration', 'variable', 'variables']"
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_C,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:277,Config,Configuration,277,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:273,Config,Configuration,273,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:272,Config,Configuration,272,docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cult,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:268,Config,Configuration,268,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:264,Config,Configuration,264,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_f,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html:262,Config,Configuration,262,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,3,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibro,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html:258,Config,Configuration,258,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,2,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibrob,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html:257,Config,Configuration,257,docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibrobla,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html:255,Config,Configuration,255,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,2,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblas,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html:254,Config,Configuration,254,docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblast,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html:253,Config,Configuration,253,docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html:251,Config,Configuration,251,docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_a,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html:250,Config,Configuration,250,docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,2,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_al,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html:249,Config,Configuration,249,docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,2,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html:248,Config,Configuration,248,docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,4,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html:247,Config,Configuration,247,docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,2,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_s,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html:246,Config,Configuration,246,docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_ge,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html:241,Config,Configuration,241,docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html:239,Config,Configuration,239,docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html:238,Config,Configuration,238,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_ass,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html:235,Config,Configuration,235,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,6,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_asso,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html:234,Config,Configuration,234,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_assoc,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html:233,Config,Configuration,233,docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html:232,Config,Configuration,232,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associat,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html:230,Config,Configuration,230,docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,1,['Config'],['Configuration']
Modifiability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html:226,Config,Configuration,226,docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,7,['Config'],['Configuration']
Modifiability," ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.contig_length(contig, reference_genome='default')[source]; Returns the length of contig in reference_genome.; Examples; >>> hl.eval(hl.contig_length('5', reference_genome='GRCh37')); 180915260. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; Int32Expression. hail.expr.functions.allele_type(ref, alt)[source]; Returns the type of the polymorphism as a string.; Examples; >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes. The possible return values are:; ""SNP""; ""MNP""; ""Insertion""; ""Deletion""; ""Complex""; ""Star""; ""Symbolic""; ""Unknown"". Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; StringExpression. hail.expr.functions.numeric_allele_type(ref, alt)[source]; Returns the type of the polymorphism as an integer. The value returned; is the integer value of AlleleType representing that kind of; polymorphism.; Examples; >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; The values of AlleleType are not stable and thus should not be; relied upon across hail versions. hail.expr.functions.pl_dosage(pl)[source]; Return expected genotype dosage from array of Phred-scaled genotype; likelihoods with uniform prior. Only defined for bi-allelic variants. The; pl argument must be length 3.; For a PL array [a, b, c], let:. \[a^\prime = 10^{-a/10} \\; b^\prime = 10^{-b/10} \\; c^\prime = 10^{-c/10} \\\]; The genotype dosage is given by:. \[\frac{b^\prime + 2 c^\prime}; {a^\prime + b^\prime +c ^\prime}\]; Examples; >>> hl.eval(hl.pl_dosage([5, 10, 100])); 0.24025307377482674. Parameters:; pl (ArrayNumericExpression of type tint32) – Length 3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hail.expr.functions.gp_dosage(gp)[source]; Return expected genotype dosage from array of genoty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:17292,polymorphi,polymorphism,17292,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability," Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: Use the linear_regression_rows() method for each group; >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.group_by() and aggregators.linreg() aggregators; >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). dependencies:; linear_regression_rows(), aggregators.group_by(), aggregators.linreg(). understanding:. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the linear_regression_rows() method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression stati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:12218,variab,variable,12218,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['variab'],['variable']
Modifiability," Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plotting With hail.ggplot Overview. View page source. Plotting With hail.ggplot Overview. Warning; Plotting functionality is in early stages and is experimental. The hl.ggplot module is designed based on R’s tidyverse ggplot2 library. This module provides a subset of ggplot2’s; functionality to allow users to generate plots in much the same way they would in ggplot2.; This module is intended to be a new, more flexible way of plotting compared to the hl.plot module. This module; currently uses plotly to generate plots, as opposed to hl.plot, which uses bokeh. Core functions. ggplot; Create the initial plot object. aes; Create an aesthetic mapping. coord_cartesian; Set the boundaries of the plot. hail.ggplot.ggplot(table, mapping={})[source]; Create the initial plot object.; This function is the beginning of all plots using the hail.ggplot interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result.; Examples; Create a y = x^2 scatter plot; >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters:. table – The table containing the data to plot.; mapping – Default list of aesthetic mappings from table data to plot attributes. Returns:; GGPlot. hail.ggplot.aes(**kwargs)[source]; Create an aesthetic mapp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:1080,flexible,flexible,1080,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['flexible'],['flexible']
Modifiability," The type of the resulting struct is the same as the type of; col_value(). Returns:; Expression. index_entries(row_exprs, col_exprs)[source]; Expose the entries as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2.index_entries(dataset.row_key, dataset.col_key).GQ). Or equivalently:; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Parameters:. row_exprs (tuple of Expression) – Row index expressions.; col_exprs (tuple of Expression) – Column index expressions. Notes; The type of the resulting struct is the same as the type of; entry(). Note; There is a shorthand syntax for MatrixTable.index_entries() using; square brackets (the Python __getitem__ syntax). This syntax is; preferred.; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Returns:; StructExpression. index_globals()[source]; Return this matrix table’s global variables for use in another; expression context.; Examples; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns:; StructExpression. index_rows(*exprs, all_matches=False)[source]; Expose the row values as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Notes; index_rows(exprs) is equivalent to rows().index(exprs); or rows()[exprs].; The type of the resulting struct is the same as the type of; row_value().",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:42935,variab,variables,42935,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variables']
Modifiability," This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:13406,extend,extended,13406,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['extend'],['extended']
Modifiability," UCSC BED file as a Table.; Examples; The file formats are; $ cat data/file1.bed; track name=""BedTest""; 20 1 14000000; 20 17000000 18000000; ... $ cat file2.bed; track name=""BedTest""; 20 1 14000000 cnv1; 20 17000000 18000000 cnv2; ... Add the row field cnv_region indicating inclusion in; at least one interval of the three-column BED file:; >>> bed = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(cnv_region = hl.is_defined(bed[dataset.locus])). Add a row field cnv_id with the value given by the; fourth column of a BED file:; >>> bed = hl.import_bed('data/file2.bed'); >>> result = dataset.annotate_rows(cnv_id = bed[dataset.locus].target). Notes; The table produced by this method has one of two possible structures. If; the .bed file has only three fields (chrom, chromStart, and; chromEnd), then the produced table has only one column:. interval (tinterval) - Row key. Genomic interval. If; reference_genome is defined, the point type of the interval will be; tlocus parameterized by the reference_genome. Otherwise,; the point type is a tstruct with two fields: contig with; type tstr and position with type tint32. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. interval (tinterval) - Row key. Genomic interval. Same schema as above.; target (tstr) - Fourth column of .bed file. UCSC bed files can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; Intervals in UCSC BED files are 0-indexed and half open.; The line “5 100 105” correpsonds to the interval [5:101-5:106) in Hail’s; 1-indexed notation. Details; here. Parameters:. path (str) – Path to .bed file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recodi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:6133,parameteriz,parameterized,6133,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['parameteriz'],['parameterized']
Modifiability," VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant annotation that is a String representing the chromosome and start position. va.altName = v.contig + "":"" + v.start. Add a new variant annotation that splits a comma-separated string with gene names and keeps the first element of the resulting array. va.geneName = va.geneNames.split("","")[0]. Add a new variant annotation that is the log of an existing annotation. va.logIntensity = log(va.intensity). Add a new global annotation computed from existing global annotations. global.callRate = global.nCalled / global.nGenotypes. Variant Annotation Comput",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:5842,variab,variables,5842,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variables']
Modifiability," X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; This function is always followed by GroupedTable.aggregate(). Follow the; link for documentation on the aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, x:; >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:; >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); .",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:32017,variab,variable-length,32017,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability," [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud paramete",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:10059,config,config,10059,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,6,['config'],"['config', 'configuration']"
Modifiability," \frac{f(x_i)}{f'(x_i)}`; until the difference between :math:`x_{i}` and :math:`x_{i+1}` falls below; our convergence threshold:. >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; -------; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters; ----------; f : function ( (marker, \*args) -> :class:`.Expression`; Function of one callable marker, denoting where the recursive call (or calls) is located,; and many `args`, the loop variables.; typ : :class:`str` or :class:`.HailType`; Type the loop returns.; args : variable-length args of :class:`.Expression`; Expressions to initialize the loop values.; Returns; -------; :class:`.Expression`; Result of the loop with `args` as initial loop values.; """""". loop_name = Env.get_uid(). def contains_recursive_call(non_recursive):; if isinstance(non_recursive, ir.Recur) and non_recursive.name == loop_name:; return True; return any([contains_recursive_call(c) for c in non_recursive.children]). def check_tail_recursive(loop_ir):; if isinstance(loop_ir, ir.If):; if contains_recursive_call(loop_ir.cond):; raise TypeError(""branch condition can't contain recursive call!""); check_tail_recursive(loop_ir.cnsq); check_tail_recursive(loop_ir.altr); elif isinstance(loop_ir, ir.Let):; if contains_recursive_call(loop_ir.value):; raise TypeError(""bound value used in other expression can't contain recursive call!""); check_tail_recursive(loop_ir.body); elif isinstance(loop_ir, ir.TailLoop):; if any(contains_recursive_call(x) for n, x in loop_ir.params):; raise TypeError(""parameters p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:3900,variab,variable-length,3900,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,2,['variab'],['variable-length']
Modifiability," acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints “hello”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1785,variab,variable,1785,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,6,"['config', 'variab']","['config', 'variable']"
Modifiability," all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:7953,inherit,inherits,7953,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['inherit'],['inherits']
Modifiability," alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute an insertion or deletion. Examples; --------. >>> hl.eval(hl.is_indel('ATT', 'A')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return hl.bind(lambda t: (t == AlleleType.INSERTION) | (t == AlleleType.DELETION), numeric_allele_type(ref, alt)). [docs]@typecheck(ref=expr_str, alt=expr_str); def is_star(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute an upstream deletion. Examples; --------. >>> hl.eval(hl.is_star('A', '*')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return numeric_allele_type(ref, alt) == AlleleType.STAR. [docs]@typecheck(ref=expr_str, alt=expr_str); def is_complex(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute a complex polymorphism. Examples; --------. >>> hl.eval(hl.is_complex('ATT', 'GCAC')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return numeric_allele_type(ref, alt) == AlleleType.COMPLEX. [docs]@typecheck(ref=expr_str, alt=expr_str); def is_strand_ambiguous(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles are strand ambiguous. Strand ambiguous allele pairs are ``A/T``, ``T/A``,; ``C/G``, and ``G/C`` where the first allele is `ref`; and the second allele is `alt`. Examples; --------. >>> hl.eval(hl.is_strand_ambiguous('A', 'T')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; alleles = hl.literal({('A', 'T'), ('T', 'A'), ('G', 'C",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:101363,polymorphi,polymorphism,101363,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['polymorphi'],['polymorphism']
Modifiability," always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""; if Env.hc()._warn_entries_order and len(self.col_key) > 0:; warning(; ""entries(): Resulting entries table is sorted by '(row_key, col_key)'.""; ""\n To preserve row-major matrix table order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_entries_order = False. return Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(ir.MatrixRowsTable(self._mir)), self.globals.dtype). [docs] def index_rows(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:. >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:90339,variab,variables,90339,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['variab'],['variables']
Modifiability," approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:14938,variab,variable,14938,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability," at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:8094,variab,variable,8094,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability," being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, output; + '.bgen' will be a directory containing many BGEN files. In either case, the SAMPLE file is; written to output + '.sample'. For",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:51737,config,config,51737,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['config'],['config']
Modifiability," called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3946,polymorphi,polymorphism,3946,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability," calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean samp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:5041,variab,variable,5041,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability," column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a)[source]; Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; If f returns False for every element, then the result is missing. Parameters:; f (function ( (a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:7920,extend,extend,7920,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['extend'],['extend']
Modifiability," dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27753,variab,variables,27753,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability," def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:3540,polymorphi,polymorphism,3540,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability," exists for annotation dataset: {self.name}. Rows have been'; f' annotated with version {indexed_values[1]}.'; ); return indexed_values[0]. [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:9989,config,configuration,9989,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['config'],['configuration']
Modifiability," field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified column fields.; """"""; caller = 'MatrixTable.select_cols'; new_col = get_select_exprs(caller, exprs, named_exprs, self._col_indices, self._col); return self._select_cols(caller, new_col). [docs] def select_entries(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing entry fields or create new fields by name, dropping the rest. Examples; --------; Drop all entry fields aside from `GT`:. >>> dataset_result = dataset.select_entries(dataset.GT). Notes; -----; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified entry fields.; """"""; caller = 'MatrixTable.select_entries'; new_entry = get_select_exprs(caller, exprs, named_exprs, self._entry_indices, self._entry); return self._select_entries(caller, new_entry). [docs] @typecheck_method(exprs=oneof(str, Expression)); def drop(self, *exprs) -> 'MatrixTable':; """"""Drop fields. Examples; --------. Drop fields `PL` (an entry field), `info` (a row field), and `pheno` (a column; field): using strings:. >>> dataset_result = dataset.drop('PL', 'info', 'pheno'). Drop fields `PL` (an entry field), `info` (a row field), and `pheno` (a column; field): using field references:. >>> dataset_result = dataset.drop(dataset.PL, dataset.info, dataset.pheno). Drop a list of fie",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:41615,variab,variable-length,41615,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['variab'],['variable-length']
Modifiability," for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:2351,variab,variable,2351,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],"['variable', 'variables']"
Modifiability," for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType) – Value type. See also; DictExpression, dict(), Collection functions. class hail.expr.types.tstruct(**field_types)[source]; Hail type for structured groups of heterogeneous fields.; In Python, these are represented as Struct.; Hail’s tstruct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and MatrixTable; is a struct:. Table.row(); Table.globals(); MatrixTable.row(); MatrixTable.col(); MatrixTable.entry(); MatrixTable.globals(). Structs appear below the top-level component types as well. Consider the following join:; >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to table1 called table2_fields. In ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:9486,parameteriz,parameterize,9486,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['parameteriz'],['parameterize']
Modifiability," highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:6843,config,config,6843,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['config']
Modifiability," ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_rows_with_themselves(ht.Qtys[i].T); ),; ). ht = setup_globals(ht). def process_block(block):; rows_in_block = hl.len(block). # Processes one block group based on given idx. Returns a single struct.; def process_y_group(idx):; if weights ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22241,variab,variables,22241,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],['variables']
Modifiability," if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38537,variab,variables,38537,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['variab'],['variables']
Modifiability," in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Explodes a row field of type array or set, copying the entire row for each element. filter_cols; Filter columns of the matrix. filter_entries; Filter entries of the matrix. filter_rows; Filter rows of the matrix. from_parts; Create a MatrixTable from its component parts. from_rows_table; Construct matrix table with no columns from a table. globals_table; Returns a table with a single row with the globals of the matrix table. group_cols_by; Group columns, used with GroupedMatrixTable.aggregate(). group_rows_by; Group rows, used with GroupedMatrixTable.aggregate(). head; Subset matrix to first n_rows rows and n_cols cols. index_cols; Expose the column values as if looked up in a dictionary, indexing with exprs. index_entries; Expose the entries as if looked up in a dictionary, indexing with exprs. index_globals; Return this matrix table's global variables for use in another expression context. index_rows; Expose the row values as if looked up in a dictionary, indexing with exprs. key_cols_by; Key columns by a new set of fields. key_rows_by; Key rows by a new set of fields. localize_entries; Convert the matrix table to a table with entries localized as an array of structs. make_table; Make a table from a matrix table with one field per sample. n_partitions; Number of partitions. naive_coalesce; Naively decrease the number of partitions. persist; Persist this table in memory or on disk. rename; Rename fields of a matrix table. repartition; Change the number of partitions. rows; Returns a table with all row fields in the matrix. sample_cols; Downsample the matrix table by keeping each column with probability p. sample_rows; Downsample the matrix table by keeping each row with probability p. select_cols; Select existing column fields or create new fields by name, dropping the rest. select_entries; Select existing entry fields or ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:5378,variab,variables,5378,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variables']
Modifiability," install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/dis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2899,variab,variables,2899,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['variab'],['variables']
Modifiability," is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argumen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:6802,config,configuration,6802,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configuration']
Modifiability," key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175610,plugin,plugin,175610,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['plugin'],['plugin']
Modifiability," list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:3570,variab,variable,3570,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,2,['variab'],"['variable', 'variables']"
Modifiability," log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:5425,config,configuration,5425,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configuration']
Modifiability," math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74404,variab,variables,74404,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability," num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. num_mismatch()[source]¶; Returns the number of mismatched bases in this alternate allele.; Fails if the ref and alt alleles are not the same length. Return type:int. ref¶; Reference allele. Return type:str. stripped_snp()[source]¶; Returns the one-character reduced SNP.; Fails if called on an alternate allele that is not a SNP. Return type:str, str. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:2841,polymorphi,polymorphism,2841,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,2,['polymorphi'],['polymorphism']
Modifiability," of the slice (not included in result). [i:]: Array[T]. Returns a slice of the array from the i*th* element (0-indexed) to the; end. Negative indices are interpreted as offsets from the end of the array.; let a = [0, 2, 4, 6, 8, 10] in a[3:]; result: [6, 8, 10]. let a = [0, 2, 4, 6, 8, 10] in a[-5:]; result: [2, 4, 6, 8, 10]. Arguments. i (Int) – Starting index of the slice. [i]: T. Returns the i*th* element (0-indexed) of the array, or throws an exception if i is an invalid index.; let a = [0, 2, 4, 6, 8, 10] in a[2]; result: 4. Arguments. i (Int) – Index of the element to return. append(a: T): Array[T] – Returns the result of adding the element a to the end of this Array. exists(expr: T => Boolean): Boolean. Returns a boolean which is true if any element in the array satisfies the condition given by expr. false otherwise.; let a = [1, 2, 3, 4, 5, 6] in a.exists(e => e > 4); result: true. Arguments. expr (T => Boolean) – Lambda expression. extend(a: Array[T]): Array[T] – Returns the concatenation of this Array followed by Array a. filter(expr: T => Boolean): Array[T]. Returns a new array subsetted to the elements where expr evaluates to true.; let a = [1, 4, 5, 6, 10] in a.filter(e => e % 2 == 0); result: [4, 6, 10]. Arguments. expr (T => Boolean) – Lambda expression. find(expr: T => Boolean): T. Returns the first non-missing element of the array for which expr is true. If no element satisfies the predicate, find returns NA.; let a = [""cat"", ""dog"", ""rabbit""] in a.find(e => 'bb' ~ e); result: ""rabbit"". Arguments. expr (T => Boolean) – Lambda expression. flatMap(expr: T => Array[U]): Array[U]. Returns a new array by applying a function to each subarray and concatenating the resulting arrays.; let a = [[1, 2, 3], [4, 5], [6]] in a.flatMap(e => e + 1); result: [2, 3, 4, 5, 6, 7]. Arguments. expr (T => Array[U]) – Lambda expression. forall(expr: T => Boolean): Boolean. Returns a boolean which is true if all elements in the array satisfies the condition given by expr and ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/types.html:27483,extend,extend,27483,docs/0.1/types.html,https://hail.is,https://hail.is/docs/0.1/types.html,1,['extend'],['extend']
Modifiability," operand by the right (modulus). 7 % 2; 1. 10 % 4; 2. // – Floor division - division that results into whole number adjusted to the left in the number line. 7 // 2; 3. -7 // 2; -4. Array[Numeric]¶; If one of the two operands is a scalar, the operation will be applied to each element of the Array. If both operands are Arrays, the operation will be applied positionally. This will fail if the array dimensions do not match. + – Add two operands. [1, 2, 3] + [1, 1, 1]; [2, 3, 4]. [2, 0, 1] + 5; [7, 5, 6]. - – Subtract right operand from the left. [1, 2, 3] - [1, 1, 1]; [0, 1, 2]. [2, 0, 1] - 5; [-3, -5, -4]. 3 - [2, 4, 5]; [1, -1, -2]. * – Multiply two operands. [1, 2, 3] * [1, 1, 1]; [1, 2, 3]. [2, 0, 1] * 5; [10, 0, 5]. / – Divide left operand by the right one. Always results in a Double. [1, 2, 3] / [1, 4, 9]; [1.0, 0.5, 0.333]. [2, 0, 1] / 5; [0.4, 0.0, 0.2]. 5 / [2, 4, 1]; [2.5, 1.25, 5.0]. Comparison¶. == – True if the left operand is equal to the right operand. [1, 2, 3] == [1, 2, 3]; true. != – True if the left operand is not equal to the right operand. [1, 2, 3] != [4, 5, 6]; true. < – True if the left operand is less than the right operand. 5 < 3; False. <= – True if the left operand is less than or equal to the right operand. 3 <= 5; True. > – True if the left operand is greater than the right operand. 7 > 2; True. >= – True if the left operand is greater than or equal to the right operand. 3 >= 9; False. ~ – True if a regular expression pattern matches the target string. ""1KG"" ~ ""Cohort_1KG_NA12878""; True. Logical¶. && – True if both the left and right operands are true. (5 >= 3) && (2 < 10); True. || – True if at least one operand is true. (5 <= 3) || (2 < 10); True. ! – Negates a boolean variable. Returns false if the variable is true and true if the variable is false. !(5 >= 3); False. String¶. + – Concatenate two strings together. ""a"" + ""b""; ""ab"". Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/operators.html:2434,variab,variable,2434,docs/0.1/operators.html,https://hail.is,https://hail.is/docs/0.1/operators.html,3,['variab'],['variable']
Modifiability," or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, ke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:48202,inherit,inheritance,48202,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['inherit'],['inheritance']
Modifiability," or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1790,polymorphi,polymorphism,1790,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability," package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:81976,variab,variable,81976,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variable']
Modifiability," parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:5990,variab,variables,5990,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variables']
Modifiability," parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. Constructors. bool(x); Convert to a Boolean expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. interval(s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:3507,variab,variable,3507,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['variab'],['variable']
Modifiability," r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74490,variab,variables,74490,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability," ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:55808,variab,variable-length,55808,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability," reference and one alternate allele. :rtype: bool; """""". return self._jrep.isHetRef(). [docs] def is_not_called(self):; """"""True if the genotype call is missing. :rtype: bool; """""". return self._jrep.isNotCalled(). [docs] def is_called(self):; """"""True if the genotype call is non-missing. :rtype: bool; """""". return self._jrep.isCalled(). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return from_option(self._jrep.nNonRefAlleles()). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; ho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:5360,variab,variables,5360,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['variab'],['variables']
Modifiability," regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a config",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1521,config,config,1521,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,4,['config'],['config']
Modifiability," representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1412,polymorphi,polymorphism,1412,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability," sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:3662,config,config,3662,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['config']
Modifiability," source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2145,config,configure,2145,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configure']
Modifiability," the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. .. note::. Using the **FILTER** field:. The information in the FILTER field of a VCF is contained in the; ``filters`` row field. This annotation is a ``set<str>`` and can be; queried for filter membership with expressions like; ``ds.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged; as ""PASS"" will have no filters applied; for these variants,; ``hl.len(ds.filters)`` is ``0``. Thus, filtering to PASS variants; can be done with :meth:`.MatrixTable.filter_rows` as follows:. >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome (CHROM field) and position (POS field). If `reference_genome`; is defined, the type will be :class:`.tlocus` parameterized by; `reference_genome`. Otherwise, the type will be a :class:`.tstruct` with; two fields: `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (REF field) is; the first element in the array and the alternate alleles (ALT field) are; the subsequent elements.; - `filters` (:class:`.tset` of :py:data:`.tstr`) -- Set containing all filters applied to a; variant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:100358,parameteriz,parameterized,100358,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['parameteriz'],['parameterized']
Modifiability," the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper); fx, fy = xi, yi; new_y[i] = fy; keep[i] = True. min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True). # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low. while True:; lower_slope = slope_from_fixe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:15394,variab,variables,15394,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['variab'],['variables']
Modifiability," this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1990,polymorphi,polymorphism,1990,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,1,['polymorphi'],['polymorphism']
Modifiability," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:43228,variab,variables,43228,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,2,['variab'],"['variable-length', 'variables']"
Modifiability," to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:222770,plugin,plugin,222770,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,3,['plugin'],['plugin']
Modifiability," true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:14033,variab,variable,14033,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability," u'GENE1',; u'isoform': u'GENE1.1'},; {u'canonical': True,; u'consequence': u'LOF',; u'gene': u'GENE1',; u'isoform': u'GENE1.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.1'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.3'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.1'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.2'}]},; Struct{; info: Struct{; AC: Array[Int],; AN: Int,; AF: Array[Double]; },; transcripts: Array[Struct{; gene: String,; isoform: String,; canonical: Boolean,; consequence: String; }]; }). You’ll rarely need to construct a Variant or Genotype object; inside the Hail expression language. More commonly, these objects will; be provided to you as variables. In the remainder of this notebook, we; will explore how to to manipulate the demo variables. In the next; notebook, we start using the expression langauge to annotate and filter; a dataset.; First, a short demonstration of some of the methods accessible on; Variant and Genotype objects:. In [52]:. hc.eval_expr_typed('v'). Out[52]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [53]:. hc.eval_expr_typed('v.contig'). Out[53]:. (u'16', String). In [54]:. hc.eval_expr_typed('v.start'). Out[54]:. (19200405, Int). In [55]:. hc.eval_expr_typed('v.ref'). Out[55]:. (u'C', String). In [56]:. hc.eval_expr_typed('v.altAlleles'). Out[56]:. ([AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)], Array[AltAllele]). In [57]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isSNP())'). Out[57]:. ([True, False], Array[Boolean]). In [58]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isInsertion())'). Out[58]:. ([False, True], Array[Boolean]). In [59]:. hc.eval_expr_typed('g'). Out",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:15653,variab,variables,15653,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['variab'],['variables']
Modifiability," url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:11806,config,config,11806,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['config'],['config']
Modifiability," v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype: int; """"""; return self._start. @property; def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:2858,polymorphi,polymorphism,2858,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability," variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified fields.; """"""; row = get_select_exprs('Table.select', exprs, named_exprs, self._row_indices, self._row). return self._select('Table.select', row). [docs] @typecheck_method(exprs=oneof(str, Expression)); def drop(self, *exprs) -> 'Table':; """"""Drop fields from the table. Examples; --------. Drop fields `C1` and `C2` using strings:. >>> table_result = table1.drop('C1', 'C2'). Drop fields `C1` and `C2` using field references:. >>> table_result = table1.drop(table1.C1, table1.C2). Drop a list of fields:. >>> fields_to_drop = ['C1', 'C2']; >>> table_result = table1.drop(*fields_to_drop). Notes; -----. This method can be used to drop global or row-indexed fields. The arguments; can be either strings (``'field'``)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:50327,variab,variable-length,50327,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability," vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of elements that fall in each bin.; nLess (Long) – Number of elements less than the minimum bin; nGreater (Long) – Number of elements greater than the maximum bin. Compute frequency distributions of numeric parameters.; Examples; Compute GQ-distributions per variant:; >>> vds_result = vds.annotate_variants_expr('va.gqHist = gs.map(g => g.gq).hist(0, 100, 20)'). Compute global GQ-distribution:; >>> gq_hist = vds.query_genotypes('gs.map(g => g.gq).hist(0, 100, 100)'). Notes. The start, end, and bins params are no-scope parameters, which means that while computations like 100 / 4 are acceptable, variable references like global.nBins are not.; Bin size is calculated from (end - start) / bins; (bins + 1) breakpoints are generated from the range (start to end by binsize); Each bin is left-inclusive, right-exclusive except the last bin, which includes the maximum value. This means that if there are N total bins, there will be N + 1 elements in binEdges. For the invocation hist(0, 3, 3), binEdges would be [0, 1, 2, 3] where the bins are [0, 1), [1, 2), [2, 3]. Arguments. start (Double) – Starting point of first bin; end (Double) – End point of last bin; bins (Int) – Number of bins to create. max(): Double – Compute the maximum of all non-missing elements. The empty max is missing. min(): Double – Compute the minimum of all non-missing elements. The empty min is missing. product(): Double – Compute the product of all non-missing elements. The empty product is one. stats(): Struct{mean:Double,stdev:Double,min:Double,max:Double,nNotMissing:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/types.html:2634,variab,variable,2634,docs/0.1/types.html,https://hail.is,https://hail.is/docs/0.1/types.html,1,['variab'],['variable']
Modifiability," y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hsta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38665,config,config,38665,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability," | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:75673,variab,variable,75673,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variable']
Modifiability," }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.alleles_per_variant = alleles_per_variant; self.variants_per_contig = variants_per_contig; self.allele_types = allele_types; self.nti = nti; self.ntv = ntv. def __repr__(self):; return self.__str__(). def _repr_html_(self):; return self._html_string(). def __str__(self):; contig_idx = {contig: i for i, contig in enumerate(self.rg.contigs)}; max_contig_len = max(len(contig) for contig in self.variants_per_contig); contig_formatter = f'%{max_contig_len}s'. max_allele_count_len = max(len(str(x)) for x in self.alleles_per_variant); allele_count_formatter = f'%{max_allele_cou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:52285,config,config,52285,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['config'],['config']
Modifiability," – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:6307,config,configure,6307,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configure']
Modifiability,"""""""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransition(). [docs] def is_transversion(self):; """"""True if this alternate allele is a transversion SNP. This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransversion(). [docs] @handle_py4j; def category(self):; """"""Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. :rtype: str; """"""; return self._jrep.altAlleleType(). [docs]class Locus(object):; """"""; An object that represents a location in the genome. :param contig: chromosome identifier; :type contig: str or int; :param int position: chromosomal position (1-indexed); """""". @handle_py4j; def __init__(self, contig, position):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Locus').apply(contig, position); self._init_from_java(jrep); self._contig = contig; self._position = position. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Locus(contig=%s, position=%s)' % (self.contig, self.position). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:8779,polymorphi,polymorphism,8779,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"""sqrt"", tfloat64, x). [docs]@typecheck(x=expr_array(expr_float64), y=expr_array(expr_float64)); def corr(x, y) -> Float64Expression:; """"""Compute the; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between `x` and `y`. Examples; --------; >>> hl.eval(hl.corr([1, 2, 4], [2, 3, 1])); -0.6546536707079772. Notes; -----; Only indices where both `x` and `y` are non-missing will be included in the; calculation. If `x` and `y` have length zero, then the result is missing. Parameters; ----------; x : :class:`.Expression` of type ``array<tfloat64>``; y : :class:`.Expression` of type ``array<tfloat64>``. Returns; -------; :class:`.Float64Expression`; """"""; return _func(""corr"", tfloat64, x, y). [docs]@typecheck(ref=expr_str, alt=expr_str); @ir.udf(tstr, tstr); def numeric_allele_type(ref, alt) -> Int32Expression:; """"""Returns the type of the polymorphism as an integer. The value returned; is the integer value of :class:`.AlleleType` representing that kind of; polymorphism. Examples; --------. >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; -----; The values of :class:`.AlleleType` are not stable and thus should not be; relied upon across hail versions.; """"""; _base_regex = ""^([ACGTNM])+$""; _symbolic_regex = r""(^\.)|(\.$)|(^<)|(>$)|(\[)|(\])""; return hl.bind(; lambda r, a: hl.if_else(; r.matches(_base_regex),; hl.case(); .when(; a.matches(_base_regex),; hl.case(); .when(; r.length() == a.length(),; hl.if_else(; r.length() == 1,; hl.if_else(r != a, AlleleType.SNP, AlleleType.UNKNOWN),; hl.if_else(hamming(r, a) == 1, AlleleType.SNP, AlleleType.MNP),; ),; ); .when((r.length() < a.length()) & (r[0] == a[0]) & a.endswith(r[1:]), AlleleType.INSERTION); .when((r[0] == a[0]) & r.endswith(a[1:]), AlleleType.DELETION); .default(AlleleType.COMPLEX),; ); .when(a == '*', AlleleType.STAR); .when(a.matches(_symbolic_regex), AlleleType.SYMBOLIC); .default(AlleleType.UNKNOWN),; AlleleType.UNKNOWN,; ),; ref,; alt,; ).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:95632,polymorphi,polymorphism,95632,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['polymorphi'],['polymorphism']
Modifiability,"& (child_n == 1), 1); .when((father_n == 0) & (mother_n == 0) & (child_n == 1), 2); .when((father_n == 0) & (mother_n == 0) & (child_n == 2), 5); .when((father_n == 2) & (mother_n == 2) & (child_n == 0), 8); .when((father_n == 0) & (child_n == 2), 3); .when((mother_n == 0) & (child_n == 2), 4); .when((father_n == 2) & (child_n == 0), 6); .when((mother_n == 2) & (child_n == 0), 7); .or_missing(); ). hemi_x_cond = (; hl.case(missing_false=True); .when((mother_n == 2) & (child_n == 0), 9); .when((mother_n == 0) & (child_n > 0), 10); .or_missing(); ). hemi_y_cond = (; hl.case(missing_false=True); .when((father_n > 0) & (child_n == 0), 11); .when((father_n == 0) & (child_n > 0), 12); .or_missing(); ). return (; hl.case(); .when(locus.in_autosome_or_par() | is_female, auto_cond); .when(locus.in_x_nonpar() & (~is_female), hemi_x_cond); .when(locus.in_y_nonpar() & (~is_female), hemi_y_cond); .or_missing(); ). [docs]@typecheck(locus=expr_locus(), alleles=expr_array(expr_str)); def min_rep(locus, alleles):; """"""Computes the minimal representation of a (locus, alleles) polymorphism. Examples; --------. >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; -----; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters; ----------; locus : :class:`.LocusExpression`; alleles : :class:`.ArrayExpression` of type :py:data:`.tstr`. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `locus`; (:class:`.LocusExpression`) and `alleles`; (:class:`.ArrayExpression` of type :py:data:`.tstr`).; """"""; ret_type = tstruct(locus=locus.dtype, alleles=alleles.dtype); return _func('min_rep', ret_type, locus, alleles). [docs]@typecheck(; x=o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:168248,polymorphi,polymorphism,168248,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['polymorphi'],['polymorphism']
Modifiability,"&= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73602,rewrite,rewrite,73602,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,"['rewrite', 'variab']","['rewrite', 'variables']"
Modifiability,"'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, Ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:11455,config,config,11455,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,12,['config'],['config']
Modifiability,"'str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType) – Value type. See also; DictExpression, dict(), Collection functions. class hail.expr.types.tstruct(**field_types)[source]; Hail type for structured ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:8802,parameteriz,parameterized,8802,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['parameteriz'],['parameterized']
Modifiability,"(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an arra",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:2968,variab,variable,2968,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['variab'],['variable']
Modifiability,"(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . hail.expr.functions.locus(contig, pos, reference_genome='default')[source]; Construct a locus expression from a chromosome and position.; Examples; >>> hl.eval(hl.locus(""1"", 10000, reference_genome='GRCh37')); Locus(contig=1, position=10000, reference_genome=GRCh37). Parameters:. contig (str or StringExpression) – Chromosome.; pos (int or Expression of type tint32) – Base position along the chromosome.; reference_genome (str or ReferenceGenome) – Reference genome to use. Returns:; LocusExpression. hail.expr.functions.locus_from_global_position(global_pos, reference_genome='default')[source]; Constructs a locus expression from a global position and a reference genome.; The inverse of LocusExpression.global_position().; Examples; >>> hl.eval(hl.locus_from_global_position(0)); Locus(contig=1, position=1, reference_genome=GRCh37). >>> hl.ev",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:3671,polymorphi,polymorphism,3671,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"(dataset, method); return dataset._select_rows(; method,; hl.case(); .when(dataset.alleles.length() == 2, dataset._rvrow); .or_error(; f""'{method}' expects biallelic variants ('alleles' field of length 2), found ""; + hl.str(dataset.locus); + "", ""; + hl.str(dataset.alleles); ),; ). [docs]@typecheck(dataset=MatrixTable, name=str); def rename_duplicates(dataset, name='unique_id') -> MatrixTable:; """"""Rename duplicate column keys. .. include:: ../_templates/req_tstring.rst. Examples; --------. >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; -----. This method produces a new column field from the string column key by; appending a unique suffix ``_N`` as necessary. For example, if the column; key ""NA12878"" appears three times in the dataset, the first will produce; ""NA12878"", the second will produce ""NA12878_1"", and the third will produce; ""NA12878_2"". The name of this new field is parameterized by `name`. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name of new field. Returns; -------; :class:`.MatrixTable`; """""". require_col_key_str(dataset, 'rename_duplicates'); ids = dataset.col_key[0].collect(). mapping, new_ids = deduplicate(ids). if mapping:; info(; f'Renamed {len(mapping)} duplicate {plural(""sample ID"", len(mapping))}. Mangled IDs as follows:'; + ''.join(f'\n ""{pre}"" => ""{post}""' for pre, post in mapping); ); else:; info('No duplicate sample IDs found.'); return dataset.annotate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:11040,parameteriz,parameterized,11040,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,2,['parameteriz'],['parameterized']
Modifiability,"(one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, output; + '.bgen' will be a directory containing many BGEN files. In either case, the SAMPLE file is; written to output + '.sample'. For example,; >>> hl.export_bgen(mt, '/path/to/dataset') . Will write two files: /path/to/dataset.bgen and /path/to/dataset.sample. In contrast,; >>> hl.export_bgen(mt, '/path/to/dataset', parallel='header_per_shard') . Will create /path/to/dataset.sample and will create mt.n_partitions() files into the; directory /path/to/dataset.bgen/.; Notes; The export_bgen() function requires genotype probabilities, either as an entry; field of mt (of t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:52197,config,config,52197,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['config'],['config']
Modifiability,") is a PythonResult which can be; used as either arguments to another PythonJob or to other BashJob by using one; of the methods to convert a PythonResult to a file: PythonResult.as_str(),; PythonResult.as_repr(), and PythonResult.as_json().; In the example below, we first define two Python functions: hello_world() and upper().; Next, we create a batch and then create a new PythonJob with Batch.new_python_job().; Then we use PythonJob.call() and pass the hello_world function that we want to call.; Notice we just passed the reference to the function and not hello_world(). We also add; a Python string alice as an argument to the function. The result of the j.call() is; a PythonResult which we’ve assigned to the variable hello_str.; We want to use the hello_str result and make all the letters in upper case. We call; PythonJob.call() and pass a reference to the upper function.; But now the argument is hello_str which holds the result from calling hello_world; above. We assign the new output to the variable result.; At this point, we want to write out the transformed hello world result to a text file.; However, result is a PythonResult. Therefore, we need to use the PythonResult.as_str(); to convert result to a JobResourceFile with the string output HELLO WORLD ALICE. Now; we can write the result to a file.; def hello_world(name):; return f'hello {name}'. def upper(s):; return s.upper(). b = hb.Batch(name='hello'); j = b.new_python_job(); hello_str = j.call(hello_world, 'alice'); result = j.call(upper, hello_str); b.write_output(result.as_str(), 'output/hello-alice.txt'); b.run(). Backends; There are two backends that execute batches: the LocalBackend and the; ServiceBackend. The local backend is used by default and executes jobs; on your local computer. The service backend executes jobs in a shared compute cluster; managed by the Hail team. To use the Batch Service, follow the directions here. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; them",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:18124,variab,variable,18124,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['variab'],['variable']
Modifiability,") – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--forma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102066,config,config,102066,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['config']
Modifiability,"))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:6629,config,configuration,6629,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"), named_exprs=anytype); def select(self, *exprs, **named_exprs) -> 'Table':; """"""Select existing fields or create new fields by name, dropping the rest. Examples; --------; Select a few old fields and compute a new one:. >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; -----; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:48919,Variab,Variable-length,48919,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['Variab'],['Variable-length']
Modifiability,"),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; backend=nullable(enumeration(*BackendType.__args__)),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:6506,config,configure,6506,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configure']
Modifiability,"). t = t.add_index(index_uid); unique_cols = t.aggregate(; hl.agg.group_by(hl.struct(**{f: t[f] for f in col_key_fields}), hl.agg.take(t[index_uid], 1)); ); unique_cols = sorted([v[0] for _, v in unique_cols.items()]). return self.choose_cols(unique_cols). [docs] @deprecated(version=""0.2.129""); @typecheck_method(separator=str); def make_table(self, separator='.') -> Table:; """"""Make a table from a matrix table with one field per sample. .. deprecated:: 0.2.129; use :meth:`.localize_entries` instead because it supports more; columns. Parameters; ----------; separator : :class:`str`; Separator between sample IDs and entry field names. Returns; -------; :class:`.Table`. See Also; --------; :meth:`.localize_entries`. Notes; -----; The table has one row for each row of the input matrix. The; per sample and entry fields are formed by concatenating the; sample ID with the entry field name using `separator`. If the; entry field name is empty, the separator is omitted. The table inherits the globals from the matrix table. Examples; --------; Consider a matrix table with the following schema:. .. code-block:: text. Global fields:; 'batch': str; Column fields:; 's': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; Entry fields:; 'GT': call; 'GQ': int32; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>. and three sample IDs: `A`, `B` and `C`. Then the result of; :meth:`.make_table`:. >>> ht = mt.make_table() # doctest: +SKIP. has the original row fields along with 6 additional fields,; one for each sample and entry field:. .. code-block:: text. Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>; """"""; if not (len(self.col_key) == 1 and self.col_key[0].dtype == hl.tstr):; raise ValueError(""column key must be a single field of type str""). col_keys = self.col_key",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:132296,inherit,inherits,132296,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['inherit'],['inherits']
Modifiability,").; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Argumen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:15160,variab,variable,15160,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability,"):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8130,inherit,inherited,8130,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['inherit'],['inherited']
Modifiability,"); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.tidyr. Source code for hail.experimental.tidyr; import hail as hl; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(ht=Table, key=str, value=str, fields=str); def gather(ht, key, value, *fields) -> Table:; """"""Collapse fields into key-value pairs. :func:`.gather` mimics the functionality of the `gather()` function found in R's; ``tidyr`` package. This is a way to turn ""wide"" format data into ""long""; format data. Parameters; ----------; ht : :class:`.Table`; A Hail table.; key : :class:`str`; The name of the key field in the gathered table.; value : :class:`str`; The name of the value field in the gathered table.; fields : variable-length args of obj:`str`; Names of fields to gather in ``ht``. Returns; -------; :class:`.Table`; Table with original ``fields`` gathered into ``key`` and ``value`` fields."""""". ht = ht.annotate(_col_val=hl.array([hl.struct(field_name=field, value=ht[field]) for field in fields])); ht = ht.drop(*fields); ht = ht.explode(ht['_col_val']); ht = ht.annotate(**{key: ht['_col_val'][0], value: ht['_col_val'][1]}); ht = ht.drop('_col_val'). ht_tmp = new_temp_file(); ht.write(ht_tmp). return hl.read_table(ht_tmp). [docs]@typecheck(ht=Table, field=str, value=str, key=nullable(oneof(str, sequenceof(str)))); def spread(ht, field, value, key=None) -> Table:; """"""Spread a key-value pair of fields across multiple fields. :func:`.spread` mimics the functionality of the `spread()` function in R's; `tidyr` package. This is a way to turn ""long"" format data into ""wide""; format data. Given a ``field``, :func:`.spread` will create a new table by grouping; ``ht`` by its row key and, optionally, any additional fields passed to the;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html:1196,variab,variable-length,1196,docs/0.2/_modules/hail/experimental/tidyr.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html,2,['variab'],['variable-length']
Modifiability,"); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:. .. code-block:: text. (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:. .. code-block:: text. (AB > 0.2). HIGH-quality indel:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:. .. code-block:: text. (p > 0.5) AND (AB > 0.3) A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:25011,variab,variables,25011,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['variab'],['variables']
Modifiability,"); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef; HomRef; HomVar; Auto; Mom, Kid. 5; HomRef; HomRef; HomVar; Auto; Kid. 6; HomVar; ~HomVar; HomRef; Auto; Dad, Kid. 7; ~HomVar; HomVar; HomRef; Auto; Mom, Kid. 8; HomVar; HomVar; HomRef; Auto; Kid. 9; Any; HomVar; HomRef; HemiX; Mom, Kid. 10; Any; HomRef; HomVar; HemiX; Mom, Kid. 11; HomVar; Any; HomRef; HemiY; D",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:50676,extend,extending,50676,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['extend'],['extending']
Modifiability,"); for version in self.versions; if version.maybe_index(key_expr, all_matches) is not None; ]; if len(compatible_indexed_values) == 0:; versions = [f'{(v.version, v.reference_genome)}' for v in self.versions]; raise ValueError(; f'Could not find compatible version of {self.name} for user'; f' dataset with key {key_expr.dtype}.\n'; f'This annotation dataset is available for the following'; f' versions and reference genome builds: {"", "".join(versions)}.'; ); else:; indexed_values = sorted(compatible_indexed_values, key=lambda x: x[1])[-1]. if len(compatible_indexed_values) > 1:; info(; f'index_compatible_version: More than one compatible version'; f' exists for annotation dataset: {self.name}. Rows have been'; f' annotated with version {indexed_values[1]}.'; ); return indexed_values[0]. [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:9324,config,configuration,9324,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['config'],['configuration']
Modifiability,"*group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = (table1.group_by(table1.C1, 'C2', height_bin = table1.HT // 20); ... .aggregate(meanX = hl.agg.mean(table1.X))). Note; ----; This method does not support aggregation in key expressions. Arguments; ---------; exprs : varargs of type str or :class:`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:56115,variab,variable-length,56115,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"+ '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:45994,Config,Configuration,45994,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Config'],['Configuration']
Modifiability,"+ NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Comp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6235,variab,variables,6235,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['variab'],['variables']
Modifiability,"+; | ``gqStDev`` | Double | Genotype quality standard deviation across all samples |; +---------------------------+--------+--------------------------------------------------------+. Missing values ``NA`` may result (for example, due to division by zero) and are handled properly ; in filtering and written as ""NA"" in export modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VE",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:221465,plugin,plugin,221465,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['plugin'],['plugin']
Modifiability,", 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep config",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30900,Plugin,Plugins,30900,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Plugin'],['Plugins']
Modifiability,", ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:4419,config,configuration,4419,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configuration']
Modifiability,", k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 0.516439358616939; >>> hl.eval(hl.pgenchisq(10 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0; >>> hl.eval(hl.pgenchisq(40 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:18376,variab,variables,18376,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variables']
Modifiability,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:6295,variab,variable,6295,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],['variable']
Modifiability,", start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1508,polymorphi,polymorphism,1508,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,", the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:4970,polymorphi,polymorphism,4970,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,",; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will chang",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37856,Config,Configuration,37856,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Config'],['Configuration']
Modifiability,",; variant_allele=tstr,; ); ),; seq_region_name=tstr,; start=tint32,; strand=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23037,config,configuring,23037,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuring']
Modifiability,"--+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Retu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94966,variab,variable,94966,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"----+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. -",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95070,variab,variable,95070,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"----+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the nul",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79365,variab,variable,79365,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Parameters:. p (float) – Probability of keeping each row.; seed (int) – Random seed. Returns:; Table – Table with approximately p * n_rows rows. select(*exprs, **named_exprs)[source]; Select existing fields or create new fields by name, dropping the rest.; Examples; Select a few old fields and compute a new one:; >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; Using select; Select and its sibling methods (Table.select_globals(),; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). Th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:59815,variab,variable-length,59815,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"-------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106727,variab,variable,106727,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"-----; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; return construct_expr(ir.NA(t), t). [docs]@deprecated(version=""0.2.62"", reason=""Replaced by hl.missing""); @typecheck(t=hail_type); def null(t: Union[HailType, str]):; """"""Deprecated in favor of :func:`.missing`. Creates an expression representing a missing value of a specified type. Examples; --------. >>> hl.eval(hl.null(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.null('array<str>')); None. Notes; -----; This method is useful for constructing an expression that includes missing; values, since :obj:`None` cannot be interpreted as an expression. Parameters; ----------; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; return missing(t). [docs]@typecheck(x=anytype, dtype=nullable(hail_type)); def literal(x: Any, dtype: Optional[Union[HailType, str]] = None):; """"""Captures and broadcasts a Python variable or object as an expression. Examples; --------. >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""; wrapper = {'has_expr': False, 'has_free_vars': False}. def ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:9247,variab,variable,9247,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"---; Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'target_date': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------. Fields may also be selected by their name:. >>> ht = ht.select_globals('target_date'); >>> ht.globals.show(); +--------------------+; | <expr>.target_date |; +--------------------+; | str |; +--------------------+; | ""2025-01-01"" |; +--------------------+. Notes; -----; This method creates new global fields. If a created field shares its name; with a row-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified global fields. """"""; caller = 'Table.select_globals'; new_globals = get_select_exprs(caller, exprs, named_exprs, self._global_indices, self._globals). return self._select_globals(caller, new_globals). [docs] @typecheck_method(named_exprs=expr_any); def transmute_globals(self, **named_exprs) -> 'Table':; """"""Similar to :meth:`.Table.annotate_globals`, but drops referenced fields. Notes; -----; Consider a table with global fields `population`, `area`, and `year`:. >>> ht = hl.utils.range_table(1); >>> ht = ht.annotate_globals(population=1000000, area=500, year=2020). Compute a new field, `density` from `population` and `area` and also drop the latter two; fields:. >>> ht = ht.transmute_globals(density=ht.popu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:31306,variab,variable-length,31306,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"-. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryR",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46867,config,configuration,46867,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"-; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; if ncp is None:; return _func(""pchisqtail"", tfloat64, x, df, lower_tail, log_p); else:; return _func(""pnchisqtail"", tfloat64, x, df, ncp, lower_tail, log_p). PGENCHISQ_RETURN_TYPE = tstruct(value=tfloat64, n_iterations=tint32, converged=tbool, fault=tint32). [docs]@typecheck(; x=expr_float64,; w=expr_array(expr_float64),; k=expr_array(expr_int32),; lam=expr_array(expr_float64),; mu=expr_float64,; sigma=expr_float64,; max_iterations=nullable(expr_int32),; min_accuracy=nullable(expr_float64),; ); def pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None) -> Float64Expression:; r""""""The cumulative probability function of a `generalized chi-squared distribution; <https://en.wikipedia.org/wiki/Generalized_chi-squared_distribution>`__. The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this distribution:. 1. A linear combination of normal variables and squares of normal variables. 2. A weighted sum of sums of squares of normally distributed values plus a normally distributed; value. 3. A weighted sum of chi-squared distributed values plus a normally distributed value. 4. A `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form_(statistics)>`__ in a vector; of uncorrelated `standard normal; <https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution>`__ values. The parameters of this function correspond to the parameters of the third interpretation. .. math::. \begin{aligned}; w &: R^n \quad k : Z^n \quad lam : R^n \quad mu : R \quad sigma : R \\; \\; x &\sim N(mu, sigma^2) \\; y_i &\sim \mathrm{NonCentralChiSquared}(k_i, lam_i) \\; \\; Z &= x + w y^T \\; &= x + \sum_i w_i y_i \\; Z &\sim \mathrm{GeneralizedNonCentralChiSquared}(w, k, lam, mu, sigma); \end{aligned}. The generalized chi-squared distribution often arises when working on linear models with standard; normal noise because the sum of th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:64929,variab,variables,64929,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,4,['variab'],['variables']
Modifiability,"-; :class:`.Table`; Filtered table. """"""; analyze('Table.filter', expr, self._row_indices); base, cleanup = self._process_joins(expr). return cleanup(Table(ir.TableFilter(base._tir, ir.filter_predicate_with_keep(expr._ir, keep)))). [docs] @typecheck_method(exprs=oneof(Expression, str), named_exprs=anytype); def select(self, *exprs, **named_exprs) -> 'Table':; """"""Select existing fields or create new fields by name, dropping the rest. Examples; --------; Select a few old fields and compute a new one:. >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; -----; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:48670,variab,variable-length,48670,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:59857,variab,variable,59857,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variable']
Modifiability,"-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound**3:; return _svd(self.to_numpy(), full_matrices=False, compute_uv=compute_uv, overwrite_a=True); else:; return self._svd_gr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76686,config,configuration-dependent,76686,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['config'],['configuration-dependent']
Modifiability,". 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial; The Grammar of Graphics; Geoms that group; Labels and Axes. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GGPlot Tutorial. View page source. GGPlot Tutorial. [1]:. import hail as hl; from hail.ggplot import *. import plotly. Loading BokehJS ... The Hail team has implemented a plotting module for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI ava",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:1012,flexible,flexible,1012,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['flexible'],['flexible']
Modifiability,". :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5114,polymorphi,polymorphism,5114,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,". Backend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; Backend. LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Backend. View page source. Backend. class hailtop.batch.backend.Backend(requester_pays_fses); Bases: ABC, Generic[RunningBatchType]; Abstract class for backends.; Methods. _async_run; Execute a batch. _run; See _async_run(). async_close. close; Close a Hail Batch Backend. requester_pays_fs. rtype:; RouterAsyncFS. validate_file. rtype:; None. abstract async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch.; :rtype: Optional[TypeVar(RunningBatchType)]. Warning; This method should not be called directly. Instead, use batch.Batch.run(). _run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); See _async_run().; :rtype: Optional[TypeVar(RunningBatchType)]. Warning; This method should not be called directly. Instead, use batch.Batch.run(). async async_close(). close(); Close a Hail Batch Backend.; Notes; This method should be called after executing your batches at the; end of your script. requester_pays_fs(requester_pays_config). Return type:; RouterAsyncFS. async validate_file(uri, requester_pays_config=None). Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.Backend.html:265,Config,Configuration,265,docs/batch/api/backend/hailtop.batch.backend.Backend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.Backend.html,1,['Config'],['Configuration']
Modifiability,". BashJob — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; BashJob. PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BashJob. View page source. BashJob. class hailtop.batch.job.BashJob(batch, token, *, name=None, attributes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:240,Config,Configuration,240,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,1,['Config'],['Configuration']
Modifiability,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Batch. Job; BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Batch. View page source. Batch. class hailtop.batch.batch.Batch(name=None, backend=None, attributes=None, requester_pays_project=None, default_image=None, default_memory=None, default_cpu=None, default_storage=None, default_regions=None, default_timeout=None, default_shell=None, default_python_image=None, default_spot=None, project=None, cancel_after_n_failures=None); Bases: object; Object representing the distributed acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints “hello”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respective",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:236,Config,Configuration,236,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['Config'],['Configuration']
Modifiability,". BatchPoolExecutor — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:264,Config,Configuration,264,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['Config'],['Configuration']
Modifiability,". BatchPoolFuture — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture; BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:260,Config,Configuration,260,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,1,['Config'],['Configuration']
Modifiability,". HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.context. Source code for hail.context; from __future__ import print_function # Python 2 and 3 print compatibility. from hail.typecheck import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:1052,config,configuration,1052,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['config'],['configuration']
Modifiability,". InputResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; InputResourceFile. JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; InputResourceFile. View page source. InputResourceFile. class hailtop.batch.resource.InputResourceFile; Bases: ResourceFile; Class representing a resource from an input file.; Examples; input is an InputResourceFile of the batch b; and is used in job j:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(name='hello'); >>> j.command(f'cat {input}'); >>> b.run(). Methods. source. rtype:; None. source(). Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.InputResourceFile.html:317,Config,Configuration,317,docs/batch/api/resource/hailtop.batch.resource.InputResourceFile.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.InputResourceFile.html,1,['Config'],['Configuration']
Modifiability,". Job — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; Job. BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:232,Config,Configuration,232,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['Config'],['Configuration']
Modifiability,". JobResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; JobResourceFile. ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; JobResourceFile. View page source. JobResourceFile. class hailtop.batch.resource.JobResourceFile(value, source); Bases: ResourceFile; Class representing an intermediate file from a job.; Examples; j.ofile is a JobResourceFile on the job`j`:; >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; All JobResourceFile are temporary files and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved.; Methods. add_extension; Specify the file extension to use. source. rtype:; Job. add_extension(extension); Specify the file extension to use.; Examples; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; The default file name for a JobResourceFile is the name; of the identifier. Parameters:; extension (str) – File extension to use. Return type:; JobResourceFile. Returns:; JobResourceFile – Same resource file with the extension specified. source(). Return type:; Job. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html:313,Config,Configuration,313,docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,1,['Config'],['Configuration']
Modifiability,". LocalBackend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; LocalBackend; LocalBackend. ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; LocalBackend. View page source. LocalBackend. class hailtop.batch.backend.LocalBackend(tmp_dir='/tmp/', gsa_key_file=None, extra_docker_run_flags=None); Bases: Backend[None]; Backend that executes batches on a local computer.; .. rubric:: Examples; >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters:. tmp_dir (str) – Temporary directory to use.; gsa_key_file (Optional[str]) – Mount a file with a gsa key to /gsa-key/key.json. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable HAIL_BATCH_GSA_KEY_FILE.; extra_docker_run_flags (Optional[str]) – Additional flags to pass to docker run. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS. Methods. _async_run; Execute a batch. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(). Parameters:. batch (Batch) – Batch to execute.; dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files. Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html:983,variab,variable,983,docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html,5,"['Config', 'variab']","['Configuration', 'variable']"
Modifiability,". PythonJob — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; PythonJob. View page source. PythonJob. class hailtop.batch.job.PythonJob(batch, token, *, name=None, attributes=None); Bases: Job; Object representing a single Python job to execute.; Examples; Create a new Python job that multiplies two numbers and then adds 5 to the result:; # Create a batch object with a default Python image. b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def multiply(x, y):; return x * y. def add(x, y):; return x + y. j = b.new_python_job(); result = j.call(multiply, 2, 3); result = j.call(add, result, 5). # Write out the str representation of result to a file. b.write_output(result.as_str(), 'hello.txt'). b.run(). Notes; This class should never be created directly by the user. Use Batch.new_python_job(); instead.; Methods. call; Execute a Python function. image; Set the job's docker image. call(unapplied, *args, **kwargs); Execute a Python function.; Examples; import json. def add(x, y):; return x + y. def multiply(x, y):; return x * y. def format_as_csv(x, y, add_result, mult_result):; return f'{x},{y},{add_result},{mult_result}'. def csv_to_json(path):; data = []; with open(path) as f:; for line in f:; line = line.rstrip(); fields = line.split(','); d = {'x': int(fields[0]),; 'y': int(fields[1]),; 'add': int(fields[2]),; 'mult': int(fields[3])}; data.append(d); return json.dumps(data). # Get all the multiplication and addition table results. b = Batch(name='add-mult-table'). formatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:244,Config,Configuration,244,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,1,['Config'],['Configuration']
Modifiability,". PythonResult — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; PythonResult. View page source. PythonResult. class hailtop.batch.resource.PythonResult(value, source); Bases: Resource, str; Class representing a result from a Python job.; Examples; Add two numbers and then square the result:; def add(x, y):; return x + y. def square(x):; return x ** 2. b = Batch(); j = b.new_python_job(name='add'); result = j.call(add, 3, 2); result = j.call(square, result); b.write_output(result.as_str(), 'output/squared.txt'); b.run(). Notes; All PythonResult are temporary Python objects and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved. In most cases, you’ll want to convert the PythonResult; to a JobResourceFile in a human-readable format.; Methods. as_json; Convert a Python result to a file with a JSON representation of the object. as_repr; Convert a Python result to a file with the repr representation of the object. as_str; Convert a Python result to a file with the str representation of the object. source; Get the job that created the Python result. as_json(); Convert a Python result to a file with a JSON representation of the object.; Examples; def add(x, y):; return {'result': x + y}. b = Batch(); j = b.new_python_job(name='add'); result = j.call(add, 3, 2); b.write_output(result.as_json(), 'output/add.json'); b.run(). Return type:; JobResourceFile. Returns:; JobResourceFile – A new resource file where the contents are a Python object; that has been converted to JSON. as_repr(); Convert a Python result to a file with the repr representation of the object.; Examples",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.PythonResult.html:307,Config,Configuration,307,docs/batch/api/resource/hailtop.batch.resource.PythonResult.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.PythonResult.html,1,['Config'],['Configuration']
Modifiability,". Random Forest Model — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Rand",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:355,Config,Configuration,355,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['Config'],['Configuration']
Modifiability,". Resource — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; Resource. ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Resource. View page source. Resource. class hailtop.batch.resource.Resource; Bases: object; Abstract class for resources.; Methods. source. rtype:; Optional[Job]. abstract source(). Return type:; Optional[Job]. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.Resource.html:299,Config,Configuration,299,docs/batch/api/resource/hailtop.batch.resource.Resource.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.Resource.html,1,['Config'],['Configuration']
Modifiability,". ResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; ResourceFile. InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ResourceFile. View page source. ResourceFile. class hailtop.batch.resource.ResourceFile(value); Bases: Resource, str; Class representing a single file resource. There exist two subclasses:; InputResourceFile and JobResourceFile. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceFile.html:307,Config,Configuration,307,docs/batch/api/resource/hailtop.batch.resource.ResourceFile.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceFile.html,1,['Config'],['Configuration']
Modifiability,". ResourceGroup — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; ResourceGroup. PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ResourceGroup. View page source. ResourceGroup. class hailtop.batch.resource.ResourceGroup(source, root, **values); Bases: Resource; Class representing a mapping of identifiers to a resource file.; Examples; Initialize a batch and create a new job:; >>> b = Batch(); >>> j = b.new_job(). Read a set of input files as a resource group:; >>> bfile = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'). Create a resource group from a job intermediate:; >>> j.declare_resource_group(ofile={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam'}); >>> j.command(f'plink --bfile {bfile} --make-bed --out {j.ofile}'). Reference the entire file group:; >>> j.command(f'plink --bfile {bfile} --geno 0.2 --make-bed --out {j.ofile}'). Reference a single file:; >>> j.command(f'wc -l {bfile.fam}'). Execute the batch:; >>> b.run() . Notes; All files in the resource group are copied between jobs even if only one; file in the resource group is mentioned. This is to account for files that; are implicitly assumed to always be together such as a FASTA file and its; index.; Methods. source. source(). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceGroup.html:309,Config,Configuration,309,docs/batch/api/resource/hailtop.batch.resource.ResourceGroup.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceGroup.html,1,['Config'],['Configuration']
Modifiability,". RunningBatchType — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; RunningBatchType. Backend; LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; RunningBatchType. View page source. RunningBatchType. class hailtop.batch.backend.RunningBatchType; The type of value returned by Backend._run(). The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; alias of TypeVar(‘RunningBatchType’). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html:283,Config,Configuration,283,docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,1,['Config'],['Configuration']
Modifiability,". ServiceBackend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:279,Config,Configuration,279,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['Config'],['Configuration']
Modifiability,". Tutorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:913,variab,variable,913,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['variab'],['variable']
Modifiability,". Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word arguments. Parameters:. batch (Batch) – Batch to execute.; dry_run (bool) – If True, don’t execute code.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4372,variab,variables,4372,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['variab'],['variables']
Modifiability,". hailtop.batch.docker.build_python_image — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; build_python_image(). hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.docker.build_python_image. View page source. hailtop.batch.docker.build_python_image. hailtop.batch.docker.build_python_image(fullname, requirements=None, python_version=None, _tmp_dir='/tmp', *, show_docker_output=False); Build a new Python image with dill and the specified pip packages installed.; Notes; This function is used to build Python images for PythonJob.; Examples; >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) . Parameters:. fullname (str) – Full name of where to build the image including any repository prefix and tags; if desired (default tag is latest).; requirements (Optional[List[str]]) – List of pip packages to install.; python_version (Optional[str]) – String in the format of major_version.minor_version (ex: 3.9). Defaults to; current version of Python that is running.; _tmp_dir (str) – Location to place local temporary files used while building the image.; show_docker_output (bool) – Print the output from Docker when building / pushing the image. Return type:; str. Returns:; Full name where built image is located. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html:360,Config,Configuration,360,docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,1,['Config'],['Configuration']
Modifiability,". hailtop.batch.utils.concatenate — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; concatenate(). hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.utils.concatenate. View page source. hailtop.batch.utils.concatenate. hailtop.batch.utils.concatenate(b, files, image=None, branching_factor=100); Concatenate files using tree aggregation.; Examples; Create and execute a batch that concatenates output files:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'touch {j1.ofile}'); >>> j2 = b.new_job(); >>> j2.command(f'touch {j2.ofile}'); >>> j3 = b.new_job(); >>> j3.command(f'touch {j3.ofile}'); >>> files = [j1.ofile, j2.ofile, j3.ofile]; >>> ofile = concatenate(b, files, branching_factor=2); >>> b.run(). Parameters:. b (Batch) – Batch to add concatenation jobs to.; files (List[ResourceFile]) – List of files to concatenate.; branching_factor (int) – Grouping factor when concatenating files.; image (Optional[str]) – Image to use. Must have the cat command. Return type:; ResourceFile. Returns:; Concatenated output file. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.concatenate.html:345,Config,Configuration,345,docs/batch/api/utils/hailtop.batch.utils.concatenate.html,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.concatenate.html,1,['Config'],['Configuration']
Modifiability,". hailtop.batch.utils.plink_merge — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge; plink_merge(). Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.utils.plink_merge. View page source. hailtop.batch.utils.plink_merge. hailtop.batch.utils.plink_merge(b, bfiles, image=None, branching_factor=100); Merge binary PLINK files using tree aggregation. Parameters:. b (Batch) – Batch to add merge jobs to.; bfiles (List[ResourceGroup]) – List of binary PLINK file roots to merge.; image (Optional[str]) – Image name that contains PLINK.; branching_factor (int) – Grouping factor when merging files. Return type:; ResourceGroup. Returns:; Merged binary PLINK file. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.plink_merge.html:345,Config,Configuration,345,docs/batch/api/utils/hailtop.batch.utils.plink_merge.html,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.plink_merge.html,1,['Config'],['Configuration']
Modifiability,". ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; -----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44826,config,config,44826,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['config'],['config']
Modifiability,". map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2886,variab,variables,2886,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['variab'],['variables']
Modifiability,".. ]); ... ); >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=0.2188830334629822, p_value=0.6398923118508772). Notes; -----; See the `Wikipedia article <https://en.m.wikipedia.org/wiki/Cochran%E2%80%93Mantel%E2%80%93Haenszel_statistics>`_; for more details. Parameters; ----------; a : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the upper-left cell in the contingency tables.; b : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the upper-right cell in the contingency tables.; c : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the lower-left cell in the contingency tables.; d : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the lower-right cell in the contingency tables. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `test_statistic`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; # The variable names below correspond to the notation used in the Wikipedia article.; # https://en.m.wikipedia.org/wiki/Cochran%E2%80%93Mantel%E2%80%93Haenszel_statistics; n1 = hl.zip(a, b).map(lambda ab: ab[0] + ab[1]); n2 = hl.zip(c, d).map(lambda cd: cd[0] + cd[1]); m1 = hl.zip(a, c).map(lambda ac: ac[0] + ac[1]); m2 = hl.zip(b, d).map(lambda bd: bd[0] + bd[1]); t = hl.zip(n1, n2).map(lambda nn: nn[0] + nn[1]). def numerator_term(a, n1, m1, t):; return a - n1 * m1 / t. # The numerator comes from the link below, not from the Wikipedia article.; # https://www.biostathandbook.com/cmh.html; numerator = (hl.abs(hl.sum(hl.zip(a, n1, m1, t).map(lambda tup: numerator_term(*tup)))) - 0.5) ** 2. def denominator_term(n1, n2, m1, m2, t):; return n1 * n2 * m1 * m2 / (t**3 - t**2). denominator = hl.sum(hl.zip(n1, n2, m1, m2, t).map(lambda tup: denominator_term(*tup))). test_statistic = numerator / denominator; p_value = pchisqtail(test_statistic, 1); return struct(test_statistic=test_statistic, p_value=p_value). [docs]@typecheck(; col",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:24318,variab,variable,24318,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. See also; SwitchBuilder, case(), cond(). Parameters:; expr (Expression) – Value to match against. Returns:; SwitchBuilder. hail.expr.functions.case(missing_false=False)[source]; Chain multiple if-else statements with a CaseBuilder.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(hl.len(x) == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cannot be interpreted as an expression. Parameters:; t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:6263,variab,variable-length,6263,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable-length']
Modifiability,"... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ======",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48974,variab,variable,48974,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"..]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix wher",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:2467,config,config,2467,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['config']
Modifiability,".0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:14549,variab,variable,14549,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability,".1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; AltAllele. View page source. AltAllele¶. class hail.representation.AltAllele(ref, alt)[source]¶; An object that represents an allele in a polymorphism deviating from the reference allele. Parameters:; ref (str) – reference allele; alt (str) – alternate allele. Attributes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1091,polymorphi,polymorphism,1091,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,1,['polymorphi'],['polymorphism']
Modifiability,".131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12361,config,configuration,12361,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,".; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. Parameters:num_genotypes (int) – number of possible genotypes. Return type:list of int or None. p_ab(theta=0.5)[source]¶; Returns the p-value associated with finding the given allele depth ratio.; This function uses a one-tailed binomial test.; This function returns None if the allelic depth (ad) is missing. Parameters:theta (float) – null reference probability for binomial model. Return type:float. pl¶; Returns the phred-scaled genotype posterior likelihoods. Return type:list of int or None. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html:5429,variab,variables,5429,docs/0.1/representation/hail.representation.Genotype.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html,1,['variab'],['variables']
Modifiability,".alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3702,polymorphi,polymorphism,3702,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,".annotate_globals(; weight_nds=hl.enumerate(ht.kept_samples).starmap(; lambda group_idx, group_sample_indices: hl.nd.array(; group_sample_indices.map(lambda group_sample_idx: ht.weight_arrays[group_sample_idx][group_idx]); ); ); ); ht = ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22005,variab,variables,22005,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],['variables']
Modifiability,".first(), self). [docs] @typecheck_method(item=expr_any); def append(self, item):; """"""Append an element to the array and return the result. Examples; --------. >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; ----; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17307,extend,extend,17307,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['extend'],['extend']
Modifiability,".gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See Also; --------; :meth:`.aggregate`. Returns; -------; :class:`.MatrixTable`; Aggregated matrix table.; """"""; assert self._row_keys is not None or self._col_keys is not None. defined_exprs = []; for e in [self._row_fields, self._col_fields, self._entry_fields]:; if e is not None:; defined_exprs.append(e); for e in [self._computed_row_key, self._computed_col_key]:; if e is not None:; defined_exprs.extend(e.values()). def promote_none(e):; return hl.struct() if e is None else e. entry_exprs = promote_none(self._entry_fields); if len(entry_exprs) == 0:; warning(""'GroupedMatrixTable.result': No entry fields were defined.""). base, cleanup = self._parent._process_joins(*defined_exprs). if self._col_keys is not None:; cck = self._computed_col_key or {}; computed_key_uids = {k: Env.get_uid() for k in cck}; modified_keys = [computed_key_uids.get(k, k) for k in self._col_keys]; mt = MatrixTable(; ir.MatrixAggregateColsByKey(; ir.MatrixMapCols(; base._mir,; self._parent.col.annotate(**{computed_key_uids[k]: v for k, v in cck.items()})._ir,; modified_keys,; ),; entry_exprs._ir,; promote_none(self._col_fields)._ir,; ); ); if cck:; mt = mt.rename({v: k for k, v in computed_key_uids.items()}); else:; cck = self._computed_row_key or {}; computed_key_uids = {k: Env.get_uid() for k in cck}; modified_keys = [computed_key_uids.get(k, k) for k in self._row_keys]; mt = MatrixTable(; ir.MatrixAggregateRow",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:13640,extend,extend,13640,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['extend'],['extend']
Modifiability,".hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:7519,polymorphi,polymorphism,7519,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,".select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:49541,variab,variable-length,49541,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not speci",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2716,config,config,2716,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['config']
Modifiability,"0961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. Types in action¶; We’ll produce some sample annotations with the; sample_qc; method, then use these annotations to demonstrate some of the expression; language features. In [5]:. vds = vds.variant_qc().cache().sample_qc(). In [6]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; }; }. Filtering with expressions¶; The schema printed above is the type of the sample annotations, which; are given the variable name ‘sa’ wherever they appear. Here, we use the; filter_samples_expr method to filter samples based on these; annotations. If we want to filter on the “dpMean” above, we need to; select the ‘qc’ field from the ‘sa’ struct, then select the ‘dpMean’; field from the ‘qc’ struct. These selections are done with dots.; There are four Hail methods that use the expression language to filter a; dataset: -; filter_variants_expr; -; filter_samples_expr; -; filter_genotypes; -; filter_alleles; All these methods take a Hail expression as a string argument, and; return a filtered dataset. In [7]:. # unfiltered; vds.num_samples. Out[7]:. 1000. In [8]:. vds.filter_samples_expr('sa.qc.dpMean > 5', keep=True).num_samples. Out[8]:. 699. In [9]:. vds.filter_samples_expr('sa.qc.dpMean <= 5', keep=False).num_samples. Out[9]:. 699. In [10]:. vds.filter_samples_expr('sa.qc.callRate > 0.95', keep=True).num_samples. Out[10]:. 928. In [11]:. vds.filter_samples_expr(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:3967,variab,variable,3967,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['variab'],['variable']
Modifiability,"1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26844,config,configuration,26844,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"1,1,1],""fst"":[0.1,0.1,0.1],""mixture"":false}. Notes; For entry-indexed expressions, if there is one column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a)[source]; Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Not",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:7825,extend,extend,7825,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['extend'],['extend']
Modifiability,"102},; u'transcripts': [{u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE1',; u'isoform': u'GENE1.1'},; {u'canonical': True,; u'consequence': u'LOF',; u'gene': u'GENE1',; u'isoform': u'GENE1.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.1'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.3'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.1'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.2'}]},; Struct{; info: Struct{; AC: Array[Int],; AN: Int,; AF: Array[Double]; },; transcripts: Array[Struct{; gene: String,; isoform: String,; canonical: Boolean,; consequence: String; }]; }). You’ll rarely need to construct a Variant or Genotype object; inside the Hail expression language. More commonly, these objects will; be provided to you as variables. In the remainder of this notebook, we; will explore how to to manipulate the demo variables. In the next; notebook, we start using the expression langauge to annotate and filter; a dataset.; First, a short demonstration of some of the methods accessible on; Variant and Genotype objects:. In [52]:. hc.eval_expr_typed('v'). Out[52]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [53]:. hc.eval_expr_typed('v.contig'). Out[53]:. (u'16', String). In [54]:. hc.eval_expr_typed('v.start'). Out[54]:. (19200405, Int). In [55]:. hc.eval_expr_typed('v.ref'). Out[55]:. (u'C', String). In [56]:. hc.eval_expr_typed('v.altAlleles'). Out[56]:. ([AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)], Array[AltAllele]). In [57]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isSNP())'). Out[57]:. ([True, False], Array[Boolean]). In [58]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isInsertion())'). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:15560,variab,variables,15560,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['variab'],['variables']
Modifiability,"2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:2852,variab,variable,2852,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],"['variable', 'variables']"
Modifiability,"20, 200), (None, None, 300)]. Notes; -----; The element type of the resulting array is a :class:`.ttuple` with a field; for each array. Parameters; ----------; arrays: : variable-length args of :class:`.ArrayExpression`; Array expressions.; fill_missing : :obj:`bool`; If ``False``, return an array with length equal to the shortest length; of the `arrays`. If ``True``, return an array equal to the longest; length of the `arrays`, by extending the shorter arrays with missing; values. Returns; -------; :class:`.ArrayExpression`; """"""; return _zip_streams(*(a._to_stream() for a in arrays), fill_missing=fill_missing).to_array(). def _zip_func(*arrays, fill_missing=False, f):; n_arrays = builtins.len(arrays); uids = [Env.get_uid() for _ in builtins.range(n_arrays)]; refs = [; construct_expr(ir.Ref(uid, a.dtype.element_type), a.dtype.element_type, a._indices, a._aggregations); for uid, a in builtins.zip(uids, arrays); ]; body_result = f(*refs); indices, aggregations = unify_all(*arrays, body_result); behavior = 'ExtendNA' if fill_missing else 'TakeMinLength'; return construct_expr(; ir.toArray(ir.StreamZip([ir.toStream(a._ir) for a in arrays], uids, body_result._ir, behavior)),; tarray(body_result.dtype),; indices,; aggregations,; ). [docs]@typecheck(a=expr_array(), start=expr_int32, index_first=bool); def enumerate(a, start=0, *, index_first=True):; """"""Returns an array of (index, element) tuples. Examples; --------. >>> hl.eval(hl.enumerate(['A', 'B', 'C'])); [(0, 'A'), (1, 'B'), (2, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], start=3)); [(3, 'A'), (4, 'B'), (5, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], index_first=False)); [('A', 0), ('B', 1), ('C', 2)]. Parameters; ----------; a : :class:`.ArrayExpression`; start : :class:`.Int32Expression`; The index value from which the counter is started, 0 by default.; index_first: :obj:`bool`; If ``True``, the index is the first value of the element tuples. If; ``False``, the index is the second value. Returns; -------; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:117593,Extend,ExtendNA,117593,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Extend'],['ExtendNA']
Modifiability,"20e-01; 7 9.7540e-01; 8 8.4848e-01; 9 3.7423e-01. Due to overhead and file system limits related to having large numbers; of open files, this function will iteratively export groups of columns.; The batch_size parameter can control the size of these groups. Parameters:. mt (MatrixTable); path (int) – Path (directory to write to.; batch_size (int) – Number of columns to write per iteration.; bgzip (bool) – BGZip output files.; header_json_in_file (bool) – Include JSON header in each component file (if False, only written to index.tsv). hail.experimental.gather(ht, key, value, *fields)[source]; Collapse fields into key-value pairs.; gather() mimics the functionality of the gather() function found in R’s; tidyr package. This is a way to turn “wide” format data into “long”; format data. Parameters:. ht (Table) – A Hail table.; key (str) – The name of the key field in the gathered table.; value (str) – The name of the value field in the gathered table.; fields (variable-length args of obj:str) – Names of fields to gather in ht. Returns:; Table – Table with original fields gathered into key and value fields. hail.experimental.separate(ht, field, into, delim)[source]; Separate a field into multiple fields by splitting on a delimiter; character or position.; separate() mimics the functionality of the separate() function in R’s; tidyr package.; This function will create a new table where field has been split into; multiple new fields, whose names are given by into.; If delim is a str (including regular expression strings), field; will be separated into columns by that string. In this case, the length; of into must match the number of resulting fields.; If delim is an int, field will be separated into two row fields,; where the first field contains the first delim characters of field; and the second field contains the remaining characters. Parameters:. ht (Table) – A Hail table.; field (str) – The name of the field to separate in ht.; into (list of str) – The names of the fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:31194,variab,variable-length,31194,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['variab'],['variable-length']
Modifiability,"22-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49025,config,configuration,49025,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,; input_file=None,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ); env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:32370,config,config,32370,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['config'],['config']
Modifiability,"3]:. (None, Dict[String,Int]). In [14]:. hc.eval_expr_typed('1 + NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the erro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6124,variab,variable,6124,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['variab'],['variable']
Modifiability,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:95604,config,configurations,95604,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['config'],['configurations']
Modifiability,"8e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-01,1.03e-02,2.65e-02,-8.51e-02,2.49e-02,5.67e-02,-8.31e-03]; showing top 5 rows. Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:22683,variab,variable,22683,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,2,['variab'],['variable']
Modifiability,": String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; s: Sample; sa: sample annotations; }. with a single key ``s``. :return: Key table with samples and sample annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.samplesKT()). [docs] @handle_py4j; def genotypes_table(self):; """"""Generate a fully expanded genotype table. **Examples**. >>> gs = vds.genotypes_table(). **Notes**. This produces a (massive) flat table from all the; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:227542,config,config,227542,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['config'],['config']
Modifiability,":; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransition(). [docs] def is_transversion(self):; """"""True if this alternate allele is a transversion SNP. This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransversion(). [docs] @handle_py4j; def category(self):; """"""Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. :rtype: str; """"""; return self._jrep.altAlleleType(). [docs]class Locus(object):; """"""; An object that represents a location in the genome. :param contig: chromosome identifier; :type contig: str or int; :param int position: chromosomal position (1-indexed); """""". @handle_py4j; def __init__(self, contig, position):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Locus').apply(contig, position); self._init_from_java(jrep",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:8466,polymorphi,polymorphism,8466,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,":; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; Table – Table with specified fields. select_globals(*exprs, **named_exprs)[source]; Select existing global fields or create new fields by name, dropping the rest.; Examples; Selecting two global fields, one by name and one new one, replacing any previously annotated; global fields.; >>> ht = hl.utils.range_table(1); >>> ht = ht.annotate_globals(pops = ['EUR', 'AFR', 'EAS', 'SAS']); >>> ht = ht.annotate_globals(study_name = 'HGDP+1kg'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'study_name': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ---------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:61421,variab,variable-length,61421,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,":; return _func(""qchisqtail"", tfloat64, p, df, lower_tail, log_p); else:; return _func(""qnchisqtail"", tfloat64, p, df, ncp, lower_tail, log_p). [docs]@typecheck(p=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The quantile function of a normal distribution with mean `mu` and; standard deviation `sigma`, inverts :func:`~.pnorm`. Returns quantile of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; -----; Returns left-quantile `x` for which p = Prob(:math:`Z` < x) with :math:`Z`; a normal random variable with mean `mu` and standard deviation `sigma`.; Defaults to a standard normal random variable, and the probability `p` must; satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pnorm`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pnorm`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""qnorm"", tfloat64, p, mu, sigma, lower_tail, log_p). [docs]@typecheck(p=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def qpois(p, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The quantile function of a Poisson distribution with rate parameter; `lamb`, inverts :func:`~.ppois`. Examples; --------. >>> hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:79481,variab,variable,79481,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,":py:data:`.tfloat64`; Probability.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pnorm`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pnorm`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""qnorm"", tfloat64, p, mu, sigma, lower_tail, log_p). [docs]@typecheck(p=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def qpois(p, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The quantile function of a Poisson distribution with rate parameter; `lamb`, inverts :func:`~.ppois`. Examples; --------. >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; -----; Returns the smallest integer :math:`x` such that Prob(:math:`X \leq x`) :math:`\geq` `p` where :math:`X`; is a Poisson random variable with rate parameter `lambda`. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; lamb : float or :class:`.Expression` of type :py:data:`.tfloat64`; Rate parameter of Poisson distribution.; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in inverse :func:`.ppois`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p` before testing. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""qpois"", tint32, p, lamb, lower_tail, log_p). [docs]@typecheck(start=expr_int32, stop=nullable(expr_int32), step=expr_int32); def range(start, stop=None, step=1) -> ArrayNumericExpression:; """"""Returns an array of integers from `start` to `stop` by `step`. Examples; --------. >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:80660,variab,variable,80660,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invok",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37894,config,configuration,37894,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"; .when(; a.matches(_base_regex),; hl.case(); .when(; r.length() == a.length(),; hl.if_else(; r.length() == 1,; hl.if_else(r != a, AlleleType.SNP, AlleleType.UNKNOWN),; hl.if_else(hamming(r, a) == 1, AlleleType.SNP, AlleleType.MNP),; ),; ); .when((r.length() < a.length()) & (r[0] == a[0]) & a.endswith(r[1:]), AlleleType.INSERTION); .when((r[0] == a[0]) & r.endswith(a[1:]), AlleleType.DELETION); .default(AlleleType.COMPLEX),; ); .when(a == '*', AlleleType.STAR); .when(a.matches(_symbolic_regex), AlleleType.SYMBOLIC); .default(AlleleType.UNKNOWN),; AlleleType.UNKNOWN,; ),; ref,; alt,; ). @deprecated(version='0.2.129', reason=""Replaced by the public numeric_allele_type""); @typecheck(ref=expr_str, alt=expr_str); def _num_allele_type(ref, alt) -> Int32Expression:; """"""Provided for backwards compatibility, don't use it in new code, or; within the hail library itself; """"""; return numeric_allele_type(ref, alt). [docs]@typecheck(ref=expr_str, alt=expr_str); def is_snp(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute a single nucleotide polymorphism. Examples; --------. >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return numeric_allele_type(ref, alt) == AlleleType.SNP. [docs]@typecheck(ref=expr_str, alt=expr_str); def is_mnp(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute a multiple nucleotide polymorphism. Examples; --------. >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return numeric_allele_type(ref, alt) == AlleleType.MNP. [docs]@typecheck(ref=expr_str, alt=expr_str); def is_transition(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute a t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:97099,polymorphi,polymorphism,97099,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['polymorphi'],['polymorphism']
Modifiability,"; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175356,plugin,plugin,175356,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,3,['plugin'],['plugin']
Modifiability,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1527,config,config,1527,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,3,['config'],"['config', 'configuration']"
Modifiability,"; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the --make-bed option.; Hail uses the individual ID (column 2 in FAM file) as the sample id (s).; The individual IDs must be unique.; The resulting MatrixTable has the following fields:. Row fields:. locus (tlocus or tstruct) – Row key. The; chromosome and position. If reference_genome is defined, the type; will be tlocus parameterized by reference_genome.; Otherwise, the type will be a tstruct with two fields:; contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference allele (A2; if a2_reference is True) is the first element in the array.; rsid (tstr) – Column 2 in the BIM file.; cm_position (tfloat64) – Column 3 in the BIM file,; the position in centimorgans. Column fields:. s (tstr) – Column 2 in the Fam file (key field).; fam_id (tstr) – Column 1 in the FAM file. Set to; missing if ID equals “0”.; pat_id (tstr) – Column 3 in the FAM file. Set to; missing if ID equals “0”.; mat_id (tstr) – Column 4 in the FAM file. Set to; missing if ID equals “0”.; is_female (tstr) – Column 5 in the FAM file. Set to; missing if value equals “-9”, “0”, or “N/A”. Set to true if value; equals “2”. Set to false if value equals “1”.; is_case (tbool) – Column 6 in the FAM file. Only; present if quant_pheno equals Fa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:27750,parameteriz,parameterized,27750,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['parameteriz'],['parameterized']
Modifiability,"; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; Table – Table with specified fields. se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:60656,variab,variable-length,60656,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"; SwitchBuilder. hail.expr.functions.case(missing_false=False)[source]; Chain multiple if-else statements with a CaseBuilder.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(hl.len(x) == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cannot be interpreted as an expression. Parameters:; t (str or HailType) – Type of the missing expression. Returns:; Expression – A missing expression of type t. hail.expr.functions.null(t)[source]; Deprecated in favor of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:6465,variab,variable,6465,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable']
Modifiability,"; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102615,variab,variables,102615,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variables']
Modifiability,"; True if the call contains two different alternate alleles. Return type:bool. is_het_ref()[source]¶; True if the call contains one reference and one alternate allele. Return type:bool. is_hom_ref()[source]¶; True if the call is 0/0. Return type:bool. is_hom_var()[source]¶; True if the call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Call.html:2783,variab,variables,2783,docs/0.1/representation/hail.representation.Call.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Call.html,1,['variab'],['variables']
Modifiability,"; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:2456,variab,variable,2456,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability,"; [[3,4], []]. a; 2; [[3,4], []]. Explode c2 once and c3 twice:; >>> kt3.explode(['c2', 'c3', 'c3']). c1; c2; c3. a; 1; 3. a; 2; 3. a; 1; 4. a; 2; 4. Parameters:column_names (str or list of str) – Column name(s) to be exploded. Returns:Key table with columns exploded. Return type:KeyTable. export(output, types_file=None, header=True, parallel=False)[source]¶; Export to a TSV file.; Examples; Rename column names of key table and export to file:; >>> (kt1.rename({'HT' : 'Height'}); ... .export(""output/kt1_renamed.tsv"")). Parameters:; output (str) – Output file path.; types_file (str) – Output path of types file.; header (bool) – Write a header using the column names.; parallel (bool) – If true, writes a set of files (one per partition) rather than serially concatenating these files. export_cassandra(address, keyspace, table, block_size=100, rate=1000)[source]¶; Export to Cassandra. Warning; export_cassandra() is EXPERIMENTAL. export_elasticsearch(host, port, index, index_type, block_size, config=None, verbose=True)[source]¶; Export to Elasticsearch. Warning; export_elasticsearch() is EXPERIMENTAL. export_mongodb(mode='append')[source]¶; Export to MongoDB. Warning; export_mongodb() is EXPERIMENTAL. export_solr(zk_host, collection, block_size=100)[source]¶; Export to Solr. Warning; export_solr() is EXPERIMENTAL. filter(expr, keep=True)[source]¶; Filter rows.; Examples; Keep rows where C1 equals 5:; >>> kt_result = kt1.filter(""C1 == 5""). Remove rows where C1 equals 10:; >>> kt_result = kt1.filter(""C1 == 10"", keep=False). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Caution; When expr evaluates to missing, the row will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep rows where expr is true. Returns:Filtered key table. Return type:KeyTable. flatten()[source]¶; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:8659,config,config,8659,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['config'],['config']
Modifiability,"; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`. Returns; -------; :class:`.ArrayExpression`.; """"""; return self._to_stream().scan(lambda x, y: f(x, y), zero).to_array(). [docs] @typecheck_method(group_size=expr_int32); def grouped(self, group_size):; """"""Partition an array into fixed size subarrays. Examples; --------; >>> a = hl.array([0, 1,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17598,extend,extend,17598,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['extend'],['extend']
Modifiability,"; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:22386,config,configuration,22386,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['config'],['configuration']
Modifiability,"; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[sourc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:1316,variab,variable,1316,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],['variable']
Modifiability,"; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). depen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:8400,flexible,flexible,8400,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['flexible'],['flexible']
Modifiability,"; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:2046,polymorphi,polymorphism,2046,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"= '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(p_value, approximate=True):; chisq = hl.qchisqtail(p_value, 1); if approximate:; med_chisq = hl.agg.filter(~hl.is_nan(p_value), hl.agg.approx_quantiles(chisq, 0.5)); else:; med_chisq = hl.agg.filter(~hl.is_nan(p_value), hl.m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109689,config,config,109689,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability,"= [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_names), itertools.chain.from_iterable(y))); func = 'LinearRegressionRowsChained'. else:; y_field_names = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_names, y)); func = 'LinearRegressionRowsSingle'. cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). row_fields = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': func,; 'yFields': y_field_names,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'rowBlockSize': block_size,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; }; ht_result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}). return ht_result.persist(). @typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; pass_through=sequenceof(oneof(str, Expression)),; ); def _linear_regression_rows_nd(y, x, covariates, block_size=16, weights=None, pass_through=()) -> Table:; mt = matrix_table_source('linear_regression_rows_nd/x', x); raise_unless_entry_indexed('linear_regression_rows_nd/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows_nd': found no values for 'y'""); is_chained = y_is_list and isinstance(y[0], list). if is_chained and any(len(lst) == 0 for lst in y):; raise ValueError(""'linear_regression_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:14762,config,config,14762,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability,"=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23077,config,configuration,23077,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,"['config', 'inherit']","['configuration', 'inherits']"
Modifiability,">>> a = [(1, 5), (3, 2), (7, 8)]. >>> hl.eval(hl.starmap(lambda x, y: hl.if_else(x < y, x, y), a)); [1, 2, 7]. Parameters:. f (function ( (*args) -> Expression)) – Function to transform each element of the collection.; collection (ArrayExpression or SetExpression) – Collection expression. Returns:; ArrayExpression or SetExpression. – Collection where each element has been transformed by f. hail.expr.functions.zip(*arrays, fill_missing=False)[source]; Zip together arrays into a single array.; Examples; >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the fill_missing parameter.; >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; The element type of the resulting array is a ttuple with a field; for each array. Parameters:. arrays (: variable-length args of ArrayExpression) – Array expressions.; fill_missing (bool) – If False, return an array with length equal to the shortest length; of the arrays. If True, return an array equal to the longest; length of the arrays, by extending the shorter arrays with missing; values. Returns:; ArrayExpression. hail.expr.functions.enumerate(a, start=0, *, index_first=True)[source]; Returns an array of (index, element) tuples.; Examples; >>> hl.eval(hl.enumerate(['A', 'B', 'C'])); [(0, 'A'), (1, 'B'), (2, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], start=3)); [(3, 'A'), (4, 'B'), (5, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], index_first=False)); [('A', 0), ('B', 1), ('C', 2)]. Parameters:. a (ArrayExpression); start (Int32Expression) – The index value from which the counter is started, 0 by default.; index_first (bool) – If True, the index is the first value of the element tuples. If; False, the index is the second value. Returns:; ArrayExpression – Array of (index, element) or (element, index",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/collections.html:5357,variab,variable-length,5357,docs/0.2/functions/collections.html,https://hail.is,https://hail.is/docs/0.2/functions/collections.html,1,['variab'],['variable-length']
Modifiability,">>> hl.export_vcf(split_ds, 'output/export.vcf') . The info field AC in data/export.vcf will have Number=1.; New Fields; split_multi_hts() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2. See also; split_multi(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root (str) – Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table – A biallelic variant dataset. hail.methods.summarize_variants(mt, show=True, *, handler=None)[source]; Summarize the variants present in a dataset and print the results.; Examples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346 variants; ==============================; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ========",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:91669,variab,variable-length,91669,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variable-length']
Modifiability,"A | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of vari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:75768,variab,variable,75768,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variable']
Modifiability,"Batch to localize the files as inputs to a Job. Therefore, we; use the method Batch.read_input() to tell Batch to localize these files; when they are referenced by a Job.; df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). We initialize a list to keep track of all of the output files to concatenate; later on.; results = []. We now have all of our inputs ready and can iterate through each window in the; y dataframe. For each window, we create a new PythonJob using the; method Batch.new_python_job(). We then use the method PythonJob.call(); to run the function random_forest. The inputs to random_forest are the Batch inputs; df_x_input and df_y_input as well as the window name. Notice that the first argument to; PythonJob.call() is the reference to the function to call (i.e random_forest and; not random_forest(…). The rest of the arguments are the usual positional arguments and; key-word arguments to the function. Lastly, we assign the result of calling the function; to the variable result which is a PythonResult. A PythonResult; can be thought of as a Python object and used in subsequent calls to PythonJob.call().; Since the type of result is a tuple of (str, float, float), we need to convert the Python; tuple to a tab-delimited string that can later be concatenated. We use the as_tsv function; we wrote above to do so. The input to as_tsv is result and we assign the output to tsv_result.; Lastly in the for loop for each window, we append the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:6683,variab,variable,6683,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['variab'],['variable']
Modifiability,"Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Imp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:1029,config,configured,1029,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['config'],['configured']
Modifiability,"Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The ou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:3818,config,configuration,3818,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,"['config', 'inherit']","['configuration', 'inherits']"
Modifiability,"DB. .. warning::. :py:meth:`~.export_mongodb` is EXPERIMENTAL. """""". (scala_package_object(self.hc._hail.driver); .exportMongoDB(self.hc._jsql_context, self._jkt, mode)). [docs] @handle_py4j; @typecheck_method(zk_host=strlike,; collection=strlike,; block_size=integral); def export_solr(self, zk_host, collection, block_size=100):; """"""Export to Solr.; ; .. warning::. :py:meth:`~.export_solr` is EXPERIMENTAL. """""". self._jkt.exportSolr(zk_host, collection, block_size). [docs] @handle_py4j; @typecheck_method(address=strlike,; keyspace=strlike,; table=strlike,; block_size=integral,; rate=integral); def export_cassandra(self, address, keyspace, table, block_size=100, rate=1000):; """"""Export to Cassandra. .. warning::. :py:meth:`~.export_cassandra` is EXPERIMENTAL. """""". self._jkt.exportCassandra(address, keyspace, table, block_size, rate). [docs] @handle_py4j; @typecheck_method(host=strlike,; port=integral,; index=strlike,; index_type=strlike,; block_size=integral,; config=nullable(dictof(strlike, strlike)),; verbose=bool); def export_elasticsearch(self, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export to Elasticsearch. .. warning::. :py:meth:`~.export_elasticsearch` is EXPERIMENTAL. """""". self._jkt.exportElasticsearch(host, port, index, index_type, block_size, config, verbose). [docs] @handle_py4j; @typecheck_method(column_names=oneof(strlike, listof(strlike))); def explode(self, column_names):; """"""Explode columns of this key table. The explode operation unpacks the elements in a column of type ``Array`` or ``Set`` into its own row.; If an empty ``Array`` or ``Set`` is exploded, the entire row is removed from the :py:class:`.KeyTable`. **Examples**. Assume ``kt3`` is a :py:class:`.KeyTable` with three columns: c1, c2 and; c3. >>> kt3 = hc.import_table('data/kt_example3.tsv', impute=True,; ... types={'c1': TString(), 'c2': TArray(TInt()), 'c3': TArray(TArray(TInt()))}). The types of each column are ``String``, ``Array[Int]``, and ``Array[Array[I",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:17144,config,config,17144,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,2,['config'],['config']
Modifiability,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:2349,config,configured,2349,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['config'],['configured']
Modifiability,"ERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223168,plugin,plugin,223168,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['plugin'],['plugin']
Modifiability,"False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transition(ref, alt)[source]; Returns True if the alleles constitute a transition.; Examples; >>> hl.eval(hl.is_transition('A', 'T')); False. >>> hl.eval(hl.is_transition('AAA', 'AGA')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transversion(ref, alt)[source]; Returns True if the alleles constitute a transversion.; Examples; >>> hl.eval(hl.is_transversion('A', 'T')); True. >>> hl.eval(hl.is_transversion('AAA', 'AGA')); False. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_insertion(ref, alt)[source]; Returns True i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:12630,polymorphi,polymorphism,12630,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:43055,config,config,43055,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; ArrayExpression. View page source. ArrayExpression. class hail.expr.ArrayExpression[source]; Expression of type tarray.; >>> names = hl.literal(['Alice', 'Bob', 'Charlie']). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. aggregate; Uses the aggregator library to compute a summary from an array. append; Append an element to the array and return the result. contains; Returns a boolean indicating whether item is found in the array. extend; Concatenate two arrays and return the result. first; Returns the first element of the array, or missing if empty. grouped; Partition an array into fixed size subarrays. head; Deprecated in favor of first(). index; Returns the first index of x, or missing. last; Returns the last element of the array, or missing if empty. scan; Map each element of the array to cumulative value of function f, with initial value zero. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:1141,extend,extend,1141,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['extend'],['extend']
Modifiability,"Hc: int32,; asjAf: float64,; asjAc: int32,; asjAn: int32,; asjHc: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32,; sasHc: int32,; failedFilter: bool; },; topmed: struct {; failedFilter: bool,; allAc: int32,; allAn: int32,; allAf: float64,; allHc: int32; },; oneKg: struct {; ancestralAllele: str,; allAf: float64,; allAc: int32,; allAn: int32,; afrAf: float64,; afrAc: int32,; afrAn: int32,; amrAf: float64,; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:64845,inherit,inheritance,64845,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,['inherit'],['inheritance']
Modifiability,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102953,config,configuration,102953,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,"['config', 'plugin']","['configuration', 'plugin']"
Modifiability,"Int; C.gq: Int. and values; v A.gt A.gq B.gt B.gq C.gt C.gq; 1:1:A:T 1 99 NA NA 0 99; 1:2:G:C 1 89 1 99 2 93. The above table can be generated and exported as a TSV using KeyTable export().; Notes; Per sample field names in the result are formed by; concatenating the sample ID with the genotype_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:123350,inherit,inheritance,123350,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['inherit'],['inheritance']
Modifiability,"Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'target_date': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------. Fields may also be selected by their name:; >>> ht = ht.select_globals('target_date'); >>> ht.globals.show(); +--------------------+; | <expr>.target_date |; +--------------------+; | str |; +--------------------+; | ""2025-01-01"" |; +--------------------+. Notes; This method creates new global fields. If a created field shares its name; with a row-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; Table – Table with specified global fields. semi_join(other)[source]; Filters the table to rows whose key appears in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; Table. Notes; The key type of the table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the table to keys present in another table.; To discard keys present in other, use anti_join().; Examples; >>> table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:63256,variab,variable-length,63256,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"Legend,; LegendItem,; LinearColorMapper,; LogColorMapper,; LogTicker,; Plot,; Renderer,; Select,; Slope,; Span,; ); from bokeh.plotting import figure; from bokeh.transform import transform. import hail; from hail.expr import aggregators; from hail.expr.expressions import (; Expression,; Float32Expression,; Float64Expression,; Int32Expression,; Int64Expression,; LocusExpression,; NumericExpression,; StringExpression,; expr_any,; expr_float64,; expr_locus,; expr_numeric,; expr_str,; raise_unless_row_indexed,; ); from hail.expr.functions import _error_from_cdf_python; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import dictof, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils.java import warning; from hail.utils.struct import Struct. palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']. [docs]def output_notebook():; """"""Configure the Bokeh output state to generate output in notebook; cells when :func:`bokeh.io.show` is called. Calls; :func:`bokeh.io.output_notebook`. """"""; bokeh.io.output_notebook(). def show(obj, interact=None):; """"""Immediately display a Bokeh object or application. Calls; :func:`bokeh.io.show`. Parameters; ----------; obj; A Bokeh object to display.; interact; A handle returned by a plotting method with `interactive=True`.; """"""; if interact is None:; bokeh.io.show(obj); else:; handle = bokeh.io.show(obj, notebook_handle=True); interact(handle). [docs]def cdf(data, k=350, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:1978,Config,Configure,1978,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['Config'],['Configure']
Modifiability,"Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223042,plugin,plugin,223042,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['plugin'],['plugin']
Modifiability,"N=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2893,config,configuration,2893,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['config'],['configuration']
Modifiability,"NDArrayNumericExpression full of ones.; Examples; Create a 5 by 7 NDArray of type tfloat64 ones.; >>> hl.nd.ones((5, 7)). It is possible to specify a type other than tfloat64 with the dtype argument.; >>> hl.nd.ones((5, 7), dtype=hl.tfloat32). Parameters:. shape (tuple or TupleExpression) – Desired shape.; dtype (HailType) – Desired hail type. Default: float64. See also; full(). Returns:; NDArrayNumericExpression – ndarray of the specified size full of ones. hail.nd.diagonal(nd)[source]; Gets the diagonal of a 2 dimensional NDArray.; Examples; >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters:; nd (NDArrayNumericExpression) – A 2 dimensional NDArray, shape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:6139,variab,variables,6139,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['variab'],['variables']
Modifiability,"NPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is set to requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. hail.methods.balding_nichols_model(n_populations, n_samples, n_variants, n_partitions=None, pop_dist=None, fst=None, af_dist=None, reference_genome='default', mixture=False, *, phased=False)[source]; Generate a matrix table of variants, samples, and genotypes using the; Balding-Nichols or Pritchard-Stephens-Donnelly model.; Examples; Generate a matrix table of genotypes with 1000 variants and 100 samples; across 3 populations:; >>> hl.r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:7602,config,configuration,7602,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (; 'intergenic_consequences',; 'motif_feature_consequences',; 'regulatory_feature_consequences',; 'transcript_consequences',; ); }). if isinstance(ds, Table):; return split.annotate(**update_rows_expression).drop('",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122546,variab,variable-length,122546,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable-length']
Modifiability,"None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7126,config,config,7126,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['config']
Modifiability,"ODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1140,variab,variables,1140,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['variab'],['variables']
Modifiability,"OT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3602,config,config,3602,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,12,['config'],['config']
Modifiability,"PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the locat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223197,plugin,plugin,223197,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['plugin'],['plugin']
Modifiability,"Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-or x and y.; Examples; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_lshift(x, y)[source]; Bitwise left-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for mo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:5360,extend,extended,5360,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['extend'],['extended']
Modifiability,"Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1316,polymorphi,polymorphism,1316,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. Th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1216,flexible,flexible,1216,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['flexible'],['flexible']
Modifiability,"Python, these are represented as int. See also; Int64Expression, int64(). hail.expr.types.tfloat = dtype('float64'); Alias for tfloat64. hail.expr.types.tfloat32 = dtype('float32'); Hail type for 32-bit floating point numbers.; In Python, these are represented as float. See also; Float32Expression, float64(). hail.expr.types.tfloat64 = dtype('float64'); Hail type for 64-bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:8275,parameteriz,parameterized,8275,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['parameteriz'],['parameterized']
Modifiability,"Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21031,rewrite,rewrite,21031,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['rewrite'],['rewrite']
Modifiability,"Returns:; NDArrayNumericExpression – ndarray of the specified size full of ones. hail.nd.diagonal(nd)[source]; Gets the diagonal of a 2 dimensional NDArray.; Examples; >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters:; nd (NDArrayNumericExpression) – A 2 dimensional NDArray, shape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:6521,variab,variables,6521,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['variab'],['variables']
Modifiability,"S f INNER JOIN annotations AS a ON f.file_id = a.file_id; WHERE f.file_id IN ({}); GROUP BY file_path"""""".format(sub). # collect counts in file_id: count dictionary; cnts = {x[0]: x[1] for x in curs.execute(qry, file_ids).fetchall()}. # close database connection; conn.close(). # collect dictionary of file_path: expr entries; file_exprs = {}; for r in results:; expr = r[1] + '=' + 'table' if r[2] == 'table' and cnts[r[0]] < 2 else r[1] + '=' + r[2] + '.' + r[3]; try:; file_exprs[r[0]] += ',' + expr; except KeyError:; file_exprs[r[0]] = expr. # are there any gene annotations?; are_genes = 'gs://annotationdb/gene/gene.kt' in file_exprs #any([x.startswith('gs://annotationdb/gene/') for x in file_exprs]). # subset to VEP annotations; veps = any([x == 'vep' for x in file_exprs]). # if VEP annotations are selected, or if gene-level annotations are selected with no specified gene_key, annotate with VEP; if veps or (are_genes and not gene_key):. # VEP annotate the VDS; self = self.vep(config='/vep/vep-gcloud.properties', root='va.vep'). # extract 1 gene symbol per variant from VEP annotations if a gene_key parameter isn't provided; if are_genes:. # hierarchy of possible variant consequences, from most to least severe; csq_terms = [; 'transcript_ablation',; 'splice_acceptor_variant',; 'splice_donor_variant',; 'stop_gained',; 'frameshift_variant',; 'stop_lost',; 'start_lost',; 'transcript_amplification',; 'inframe_insertion',; 'inframe_deletion',; 'missense_variant',; 'protein_altering_variant',; 'incomplete_terminal_codon_variant',; 'stop_retained_variant',; 'synonymous_variant',; 'splice_region_variant',; 'coding_sequence_variant',; 'mature_miRNA_variant',; '5_prime_UTR_variant',; '3_prime_UTR_variant',; 'non_coding_transcript_exon_variant',; 'intron_variant',; 'NMD_transcript_variant',; 'non_coding_transcript_variant',; 'upstream_gene_variant',; 'downstream_gene_variant',; 'TFBS_ablation',; 'TFBS_amplification',; 'TF_binding_site_variant',; 'regulatory_region_ablation',; 're",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:37354,config,config,37354,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['config'],['config']
Modifiability,"Table tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, you",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:18150,config,configuration,18150,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42372,Config,Configuration,42372,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Config'],['Configuration']
Modifiability,"[docs] def select_rows(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing row fields or create new fields by name, dropping all; other non-key fields. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; -----; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail. Row keys are preserved. To drop or change a row key field, use; :meth:`MatrixTable.key_rows_by`. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method supports aggregation over columns. For instance, the usage:. >>> dataset_result = dataset.select_rows(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per row. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified row fields.; """"""; caller = 'MatrixTable.select_rows'; new_row = get_select_exprs(caller, exprs, named_exprs, self._row_indices, self._rvrow); return self._select_rows(caller, new_row). [docs] def select_cols(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing column fields or create new fields by name, dropping the rest. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_cols(; ... dataset.sample_qc,; ... dataset.pheno.age,; ... isCohort1 = dataset.pheno.cohort_name == 'Cohort1'). Notes; -----; This method creates new column fields. If a created field shares its name; with a differently-indexed field of the table, the method wi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:39184,variab,variable-length,39184,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['variab'],['variable-length']
Modifiability,"[missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. hail.expr.functions.literal(x, dtype=None)[source]; Captures and broadcasts a Python variable or object as an expression.; Examples; >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; Table or MatrixTable. Parameters:; x – Object to capture and broadcast as an expression. Returns:; Expression. hail.expr.functions.cond(condition, consequent, alternate, missing_false=False)[source]; Deprecated in favor of if_else().; Expression for an if/else statement; tests a condi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:2154,variab,variable,2154,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable']
Modifiability,"],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_samples_expr(expr)[source]¶; Annotate samples with expression.; Examples; Compute per-sample GQ statistics for hets:; >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:; >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:13023,variab,variable,13023,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['variab'],['variable']
Modifiability,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48533,variab,variable,48533,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],['variable']
Modifiability,"_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:6973,config,configuration,6973,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61253,variab,variable,61253,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],['variable']
Modifiability,"_independent_set now does not; produce duplicates.; (#5725) Docs now; consistently refer to hl.agg not agg.; (#5730)(#5782); Taught import_bgen to optimize its variants argument. Experimental. (#5732) The; hl.agg.approx_quantiles aggregate computes an approximation of; the quantiles of an expression.; (#5693)(#5396); Table._multi_way_zip_join now correctly handles keys that have; been truncated. Version 0.2.12; Released 2019-03-28. New features. (#5614) Add support; for multiple missing values in hl.import_table.; (#5666) Produce HTML; table output for Table.show() when running in Jupyter notebook. Bug fixes. (#5603)(#5697); Fixed issue where min_partitions on hl.import_table was; non-functional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer joins on; MatrixTables. Version 0.2.11; Released 2019-03-06. New features. (#5374) Add default; arguments to hl.add_sequence for running on GCP.; (#5481) Added; sample_cols method to MatrixTable.; (#5501) Exposed; MatrixTable.unfilter_entries. See filter_entries; documentation for more information.; (#5480) Added; n_cols argument to MatrixTable.head.; (#5529) Added; Table.{semi_join, anti_join} and; MatrixTable.{semi_join_rows, semi_join_cols, anti_join_rows, anti_join_cols}.; (#5528) Added; {MatrixTable, Table}.checkpoint methods as wrappers around; write / read_{matrix_table, table}. Bug fixes. (#5416) Resolved; issue wherein VEP and certain regressions were recomputed on each; use, rather than once.; (#5419) Resolved; issue with import_vcf force_bgz and file size checks.; (#5427) Resolved; issue with Table.show and dictionary field types.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:97946,extend,extend,97946,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['extend'],['extend']
Modifiability,"_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is missing, False otherwise. hail.expr.functions.is_defined(expression)[source]; Returns True if the argument is not missing.; Examples; >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is not missing, False otherwise. hail.expr.functions.coalesce(*args)[source]; Returns the first non-missing value of args.; Examples; >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See also; or_else(). Parameters:; args (variable-length args of Expression). Returns:; Expression. hail.expr.functions.or_else(a, b)[source]; If a is missing, return b.; Examples; >>> hl.eval(hl.or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The ra",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:9211,variab,variable-length,9211,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable-length']
Modifiability,"_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:1871,polymorphi,polymorphism,1871,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:13616,polymorphi,polymorphism,13616,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['polymorphi'],['polymorphism']
Modifiability,"_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nest",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:53100,variab,variable,53100,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['variab'],['variable']
Modifiability,"_region_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:227096,config,config,227096,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['config'],"['config', 'configuration']"
Modifiability,"_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63919,config,config,63919,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability,"_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('linear_regression_rows', covariates). x_field_name = Env.get_uid(); if is_chained:; y_field_names = [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_names), itertools.chain.from_iterable(y))); func = 'LinearRegressionRowsChained'. else:; y_field_names = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_names, y)); func = 'LinearRegressionRowsSingle'. cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). row_fields = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': func,; 'yFields': y_field_names,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'rowBlockSize': block_size,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; }; ht_result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}). return ht_result.persist(). @typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; pass_through=sequenceof(oneof(str, Expression)),; ); def _linear_regression_rows_nd(y, x, covariates, block_size=16, weights=None, pass_through=()) -> Table:; mt = matrix_table_source('linear_regression_rows_nd/x', x); raise_unless_entry_indexed('linear_regression_rows_nd/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows_nd': found ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:14507,config,config,14507,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability,"_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nested_dim; covs_defined = hl.all(lambda cov: hl.is_defined(cov), x); tup = hl.agg.filter(covs_defined, hl.tuple([hl.a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:53215,variab,variables,53215,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['variab'],['variables']
Modifiability,"_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with distribution \(F`(df1, df2). If `lower_tail\); is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); df1 (float or Expression of type tfloat64) – Parameter of the F-distribution; df2 (float or Expression of type tfloat64) – Parameter of the F-distribution; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.ppois(x, lamb, lower_tail=True, log_p=False)[source]; The cumulative probability function of a Poisson distribution.; Examples; >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is a; Poisson random variable with rate parameter lamb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False)[source]; The quantile function of a chi-squared distribution with df degrees of; freedom, inverts pchisqtail().; Examples; >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; Returns r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:24462,variab,variable,24462,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"_tbool). def _parsable_string(self):; return ""Boolean"". def unify(self, t):; return t == tbool. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return bool. def _byte_size(self):; return 1. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> bool:; return byte_reader.read_bool(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_bool(value). class _trngstate(HailType):; def __init__(self):; super(_trngstate, self).__init__(). def __str__(self):; return ""rng_state"". def _eq(self, other):; return isinstance(other, _trngstate). def _parsable_string(self):; return ""RNGState"". def unify(self, t):; return t == trngstate. def subst(self):; return self. def clear(self):; pass. [docs]class tndarray(HailType):; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """""". @typecheck_method(element_type=hail_type, ndim=oneof(NatBase, int)); def __init__(self, element_type, ndim):; self._element_type = element_type; self._ndim = NatLiteral(ndim) if isinstance(ndim, int) else ndim; super(tndarray, self).__init__(). @property; def element_type(self):; """"""NDArray element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. @property; def ndim(self):; """"""NDArray number of dimensions. Returns; -------; :obj:`int`; Number of dimensions.; """"""; assert isinstance(self._ndim, NatLiteral), ""tndarray must be realized with a concrete number of dimensions""; return self._ndim.n. def _traverse(self, obj, f):; if f(self, obj):; for elt in np.nditer(obj, ['zerosize_ok']):; self.element_type._traverse(elt.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:14412,parameteriz,parameterized,14412,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['parameteriz'],['parameterized']
Modifiability,"`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3419,config,config,3419,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,['config'],['config']
Modifiability,"`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223013,plugin,plugin,223013,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['plugin'],['plugin']
Modifiability,"`log_p` parameter in :func:`.pchisqtail`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; if ncp is None:; return _func(""qchisqtail"", tfloat64, p, df, lower_tail, log_p); else:; return _func(""qnchisqtail"", tfloat64, p, df, ncp, lower_tail, log_p). [docs]@typecheck(p=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The quantile function of a normal distribution with mean `mu` and; standard deviation `sigma`, inverts :func:`~.pnorm`. Returns quantile of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; -----; Returns left-quantile `x` for which p = Prob(:math:`Z` < x) with :math:`Z`; a normal random variable with mean `mu` and standard deviation `sigma`.; Defaults to a standard normal random variable, and the probability `p` must; satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pnorm`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pnorm`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""qnorm"", tfloat64, p, mu, sigma, lower_tail, log_p). [docs]@typecheck(p=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def qpois(p, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:79387,variab,variable,79387,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"a PI_HAT value greater than or equal to 0.6.; ; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:. >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). **Notes**. The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted. ; ; The tiebreaking_expr namespace has the following variables available:; ; - ``s1``: The first sample id.; - ``sa1``: The annotations associated with s1.; - ``s2``: The second sample id. ; - ``sa2``: The annotations associated with s2. ; ; The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping ``s1``. Any positive integer expresses a preference for keeping ``s2``. A zero expresses no preference. This function must induce a `preorder <https://en.wikipedia.org/wiki/Preorder>`__ on the samples, in particular:. - ``tiebreaking_expr(sample1, sample2)`` must equal ``-1 * tie breaking_expr(sample2, sample1)``, which evokes the common sense understanding that if ``x < y`` then `y > x``.; - ``tiebreaking_expr(sample1, sample1)`` must equal 0, i.e. ``x = x``; - if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related sample",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:85462,variab,variables,85462,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['variab'],['variables']
Modifiability,"able.select_rows'; new_row = get_select_exprs(caller, exprs, named_exprs, self._row_indices, self._rvrow); return self._select_rows(caller, new_row). [docs] def select_cols(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing column fields or create new fields by name, dropping the rest. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_cols(; ... dataset.sample_qc,; ... dataset.pheno.age,; ... isCohort1 = dataset.pheno.cohort_name == 'Cohort1'). Notes; -----; This method creates new column fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method supports aggregation over rows. For instance, the usage:. >>> dataset_result = dataset.select_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified column fields.; """"""; caller = 'MatrixTable.select_cols'; new_col = get_select_exprs(caller, exprs, named_exprs, self._col_indices, self._col); return self._select_cols(caller, new_col). [docs] def select_entries(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing entry fields or create new fields by name, dropping the rest. Examples; --------; Drop all entry fields aside from `GT`:. >>> dataset_result = dataset.select_entries(dataset.GT). Notes; -----; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` metho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:40517,variab,variable-length,40517,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['variab'],['variable-length']
Modifiability,"ail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`. """"""; return _func(""pT"", tfloat64, x, n, lower_tail, log_p). [docs]@typecheck(x=expr_float64, df1=expr_float64, df2=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pF(x, df1, df2, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `F-distribution; <https://en.wikipedia.org/wiki/F-distribution>`__ with parameters; `df1` and `df2`. Examples; --------. >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is; a random variable with distribution :math:`F`(df1, df2). If `lower_tail`; is false, returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; df1 : float or :class:`.Expression` of type :py:data:`.tfloat64`; Parameter of the F-distribution; df2 : float or :class:`.Expression` of type :py:data:`.tfloat64`; Parameter of the F-distribution; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pF"", tfloat64, x, df1, df2, lower_tail, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def ppois(x, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:75186,variab,variable,75186,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"ail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for detai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:41366,config,config,41366,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['config']
Modifiability,"al @ ht.G).map(lambda x: x**2) * ht.weight).sum(0)). # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:83827,variab,variables,83827,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"allelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4100,polymorphi,polymorphism,4100,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"alling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:59554,config,configuration,59554,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"amb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False)[source]; The quantile function of a chi-squared distribution with df degrees of; freedom, inverts pchisqtail().; Examples; >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; Returns right-quantile x for which p = Prob(\(Z^2\) > x) with; \(Z^2\) a chi-squared random variable with degrees of freedom specified; by df. The probability p must satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:25517,variab,variable,25517,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
